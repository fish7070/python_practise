{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6c3lCrsIyHYn"
      },
      "source": [
        "In this example, we will download the LangChain docs from [langchain.readthedocs.io/](https://langchain.readthedocs.io/latest/en/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2vj2Y4zyHYo"
      },
      "source": [
        "This downloads all HTML into the `rtdocs` directory. Now we can use LangChain itself to process these docs. We do this using the `ReadTheDocsLoader` like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n40_0MtlsKgM",
        "outputId": "2d9e27c4-0e20-45bf-abc9-2e04704968f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\fish\\anaconda3\\envs\\mygpt\\lib\\site-packages\\langchain\\document_loaders\\readthedocs.py:30: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 30 of the file c:\\Users\\fish\\anaconda3\\envs\\mygpt\\lib\\site-packages\\langchain\\document_loaders\\readthedocs.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  _ = BeautifulSoup(\n",
            "c:\\Users\\fish\\anaconda3\\envs\\mygpt\\lib\\site-packages\\langchain\\document_loaders\\readthedocs.py:46: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 46 of the file c:\\Users\\fish\\anaconda3\\envs\\mygpt\\lib\\site-packages\\langchain\\document_loaders\\readthedocs.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  soup = BeautifulSoup(data, **self.bs_kwargs)\n",
            "c:\\Users\\fish\\anaconda3\\envs\\mygpt\\lib\\site-packages\\langchain\\document_loaders\\readthedocs.py:46: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(data, **self.bs_kwargs)\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import ReadTheDocsLoader\n",
        "\n",
        "loader = ReadTheDocsLoader('LangChaindocs')\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\robots.txt.tmp'}),\n",
              " Document(page_content='.md\\n.pdf\\nDeployments\\n Contents \\nStreamlit\\nGradio (on Hugging Face)\\nBeam\\nVercel\\nDigitalocean App Platform\\nSteamShip\\nLangchain-serve\\nBentoML\\nDeployments#\\nSo you’ve made a really cool chain - now what? How do you deploy it and make it easily sharable with the world?\\nThis section covers several options for that.\\nNote that these are meant as quick deployment options for prototypes and demos, and not for production systems.\\nIf you are looking for help with deployment of a production system, please contact us directly.\\nWhat follows is a list of template GitHub repositories aimed that are intended to be\\nvery easy to fork and modify to use your chain.\\nThis is far from an exhaustive list of options, and we are EXTREMELY open to contributions here.\\nStreamlit#\\nThis repo serves as a template for how to deploy a LangChain with Streamlit.\\nIt implements a chatbot interface.\\nIt also contains instructions for how to deploy this app on the Streamlit platform.\\nGradio (on Hugging Face)#\\nThis repo serves as a template for how deploy a LangChain with Gradio.\\nIt implements a chatbot interface, with a “Bring-Your-Own-Token” approach (nice for not wracking up big bills).\\nIt also contains instructions for how to deploy this app on the Hugging Face platform.\\nThis is heavily influenced by James Weaver’s excellent examples.\\nBeam#\\nThis repo serves as a template for how deploy a LangChain with Beam.\\nIt implements a Question Answering app and contains instructions for deploying the app as a serverless REST API.\\nVercel#\\nA minimal example on how to run LangChain on Vercel using Flask.\\nDigitalocean App Platform#\\nA minimal example on how to deploy LangChain to DigitalOcean App Platform.\\nSteamShip#\\nThis repository contains LangChain adapters for Steamship, enabling LangChain developers to rapidly deploy their apps on Steamship.\\nThis includes: production ready endpoints, horizontal scaling across dependencies, persistant storage of app state, multi-tenancy support, etc.\\nLangchain-serve#\\nThis repository allows users to serve local chains and agents as RESTful, gRPC, or Websocket APIs thanks to Jina. Deploy your chains & agents with ease and enjoy independent scaling, serverless and autoscaling APIs, as well as a Streamlit playground on Jina AI Cloud.\\nBentoML#\\nThis repository provides an example of how to deploy a LangChain application with BentoML. BentoML is a framework that enables the containerization of machine learning applications as standard OCI images. BentoML also allows for the automatic generation of OpenAPI and gRPC endpoints. With BentoML, you can integrate models from all popular ML frameworks and deploy them as microservices running on the most optimal hardware and scaling independently.\\nprevious\\nLangChain Gallery\\nnext\\nTracing\\n Contents\\n  \\nStreamlit\\nGradio (on Hugging Face)\\nBeam\\nVercel\\nDigitalocean App Platform\\nSteamShip\\nLangchain-serve\\nBentoML\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\deployments.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nLangChain Ecosystem\\nLangChain Ecosystem#\\nGuides for how other companies/products can be used with LangChain\\nAI21 Labs\\nAim\\nApify\\nAtlasDB\\nBanana\\nCerebriumAI\\nChroma\\nClearML Integration\\nCohere\\nComet\\nDataberry\\nDeepInfra\\nDeep Lake\\nForefrontAI\\nGoogle Search Wrapper\\nGoogle Serper Wrapper\\nGooseAI\\nGPT4All\\nGraphsignal\\nHazy Research\\nHelicone\\nHugging Face\\nJina\\nLlama.cpp\\nMilvus\\nModal\\nNLPCloud\\nOpenAI\\nOpenSearch\\nPetals\\nPGVector\\nPinecone\\nPromptLayer\\nQdrant\\nReplicate\\nRunhouse\\nRWKV-4\\nSearxNG Search API\\nSerpAPI\\nStochasticAI\\nUnstructured\\nWeights & Biases\\nWeaviate\\nWolfram Alpha Wrapper\\nWriter\\nYeager.ai\\nZilliz\\nprevious\\nAgents\\nnext\\nAI21 Labs\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nLangChain Gallery\\n Contents \\nOpen Source\\nMisc. Colab Notebooks\\nProprietary\\nLangChain Gallery#\\nLots of people have built some pretty awesome stuff with LangChain.\\nThis is a collection of our favorites.\\nIf you see any other demos that you think we should highlight, be sure to let us know!\\nOpen Source#\\nHowDoI.ai\\nThis is an experiment in building a large-language-model-backed chatbot. It can hold a conversation, remember previous comments/questions,\\nand answer all types of queries (history, web search, movie data, weather, news, and more).\\nYouTube Transcription QA with Sources\\nAn end-to-end example of doing question answering on YouTube transcripts, returning the timestamps as sources to legitimize the answer.\\nQA Slack Bot\\nThis application is a Slack Bot that uses Langchain and OpenAI’s GPT3 language model to provide domain specific answers. You provide the documents.\\nThoughtSource\\nA central, open resource and community around data and tools related to chain-of-thought reasoning in large language models.\\nLLM Strategy\\nThis Python package adds a decorator llm_strategy that connects to an LLM (such as OpenAI’s GPT-3) and uses the LLM to “implement” abstract methods in interface classes. It does this by forwarding requests to the LLM and converting the responses back to Python data using Python’s @dataclasses.\\nZero-Shot Corporate Lobbyist\\nA notebook showing how to use GPT to help with the work of a corporate lobbyist.\\nDagster Documentation ChatBot\\nA jupyter notebook demonstrating how you could create a semantic search engine on documents in one of your Google Folders\\nGoogle Folder Semantic Search\\nBuild a GitHub support bot with GPT3, LangChain, and Python.\\nTalk With Wind\\nRecord sounds of anything (birds, wind, fire, train station) and chat with it.\\nChatGPT LangChain\\nThis simple application demonstrates a conversational agent implemented with OpenAI GPT-3.5 and LangChain. When necessary, it leverages tools for complex math, searching the internet, and accessing news and weather.\\nGPT Math Techniques\\nA Hugging Face spaces project showing off the benefits of using PAL for math problems.\\nGPT Political Compass\\nMeasure the political compass of GPT.\\nNotion Database Question-Answering Bot\\nOpen source GitHub project shows how to use LangChain to create a chatbot that can answer questions about an arbitrary Notion database.\\nLlamaIndex\\nLlamaIndex (formerly GPT Index) is a project consisting of a set of data structures that are created using GPT-3 and can be traversed using GPT-3 in order to answer queries.\\nGrover’s Algorithm\\nLeveraging Qiskit, OpenAI and LangChain to demonstrate Grover’s algorithm\\nQNimGPT\\nA chat UI to play Nim, where a player can select an opponent, either a quantum computer or an AI\\nReAct TextWorld\\nLeveraging the ReActTextWorldAgent to play TextWorld with an LLM!\\nFact Checker\\nThis repo is a simple demonstration of using LangChain to do fact-checking with prompt chaining.\\nDocsGPT\\nAnswer questions about the documentation of any project\\nMisc. Colab Notebooks#\\nWolfram Alpha in Conversational Agent\\nGive ChatGPT a WolframAlpha neural implant\\nTool Updates in Agents\\nAgent improvements (6th Jan 2023)\\nConversational Agent with Tools (Langchain AGI)\\nLangchain AGI (23rd Dec 2022)\\nProprietary#\\nDaimon\\nA chat-based AI personal assistant with long-term memory about you.\\nSummarize any file with AI\\nSummarize not only long docs, interview audio or video files quickly, but also entire websites and YouTube videos. Share or download your generated summaries to collaborate with others, or revisit them at any time! Bonus: @anysummary on Twitter will also summarize any thread it is tagged in.\\nAI Assisted SQL Query Generator\\nAn app to write SQL using natural language, and execute against real DB.\\nClerkie\\nStack Tracing QA Bot to help debug complex stack tracing (especially the ones that go multi-function/file deep).\\nSales Email Writer\\nBy Raza Habib, this demo utilizes LangChain + SerpAPI + HumanLoop to write sales emails. Give it a company name and a person, this application will use Google Search (via SerpAPI) to get more information on the company and the person, and then write them a sales message.\\nQuestion-Answering on a Web Browser\\nBy Zahid Khawaja, this demo utilizes question answering to answer questions about a given website. A followup added this for YouTube videos, and then another followup added it for Wikipedia.\\nMynd\\nA journaling app for self-care that uses AI to uncover insights and patterns over time.\\nprevious\\nGlossary\\nnext\\nDeployments\\n Contents\\n  \\nOpen Source\\nMisc. Colab Notebooks\\nProprietary\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\gallery.html'}),\n",
              " Document(page_content='Index\\n_\\n | A\\n | B\\n | C\\n | D\\n | E\\n | F\\n | G\\n | H\\n | I\\n | J\\n | K\\n | L\\n | M\\n | N\\n | O\\n | P\\n | Q\\n | R\\n | S\\n | T\\n | U\\n | V\\n | W\\n | Z\\n_\\n__call__() (langchain.llms.AI21 method)\\n(langchain.llms.AlephAlpha method)\\n(langchain.llms.Anthropic method)\\n(langchain.llms.AzureOpenAI method)\\n(langchain.llms.Banana method)\\n(langchain.llms.CerebriumAI method)\\n(langchain.llms.Cohere method)\\n(langchain.llms.DeepInfra method)\\n(langchain.llms.ForefrontAI method)\\n(langchain.llms.GooseAI method)\\n(langchain.llms.GPT4All method)\\n(langchain.llms.HuggingFaceEndpoint method)\\n(langchain.llms.HuggingFaceHub method)\\n(langchain.llms.HuggingFacePipeline method)\\n(langchain.llms.LlamaCpp method)\\n(langchain.llms.Modal method)\\n(langchain.llms.NLPCloud method)\\n(langchain.llms.OpenAI method)\\n(langchain.llms.OpenAIChat method)\\n(langchain.llms.Petals method)\\n(langchain.llms.PromptLayerOpenAI method)\\n(langchain.llms.PromptLayerOpenAIChat method)\\n(langchain.llms.Replicate method)\\n(langchain.llms.RWKV method)\\n(langchain.llms.SagemakerEndpoint method)\\n(langchain.llms.SelfHostedHuggingFaceLLM method)\\n(langchain.llms.SelfHostedPipeline method)\\n(langchain.llms.StochasticAI method)\\n(langchain.llms.Writer method)\\nA\\naadd_documents() (langchain.vectorstores.VectorStore method)\\naadd_texts() (langchain.vectorstores.VectorStore method)\\naapply() (langchain.chains.LLMChain method)\\naapply_and_parse() (langchain.chains.LLMChain method)\\nadd() (langchain.docstore.InMemoryDocstore method)\\nadd_documents() (langchain.vectorstores.VectorStore method)\\nadd_embeddings() (langchain.vectorstores.FAISS method)\\nadd_example() (langchain.prompts.example_selector.LengthBasedExampleSelector method)\\n(langchain.prompts.example_selector.SemanticSimilarityExampleSelector method)\\nadd_texts() (langchain.vectorstores.Annoy method)\\n(langchain.vectorstores.AtlasDB method)\\n(langchain.vectorstores.Chroma method)\\n(langchain.vectorstores.DeepLake method)\\n(langchain.vectorstores.ElasticVectorSearch method)\\n(langchain.vectorstores.FAISS method)\\n(langchain.vectorstores.Milvus method)\\n(langchain.vectorstores.OpenSearchVectorSearch method)\\n(langchain.vectorstores.Pinecone method)\\n(langchain.vectorstores.Qdrant method)\\n(langchain.vectorstores.SupabaseVectorStore method)\\n(langchain.vectorstores.VectorStore method)\\n(langchain.vectorstores.Weaviate method)\\nadd_vectors() (langchain.vectorstores.SupabaseVectorStore method)\\nafrom_documents() (langchain.vectorstores.VectorStore class method)\\nafrom_texts() (langchain.vectorstores.VectorStore class method)\\nagenerate() (langchain.chains.LLMChain method)\\n(langchain.llms.AI21 method)\\n(langchain.llms.AlephAlpha method)\\n(langchain.llms.Anthropic method)\\n(langchain.llms.AzureOpenAI method)\\n(langchain.llms.Banana method)\\n(langchain.llms.CerebriumAI method)\\n(langchain.llms.Cohere method)\\n(langchain.llms.DeepInfra method)\\n(langchain.llms.ForefrontAI method)\\n(langchain.llms.GooseAI method)\\n(langchain.llms.GPT4All method)\\n(langchain.llms.HuggingFaceEndpoint method)\\n(langchain.llms.HuggingFaceHub method)\\n(langchain.llms.HuggingFacePipeline method)\\n(langchain.llms.LlamaCpp method)\\n(langchain.llms.Modal method)\\n(langchain.llms.NLPCloud method)\\n(langchain.llms.OpenAI method)\\n(langchain.llms.OpenAIChat method)\\n(langchain.llms.Petals method)\\n(langchain.llms.PromptLayerOpenAI method)\\n(langchain.llms.PromptLayerOpenAIChat method)\\n(langchain.llms.Replicate method)\\n(langchain.llms.RWKV method)\\n(langchain.llms.SagemakerEndpoint method)\\n(langchain.llms.SelfHostedHuggingFaceLLM method)\\n(langchain.llms.SelfHostedPipeline method)\\n(langchain.llms.StochasticAI method)\\n(langchain.llms.Writer method)\\nagenerate_prompt() (langchain.llms.AI21 method)\\n(langchain.llms.AlephAlpha method)\\n(langchain.llms.Anthropic method)\\n(langchain.llms.AzureOpenAI method)\\n(langchain.llms.Banana method)\\n(langchain.llms.CerebriumAI method)\\n(langchain.llms.Cohere method)\\n(langchain.llms.DeepInfra method)\\n(langchain.llms.ForefrontAI method)\\n(langchain.llms.GooseAI method)\\n(langchain.llms.GPT4All method)\\n(langchain.llms.HuggingFaceEndpoint method)\\n(langchain.llms.HuggingFaceHub method)\\n(langchain.llms.HuggingFacePipeline method)\\n(langchain.llms.LlamaCpp method)\\n(langchain.llms.Modal method)\\n(langchain.llms.NLPCloud method)\\n(langchain.llms.OpenAI method)\\n(langchain.llms.OpenAIChat method)\\n(langchain.llms.Petals method)\\n(langchain.llms.PromptLayerOpenAI method)\\n(langchain.llms.PromptLayerOpenAIChat method)\\n(langchain.llms.Replicate method)\\n(langchain.llms.RWKV method)\\n(langchain.llms.SagemakerEndpoint method)\\n(langchain.llms.SelfHostedHuggingFaceLLM method)\\n(langchain.llms.SelfHostedPipeline method)\\n(langchain.llms.StochasticAI method)\\n(langchain.llms.Writer method)\\nagent (langchain.agents.AgentExecutor attribute)\\n(langchain.agents.MRKLChain attribute)\\n(langchain.agents.ReActChain attribute)\\n(langchain.agents.SelfAskWithSearchChain attribute)\\nAgentType (class in langchain.agents)\\nai_prefix (langchain.agents.ConversationalAgent attribute)\\naiosession (langchain.serpapi.SerpAPIWrapper attribute)\\n(langchain.utilities.searx_search.SearxSearchWrapper attribute)\\naleph_alpha_api_key (langchain.llms.AlephAlpha attribute)\\nallowed_special (langchain.llms.AzureOpenAI attribute)\\n(langchain.llms.OpenAIChat attribute)\\n(langchain.llms.PromptLayerOpenAIChat attribute)\\nallowed_tools (langchain.agents.Agent attribute)\\namax_marginal_relevance_search() (langchain.vectorstores.VectorStore method)\\namax_marginal_relevance_search_by_vector() (langchain.vectorstores.VectorStore method)\\nAnnoy (class in langchain.vectorstores)\\nanswers (langchain.utilities.searx_search.SearxResults property)\\napi_answer_chain (langchain.chains.APIChain attribute)\\napi_docs (langchain.chains.APIChain attribute)\\napi_operation (langchain.chains.OpenAPIEndpointChain attribute)\\napi_request_chain (langchain.chains.APIChain attribute)\\n(langchain.chains.OpenAPIEndpointChain attribute)\\napi_response_chain (langchain.chains.OpenAPIEndpointChain attribute)\\napi_url (langchain.llms.StochasticAI attribute)\\naplan() (langchain.agents.Agent method)\\n(langchain.agents.BaseMultiActionAgent method)\\n(langchain.agents.BaseSingleActionAgent method)\\n(langchain.agents.LLMSingleActionAgent method)\\napply() (langchain.chains.LLMChain method)\\napply_and_parse() (langchain.chains.LLMChain method)\\napredict() (langchain.chains.LLMChain method)\\napredict_and_parse() (langchain.chains.LLMChain method)\\naprep_prompts() (langchain.chains.LLMChain method)\\nare_all_true_prompt (langchain.chains.LLMSummarizationCheckerChain attribute)\\naresults() (langchain.utilities.searx_search.SearxSearchWrapper method)\\nargs (langchain.agents.Tool property)\\narun() (langchain.serpapi.SerpAPIWrapper method)\\n(langchain.utilities.searx_search.SearxSearchWrapper method)\\nas_retriever() (langchain.vectorstores.VectorStore method)\\nasimilarity_search() (langchain.vectorstores.VectorStore method)\\nasimilarity_search_by_vector() (langchain.vectorstores.VectorStore method)\\nAtlasDB (class in langchain.vectorstores)\\natransform_documents() (langchain.text_splitter.TextSplitter method)\\nB\\nbad_words (langchain.llms.NLPCloud attribute)\\nbase_embeddings (langchain.chains.HypotheticalDocumentEmbedder attribute)\\nbase_url (langchain.llms.AI21 attribute)\\n(langchain.llms.ForefrontAI attribute)\\n(langchain.llms.Writer attribute)\\nbatch_size (langchain.llms.AzureOpenAI attribute)\\nbeam_search_diversity_rate (langchain.llms.Writer attribute)\\nbeam_width (langchain.llms.Writer attribute)\\nbest_of (langchain.llms.AlephAlpha attribute)\\n(langchain.llms.AzureOpenAI attribute)\\nC\\ncache_folder (langchain.embeddings.HuggingFaceEmbeddings attribute)\\n(langchain.embeddings.HuggingFaceInstructEmbeddings attribute)\\ncallback_manager (langchain.agents.MRKLChain attribute)\\n(langchain.agents.ReActChain attribute)\\n(langchain.agents.SelfAskWithSearchChain attribute)\\ncategories (langchain.utilities.searx_search.SearxSearchWrapper attribute)\\nchain (langchain.chains.ConstitutionalChain attribute)\\nchains (langchain.chains.SequentialChain attribute)\\n(langchain.chains.SimpleSequentialChain attribute)\\nCharacterTextSplitter (class in langchain.text_splitter)\\nCHAT_CONVERSATIONAL_REACT_DESCRIPTION (langchain.agents.AgentType attribute)\\nCHAT_ZERO_SHOT_REACT_DESCRIPTION (langchain.agents.AgentType attribute)\\ncheck_assertions_prompt (langchain.chains.LLMCheckerChain attribute)\\n(langchain.chains.LLMSummarizationCheckerChain attribute)\\nChroma (class in langchain.vectorstores)\\nCHUNK_LEN (langchain.llms.RWKV attribute)\\nchunk_size (langchain.embeddings.OpenAIEmbeddings attribute)\\nclient (langchain.llms.Petals attribute)\\ncombine_docs_chain (langchain.chains.AnalyzeDocumentChain attribute)\\ncombine_documents_chain (langchain.chains.MapReduceChain attribute)\\ncombine_embeddings() (langchain.chains.HypotheticalDocumentEmbedder method)\\ncompletion_bias_exclusion_first_token_only (langchain.llms.AlephAlpha attribute)\\ncompress_to_size (langchain.embeddings.AlephAlphaAsymmetricSemanticEmbedding attribute)\\nconstitutional_principles (langchain.chains.ConstitutionalChain attribute)\\nconstruct() (langchain.llms.AI21 class method)\\n(langchain.llms.AlephAlpha class method)\\n(langchain.llms.Anthropic class method)\\n(langchain.llms.AzureOpenAI class method)\\n(langchain.llms.Banana class method)\\n(langchain.llms.CerebriumAI class method)\\n(langchain.llms.Cohere class method)\\n(langchain.llms.DeepInfra class method)\\n(langchain.llms.ForefrontAI class method)\\n(langchain.llms.GooseAI class method)\\n(langchain.llms.GPT4All class method)\\n(langchain.llms.HuggingFaceEndpoint class method)\\n(langchain.llms.HuggingFaceHub class method)\\n(langchain.llms.HuggingFacePipeline class method)\\n(langchain.llms.LlamaCpp class method)\\n(langchain.llms.Modal class method)\\n(langchain.llms.NLPCloud class method)\\n(langchain.llms.OpenAI class method)\\n(langchain.llms.OpenAIChat class method)\\n(langchain.llms.Petals class method)\\n(langchain.llms.PromptLayerOpenAI class method)\\n(langchain.llms.PromptLayerOpenAIChat class method)\\n(langchain.llms.Replicate class method)\\n(langchain.llms.RWKV class method)\\n(langchain.llms.SagemakerEndpoint class method)\\n(langchain.llms.SelfHostedHuggingFaceLLM class method)\\n(langchain.llms.SelfHostedPipeline class method)\\n(langchain.llms.StochasticAI class method)\\n(langchain.llms.Writer class method)\\ncontent_handler (langchain.embeddings.SagemakerEndpointEmbeddings attribute)\\n(langchain.llms.SagemakerEndpoint attribute)\\nCONTENT_KEY (langchain.vectorstores.Qdrant attribute)\\ncontextual_control_threshold (langchain.embeddings.AlephAlphaAsymmetricSemanticEmbedding attribute)\\n(langchain.llms.AlephAlpha attribute)\\ncontrol_log_additive (langchain.embeddings.AlephAlphaAsymmetricSemanticEmbedding attribute)\\n(langchain.llms.AlephAlpha attribute)\\nCONVERSATIONAL_REACT_DESCRIPTION (langchain.agents.AgentType attribute)\\ncopy() (langchain.llms.AI21 method)\\n(langchain.llms.AlephAlpha method)\\n(langchain.llms.Anthropic method)\\n(langchain.llms.AzureOpenAI method)\\n(langchain.llms.Banana method)\\n(langchain.llms.CerebriumAI method)\\n(langchain.llms.Cohere method)\\n(langchain.llms.DeepInfra method)\\n(langchain.llms.ForefrontAI method)\\n(langchain.llms.GooseAI method)\\n(langchain.llms.GPT4All method)\\n(langchain.llms.HuggingFaceEndpoint method)\\n(langchain.llms.HuggingFaceHub method)\\n(langchain.llms.HuggingFacePipeline method)\\n(langchain.llms.LlamaCpp method)\\n(langchain.llms.Modal method)\\n(langchain.llms.NLPCloud method)\\n(langchain.llms.OpenAI method)\\n(langchain.llms.OpenAIChat method)\\n(langchain.llms.Petals method)\\n(langchain.llms.PromptLayerOpenAI method)\\n(langchain.llms.PromptLayerOpenAIChat method)\\n(langchain.llms.Replicate method)\\n(langchain.llms.RWKV method)\\n(langchain.llms.SagemakerEndpoint method)\\n(langchain.llms.SelfHostedHuggingFaceLLM method)\\n(langchain.llms.SelfHostedPipeline method)\\n(langchain.llms.StochasticAI method)\\n(langchain.llms.Writer method)\\ncoroutine (langchain.agents.Tool attribute)\\ncountPenalty (langchain.llms.AI21 attribute)\\ncreate_assertions_prompt (langchain.chains.LLMSummarizationCheckerChain attribute)\\ncreate_csv_agent() (in module langchain.agents)\\ncreate_documents() (langchain.text_splitter.TextSplitter method)\\ncreate_draft_answer_prompt (langchain.chains.LLMCheckerChain attribute)\\ncreate_index() (langchain.vectorstores.AtlasDB method)\\ncreate_json_agent() (in module langchain.agents)\\ncreate_llm_result() (langchain.llms.AzureOpenAI method)\\n(langchain.llms.OpenAI method)\\n(langchain.llms.PromptLayerOpenAI method)\\ncreate_openapi_agent() (in module langchain.agents)\\ncreate_outputs() (langchain.chains.LLMChain method)\\ncreate_pandas_dataframe_agent() (in module langchain.agents)\\ncreate_prompt() (langchain.agents.Agent class method)\\n(langchain.agents.ConversationalAgent class method)\\n(langchain.agents.ConversationalChatAgent class method)\\n(langchain.agents.ReActTextWorldAgent class method)\\n(langchain.agents.ZeroShotAgent class method)\\ncreate_sql_agent() (in module langchain.agents)\\ncreate_vectorstore_agent() (in module langchain.agents)\\ncreate_vectorstore_router_agent() (in module langchain.agents)\\ncredentials_profile_name (langchain.embeddings.SagemakerEndpointEmbeddings attribute)\\n(langchain.llms.SagemakerEndpoint attribute)\\ncritique_chain (langchain.chains.ConstitutionalChain attribute)\\nD\\ndatabase (langchain.chains.SQLDatabaseChain attribute)\\ndecider_chain (langchain.chains.SQLDatabaseSequentialChain attribute)\\nDeepLake (class in langchain.vectorstores)\\ndelete() (langchain.vectorstores.DeepLake method)\\ndelete_collection() (langchain.vectorstores.Chroma method)\\ndelete_dataset() (langchain.vectorstores.DeepLake method)\\ndeployment_name (langchain.llms.AzureOpenAI attribute)\\ndescription (langchain.agents.Tool attribute)\\ndeserialize_json_input() (langchain.chains.OpenAPIEndpointChain method)\\ndevice (langchain.llms.SelfHostedHuggingFaceLLM attribute)\\ndict() (langchain.agents.BaseMultiActionAgent method)\\n(langchain.agents.BaseSingleActionAgent method)\\n(langchain.llms.AI21 method)\\n(langchain.llms.AlephAlpha method)\\n(langchain.llms.Anthropic method)\\n(langchain.llms.AzureOpenAI method)\\n(langchain.llms.Banana method)\\n(langchain.llms.CerebriumAI method)\\n(langchain.llms.Cohere method)\\n(langchain.llms.DeepInfra method)\\n(langchain.llms.ForefrontAI method)\\n(langchain.llms.GooseAI method)\\n(langchain.llms.GPT4All method)\\n(langchain.llms.HuggingFaceEndpoint method)\\n(langchain.llms.HuggingFaceHub method)\\n(langchain.llms.HuggingFacePipeline method)\\n(langchain.llms.LlamaCpp method)\\n(langchain.llms.Modal method)\\n(langchain.llms.NLPCloud method)\\n(langchain.llms.OpenAI method)\\n(langchain.llms.OpenAIChat method)\\n(langchain.llms.Petals method)\\n(langchain.llms.PromptLayerOpenAI method)\\n(langchain.llms.PromptLayerOpenAIChat method)\\n(langchain.llms.Replicate method)\\n(langchain.llms.RWKV method)\\n(langchain.llms.SagemakerEndpoint method)\\n(langchain.llms.SelfHostedHuggingFaceLLM method)\\n(langchain.llms.SelfHostedPipeline method)\\n(langchain.llms.StochasticAI method)\\n(langchain.llms.Writer method)\\n(langchain.prompts.BasePromptTemplate method)\\n(langchain.prompts.FewShotPromptTemplate method)\\n(langchain.prompts.FewShotPromptWithTemplates method)\\ndisallowed_special (langchain.llms.AzureOpenAI attribute)\\n(langchain.llms.OpenAIChat attribute)\\n(langchain.llms.PromptLayerOpenAIChat attribute)\\ndo_sample (langchain.llms.NLPCloud attribute)\\n(langchain.llms.Petals attribute)\\nE\\nearly_stopping (langchain.llms.NLPCloud attribute)\\nearly_stopping_method (langchain.agents.AgentExecutor attribute)\\n(langchain.agents.MRKLChain attribute)\\n(langchain.agents.ReActChain attribute)\\n(langchain.agents.SelfAskWithSearchChain attribute)\\necho (langchain.llms.AlephAlpha attribute)\\n(langchain.llms.GPT4All attribute)\\n(langchain.llms.LlamaCpp attribute)\\nElasticVectorSearch (class in langchain.vectorstores)\\nembed_documents() (langchain.chains.HypotheticalDocumentEmbedder method)\\n(langchain.embeddings.AlephAlphaAsymmetricSemanticEmbedding method)\\n(langchain.embeddings.AlephAlphaSymmetricSemanticEmbedding method)\\n(langchain.embeddings.CohereEmbeddings method)\\n(langchain.embeddings.FakeEmbeddings method)\\n(langchain.embeddings.HuggingFaceEmbeddings method)\\n(langchain.embeddings.HuggingFaceHubEmbeddings method)\\n(langchain.embeddings.HuggingFaceInstructEmbeddings method)\\n(langchain.embeddings.LlamaCppEmbeddings method)\\n(langchain.embeddings.OpenAIEmbeddings method)\\n(langchain.embeddings.SagemakerEndpointEmbeddings method)\\n(langchain.embeddings.SelfHostedEmbeddings method)\\n(langchain.embeddings.SelfHostedHuggingFaceInstructEmbeddings method)\\n(langchain.embeddings.TensorflowHubEmbeddings method)\\nembed_instruction (langchain.embeddings.HuggingFaceInstructEmbeddings attribute)\\n(langchain.embeddings.SelfHostedHuggingFaceInstructEmbeddings attribute)\\nembed_query() (langchain.chains.HypotheticalDocumentEmbedder method)\\n(langchain.embeddings.AlephAlphaAsymmetricSemanticEmbedding method)\\n(langchain.embeddings.AlephAlphaSymmetricSemanticEmbedding method)\\n(langchain.embeddings.CohereEmbeddings method)\\n(langchain.embeddings.FakeEmbeddings method)\\n(langchain.embeddings.HuggingFaceEmbeddings method)\\n(langchain.embeddings.HuggingFaceHubEmbeddings method)\\n(langchain.embeddings.HuggingFaceInstructEmbeddings method)\\n(langchain.embeddings.LlamaCppEmbeddings method)\\n(langchain.embeddings.OpenAIEmbeddings method)\\n(langchain.embeddings.SagemakerEndpointEmbeddings method)\\n(langchain.embeddings.SelfHostedEmbeddings method)\\n(langchain.embeddings.SelfHostedHuggingFaceInstructEmbeddings method)\\n(langchain.embeddings.TensorflowHubEmbeddings method)\\nembedding (langchain.llms.GPT4All attribute)\\nendpoint_kwargs (langchain.embeddings.SagemakerEndpointEmbeddings attribute)\\n(langchain.llms.SagemakerEndpoint attribute)\\nendpoint_name (langchain.embeddings.SagemakerEndpointEmbeddings attribute)\\n(langchain.llms.SagemakerEndpoint attribute)\\nendpoint_url (langchain.llms.CerebriumAI attribute)\\n(langchain.llms.ForefrontAI attribute)\\n(langchain.llms.HuggingFaceEndpoint attribute)\\n(langchain.llms.Modal attribute)\\nengines (langchain.utilities.searx_search.SearxSearchWrapper attribute)\\nentity_extraction_chain (langchain.chains.GraphQAChain attribute)\\nerror (langchain.chains.OpenAIModerationChain attribute)\\nexample_keys (langchain.prompts.example_selector.SemanticSimilarityExampleSelector attribute)\\nexample_prompt (langchain.prompts.example_selector.LengthBasedExampleSelector attribute)\\n(langchain.prompts.FewShotPromptTemplate attribute)\\n(langchain.prompts.FewShotPromptWithTemplates attribute)\\nexample_selector (langchain.prompts.FewShotPromptTemplate attribute)\\n(langchain.prompts.FewShotPromptWithTemplates attribute)\\nexample_separator (langchain.prompts.FewShotPromptTemplate attribute)\\n(langchain.prompts.FewShotPromptWithTemplates attribute)\\nexamples (langchain.prompts.example_selector.LengthBasedExampleSelector attribute)\\n(langchain.prompts.FewShotPromptTemplate attribute)\\n(langchain.prompts.FewShotPromptWithTemplates attribute)\\nF\\nf16_kv (langchain.embeddings.LlamaCppEmbeddings attribute)\\n(langchain.llms.GPT4All attribute)\\n(langchain.llms.LlamaCpp attribute)\\nFAISS (class in langchain.vectorstores)\\nfetch_k (langchain.prompts.example_selector.MaxMarginalRelevanceExampleSelector attribute)\\nformat() (langchain.prompts.BaseChatPromptTemplate method)\\n(langchain.prompts.BasePromptTemplate method)\\n(langchain.prompts.ChatPromptTemplate method)\\n(langchain.prompts.FewShotPromptTemplate method)\\n(langchain.prompts.FewShotPromptWithTemplates method)\\n(langchain.prompts.PromptTemplate method)\\nformat_messages() (langchain.prompts.BaseChatPromptTemplate method)\\n(langchain.prompts.ChatPromptTemplate method)\\n(langchain.prompts.MessagesPlaceholder method)\\nformat_prompt() (langchain.prompts.BaseChatPromptTemplate method)\\n(langchain.prompts.BasePromptTemplate method)\\n(langchain.prompts.StringPromptTemplate method)\\nfrequency_penalty (langchain.llms.AlephAlpha attribute)\\n(langchain.llms.AzureOpenAI attribute)\\n(langchain.llms.Cohere attribute)\\n(langchain.llms.GooseAI attribute)\\nfrequencyPenalty (langchain.llms.AI21 attribute)\\nfrom_agent_and_tools() (langchain.agents.AgentExecutor class method)\\nfrom_api_operation() (langchain.chains.OpenAPIEndpointChain class method)\\nfrom_chains() (langchain.agents.MRKLChain class method)\\nfrom_colored_object_prompt() (langchain.chains.PALChain class method)\\nfrom_documents() (langchain.vectorstores.AtlasDB class method)\\n(langchain.vectorstores.Chroma class method)\\n(langchain.vectorstores.VectorStore class method)\\nfrom_embeddings() (langchain.vectorstores.Annoy class method)\\n(langchain.vectorstores.FAISS class method)\\nfrom_examples() (langchain.prompts.example_selector.MaxMarginalRelevanceExampleSelector class method)\\n(langchain.prompts.example_selector.SemanticSimilarityExampleSelector class method)\\n(langchain.prompts.PromptTemplate class method)\\nfrom_existing_index() (langchain.vectorstores.Pinecone class method)\\nfrom_file() (langchain.prompts.PromptTemplate class method)\\nfrom_huggingface_tokenizer() (langchain.text_splitter.TextSplitter class method)\\nfrom_llm() (langchain.chains.ChatVectorDBChain class method)\\n(langchain.chains.ConstitutionalChain class method)\\n(langchain.chains.ConversationalRetrievalChain class method)\\n(langchain.chains.GraphQAChain class method)\\n(langchain.chains.HypotheticalDocumentEmbedder class method)\\n(langchain.chains.QAGenerationChain class method)\\n(langchain.chains.SQLDatabaseSequentialChain class method)\\nfrom_llm_and_api_docs() (langchain.chains.APIChain class method)\\nfrom_llm_and_tools() (langchain.agents.Agent class method)\\n(langchain.agents.BaseSingleActionAgent class method)\\n(langchain.agents.ConversationalAgent class method)\\n(langchain.agents.ConversationalChatAgent class method)\\n(langchain.agents.ZeroShotAgent class method)\\nfrom_math_prompt() (langchain.chains.PALChain class method)\\nfrom_model_id() (langchain.llms.HuggingFacePipeline class method)\\nfrom_params() (langchain.chains.MapReduceChain class method)\\nfrom_pipeline() (langchain.llms.SelfHostedHuggingFaceLLM class method)\\n(langchain.llms.SelfHostedPipeline class method)\\nfrom_string() (langchain.chains.LLMChain class method)\\nfrom_template() (langchain.prompts.PromptTemplate class method)\\nfrom_texts() (langchain.vectorstores.Annoy class method)\\n(langchain.vectorstores.AtlasDB class method)\\n(langchain.vectorstores.Chroma class method)\\n(langchain.vectorstores.DeepLake class method)\\n(langchain.vectorstores.ElasticVectorSearch class method)\\n(langchain.vectorstores.FAISS class method)\\n(langchain.vectorstores.Milvus class method)\\n(langchain.vectorstores.OpenSearchVectorSearch class method)\\n(langchain.vectorstores.Pinecone class method)\\n(langchain.vectorstores.Qdrant class method)\\n(langchain.vectorstores.SupabaseVectorStore class method)\\n(langchain.vectorstores.VectorStore class method)\\n(langchain.vectorstores.Weaviate class method)\\nfrom_tiktoken_encoder() (langchain.text_splitter.TextSplitter class method)\\nfrom_url_and_method() (langchain.chains.OpenAPIEndpointChain class method)\\nfunc (langchain.agents.Tool attribute)\\nG\\ngenerate() (langchain.chains.LLMChain method)\\n(langchain.llms.AI21 method)\\n(langchain.llms.AlephAlpha method)\\n(langchain.llms.Anthropic method)\\n(langchain.llms.AzureOpenAI method)\\n(langchain.llms.Banana method)\\n(langchain.llms.CerebriumAI method)\\n(langchain.llms.Cohere method)\\n(langchain.llms.DeepInfra method)\\n(langchain.llms.ForefrontAI method)\\n(langchain.llms.GooseAI method)\\n(langchain.llms.GPT4All method)\\n(langchain.llms.HuggingFaceEndpoint method)\\n(langchain.llms.HuggingFaceHub method)\\n(langchain.llms.HuggingFacePipeline method)\\n(langchain.llms.LlamaCpp method)\\n(langchain.llms.Modal method)\\n(langchain.llms.NLPCloud method)\\n(langchain.llms.OpenAI method)\\n(langchain.llms.OpenAIChat method)\\n(langchain.llms.Petals method)\\n(langchain.llms.PromptLayerOpenAI method)\\n(langchain.llms.PromptLayerOpenAIChat method)\\n(langchain.llms.Replicate method)\\n(langchain.llms.RWKV method)\\n(langchain.llms.SagemakerEndpoint method)\\n(langchain.llms.SelfHostedHuggingFaceLLM method)\\n(langchain.llms.SelfHostedPipeline method)\\n(langchain.llms.StochasticAI method)\\n(langchain.llms.Writer method)\\ngenerate_prompt() (langchain.llms.AI21 method)\\n(langchain.llms.AlephAlpha method)\\n(langchain.llms.Anthropic method)\\n(langchain.llms.AzureOpenAI method)\\n(langchain.llms.Banana method)\\n(langchain.llms.CerebriumAI method)\\n(langchain.llms.Cohere method)\\n(langchain.llms.DeepInfra method)\\n(langchain.llms.ForefrontAI method)\\n(langchain.llms.GooseAI method)\\n(langchain.llms.GPT4All method)\\n(langchain.llms.HuggingFaceEndpoint method)\\n(langchain.llms.HuggingFaceHub method)\\n(langchain.llms.HuggingFacePipeline method)\\n(langchain.llms.LlamaCpp method)\\n(langchain.llms.Modal method)\\n(langchain.llms.NLPCloud method)\\n(langchain.llms.OpenAI method)\\n(langchain.llms.OpenAIChat method)\\n(langchain.llms.Petals method)\\n(langchain.llms.PromptLayerOpenAI method)\\n(langchain.llms.PromptLayerOpenAIChat method)\\n(langchain.llms.Replicate method)\\n(langchain.llms.RWKV method)\\n(langchain.llms.SagemakerEndpoint method)\\n(langchain.llms.SelfHostedHuggingFaceLLM method)\\n(langchain.llms.SelfHostedPipeline method)\\n(langchain.llms.StochasticAI method)\\n(langchain.llms.Writer method)\\nget_all_tool_names() (in module langchain.agents)\\nget_allowed_tools() (langchain.agents.Agent method)\\n(langchain.agents.BaseMultiActionAgent method)\\n(langchain.agents.BaseSingleActionAgent method)\\nget_answer_expr (langchain.chains.PALChain attribute)\\nget_full_inputs() (langchain.agents.Agent method)\\nget_num_tokens() (langchain.llms.AI21 method)\\n(langchain.llms.AlephAlpha method)\\n(langchain.llms.Anthropic method)\\n(langchain.llms.AzureOpenAI method)\\n(langchain.llms.Banana method)\\n(langchain.llms.CerebriumAI method)\\n(langchain.llms.Cohere method)\\n(langchain.llms.DeepInfra method)\\n(langchain.llms.ForefrontAI method)\\n(langchain.llms.GooseAI method)\\n(langchain.llms.GPT4All method)\\n(langchain.llms.HuggingFaceEndpoint method)\\n(langchain.llms.HuggingFaceHub method)\\n(langchain.llms.HuggingFacePipeline method)\\n(langchain.llms.LlamaCpp method)\\n(langchain.llms.Modal method)\\n(langchain.llms.NLPCloud method)\\n(langchain.llms.OpenAI method)\\n(langchain.llms.OpenAIChat method)\\n(langchain.llms.Petals method)\\n(langchain.llms.PromptLayerOpenAI method)\\n(langchain.llms.PromptLayerOpenAIChat method)\\n(langchain.llms.Replicate method)\\n(langchain.llms.RWKV method)\\n(langchain.llms.SagemakerEndpoint method)\\n(langchain.llms.SelfHostedHuggingFaceLLM method)\\n(langchain.llms.SelfHostedPipeline method)\\n(langchain.llms.StochasticAI method)\\n(langchain.llms.Writer method)\\nget_num_tokens_from_messages() (langchain.llms.AI21 method)\\n(langchain.llms.AlephAlpha method)\\n(langchain.llms.Anthropic method)\\n(langchain.llms.AzureOpenAI method)\\n(langchain.llms.Banana method)\\n(langchain.llms.CerebriumAI method)\\n(langchain.llms.Cohere method)\\n(langchain.llms.DeepInfra method)\\n(langchain.llms.ForefrontAI method)\\n(langchain.llms.GooseAI method)\\n(langchain.llms.GPT4All method)\\n(langchain.llms.HuggingFaceEndpoint method)\\n(langchain.llms.HuggingFaceHub method)\\n(langchain.llms.HuggingFacePipeline method)\\n(langchain.llms.LlamaCpp method)\\n(langchain.llms.Modal method)\\n(langchain.llms.NLPCloud method)\\n(langchain.llms.OpenAI method)\\n(langchain.llms.OpenAIChat method)\\n(langchain.llms.Petals method)\\n(langchain.llms.PromptLayerOpenAI method)\\n(langchain.llms.PromptLayerOpenAIChat method)\\n(langchain.llms.Replicate method)\\n(langchain.llms.RWKV method)\\n(langchain.llms.SagemakerEndpoint method)\\n(langchain.llms.SelfHostedHuggingFaceLLM method)\\n(langchain.llms.SelfHostedPipeline method)\\n(langchain.llms.StochasticAI method)\\n(langchain.llms.Writer method)\\nget_params() (langchain.serpapi.SerpAPIWrapper method)\\nget_principles() (langchain.chains.ConstitutionalChain class method)\\nget_sub_prompts() (langchain.llms.AzureOpenAI method)\\n(langchain.llms.OpenAI method)\\n(langchain.llms.PromptLayerOpenAI method)\\nget_text_length (langchain.prompts.example_selector.LengthBasedExampleSelector attribute)\\nglobals (langchain.python.PythonREPL attribute)\\ngraph (langchain.chains.GraphQAChain attribute)\\nH\\nhardware (langchain.embeddings.SelfHostedHuggingFaceEmbeddings attribute)\\n(langchain.llms.SelfHostedHuggingFaceLLM attribute)\\n(langchain.llms.SelfHostedPipeline attribute)\\nheaders (langchain.utilities.searx_search.SearxSearchWrapper attribute)\\nhosting (langchain.embeddings.AlephAlphaAsymmetricSemanticEmbedding attribute)\\nI\\ninference_fn (langchain.embeddings.SelfHostedEmbeddings attribute)\\n(langchain.embeddings.SelfHostedHuggingFaceEmbeddings attribute)\\n(langchain.llms.SelfHostedHuggingFaceLLM attribute)\\n(langchain.llms.SelfHostedPipeline attribute)\\ninference_kwargs (langchain.embeddings.SelfHostedEmbeddings attribute)\\ninitialize_agent() (in module langchain.agents)\\nInMemoryDocstore (class in langchain.docstore)\\ninput_key (langchain.chains.QAGenerationChain attribute)\\ninput_keys (langchain.chains.ConstitutionalChain property)\\n(langchain.chains.ConversationChain property)\\n(langchain.chains.HypotheticalDocumentEmbedder property)\\n(langchain.chains.QAGenerationChain property)\\n(langchain.prompts.example_selector.SemanticSimilarityExampleSelector attribute)\\ninput_variables (langchain.chains.SequentialChain attribute)\\n(langchain.chains.TransformChain attribute)\\n(langchain.prompts.BasePromptTemplate attribute)\\n(langchain.prompts.FewShotPromptTemplate attribute)\\n(langchain.prompts.FewShotPromptWithTemplates attribute)\\n(langchain.prompts.MessagesPlaceholder property)\\n(langchain.prompts.PromptTemplate attribute)\\nJ\\njson() (langchain.llms.AI21 method)\\n(langchain.llms.AlephAlpha method)\\n(langchain.llms.Anthropic method)\\n(langchain.llms.AzureOpenAI method)\\n(langchain.llms.Banana method)\\n(langchain.llms.CerebriumAI method)\\n(langchain.llms.Cohere method)\\n(langchain.llms.DeepInfra method)\\n(langchain.llms.ForefrontAI method)\\n(langchain.llms.GooseAI method)\\n(langchain.llms.GPT4All method)\\n(langchain.llms.HuggingFaceEndpoint method)\\n(langchain.llms.HuggingFaceHub method)\\n(langchain.llms.HuggingFacePipeline method)\\n(langchain.llms.LlamaCpp method)\\n(langchain.llms.Modal method)\\n(langchain.llms.NLPCloud method)\\n(langchain.llms.OpenAI method)\\n(langchain.llms.OpenAIChat method)\\n(langchain.llms.Petals method)\\n(langchain.llms.PromptLayerOpenAI method)\\n(langchain.llms.PromptLayerOpenAIChat method)\\n(langchain.llms.Replicate method)\\n(langchain.llms.RWKV method)\\n(langchain.llms.SagemakerEndpoint method)\\n(langchain.llms.SelfHostedHuggingFaceLLM method)\\n(langchain.llms.SelfHostedPipeline method)\\n(langchain.llms.StochasticAI method)\\n(langchain.llms.Writer method)\\nK\\nk (langchain.chains.QAGenerationChain attribute)\\n(langchain.chains.VectorDBQA attribute)\\n(langchain.chains.VectorDBQAWithSourcesChain attribute)\\n(langchain.llms.Cohere attribute)\\n(langchain.prompts.example_selector.SemanticSimilarityExampleSelector attribute)\\n(langchain.utilities.searx_search.SearxSearchWrapper attribute)\\nL\\n    langchain.agents\\n      \\nmodule\\n    langchain.chains\\n      \\nmodule\\n    langchain.docstore\\n      \\nmodule\\n    langchain.embeddings\\n      \\nmodule\\n    langchain.llms\\n      \\nmodule\\n    langchain.prompts\\n      \\nmodule\\n    langchain.prompts.example_selector\\n      \\nmodule\\n    langchain.python\\n      \\nmodule\\n    langchain.serpapi\\n      \\nmodule\\n    langchain.text_splitter\\n      \\nmodule\\n    langchain.utilities.searx_search\\n      \\nmodule\\n    langchain.vectorstores\\n      \\nmodule\\nlast_n_tokens_size (langchain.llms.LlamaCpp attribute)\\nLatexTextSplitter (class in langchain.text_splitter)\\nlength (langchain.llms.ForefrontAI attribute)\\n(langchain.llms.Writer attribute)\\nlength_no_input (langchain.llms.NLPCloud attribute)\\nlength_penalty (langchain.llms.NLPCloud attribute)\\nlength_pentaly (langchain.llms.Writer attribute)\\nlist_assertions_prompt (langchain.chains.LLMCheckerChain attribute)\\nllm (langchain.chains.LLMBashChain attribute)\\n(langchain.chains.LLMChain attribute)\\n(langchain.chains.LLMCheckerChain attribute)\\n(langchain.chains.LLMMathChain attribute)\\n(langchain.chains.LLMSummarizationCheckerChain attribute)\\n(langchain.chains.PALChain attribute)\\n(langchain.chains.SQLDatabaseChain attribute)\\nllm_chain (langchain.agents.Agent attribute)\\n(langchain.agents.LLMSingleActionAgent attribute)\\n(langchain.chains.HypotheticalDocumentEmbedder attribute)\\n(langchain.chains.LLMRequestsChain attribute)\\n(langchain.chains.QAGenerationChain attribute)\\nllm_prefix (langchain.agents.Agent property)\\n(langchain.agents.ConversationalAgent property)\\n(langchain.agents.ConversationalChatAgent property)\\n(langchain.agents.ZeroShotAgent property)\\nload_agent() (in module langchain.agents)\\nload_chain() (in module langchain.chains)\\nload_fn_kwargs (langchain.embeddings.SelfHostedHuggingFaceEmbeddings attribute)\\n(langchain.llms.SelfHostedHuggingFaceLLM attribute)\\n(langchain.llms.SelfHostedPipeline attribute)\\nload_local() (langchain.vectorstores.Annoy class method)\\n(langchain.vectorstores.FAISS class method)\\nload_prompt() (in module langchain.prompts)\\nload_tools() (in module langchain.agents)\\nlocals (langchain.python.PythonREPL attribute)\\nlog_probs (langchain.llms.AlephAlpha attribute)\\nlogit_bias (langchain.llms.AlephAlpha attribute)\\n(langchain.llms.AzureOpenAI attribute)\\n(langchain.llms.GooseAI attribute)\\nlogitBias (langchain.llms.AI21 attribute)\\nlogits_all (langchain.embeddings.LlamaCppEmbeddings attribute)\\n(langchain.llms.GPT4All attribute)\\n(langchain.llms.LlamaCpp attribute)\\nlogprobs (langchain.llms.LlamaCpp attribute)\\n(langchain.llms.Writer attribute)\\nlookup_tool() (langchain.agents.AgentExecutor method)\\nM\\nMarkdownTextSplitter (class in langchain.text_splitter)\\nmax_checks (langchain.chains.LLMSummarizationCheckerChain attribute)\\nmax_execution_time (langchain.agents.AgentExecutor attribute)\\n(langchain.agents.MRKLChain attribute)\\n(langchain.agents.ReActChain attribute)\\n(langchain.agents.SelfAskWithSearchChain attribute)\\nmax_iterations (langchain.agents.AgentExecutor attribute)\\n(langchain.agents.MRKLChain attribute)\\n(langchain.agents.ReActChain attribute)\\n(langchain.agents.SelfAskWithSearchChain attribute)\\nmax_length (langchain.llms.NLPCloud attribute)\\n(langchain.llms.Petals attribute)\\n(langchain.prompts.example_selector.LengthBasedExampleSelector attribute)\\nmax_marginal_relevance_search() (langchain.vectorstores.Annoy method)\\n(langchain.vectorstores.Chroma method)\\n(langchain.vectorstores.DeepLake method)\\n(langchain.vectorstores.FAISS method)\\n(langchain.vectorstores.Milvus method)\\n(langchain.vectorstores.Qdrant method)\\n(langchain.vectorstores.SupabaseVectorStore method)\\n(langchain.vectorstores.VectorStore method)\\n(langchain.vectorstores.Weaviate method)\\nmax_marginal_relevance_search_by_vector() (langchain.vectorstores.Annoy method)\\n(langchain.vectorstores.Chroma method)\\n(langchain.vectorstores.DeepLake method)\\n(langchain.vectorstores.FAISS method)\\n(langchain.vectorstores.SupabaseVectorStore method)\\n(langchain.vectorstores.VectorStore method)\\nmax_new_tokens (langchain.llms.Petals attribute)\\nmax_retries (langchain.embeddings.OpenAIEmbeddings attribute)\\n(langchain.llms.AzureOpenAI attribute)\\n(langchain.llms.OpenAIChat attribute)\\n(langchain.llms.PromptLayerOpenAIChat attribute)\\nmax_tokens (langchain.llms.AzureOpenAI attribute)\\n(langchain.llms.Cohere attribute)\\n(langchain.llms.GooseAI attribute)\\n(langchain.llms.LlamaCpp attribute)\\nmax_tokens_for_prompt() (langchain.llms.AzureOpenAI method)\\n(langchain.llms.OpenAI method)\\n(langchain.llms.PromptLayerOpenAI method)\\nmax_tokens_limit (langchain.chains.ConversationalRetrievalChain attribute)\\n(langchain.chains.RetrievalQAWithSourcesChain attribute)\\n(langchain.chains.VectorDBQAWithSourcesChain attribute)\\nmax_tokens_per_generation (langchain.llms.RWKV attribute)\\nmax_tokens_to_sample (langchain.llms.Anthropic attribute)\\nmaximum_tokens (langchain.llms.AlephAlpha attribute)\\nmaxTokens (langchain.llms.AI21 attribute)\\nmemory (langchain.agents.MRKLChain attribute)\\n(langchain.agents.ReActChain attribute)\\n(langchain.agents.SelfAskWithSearchChain attribute)\\n(langchain.chains.ConversationChain attribute)\\nmerge_from() (langchain.vectorstores.FAISS method)\\nMETADATA_KEY (langchain.vectorstores.Qdrant attribute)\\nMilvus (class in langchain.vectorstores)\\nmin_length (langchain.llms.NLPCloud attribute)\\nmin_tokens (langchain.llms.GooseAI attribute)\\nminimum_tokens (langchain.llms.AlephAlpha attribute)\\nminTokens (langchain.llms.AI21 attribute)\\nmodel (langchain.embeddings.AlephAlphaAsymmetricSemanticEmbedding attribute)\\n(langchain.embeddings.CohereEmbeddings attribute)\\n(langchain.llms.AI21 attribute)\\n(langchain.llms.AlephAlpha attribute)\\n(langchain.llms.Anthropic attribute)\\n(langchain.llms.Cohere attribute)\\n(langchain.llms.GPT4All attribute)\\n(langchain.llms.RWKV attribute)\\nmodel_id (langchain.embeddings.SelfHostedHuggingFaceEmbeddings attribute)\\n(langchain.embeddings.SelfHostedHuggingFaceInstructEmbeddings attribute)\\n(langchain.llms.HuggingFacePipeline attribute)\\n(langchain.llms.SelfHostedHuggingFaceLLM attribute)\\n(langchain.llms.Writer attribute)\\nmodel_key (langchain.llms.Banana attribute)\\nmodel_kwargs (langchain.embeddings.HuggingFaceEmbeddings attribute)\\n(langchain.embeddings.HuggingFaceHubEmbeddings attribute)\\n(langchain.embeddings.HuggingFaceInstructEmbeddings attribute)\\n(langchain.embeddings.SagemakerEndpointEmbeddings attribute)\\n(langchain.llms.AzureOpenAI attribute)\\n(langchain.llms.Banana attribute)\\n(langchain.llms.CerebriumAI attribute)\\n(langchain.llms.GooseAI attribute)\\n(langchain.llms.HuggingFaceEndpoint attribute)\\n(langchain.llms.HuggingFaceHub attribute)\\n(langchain.llms.HuggingFacePipeline attribute)\\n(langchain.llms.Modal attribute)\\n(langchain.llms.OpenAIChat attribute)\\n(langchain.llms.Petals attribute)\\n(langchain.llms.PromptLayerOpenAIChat attribute)\\n(langchain.llms.SagemakerEndpoint attribute)\\n(langchain.llms.SelfHostedHuggingFaceLLM attribute)\\n(langchain.llms.StochasticAI attribute)\\nmodel_load_fn (langchain.embeddings.SelfHostedHuggingFaceEmbeddings attribute)\\n(langchain.llms.SelfHostedHuggingFaceLLM attribute)\\n(langchain.llms.SelfHostedPipeline attribute)\\nmodel_name (langchain.chains.OpenAIModerationChain attribute)\\n(langchain.embeddings.HuggingFaceEmbeddings attribute)\\n(langchain.embeddings.HuggingFaceInstructEmbeddings attribute)\\n(langchain.llms.AzureOpenAI attribute)\\n(langchain.llms.GooseAI attribute)\\n(langchain.llms.NLPCloud attribute)\\n(langchain.llms.OpenAIChat attribute)\\n(langchain.llms.Petals attribute)\\n(langchain.llms.PromptLayerOpenAIChat attribute)\\nmodel_path (langchain.llms.LlamaCpp attribute)\\nmodel_reqs (langchain.embeddings.SelfHostedHuggingFaceEmbeddings attribute)\\n(langchain.embeddings.SelfHostedHuggingFaceInstructEmbeddings attribute)\\n(langchain.llms.SelfHostedHuggingFaceLLM attribute)\\n(langchain.llms.SelfHostedPipeline attribute)\\nmodel_url (langchain.embeddings.TensorflowHubEmbeddings attribute)\\nmodelname_to_contextsize() (langchain.llms.AzureOpenAI method)\\n(langchain.llms.OpenAI method)\\n(langchain.llms.PromptLayerOpenAI method)\\n    module\\n      \\nlangchain.agents\\nlangchain.chains\\nlangchain.docstore\\nlangchain.embeddings\\nlangchain.llms\\nlangchain.prompts\\nlangchain.prompts.example_selector\\nlangchain.python\\nlangchain.serpapi\\nlangchain.text_splitter\\nlangchain.utilities.searx_search\\nlangchain.vectorstores\\nN\\nn (langchain.llms.AlephAlpha attribute)\\n(langchain.llms.AzureOpenAI attribute)\\n(langchain.llms.GooseAI attribute)\\nn_batch (langchain.embeddings.LlamaCppEmbeddings attribute)\\n(langchain.llms.GPT4All attribute)\\n(langchain.llms.LlamaCpp attribute)\\nn_ctx (langchain.embeddings.LlamaCppEmbeddings attribute)\\n(langchain.llms.GPT4All attribute)\\n(langchain.llms.LlamaCpp attribute)\\nn_parts (langchain.embeddings.LlamaCppEmbeddings attribute)\\n(langchain.llms.GPT4All attribute)\\n(langchain.llms.LlamaCpp attribute)\\nn_predict (langchain.llms.GPT4All attribute)\\nn_threads (langchain.embeddings.LlamaCppEmbeddings attribute)\\n(langchain.llms.GPT4All attribute)\\n(langchain.llms.LlamaCpp attribute)\\nNLTKTextSplitter (class in langchain.text_splitter)\\nnormalize (langchain.embeddings.AlephAlphaAsymmetricSemanticEmbedding attribute)\\nnum_beams (langchain.llms.NLPCloud attribute)\\nnum_return_sequences (langchain.llms.NLPCloud attribute)\\nnumResults (langchain.llms.AI21 attribute)\\nO\\nobservation_prefix (langchain.agents.Agent property)\\n(langchain.agents.ConversationalAgent property)\\n(langchain.agents.ConversationalChatAgent property)\\n(langchain.agents.ZeroShotAgent property)\\nopenai_api_key (langchain.chains.OpenAIModerationChain attribute)\\nopenai_organization (langchain.chains.OpenAIModerationChain attribute)\\nOpenSearchVectorSearch (class in langchain.vectorstores)\\noutput_key (langchain.chains.QAGenerationChain attribute)\\noutput_keys (langchain.chains.ConstitutionalChain property)\\n(langchain.chains.HypotheticalDocumentEmbedder property)\\n(langchain.chains.QAGenerationChain property)\\noutput_parser (langchain.agents.Agent attribute)\\n(langchain.agents.ConversationalAgent attribute)\\n(langchain.agents.ConversationalChatAgent attribute)\\n(langchain.agents.LLMSingleActionAgent attribute)\\n(langchain.agents.ReActTextWorldAgent attribute)\\n(langchain.agents.ZeroShotAgent attribute)\\n(langchain.prompts.BasePromptTemplate attribute)\\noutput_variables (langchain.chains.TransformChain attribute)\\nP\\np (langchain.llms.Cohere attribute)\\nparam_mapping (langchain.chains.OpenAPIEndpointChain attribute)\\nparams (langchain.serpapi.SerpAPIWrapper attribute)\\n(langchain.utilities.searx_search.SearxSearchWrapper attribute)\\nparse() (langchain.agents.AgentOutputParser method)\\npartial() (langchain.prompts.BasePromptTemplate method)\\n(langchain.prompts.ChatPromptTemplate method)\\npenalty_alpha_frequency (langchain.llms.RWKV attribute)\\npenalty_alpha_presence (langchain.llms.RWKV attribute)\\npenalty_bias (langchain.llms.AlephAlpha attribute)\\npenalty_exceptions (langchain.llms.AlephAlpha attribute)\\npenalty_exceptions_include_stop_sequences (langchain.llms.AlephAlpha attribute)\\npersist() (langchain.vectorstores.Chroma method)\\n(langchain.vectorstores.DeepLake method)\\nPinecone (class in langchain.vectorstores)\\nplan() (langchain.agents.Agent method)\\n(langchain.agents.BaseMultiActionAgent method)\\n(langchain.agents.BaseSingleActionAgent method)\\n(langchain.agents.LLMSingleActionAgent method)\\npredict() (langchain.chains.LLMChain method)\\npredict_and_parse() (langchain.chains.LLMChain method)\\nprefix (langchain.prompts.FewShotPromptTemplate attribute)\\n(langchain.prompts.FewShotPromptWithTemplates attribute)\\nprefix_messages (langchain.llms.OpenAIChat attribute)\\n(langchain.llms.PromptLayerOpenAIChat attribute)\\nprep_prompts() (langchain.chains.LLMChain method)\\nprep_streaming_params() (langchain.llms.AzureOpenAI method)\\n(langchain.llms.OpenAI method)\\n(langchain.llms.PromptLayerOpenAI method)\\npresence_penalty (langchain.llms.AlephAlpha attribute)\\n(langchain.llms.AzureOpenAI attribute)\\n(langchain.llms.Cohere attribute)\\n(langchain.llms.GooseAI attribute)\\npresencePenalty (langchain.llms.AI21 attribute)\\nprocess_index_results() (langchain.vectorstores.Annoy method)\\nPrompt (in module langchain.prompts)\\nprompt (langchain.chains.ConversationChain attribute)\\n(langchain.chains.LLMBashChain attribute)\\n(langchain.chains.LLMChain attribute)\\n(langchain.chains.LLMMathChain attribute)\\n(langchain.chains.PALChain attribute)\\n(langchain.chains.SQLDatabaseChain attribute)\\npython_globals (langchain.chains.PALChain attribute)\\npython_locals (langchain.chains.PALChain attribute)\\nPythonCodeTextSplitter (class in langchain.text_splitter)\\nQ\\nqa_chain (langchain.chains.GraphQAChain attribute)\\nQdrant (class in langchain.vectorstores)\\nquery_instruction (langchain.embeddings.HuggingFaceInstructEmbeddings attribute)\\n(langchain.embeddings.SelfHostedHuggingFaceInstructEmbeddings attribute)\\nquery_name (langchain.vectorstores.SupabaseVectorStore attribute)\\nquery_suffix (langchain.utilities.searx_search.SearxSearchWrapper attribute)\\nR\\nrandom_seed (langchain.llms.Writer attribute)\\nraw_completion (langchain.llms.AlephAlpha attribute)\\nREACT_DOCSTORE (langchain.agents.AgentType attribute)\\nRecursiveCharacterTextSplitter (class in langchain.text_splitter)\\nreduce_k_below_max_tokens (langchain.chains.RetrievalQAWithSourcesChain attribute)\\n(langchain.chains.VectorDBQAWithSourcesChain attribute)\\nregion_name (langchain.embeddings.SagemakerEndpointEmbeddings attribute)\\n(langchain.llms.SagemakerEndpoint attribute)\\nremove_end_sequence (langchain.llms.NLPCloud attribute)\\nremove_input (langchain.llms.NLPCloud attribute)\\nrepeat_last_n (langchain.llms.GPT4All attribute)\\nrepeat_penalty (langchain.llms.GPT4All attribute)\\n(langchain.llms.LlamaCpp attribute)\\nrepetition_penalties_include_completion (langchain.llms.AlephAlpha attribute)\\nrepetition_penalties_include_prompt (langchain.llms.AlephAlpha attribute)\\nrepetition_penalty (langchain.llms.ForefrontAI attribute)\\n(langchain.llms.NLPCloud attribute)\\n(langchain.llms.Writer attribute)\\nrepo_id (langchain.embeddings.HuggingFaceHubEmbeddings attribute)\\n(langchain.llms.HuggingFaceHub attribute)\\nrequest_timeout (langchain.llms.AzureOpenAI attribute)\\nrequests (langchain.chains.OpenAPIEndpointChain attribute)\\nrequests_wrapper (langchain.chains.APIChain attribute)\\n(langchain.chains.LLMRequestsChain attribute)\\nresults() (langchain.serpapi.SerpAPIWrapper method)\\n(langchain.utilities.searx_search.SearxSearchWrapper method)\\nretriever (langchain.chains.ConversationalRetrievalChain attribute)\\n(langchain.chains.RetrievalQA attribute)\\n(langchain.chains.RetrievalQAWithSourcesChain attribute)\\nreturn_all (langchain.chains.SequentialChain attribute)\\nreturn_direct (langchain.chains.SQLDatabaseChain attribute)\\nreturn_intermediate_steps (langchain.agents.AgentExecutor attribute)\\n(langchain.agents.MRKLChain attribute)\\n(langchain.agents.ReActChain attribute)\\n(langchain.agents.SelfAskWithSearchChain attribute)\\n(langchain.chains.OpenAPIEndpointChain attribute)\\n(langchain.chains.PALChain attribute)\\n(langchain.chains.SQLDatabaseChain attribute)\\n(langchain.chains.SQLDatabaseSequentialChain attribute)\\nreturn_stopped_response() (langchain.agents.Agent method)\\n(langchain.agents.BaseMultiActionAgent method)\\n(langchain.agents.BaseSingleActionAgent method)\\nreturn_values (langchain.agents.Agent property)\\n(langchain.agents.BaseMultiActionAgent property)\\n(langchain.agents.BaseSingleActionAgent property)\\nrevised_answer_prompt (langchain.chains.LLMCheckerChain attribute)\\nrevised_summary_prompt (langchain.chains.LLMSummarizationCheckerChain attribute)\\nrevision_chain (langchain.chains.ConstitutionalChain attribute)\\nrun() (langchain.python.PythonREPL method)\\n(langchain.serpapi.SerpAPIWrapper method)\\n(langchain.utilities.searx_search.SearxSearchWrapper method)\\nrwkv_verbose (langchain.llms.RWKV attribute)\\nS\\nsave() (langchain.agents.AgentExecutor method)\\n(langchain.agents.BaseMultiActionAgent method)\\n(langchain.agents.BaseSingleActionAgent method)\\n(langchain.llms.AI21 method)\\n(langchain.llms.AlephAlpha method)\\n(langchain.llms.Anthropic method)\\n(langchain.llms.AzureOpenAI method)\\n(langchain.llms.Banana method)\\n(langchain.llms.CerebriumAI method)\\n(langchain.llms.Cohere method)\\n(langchain.llms.DeepInfra method)\\n(langchain.llms.ForefrontAI method)\\n(langchain.llms.GooseAI method)\\n(langchain.llms.GPT4All method)\\n(langchain.llms.HuggingFaceEndpoint method)\\n(langchain.llms.HuggingFaceHub method)\\n(langchain.llms.HuggingFacePipeline method)\\n(langchain.llms.LlamaCpp method)\\n(langchain.llms.Modal method)\\n(langchain.llms.NLPCloud method)\\n(langchain.llms.OpenAI method)\\n(langchain.llms.OpenAIChat method)\\n(langchain.llms.Petals method)\\n(langchain.llms.PromptLayerOpenAI method)\\n(langchain.llms.PromptLayerOpenAIChat method)\\n(langchain.llms.Replicate method)\\n(langchain.llms.RWKV method)\\n(langchain.llms.SagemakerEndpoint method)\\n(langchain.llms.SelfHostedHuggingFaceLLM method)\\n(langchain.llms.SelfHostedPipeline method)\\n(langchain.llms.StochasticAI method)\\n(langchain.llms.Writer method)\\n(langchain.prompts.BasePromptTemplate method)\\n(langchain.prompts.ChatPromptTemplate method)\\nsave_agent() (langchain.agents.AgentExecutor method)\\nsave_local() (langchain.vectorstores.Annoy method)\\n(langchain.vectorstores.FAISS method)\\nsearch() (langchain.docstore.InMemoryDocstore method)\\n(langchain.docstore.Wikipedia method)\\n(langchain.vectorstores.DeepLake method)\\nsearch_kwargs (langchain.chains.ChatVectorDBChain attribute)\\n(langchain.chains.VectorDBQA attribute)\\n(langchain.chains.VectorDBQAWithSourcesChain attribute)\\nsearch_type (langchain.chains.VectorDBQA attribute)\\nsearx_host (langchain.utilities.searx_search.SearxSearchWrapper attribute)\\nSearxResults (class in langchain.utilities.searx_search)\\nseed (langchain.embeddings.LlamaCppEmbeddings attribute)\\n(langchain.llms.GPT4All attribute)\\n(langchain.llms.LlamaCpp attribute)\\nselect_examples() (langchain.prompts.example_selector.LengthBasedExampleSelector method)\\n(langchain.prompts.example_selector.MaxMarginalRelevanceExampleSelector method)\\n(langchain.prompts.example_selector.SemanticSimilarityExampleSelector method)\\nSELF_ASK_WITH_SEARCH (langchain.agents.AgentType attribute)\\nserpapi_api_key (langchain.serpapi.SerpAPIWrapper attribute)\\nsimilarity_search() (langchain.vectorstores.Annoy method)\\n(langchain.vectorstores.AtlasDB method)\\n(langchain.vectorstores.Chroma method)\\n(langchain.vectorstores.DeepLake method)\\n(langchain.vectorstores.ElasticVectorSearch method)\\n(langchain.vectorstores.FAISS method)\\n(langchain.vectorstores.Milvus method)\\n(langchain.vectorstores.OpenSearchVectorSearch method)\\n(langchain.vectorstores.Pinecone method)\\n(langchain.vectorstores.Qdrant method)\\n(langchain.vectorstores.SupabaseVectorStore method)\\n(langchain.vectorstores.VectorStore method)\\n(langchain.vectorstores.Weaviate method)\\nsimilarity_search_by_index() (langchain.vectorstores.Annoy method)\\nsimilarity_search_by_vector() (langchain.vectorstores.Annoy method)\\n(langchain.vectorstores.Chroma method)\\n(langchain.vectorstores.DeepLake method)\\n(langchain.vectorstores.FAISS method)\\n(langchain.vectorstores.SupabaseVectorStore method)\\n(langchain.vectorstores.VectorStore method)\\n(langchain.vectorstores.Weaviate method)\\nsimilarity_search_by_vector_returning_embeddings() (langchain.vectorstores.SupabaseVectorStore method)\\nsimilarity_search_by_vector_with_relevance_scores() (langchain.vectorstores.SupabaseVectorStore method)\\nsimilarity_search_with_relevance_scores() (langchain.vectorstores.SupabaseVectorStore method)\\n(langchain.vectorstores.VectorStore method)\\nsimilarity_search_with_score() (langchain.vectorstores.Annoy method)\\n(langchain.vectorstores.Chroma method)\\n(langchain.vectorstores.DeepLake method)\\n(langchain.vectorstores.FAISS method)\\n(langchain.vectorstores.Milvus method)\\n(langchain.vectorstores.Pinecone method)\\n(langchain.vectorstores.Qdrant method)\\nsimilarity_search_with_score_by_index() (langchain.vectorstores.Annoy method)\\nsimilarity_search_with_score_by_vector() (langchain.vectorstores.Annoy method)\\n(langchain.vectorstores.FAISS method)\\nSpacyTextSplitter (class in langchain.text_splitter)\\nsplit_documents() (langchain.text_splitter.TextSplitter method)\\nsplit_text() (langchain.text_splitter.CharacterTextSplitter method)\\n(langchain.text_splitter.NLTKTextSplitter method)\\n(langchain.text_splitter.RecursiveCharacterTextSplitter method)\\n(langchain.text_splitter.SpacyTextSplitter method)\\n(langchain.text_splitter.TextSplitter method)\\n(langchain.text_splitter.TokenTextSplitter method)\\nsql_chain (langchain.chains.SQLDatabaseSequentialChain attribute)\\nstop (langchain.agents.LLMSingleActionAgent attribute)\\n(langchain.chains.PALChain attribute)\\n(langchain.llms.GPT4All attribute)\\n(langchain.llms.LlamaCpp attribute)\\n(langchain.llms.Writer attribute)\\nstop_sequences (langchain.llms.AlephAlpha attribute)\\nstrategy (langchain.llms.RWKV attribute)\\nstream() (langchain.llms.Anthropic method)\\n(langchain.llms.AzureOpenAI method)\\n(langchain.llms.OpenAI method)\\n(langchain.llms.PromptLayerOpenAI method)\\nstreaming (langchain.llms.Anthropic attribute)\\n(langchain.llms.AzureOpenAI attribute)\\n(langchain.llms.GPT4All attribute)\\n(langchain.llms.OpenAIChat attribute)\\n(langchain.llms.PromptLayerOpenAIChat attribute)\\nstrip_outputs (langchain.chains.SimpleSequentialChain attribute)\\nsuffix (langchain.llms.LlamaCpp attribute)\\n(langchain.prompts.FewShotPromptTemplate attribute)\\n(langchain.prompts.FewShotPromptWithTemplates attribute)\\nSupabaseVectorStore (class in langchain.vectorstores)\\nT\\ntable_name (langchain.vectorstores.SupabaseVectorStore attribute)\\ntask (langchain.embeddings.HuggingFaceHubEmbeddings attribute)\\n(langchain.llms.HuggingFaceEndpoint attribute)\\n(langchain.llms.HuggingFaceHub attribute)\\n(langchain.llms.SelfHostedHuggingFaceLLM attribute)\\ntemp (langchain.llms.GPT4All attribute)\\ntemperature (langchain.llms.AI21 attribute)\\n(langchain.llms.AlephAlpha attribute)\\n(langchain.llms.Anthropic attribute)\\n(langchain.llms.AzureOpenAI attribute)\\n(langchain.llms.Cohere attribute)\\n(langchain.llms.ForefrontAI attribute)\\n(langchain.llms.GooseAI attribute)\\n(langchain.llms.LlamaCpp attribute)\\n(langchain.llms.NLPCloud attribute)\\n(langchain.llms.Petals attribute)\\n(langchain.llms.RWKV attribute)\\n(langchain.llms.Writer attribute)\\ntemplate (langchain.prompts.PromptTemplate attribute)\\ntemplate_format (langchain.prompts.FewShotPromptTemplate attribute)\\n(langchain.prompts.FewShotPromptWithTemplates attribute)\\n(langchain.prompts.PromptTemplate attribute)\\ntext_length (langchain.chains.LLMRequestsChain attribute)\\ntext_splitter (langchain.chains.AnalyzeDocumentChain attribute)\\n(langchain.chains.MapReduceChain attribute)\\n(langchain.chains.QAGenerationChain attribute)\\nTextSplitter (class in langchain.text_splitter)\\ntokenizer (langchain.llms.Petals attribute)\\ntokens (langchain.llms.AlephAlpha attribute)\\ntokens_path (langchain.llms.RWKV attribute)\\ntokens_to_generate (langchain.llms.Writer attribute)\\nTokenTextSplitter (class in langchain.text_splitter)\\ntool() (in module langchain.agents)\\ntool_run_logging_kwargs() (langchain.agents.Agent method)\\n(langchain.agents.BaseMultiActionAgent method)\\n(langchain.agents.BaseSingleActionAgent method)\\n(langchain.agents.LLMSingleActionAgent method)\\ntools (langchain.agents.AgentExecutor attribute)\\n(langchain.agents.MRKLChain attribute)\\n(langchain.agents.ReActChain attribute)\\n(langchain.agents.SelfAskWithSearchChain attribute)\\ntop_k (langchain.chains.SQLDatabaseChain attribute)\\n(langchain.llms.AlephAlpha attribute)\\n(langchain.llms.Anthropic attribute)\\n(langchain.llms.ForefrontAI attribute)\\n(langchain.llms.GPT4All attribute)\\n(langchain.llms.LlamaCpp attribute)\\n(langchain.llms.NLPCloud attribute)\\n(langchain.llms.Petals attribute)\\n(langchain.llms.Writer attribute)\\ntop_k_docs_for_context (langchain.chains.ChatVectorDBChain attribute)\\ntop_p (langchain.llms.AlephAlpha attribute)\\n(langchain.llms.Anthropic attribute)\\n(langchain.llms.AzureOpenAI attribute)\\n(langchain.llms.ForefrontAI attribute)\\n(langchain.llms.GooseAI attribute)\\n(langchain.llms.GPT4All attribute)\\n(langchain.llms.LlamaCpp attribute)\\n(langchain.llms.NLPCloud attribute)\\n(langchain.llms.Petals attribute)\\n(langchain.llms.RWKV attribute)\\n(langchain.llms.Writer attribute)\\ntopP (langchain.llms.AI21 attribute)\\ntransform (langchain.chains.TransformChain attribute)\\ntransform_documents() (langchain.text_splitter.TextSplitter method)\\ntruncate (langchain.embeddings.CohereEmbeddings attribute)\\n(langchain.llms.Cohere attribute)\\nU\\nunsecure (langchain.utilities.searx_search.SearxSearchWrapper attribute)\\nupdate_forward_refs() (langchain.llms.AI21 class method)\\n(langchain.llms.AlephAlpha class method)\\n(langchain.llms.Anthropic class method)\\n(langchain.llms.AzureOpenAI class method)\\n(langchain.llms.Banana class method)\\n(langchain.llms.CerebriumAI class method)\\n(langchain.llms.Cohere class method)\\n(langchain.llms.DeepInfra class method)\\n(langchain.llms.ForefrontAI class method)\\n(langchain.llms.GooseAI class method)\\n(langchain.llms.GPT4All class method)\\n(langchain.llms.HuggingFaceEndpoint class method)\\n(langchain.llms.HuggingFaceHub class method)\\n(langchain.llms.HuggingFacePipeline class method)\\n(langchain.llms.LlamaCpp class method)\\n(langchain.llms.Modal class method)\\n(langchain.llms.NLPCloud class method)\\n(langchain.llms.OpenAI class method)\\n(langchain.llms.OpenAIChat class method)\\n(langchain.llms.Petals class method)\\n(langchain.llms.PromptLayerOpenAI class method)\\n(langchain.llms.PromptLayerOpenAIChat class method)\\n(langchain.llms.Replicate class method)\\n(langchain.llms.RWKV class method)\\n(langchain.llms.SagemakerEndpoint class method)\\n(langchain.llms.SelfHostedHuggingFaceLLM class method)\\n(langchain.llms.SelfHostedPipeline class method)\\n(langchain.llms.StochasticAI class method)\\n(langchain.llms.Writer class method)\\nuse_mlock (langchain.embeddings.LlamaCppEmbeddings attribute)\\n(langchain.llms.GPT4All attribute)\\n(langchain.llms.LlamaCpp attribute)\\nuse_multiplicative_presence_penalty (langchain.llms.AlephAlpha attribute)\\nV\\nvalidate_template (langchain.prompts.FewShotPromptTemplate attribute)\\n(langchain.prompts.FewShotPromptWithTemplates attribute)\\n(langchain.prompts.PromptTemplate attribute)\\nVectorStore (class in langchain.vectorstores)\\nvectorstore (langchain.chains.ChatVectorDBChain attribute)\\n(langchain.chains.VectorDBQA attribute)\\n(langchain.chains.VectorDBQAWithSourcesChain attribute)\\n(langchain.prompts.example_selector.SemanticSimilarityExampleSelector attribute)\\nverbose (langchain.agents.MRKLChain attribute)\\n(langchain.agents.ReActChain attribute)\\n(langchain.agents.SelfAskWithSearchChain attribute)\\n(langchain.llms.AzureOpenAI attribute)\\n(langchain.llms.OpenAI attribute)\\n(langchain.llms.OpenAIChat attribute)\\nvocab_only (langchain.embeddings.LlamaCppEmbeddings attribute)\\n(langchain.llms.GPT4All attribute)\\n(langchain.llms.LlamaCpp attribute)\\nW\\nWeaviate (class in langchain.vectorstores)\\nWikipedia (class in langchain.docstore)\\nZ\\nZERO_SHOT_REACT_DESCRIPTION (langchain.agents.AgentType attribute)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\genindex.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nGlossary\\n Contents \\nChain of Thought Prompting\\nAction Plan Generation\\nReAct Prompting\\nSelf-ask\\nPrompt Chaining\\nMemetic Proxy\\nSelf Consistency\\nInception\\nMemPrompt\\nGlossary#\\nThis is a collection of terminology commonly used when developing LLM applications.\\nIt contains reference to external papers or sources where the concept was first introduced,\\nas well as to places in LangChain where the concept is used.\\nChain of Thought Prompting#\\nA prompting technique used to encourage the model to generate a series of intermediate reasoning steps.\\nA less formal way to induce this behavior is to include “Let’s think step-by-step” in the prompt.\\nResources:\\nChain-of-Thought Paper\\nStep-by-Step Paper\\nAction Plan Generation#\\nA prompt usage that uses a language model to generate actions to take.\\nThe results of these actions can then be fed back into the language model to generate a subsequent action.\\nResources:\\nWebGPT Paper\\nSayCan Paper\\nReAct Prompting#\\nA prompting technique that combines Chain-of-Thought prompting with action plan generation.\\nThis induces the to model to think about what action to take, then take it.\\nResources:\\nPaper\\nLangChain Example\\nSelf-ask#\\nA prompting method that builds on top of chain-of-thought prompting.\\nIn this method, the model explicitly asks itself follow-up questions, which are then answered by an external search engine.\\nResources:\\nPaper\\nLangChain Example\\nPrompt Chaining#\\nCombining multiple LLM calls together, with the output of one-step being the input to the next.\\nResources:\\nPromptChainer Paper\\nLanguage Model Cascades\\nICE Primer Book\\nSocratic Models\\nMemetic Proxy#\\nEncouraging the LLM to respond in a certain way framing the discussion in a context that the model knows of and that will result in that type of response. For example, as a conversation between a student and a teacher.\\nResources:\\nPaper\\nSelf Consistency#\\nA decoding strategy that samples a diverse set of reasoning paths and then selects the most consistent answer.\\nIs most effective when combined with Chain-of-thought prompting.\\nResources:\\nPaper\\nInception#\\nAlso called “First Person Instruction”.\\nEncouraging the model to think a certain way by including the start of the model’s response in the prompt.\\nResources:\\nExample\\nMemPrompt#\\nMemPrompt maintains a memory of errors and user feedback, and uses them to prevent repetition of mistakes.\\nResources:\\nPaper\\nprevious\\nZilliz\\nnext\\nLangChain Gallery\\n Contents\\n  \\nChain of Thought Prompting\\nAction Plan Generation\\nReAct Prompting\\nSelf-ask\\nPrompt Chaining\\nMemetic Proxy\\nSelf Consistency\\nInception\\nMemPrompt\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\glossary.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nWelcome to LangChain\\n Contents \\nGetting Started\\nModules\\nUse Cases\\nReference Docs\\nLangChain Ecosystem\\nAdditional Resources\\nWelcome to LangChain#\\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an API, but will also:\\nBe data-aware: connect a language model to other sources of data\\nBe agentic: allow a language model to interact with its environment\\nThe LangChain framework is designed with the above principles in mind.\\nThis is the Python specific portion of the documentation. For a purely conceptual guide to LangChain, see here. For the JavaScript documentation, see here.\\nGetting Started#\\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\\nGetting Started Documentation\\nModules#\\nThere are several main modules that LangChain provides support for.\\nFor each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides.\\nThese modules are, in increasing order of complexity:\\nModels: The various model types and model integrations LangChain supports.\\nPrompts: This includes prompt management, prompt optimization, and prompt serialization.\\nMemory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\nIndexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\\nChains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\nAgents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\nUse Cases#\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\nAutonomous Agents: Autonomous agents are long running agents that take many steps in an attempt to accomplish an objective. Examples include AutoGPT and BabyAGI.\\nAgent Simulations: Putting agents in a sandbox and observing how they interact with each other or to events can be an interesting way to observe their long-term memory abilities.\\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\\nQuestion Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\\nCode Understanding: If you want to understand how to use LLMs to query source code from github, you should read this page.\\nInteracting with APIs: Enabling LLMs to interact with APIs is extremely powerful in order to give them more up-to-date information and allow them to take actions.\\nExtraction: Extract structured information from text.\\nSummarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.\\nEvaluation: Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\\nReference Docs#\\nAll of LangChain’s reference documentation, in one place. Full documentation on all methods, classes, installation methods, and integration setups for LangChain.\\nReference Documentation\\nLangChain Ecosystem#\\nGuides for how other companies/products can be used with LangChain\\nLangChain Ecosystem\\nAdditional Resources#\\nAdditional collection of resources we think may be useful as you develop your application!\\nLangChainHub: The LangChainHub is a place to share and explore other prompts, chains, and agents.\\nGlossary: A glossary of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\nTracing: A guide on using tracing in LangChain to visualize the execution of chains and agents.\\nModel Laboratory: Experimenting with different prompts, models, and chains is a big part of developing the best possible application. The ModelLaboratory makes it easy to do so.\\nDiscord: Join us on our Discord to discuss all things LangChain!\\nYouTube: A collection of the LangChain tutorials and videos.\\nProduction Support: As you move your LangChains into production, we’d love to offer more comprehensive support. Please fill out this form and we’ll set up a dedicated support Slack channel.\\nnext\\nQuickstart Guide\\n Contents\\n  \\nGetting Started\\nModules\\nUse Cases\\nReference Docs\\nLangChain Ecosystem\\nAdditional Resources\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\index.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nModel Comparison\\nModel Comparison#\\nConstructing your language model application will likely involved choosing between many different options of prompts, models, and even chains to use. When doing so, you will want to compare these different options on different inputs in an easy, flexible, and intuitive way.\\nLangChain provides the concept of a ModelLaboratory to test out and try different models.\\nfrom langchain import LLMChain, OpenAI, Cohere, HuggingFaceHub, PromptTemplate\\nfrom langchain.model_laboratory import ModelLaboratory\\nllms = [\\n    OpenAI(temperature=0), \\n    Cohere(model=\"command-xlarge-20221108\", max_tokens=20, temperature=0), \\n    HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":1})\\n]\\nmodel_lab = ModelLaboratory.from_llms(llms)\\nmodel_lab.compare(\"What color is a flamingo?\")\\nInput:\\nWhat color is a flamingo?\\nOpenAI\\nParams: {\\'model\\': \\'text-davinci-002\\', \\'temperature\\': 0.0, \\'max_tokens\\': 256, \\'top_p\\': 1, \\'frequency_penalty\\': 0, \\'presence_penalty\\': 0, \\'n\\': 1, \\'best_of\\': 1}\\nFlamingos are pink.\\nCohere\\nParams: {\\'model\\': \\'command-xlarge-20221108\\', \\'max_tokens\\': 20, \\'temperature\\': 0.0, \\'k\\': 0, \\'p\\': 1, \\'frequency_penalty\\': 0, \\'presence_penalty\\': 0}\\nPink\\nHuggingFaceHub\\nParams: {\\'repo_id\\': \\'google/flan-t5-xl\\', \\'temperature\\': 1}\\npink\\nprompt = PromptTemplate(template=\"What is the capital of {state}?\", input_variables=[\"state\"])\\nmodel_lab_with_prompt = ModelLaboratory.from_llms(llms, prompt=prompt)\\nmodel_lab_with_prompt.compare(\"New York\")\\nInput:\\nNew York\\nOpenAI\\nParams: {\\'model\\': \\'text-davinci-002\\', \\'temperature\\': 0.0, \\'max_tokens\\': 256, \\'top_p\\': 1, \\'frequency_penalty\\': 0, \\'presence_penalty\\': 0, \\'n\\': 1, \\'best_of\\': 1}\\nThe capital of New York is Albany.\\nCohere\\nParams: {\\'model\\': \\'command-xlarge-20221108\\', \\'max_tokens\\': 20, \\'temperature\\': 0.0, \\'k\\': 0, \\'p\\': 1, \\'frequency_penalty\\': 0, \\'presence_penalty\\': 0}\\nThe capital of New York is Albany.\\nHuggingFaceHub\\nParams: {\\'repo_id\\': \\'google/flan-t5-xl\\', \\'temperature\\': 1}\\nst john s\\nfrom langchain import SelfAskWithSearchChain, SerpAPIWrapper\\nopen_ai_llm = OpenAI(temperature=0)\\nsearch = SerpAPIWrapper()\\nself_ask_with_search_openai = SelfAskWithSearchChain(llm=open_ai_llm, search_chain=search, verbose=True)\\ncohere_llm = Cohere(temperature=0, model=\"command-xlarge-20221108\")\\nsearch = SerpAPIWrapper()\\nself_ask_with_search_cohere = SelfAskWithSearchChain(llm=cohere_llm, search_chain=search, verbose=True)\\nchains = [self_ask_with_search_openai, self_ask_with_search_cohere]\\nnames = [str(open_ai_llm), str(cohere_llm)]\\nmodel_lab = ModelLaboratory(chains, names=names)\\nmodel_lab.compare(\"What is the hometown of the reigning men\\'s U.S. Open champion?\")\\nInput:\\nWhat is the hometown of the reigning men\\'s U.S. Open champion?\\nOpenAI\\nParams: {\\'model\\': \\'text-davinci-002\\', \\'temperature\\': 0.0, \\'max_tokens\\': 256, \\'top_p\\': 1, \\'frequency_penalty\\': 0, \\'presence_penalty\\': 0, \\'n\\': 1, \\'best_of\\': 1}\\n> Entering new chain...\\nWhat is the hometown of the reigning men\\'s U.S. Open champion?\\nAre follow up questions needed here: Yes.\\nFollow up: Who is the reigning men\\'s U.S. Open champion?\\nIntermediate answer: Carlos Alcaraz.\\nFollow up: Where is Carlos Alcaraz from?\\nIntermediate answer: El Palmar, Spain.\\nSo the final answer is: El Palmar, Spain\\n> Finished chain.\\nSo the final answer is: El Palmar, Spain\\nCohere\\nParams: {\\'model\\': \\'command-xlarge-20221108\\', \\'max_tokens\\': 256, \\'temperature\\': 0.0, \\'k\\': 0, \\'p\\': 1, \\'frequency_penalty\\': 0, \\'presence_penalty\\': 0}\\n> Entering new chain...\\nWhat is the hometown of the reigning men\\'s U.S. Open champion?\\nAre follow up questions needed here: Yes.\\nFollow up: Who is the reigning men\\'s U.S. Open champion?\\nIntermediate answer: Carlos Alcaraz.\\nSo the final answer is:\\nCarlos Alcaraz\\n> Finished chain.\\nSo the final answer is:\\nCarlos Alcaraz\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\model_laboratory.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nAPI References\\nAPI References#\\nAll of LangChain’s reference documentation, in one place.\\nFull documentation on all methods, classes, and APIs in LangChain.\\nPrompts\\nLLMs\\nUtilities\\nChains\\nAgents\\nprevious\\nIntegrations\\nnext\\nUtilities\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\reference.html'}),\n",
              " Document(page_content='Search\\nError\\nPlease activate JavaScript to enable the search functionality.\\nCtrl+K\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\search.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nTracing\\n Contents \\nTracing Walkthrough\\nChanging Sessions\\nTracing#\\nBy enabling tracing in your LangChain runs, you’ll be able to more effectively visualize, step through, and debug your chains and agents.\\nFirst, you should install tracing and set up your environment properly.\\nYou can use either a locally hosted version of this (uses Docker) or a cloud hosted version (in closed alpha).\\nIf you’re interested in using the hosted platform, please fill out the form here.\\nLocally Hosted Setup\\nCloud Hosted Setup\\nTracing Walkthrough#\\nWhen you first access the UI, you should see a page with your tracing sessions.\\nAn initial one “default” should already be created for you.\\nA session is just a way to group traces together.\\nIf you click on a session, it will take you to a page with no recorded traces that says “No Runs.”\\nYou can create a new session with the new session form.\\nIf we click on the default session, we can see that to start we have no traces stored.\\nIf we now start running chains and agents with tracing enabled, we will see data show up here.\\nTo do so, we can run this notebook as an example.\\nAfter running it, we will see an initial trace show up.\\nFrom here we can explore the trace at a high level by clicking on the arrow to show nested runs.\\nWe can keep on clicking further and further down to explore deeper and deeper.\\nWe can also click on the “Explore” button of the top level run to dive even deeper.\\nHere, we can see the inputs and outputs in full, as well as all the nested traces.\\nWe can keep on exploring each of these nested traces in more detail.\\nFor example, here is the lowest level trace with the exact inputs/outputs to the LLM.\\nChanging Sessions#\\nTo initially record traces to a session other than \"default\", you can set the LANGCHAIN_SESSION environment variable to the name of the session you want to record to:\\nimport os\\nos.environ[\"LANGCHAIN_HANDLER\"] = \"langchain\"\\nos.environ[\"LANGCHAIN_SESSION\"] = \"my_session\" # Make sure this session actually exists. You can create a new session in the UI.\\nTo switch sessions mid-script or mid-notebook, do NOT set the LANGCHAIN_SESSION environment variable. Instead: langchain.set_tracing_callback_manager(session_name=\"my_session\")\\nprevious\\nDeployments\\nnext\\nYouTube\\n Contents\\n  \\nTracing Walkthrough\\nChanging Sessions\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\tracing.html'}),\n",
              " Document(page_content=\".md\\n.pdf\\nYouTube\\n Contents \\nIntroduction to LangChain with Harrison Chase creator of LangChain\\nTutorials\\nVideos (sorted by views)\\nYouTube#\\nThis is a collection of LangChain tutorials and videos.\\nIntroduction to LangChain with Harrison Chase creator of LangChain#\\nLangChain Demo + Q&A with Harrison Chase by Full Stack Deep Learning\\nTutorials#\\nLangChain Crash Course - Build apps with language models by Patrick Loeber\\nLangChain for Gen AI and LLMs by James Briggs:\\n#1 Getting Started with GPT-3 vs. Open Source LLMs\\n#2 Prompt Templates for GPT 3.5 and other LLMs\\n#3 LLM Chains using GPT 3.5 and other LLMs\\n#4 Chatbot Memory for Chat-GPT, Davinci + other LLMs\\n#5 Chat with OpenAI in LangChain\\n#6 LangChain Agents Deep Dive with GPT 3.5\\nPrompt Engineering with OpenAI’s GPT-3 and other LLMs\\nLangChain 101 by Data Independent:\\nWhat Is LangChain? - LangChain + ChatGPT Overview\\nQuickstart Guide\\nBeginner Guide To 7 Essential Concepts\\nOpenAI + Wolfram Alpha\\nAsk Questions On Your Custom (or Private) Files\\nConnect Google Drive Files To OpenAI\\nYouTube Transcripts + OpenAI\\nQuestion A 300 Page Book (w/ OpenAI + Pinecone)\\nWorkaround OpenAI's Token Limit With Chain Types\\nBuild Your Own OpenAI + LangChain Web App in 23 Minutes\\nWorking With The New ChatGPT API\\nOpenAI + LangChain Wrote Me 100 Custom Sales Emails\\nStructured Output From OpenAI (Clean Dirty Data)\\nConnect OpenAI To +5,000 Tools (LangChain + Zapier)\\nUse LLMs To Extract Data From Text (Expert Mode)\\nLangChain How to and guides by Sam Witteveen:\\nLangChain Basics - LLMs & PromptTemplates with Colab\\nLangChain Basics - Tools and Chains\\nChatGPT API Announcement & Code Walkthrough with LangChain\\nConversations with Memory (explanation & code walkthrough)\\nChat with Flan20B\\nUsing Hugging Face Models locally (code walkthrough)\\nPAL : Program-aided Language Models with LangChain code\\nBuilding a Summarization System with LangChain and GPT-3 - Part 1\\nBuilding a Summarization System with LangChain and GPT-3 - Part 2\\nMicrosoft’s Visual ChatGPT using LangChain\\nLangChain Agents - Joining Tools and Chains with Decisions\\nComparing LLMs with LangChain\\nUsing Constitutional AI in LangChain\\nTalking to Alpaca with LangChain - Creating an Alpaca Chatbot\\nTalk to your CSV & Excel with LangChain\\nBabyAGI: Discover the Power of Task-Driven Autonomous Agents!\\nImprove your BabyAGI with LangChain\\nLangChain by Prompt Engineering:\\nLangChain Crash Course - All You Need to Know to Build Powerful Apps with LLMs\\nWorking with MULTIPLE PDF Files in LangChain: ChatGPT for your Data\\nChatGPT for YOUR OWN PDF files with LangChain\\nTalk to YOUR DATA without OpenAI APIs: LangChain\\nLangChain by Chat with data\\nLangChain Beginner’s Tutorial for Typescript/Javascript\\nGPT-4 Tutorial: How to Chat With Multiple PDF Files (~1000 pages of Tesla’s 10-K Annual Reports)\\nGPT-4 & LangChain Tutorial: How to Chat With A 56-Page PDF Document (w/Pinecone)\\nVideos (sorted by views)#\\nBuilding AI LLM Apps with LangChain (and more?) - LIVE STREAM by Nicholas Renotte\\nFirst look - ChatGPT + WolframAlpha (GPT-3.5 and Wolfram|Alpha via LangChain by James Weaver) by Dr Alan D. Thompson\\nLangChain explained - The hottest new Python framework by AssemblyAI\\nChatbot with INFINITE MEMORY using OpenAI & Pinecone - GPT-3, Embeddings, ADA, Vector DB, Semantic by David Shapiro ~ AI\\nLangChain for LLMs is… basically just an Ansible playbook by David Shapiro ~ AI\\nBuild your own LLM Apps with LangChain & GPT-Index by 1littlecoder\\nBabyAGI - New System of Autonomous AI Agents with LangChain by 1littlecoder\\nRun BabyAGI with Langchain Agents (with Python Code) by 1littlecoder\\nHow to Use Langchain With Zapier | Write and Send Email with GPT-3 | OpenAI API Tutorial by StarMorph AI\\nUse Your Locally Stored Files To Get Response From GPT - OpenAI | Langchain | Python by Shweta Lodha\\nLangchain JS | How to Use GPT-3, GPT-4 to Reference your own Data | OpenAI Embeddings Intro by StarMorph AI\\nThe easiest way to work with large language models | Learn LangChain in 10min by Sophia Yang\\n4 Autonomous AI Agents: “Westworld” simulation BabyAGI, AutoGPT, Camel, LangChain by Sophia Yang\\nAI CAN SEARCH THE INTERNET? Langchain Agents + OpenAI ChatGPT by tylerwhatsgood\\nBuilding the Future with LLMs, LangChain, & Pinecone by Pinecone\\nWeaviate + LangChain for LLM apps presented by Erika Cardenas by Weaviate • Vector Database\\nAnalyze Custom CSV Data with GPT-4 using Langchain by Venelin Valkov\\nLangchain Overview - How to Use Langchain & ChatGPT by Python In Office\\nCustom langchain Agent & Tools with memory. Turn any Python function into langchain tool with Gpt 3 by echohive\\nChatGPT with any YouTube video using langchain and chromadb by echohive\\nHow to Talk to a PDF using LangChain and ChatGPT by Automata Learning Lab\\nLangchain Document Loaders Part 1: Unstructured Files by Merk\\nLangChain - Prompt Templates (what all the best prompt engineers use) by Nick Daigler\\nLangChain. Crear aplicaciones Python impulsadas por GPT by Jesús Conde\\nEasiest Way to Use GPT In Your Products | LangChain Basics Tutorial by Rachel Woods\\nBabyAGI + GPT-4 Langchain Agent with Internet Access by tylerwhatsgood\\nLearning LLM Agents. How does it actually work? LangChain, AutoGPT & OpenAI by Arnoldas Kemeklis\\nGet Started with LangChain in Node.js by Developers Digest\\nLangChain + OpenAI tutorial: Building a Q&A system w/ own text data by Samuel Chan\\nLangchain + Zapier Agent by Merk\\nConnecting the Internet with ChatGPT (LLMs) using Langchain And Answers Your Questions by Kamalraj M M\\nBuild More Powerful LLM Applications for Business’s with LangChain (Beginners Guide) by No Code Blackbox\\nprevious\\nTracing\\n Contents\\n  \\nIntroduction to LangChain with Harrison Chase creator of LangChain\\nTutorials\\nVideos (sorted by views)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  \", metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\youtube.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nAI21 Labs\\n Contents \\nInstallation and Setup\\nWrappers\\nLLM\\nAI21 Labs#\\nThis page covers how to use the AI21 ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific AI21 wrappers.\\nInstallation and Setup#\\nGet an AI21 api key and set it as an environment variable (AI21_API_KEY)\\nWrappers#\\nLLM#\\nThere exists an AI21 LLM wrapper, which you can access with\\nfrom langchain.llms import AI21\\nprevious\\nLangChain Ecosystem\\nnext\\nAim\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nLLM\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\ai21.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nAim\\nAim#\\nAim makes it super easy to visualize and debug LangChain executions. Aim tracks inputs and outputs of LLMs and tools, as well as actions of agents.\\nWith Aim, you can easily debug and examine an individual execution:\\nAdditionally, you have the option to compare multiple executions side by side:\\nAim is fully open source, learn more about Aim on GitHub.\\nLet’s move forward and see how to enable and configure Aim callback.\\nTracking LangChain Executions with AimIn this notebook we will explore three usage scenarios. To start off, we will install the necessary packages and import certain modules. Subsequently, we will configure two environment variables that can be established either within the Python script or through the terminal.\\n!pip install aim\\n!pip install langchain\\n!pip install openai\\n!pip install google-search-results\\nimport os\\nfrom datetime import datetime\\nfrom langchain.llms import OpenAI\\nfrom langchain.callbacks.base import CallbackManager\\nfrom langchain.callbacks import AimCallbackHandler, StdOutCallbackHandler\\nOur examples use a GPT model as the LLM, and OpenAI offers an API for this purpose. You can obtain the key from the following link: https://platform.openai.com/account/api-keys .\\nWe will use the SerpApi to retrieve search results from Google. To acquire the SerpApi key, please go to https://serpapi.com/manage-api-key .\\nos.environ[\"OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"SERPAPI_API_KEY\"] = \"...\"\\nThe event methods of AimCallbackHandler accept the LangChain module or agent as input and log at least the prompts and generated results, as well as the serialized version of the LangChain module, to the designated Aim run.\\nsession_group = datetime.now().strftime(\"%m.%d.%Y_%H.%M.%S\")\\naim_callback = AimCallbackHandler(\\n    repo=\".\",\\n    experiment_name=\"scenario 1: OpenAI LLM\",\\n)\\nmanager = CallbackManager([StdOutCallbackHandler(), aim_callback])\\nllm = OpenAI(temperature=0, callback_manager=manager, verbose=True)\\nThe flush_tracker function is used to record LangChain assets on Aim. By default, the session is reset rather than being terminated outright.\\nScenario 1 In the first scenario, we will use OpenAI LLM.\\n# scenario 1 - LLM\\nllm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"] * 3)\\naim_callback.flush_tracker(\\n    langchain_asset=llm,\\n    experiment_name=\"scenario 2: Chain with multiple SubChains on multiple generations\",\\n)\\nScenario 2 Scenario two involves chaining with multiple SubChains across multiple generations.\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\n# scenario 2 - Chain\\ntemplate = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\\nTitle: {title}\\nPlaywright: This is a synopsis for the above play:\"\"\"\\nprompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\\nsynopsis_chain = LLMChain(llm=llm, prompt=prompt_template, callback_manager=manager)\\ntest_prompts = [\\n    {\"title\": \"documentary about good video games that push the boundary of game design\"},\\n    {\"title\": \"the phenomenon behind the remarkable speed of cheetahs\"},\\n    {\"title\": \"the best in class mlops tooling\"},\\n]\\nsynopsis_chain.apply(test_prompts)\\naim_callback.flush_tracker(\\n    langchain_asset=synopsis_chain, experiment_name=\"scenario 3: Agent with Tools\"\\n)\\nScenario 3 The third scenario involves an agent with tools.\\nfrom langchain.agents import initialize_agent, load_tools\\nfrom langchain.agents import AgentType\\n# scenario 3 - Agent with Tools\\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, callback_manager=manager)\\nagent = initialize_agent(\\n    tools,\\n    llm,\\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\\n    callback_manager=manager,\\n    verbose=True,\\n)\\nagent.run(\\n    \"Who is Leo DiCaprio\\'s girlfriend? What is her current age raised to the 0.43 power?\"\\n)\\naim_callback.flush_tracker(langchain_asset=agent, reset=False, finish=True)\\n> Entering new AgentExecutor chain...\\n I need to find out who Leo DiCaprio\\'s girlfriend is and then calculate her age raised to the 0.43 power.\\nAction: Search\\nAction Input: \"Leo DiCaprio girlfriend\"\\nObservation: Leonardo DiCaprio seemed to prove a long-held theory about his love life right after splitting from girlfriend Camila Morrone just months ...\\nThought: I need to find out Camila Morrone\\'s age\\nAction: Search\\nAction Input: \"Camila Morrone age\"\\nObservation: 25 years\\nThought: I need to calculate 25 raised to the 0.43 power\\nAction: Calculator\\nAction Input: 25^0.43\\nObservation: Answer: 3.991298452658078\\nThought: I now know the final answer\\nFinal Answer: Camila Morrone is Leo DiCaprio\\'s girlfriend and her current age raised to the 0.43 power is 3.991298452658078.\\n> Finished chain.\\nprevious\\nAI21 Labs\\nnext\\nApify\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\aim_tracking.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nApify\\n Contents \\nOverview\\nInstallation and Setup\\nWrappers\\nUtility\\nLoader\\nApify#\\nThis page covers how to use Apify within LangChain.\\nOverview#\\nApify is a cloud platform for web scraping and data extraction,\\nwhich provides an ecosystem of more than a thousand\\nready-made apps called Actors for various scraping, crawling, and extraction use cases.\\nThis integration enables you run Actors on the Apify platform and load their results into LangChain to feed your vector\\nindexes with documents and data from the web, e.g. to generate answers from websites with documentation,\\nblogs, or knowledge bases.\\nInstallation and Setup#\\nInstall the Apify API client for Python with pip install apify-client\\nGet your Apify API token and either set it as\\nan environment variable (APIFY_API_TOKEN) or pass it to the ApifyWrapper as apify_api_token in the constructor.\\nWrappers#\\nUtility#\\nYou can use the ApifyWrapper to run Actors on the Apify platform.\\nfrom langchain.utilities import ApifyWrapper\\nFor a more detailed walkthrough of this wrapper, see this notebook.\\nLoader#\\nYou can also use our ApifyDatasetLoader to get data from Apify dataset.\\nfrom langchain.document_loaders import ApifyDatasetLoader\\nFor a more detailed walkthrough of this loader, see this notebook.\\nprevious\\nAim\\nnext\\nAtlasDB\\n Contents\\n  \\nOverview\\nInstallation and Setup\\nWrappers\\nUtility\\nLoader\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\apify.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nAtlasDB\\n Contents \\nInstallation and Setup\\nWrappers\\nVectorStore\\nAtlasDB#\\nThis page covers how to use Nomic’s Atlas ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Atlas wrappers.\\nInstallation and Setup#\\nInstall the Python package with pip install nomic\\nNomic is also included in langchains poetry extras poetry install -E all\\nWrappers#\\nVectorStore#\\nThere exists a wrapper around the Atlas neural database, allowing you to use it as a vectorstore.\\nThis vectorstore also gives you full access to the underlying AtlasProject object, which will allow you to use the full range of Atlas map interactions, such as bulk tagging and automatic topic modeling.\\nPlease see the Atlas docs for more detailed information.\\nTo import this vectorstore:\\nfrom langchain.vectorstores import AtlasDB\\nFor a more detailed walkthrough of the AtlasDB wrapper, see this notebook\\nprevious\\nApify\\nnext\\nBanana\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nVectorStore\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\atlas.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nBanana\\n Contents \\nInstallation and Setup\\nDefine your Banana Template\\nBuild the Banana app\\nWrappers\\nLLM\\nBanana#\\nThis page covers how to use the Banana ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Banana wrappers.\\nInstallation and Setup#\\nInstall with pip install banana-dev\\nGet an Banana api key and set it as an environment variable (BANANA_API_KEY)\\nDefine your Banana Template#\\nIf you want to use an available language model template you can find one here.\\nThis template uses the Palmyra-Base model by Writer.\\nYou can check out an example Banana repository here.\\nBuild the Banana app#\\nBanana Apps must include the “output” key in the return json.\\nThere is a rigid response structure.\\n# Return the results as a dictionary\\nresult = {\\'output\\': result}\\nAn example inference function would be:\\ndef inference(model_inputs:dict) -> dict:\\n    global model\\n    global tokenizer\\n    # Parse out your arguments\\n    prompt = model_inputs.get(\\'prompt\\', None)\\n    if prompt == None:\\n        return {\\'message\\': \"No prompt provided\"}\\n    # Run the model\\n    input_ids = tokenizer.encode(prompt, return_tensors=\\'pt\\').cuda()\\n    output = model.generate(\\n        input_ids,\\n        max_length=100,\\n        do_sample=True,\\n        top_k=50,\\n        top_p=0.95,\\n        num_return_sequences=1,\\n        temperature=0.9,\\n        early_stopping=True,\\n        no_repeat_ngram_size=3,\\n        num_beams=5,\\n        length_penalty=1.5,\\n        repetition_penalty=1.5,\\n        bad_words_ids=[[tokenizer.encode(\\' \\', add_prefix_space=True)[0]]]\\n        )\\n    result = tokenizer.decode(output[0], skip_special_tokens=True)\\n    # Return the results as a dictionary\\n    result = {\\'output\\': result}\\n    return result\\nYou can find a full example of a Banana app here.\\nWrappers#\\nLLM#\\nThere exists an Banana LLM wrapper, which you can access with\\nfrom langchain.llms import Banana\\nYou need to provide a model key located in the dashboard:\\nllm = Banana(model_key=\"YOUR_MODEL_KEY\")\\nprevious\\nAtlasDB\\nnext\\nCerebriumAI\\n Contents\\n  \\nInstallation and Setup\\nDefine your Banana Template\\nBuild the Banana app\\nWrappers\\nLLM\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\bananadev.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nCerebriumAI\\n Contents \\nInstallation and Setup\\nWrappers\\nLLM\\nCerebriumAI#\\nThis page covers how to use the CerebriumAI ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific CerebriumAI wrappers.\\nInstallation and Setup#\\nInstall with pip install cerebrium\\nGet an CerebriumAI api key and set it as an environment variable (CEREBRIUMAI_API_KEY)\\nWrappers#\\nLLM#\\nThere exists an CerebriumAI LLM wrapper, which you can access with\\nfrom langchain.llms import CerebriumAI\\nprevious\\nBanana\\nnext\\nChroma\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nLLM\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\cerebriumai.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nChroma\\n Contents \\nInstallation and Setup\\nWrappers\\nVectorStore\\nChroma#\\nThis page covers how to use the Chroma ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Chroma wrappers.\\nInstallation and Setup#\\nInstall the Python package with pip install chromadb\\nWrappers#\\nVectorStore#\\nThere exists a wrapper around Chroma vector databases, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\nTo import this vectorstore:\\nfrom langchain.vectorstores import Chroma\\nFor a more detailed walkthrough of the Chroma wrapper, see this notebook\\nprevious\\nCerebriumAI\\nnext\\nClearML Integration\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nVectorStore\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\chroma.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nClearML Integration\\n Contents \\nGetting API Credentials\\nSetting Up\\nScenario 1: Just an LLM\\nScenario 2: Creating an agent with tools\\nTips and Next Steps\\nClearML Integration#\\nIn order to properly keep track of your langchain experiments and their results, you can enable the ClearML integration. ClearML is an experiment manager that neatly tracks and organizes all your experiment runs.\\nGetting API Credentials#\\nWe’ll be using quite some APIs in this notebook, here is a list and where to get them:\\nClearML: https://app.clear.ml/settings/workspace-configuration\\nOpenAI: https://platform.openai.com/account/api-keys\\nSerpAPI (google search): https://serpapi.com/dashboard\\nimport os\\nos.environ[\"CLEARML_API_ACCESS_KEY\"] = \"\"\\nos.environ[\"CLEARML_API_SECRET_KEY\"] = \"\"\\nos.environ[\"OPENAI_API_KEY\"] = \"\"\\nos.environ[\"SERPAPI_API_KEY\"] = \"\"\\nSetting Up#\\n!pip install clearml\\n!pip install pandas\\n!pip install textstat\\n!pip install spacy\\n!python -m spacy download en_core_web_sm\\nfrom datetime import datetime\\nfrom langchain.callbacks import ClearMLCallbackHandler, StdOutCallbackHandler\\nfrom langchain.callbacks.base import CallbackManager\\nfrom langchain.llms import OpenAI\\n# Setup and use the ClearML Callback\\nclearml_callback = ClearMLCallbackHandler(\\n    task_type=\"inference\",\\n    project_name=\"langchain_callback_demo\",\\n    task_name=\"llm\",\\n    tags=[\"test\"],\\n    # Change the following parameters based on the amount of detail you want tracked\\n    visualize=True,\\n    complexity_metrics=True,\\n    stream_logs=True\\n)\\nmanager = CallbackManager([StdOutCallbackHandler(), clearml_callback])\\n# Get the OpenAI model ready to go\\nllm = OpenAI(temperature=0, callback_manager=manager, verbose=True)\\nThe clearml callback is currently in beta and is subject to change based on updates to `langchain`. Please report any issues to https://github.com/allegroai/clearml/issues with the tag `langchain`.\\nScenario 1: Just an LLM#\\nFirst, let’s just run a single LLM a few times and capture the resulting prompt-answer conversation in ClearML\\n# SCENARIO 1 - LLM\\nllm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"] * 3)\\n# After every generation run, use flush to make sure all the metrics\\n# prompts and other output are properly saved separately\\nclearml_callback.flush_tracker(langchain_asset=llm, name=\"simple_sequential\")\\n{\\'action\\': \\'on_llm_start\\', \\'name\\': \\'OpenAI\\', \\'step\\': 3, \\'starts\\': 2, \\'ends\\': 1, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 0, \\'chain_ends\\': 0, \\'llm_starts\\': 2, \\'llm_ends\\': 1, \\'llm_streams\\': 0, \\'tool_starts\\': 0, \\'tool_ends\\': 0, \\'agent_ends\\': 0, \\'prompts\\': \\'Tell me a joke\\'}\\n{\\'action\\': \\'on_llm_start\\', \\'name\\': \\'OpenAI\\', \\'step\\': 3, \\'starts\\': 2, \\'ends\\': 1, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 0, \\'chain_ends\\': 0, \\'llm_starts\\': 2, \\'llm_ends\\': 1, \\'llm_streams\\': 0, \\'tool_starts\\': 0, \\'tool_ends\\': 0, \\'agent_ends\\': 0, \\'prompts\\': \\'Tell me a poem\\'}\\n{\\'action\\': \\'on_llm_start\\', \\'name\\': \\'OpenAI\\', \\'step\\': 3, \\'starts\\': 2, \\'ends\\': 1, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 0, \\'chain_ends\\': 0, \\'llm_starts\\': 2, \\'llm_ends\\': 1, \\'llm_streams\\': 0, \\'tool_starts\\': 0, \\'tool_ends\\': 0, \\'agent_ends\\': 0, \\'prompts\\': \\'Tell me a joke\\'}\\n{\\'action\\': \\'on_llm_start\\', \\'name\\': \\'OpenAI\\', \\'step\\': 3, \\'starts\\': 2, \\'ends\\': 1, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 0, \\'chain_ends\\': 0, \\'llm_starts\\': 2, \\'llm_ends\\': 1, \\'llm_streams\\': 0, \\'tool_starts\\': 0, \\'tool_ends\\': 0, \\'agent_ends\\': 0, \\'prompts\\': \\'Tell me a poem\\'}\\n{\\'action\\': \\'on_llm_start\\', \\'name\\': \\'OpenAI\\', \\'step\\': 3, \\'starts\\': 2, \\'ends\\': 1, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 0, \\'chain_ends\\': 0, \\'llm_starts\\': 2, \\'llm_ends\\': 1, \\'llm_streams\\': 0, \\'tool_starts\\': 0, \\'tool_ends\\': 0, \\'agent_ends\\': 0, \\'prompts\\': \\'Tell me a joke\\'}\\n{\\'action\\': \\'on_llm_start\\', \\'name\\': \\'OpenAI\\', \\'step\\': 3, \\'starts\\': 2, \\'ends\\': 1, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 0, \\'chain_ends\\': 0, \\'llm_starts\\': 2, \\'llm_ends\\': 1, \\'llm_streams\\': 0, \\'tool_starts\\': 0, \\'tool_ends\\': 0, \\'agent_ends\\': 0, \\'prompts\\': \\'Tell me a poem\\'}\\n{\\'action\\': \\'on_llm_end\\', \\'token_usage_prompt_tokens\\': 24, \\'token_usage_completion_tokens\\': 138, \\'token_usage_total_tokens\\': 162, \\'model_name\\': \\'text-davinci-003\\', \\'step\\': 4, \\'starts\\': 2, \\'ends\\': 2, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 0, \\'chain_ends\\': 0, \\'llm_starts\\': 2, \\'llm_ends\\': 2, \\'llm_streams\\': 0, \\'tool_starts\\': 0, \\'tool_ends\\': 0, \\'agent_ends\\': 0, \\'text\\': \\'\\\\n\\\\nQ: What did the fish say when it hit the wall?\\\\nA: Dam!\\', \\'generation_info_finish_reason\\': \\'stop\\', \\'generation_info_logprobs\\': None, \\'flesch_reading_ease\\': 109.04, \\'flesch_kincaid_grade\\': 1.3, \\'smog_index\\': 0.0, \\'coleman_liau_index\\': -1.24, \\'automated_readability_index\\': 0.3, \\'dale_chall_readability_score\\': 5.5, \\'difficult_words\\': 0, \\'linsear_write_formula\\': 5.5, \\'gunning_fog\\': 5.2, \\'text_standard\\': \\'5th and 6th grade\\', \\'fernandez_huerta\\': 133.58, \\'szigriszt_pazos\\': 131.54, \\'gutierrez_polini\\': 62.3, \\'crawford\\': -0.2, \\'gulpease_index\\': 79.8, \\'osman\\': 116.91}\\n{\\'action\\': \\'on_llm_end\\', \\'token_usage_prompt_tokens\\': 24, \\'token_usage_completion_tokens\\': 138, \\'token_usage_total_tokens\\': 162, \\'model_name\\': \\'text-davinci-003\\', \\'step\\': 4, \\'starts\\': 2, \\'ends\\': 2, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 0, \\'chain_ends\\': 0, \\'llm_starts\\': 2, \\'llm_ends\\': 2, \\'llm_streams\\': 0, \\'tool_starts\\': 0, \\'tool_ends\\': 0, \\'agent_ends\\': 0, \\'text\\': \\'\\\\n\\\\nRoses are red,\\\\nViolets are blue,\\\\nSugar is sweet,\\\\nAnd so are you.\\', \\'generation_info_finish_reason\\': \\'stop\\', \\'generation_info_logprobs\\': None, \\'flesch_reading_ease\\': 83.66, \\'flesch_kincaid_grade\\': 4.8, \\'smog_index\\': 0.0, \\'coleman_liau_index\\': 3.23, \\'automated_readability_index\\': 3.9, \\'dale_chall_readability_score\\': 6.71, \\'difficult_words\\': 2, \\'linsear_write_formula\\': 6.5, \\'gunning_fog\\': 8.28, \\'text_standard\\': \\'6th and 7th grade\\', \\'fernandez_huerta\\': 115.58, \\'szigriszt_pazos\\': 112.37, \\'gutierrez_polini\\': 54.83, \\'crawford\\': 1.4, \\'gulpease_index\\': 72.1, \\'osman\\': 100.17}\\n{\\'action\\': \\'on_llm_end\\', \\'token_usage_prompt_tokens\\': 24, \\'token_usage_completion_tokens\\': 138, \\'token_usage_total_tokens\\': 162, \\'model_name\\': \\'text-davinci-003\\', \\'step\\': 4, \\'starts\\': 2, \\'ends\\': 2, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 0, \\'chain_ends\\': 0, \\'llm_starts\\': 2, \\'llm_ends\\': 2, \\'llm_streams\\': 0, \\'tool_starts\\': 0, \\'tool_ends\\': 0, \\'agent_ends\\': 0, \\'text\\': \\'\\\\n\\\\nQ: What did the fish say when it hit the wall?\\\\nA: Dam!\\', \\'generation_info_finish_reason\\': \\'stop\\', \\'generation_info_logprobs\\': None, \\'flesch_reading_ease\\': 109.04, \\'flesch_kincaid_grade\\': 1.3, \\'smog_index\\': 0.0, \\'coleman_liau_index\\': -1.24, \\'automated_readability_index\\': 0.3, \\'dale_chall_readability_score\\': 5.5, \\'difficult_words\\': 0, \\'linsear_write_formula\\': 5.5, \\'gunning_fog\\': 5.2, \\'text_standard\\': \\'5th and 6th grade\\', \\'fernandez_huerta\\': 133.58, \\'szigriszt_pazos\\': 131.54, \\'gutierrez_polini\\': 62.3, \\'crawford\\': -0.2, \\'gulpease_index\\': 79.8, \\'osman\\': 116.91}\\n{\\'action\\': \\'on_llm_end\\', \\'token_usage_prompt_tokens\\': 24, \\'token_usage_completion_tokens\\': 138, \\'token_usage_total_tokens\\': 162, \\'model_name\\': \\'text-davinci-003\\', \\'step\\': 4, \\'starts\\': 2, \\'ends\\': 2, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 0, \\'chain_ends\\': 0, \\'llm_starts\\': 2, \\'llm_ends\\': 2, \\'llm_streams\\': 0, \\'tool_starts\\': 0, \\'tool_ends\\': 0, \\'agent_ends\\': 0, \\'text\\': \\'\\\\n\\\\nRoses are red,\\\\nViolets are blue,\\\\nSugar is sweet,\\\\nAnd so are you.\\', \\'generation_info_finish_reason\\': \\'stop\\', \\'generation_info_logprobs\\': None, \\'flesch_reading_ease\\': 83.66, \\'flesch_kincaid_grade\\': 4.8, \\'smog_index\\': 0.0, \\'coleman_liau_index\\': 3.23, \\'automated_readability_index\\': 3.9, \\'dale_chall_readability_score\\': 6.71, \\'difficult_words\\': 2, \\'linsear_write_formula\\': 6.5, \\'gunning_fog\\': 8.28, \\'text_standard\\': \\'6th and 7th grade\\', \\'fernandez_huerta\\': 115.58, \\'szigriszt_pazos\\': 112.37, \\'gutierrez_polini\\': 54.83, \\'crawford\\': 1.4, \\'gulpease_index\\': 72.1, \\'osman\\': 100.17}\\n{\\'action\\': \\'on_llm_end\\', \\'token_usage_prompt_tokens\\': 24, \\'token_usage_completion_tokens\\': 138, \\'token_usage_total_tokens\\': 162, \\'model_name\\': \\'text-davinci-003\\', \\'step\\': 4, \\'starts\\': 2, \\'ends\\': 2, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 0, \\'chain_ends\\': 0, \\'llm_starts\\': 2, \\'llm_ends\\': 2, \\'llm_streams\\': 0, \\'tool_starts\\': 0, \\'tool_ends\\': 0, \\'agent_ends\\': 0, \\'text\\': \\'\\\\n\\\\nQ: What did the fish say when it hit the wall?\\\\nA: Dam!\\', \\'generation_info_finish_reason\\': \\'stop\\', \\'generation_info_logprobs\\': None, \\'flesch_reading_ease\\': 109.04, \\'flesch_kincaid_grade\\': 1.3, \\'smog_index\\': 0.0, \\'coleman_liau_index\\': -1.24, \\'automated_readability_index\\': 0.3, \\'dale_chall_readability_score\\': 5.5, \\'difficult_words\\': 0, \\'linsear_write_formula\\': 5.5, \\'gunning_fog\\': 5.2, \\'text_standard\\': \\'5th and 6th grade\\', \\'fernandez_huerta\\': 133.58, \\'szigriszt_pazos\\': 131.54, \\'gutierrez_polini\\': 62.3, \\'crawford\\': -0.2, \\'gulpease_index\\': 79.8, \\'osman\\': 116.91}\\n{\\'action\\': \\'on_llm_end\\', \\'token_usage_prompt_tokens\\': 24, \\'token_usage_completion_tokens\\': 138, \\'token_usage_total_tokens\\': 162, \\'model_name\\': \\'text-davinci-003\\', \\'step\\': 4, \\'starts\\': 2, \\'ends\\': 2, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 0, \\'chain_ends\\': 0, \\'llm_starts\\': 2, \\'llm_ends\\': 2, \\'llm_streams\\': 0, \\'tool_starts\\': 0, \\'tool_ends\\': 0, \\'agent_ends\\': 0, \\'text\\': \\'\\\\n\\\\nRoses are red,\\\\nViolets are blue,\\\\nSugar is sweet,\\\\nAnd so are you.\\', \\'generation_info_finish_reason\\': \\'stop\\', \\'generation_info_logprobs\\': None, \\'flesch_reading_ease\\': 83.66, \\'flesch_kincaid_grade\\': 4.8, \\'smog_index\\': 0.0, \\'coleman_liau_index\\': 3.23, \\'automated_readability_index\\': 3.9, \\'dale_chall_readability_score\\': 6.71, \\'difficult_words\\': 2, \\'linsear_write_formula\\': 6.5, \\'gunning_fog\\': 8.28, \\'text_standard\\': \\'6th and 7th grade\\', \\'fernandez_huerta\\': 115.58, \\'szigriszt_pazos\\': 112.37, \\'gutierrez_polini\\': 54.83, \\'crawford\\': 1.4, \\'gulpease_index\\': 72.1, \\'osman\\': 100.17}\\n{\\'action_records\\':           action    name  step  starts  ends  errors  text_ctr  chain_starts  \\\\\\n0   on_llm_start  OpenAI     1       1     0       0         0             0   \\n1   on_llm_start  OpenAI     1       1     0       0         0             0   \\n2   on_llm_start  OpenAI     1       1     0       0         0             0   \\n3   on_llm_start  OpenAI     1       1     0       0         0             0   \\n4   on_llm_start  OpenAI     1       1     0       0         0             0   \\n5   on_llm_start  OpenAI     1       1     0       0         0             0   \\n6     on_llm_end     NaN     2       1     1       0         0             0   \\n7     on_llm_end     NaN     2       1     1       0         0             0   \\n8     on_llm_end     NaN     2       1     1       0         0             0   \\n9     on_llm_end     NaN     2       1     1       0         0             0   \\n10    on_llm_end     NaN     2       1     1       0         0             0   \\n11    on_llm_end     NaN     2       1     1       0         0             0   \\n12  on_llm_start  OpenAI     3       2     1       0         0             0   \\n13  on_llm_start  OpenAI     3       2     1       0         0             0   \\n14  on_llm_start  OpenAI     3       2     1       0         0             0   \\n15  on_llm_start  OpenAI     3       2     1       0         0             0   \\n16  on_llm_start  OpenAI     3       2     1       0         0             0   \\n17  on_llm_start  OpenAI     3       2     1       0         0             0   \\n18    on_llm_end     NaN     4       2     2       0         0             0   \\n19    on_llm_end     NaN     4       2     2       0         0             0   \\n20    on_llm_end     NaN     4       2     2       0         0             0   \\n21    on_llm_end     NaN     4       2     2       0         0             0   \\n22    on_llm_end     NaN     4       2     2       0         0             0   \\n23    on_llm_end     NaN     4       2     2       0         0             0   \\n    chain_ends  llm_starts  ...  difficult_words  linsear_write_formula  \\\\\\n0            0           1  ...              NaN                    NaN   \\n1            0           1  ...              NaN                    NaN   \\n2            0           1  ...              NaN                    NaN   \\n3            0           1  ...              NaN                    NaN   \\n4            0           1  ...              NaN                    NaN   \\n5            0           1  ...              NaN                    NaN   \\n6            0           1  ...              0.0                    5.5   \\n7            0           1  ...              2.0                    6.5   \\n8            0           1  ...              0.0                    5.5   \\n9            0           1  ...              2.0                    6.5   \\n10           0           1  ...              0.0                    5.5   \\n11           0           1  ...              2.0                    6.5   \\n12           0           2  ...              NaN                    NaN   \\n13           0           2  ...              NaN                    NaN   \\n14           0           2  ...              NaN                    NaN   \\n15           0           2  ...              NaN                    NaN   \\n16           0           2  ...              NaN                    NaN   \\n17           0           2  ...              NaN                    NaN   \\n18           0           2  ...              0.0                    5.5   \\n19           0           2  ...              2.0                    6.5   \\n20           0           2  ...              0.0                    5.5   \\n21           0           2  ...              2.0                    6.5   \\n22           0           2  ...              0.0                    5.5   \\n23           0           2  ...              2.0                    6.5   \\n    gunning_fog      text_standard  fernandez_huerta szigriszt_pazos  \\\\\\n0           NaN                NaN               NaN             NaN   \\n1           NaN                NaN               NaN             NaN   \\n2           NaN                NaN               NaN             NaN   \\n3           NaN                NaN               NaN             NaN   \\n4           NaN                NaN               NaN             NaN   \\n5           NaN                NaN               NaN             NaN   \\n6          5.20  5th and 6th grade            133.58          131.54   \\n7          8.28  6th and 7th grade            115.58          112.37   \\n8          5.20  5th and 6th grade            133.58          131.54   \\n9          8.28  6th and 7th grade            115.58          112.37   \\n10         5.20  5th and 6th grade            133.58          131.54   \\n11         8.28  6th and 7th grade            115.58          112.37   \\n12          NaN                NaN               NaN             NaN   \\n13          NaN                NaN               NaN             NaN   \\n14          NaN                NaN               NaN             NaN   \\n15          NaN                NaN               NaN             NaN   \\n16          NaN                NaN               NaN             NaN   \\n17          NaN                NaN               NaN             NaN   \\n18         5.20  5th and 6th grade            133.58          131.54   \\n19         8.28  6th and 7th grade            115.58          112.37   \\n20         5.20  5th and 6th grade            133.58          131.54   \\n21         8.28  6th and 7th grade            115.58          112.37   \\n22         5.20  5th and 6th grade            133.58          131.54   \\n23         8.28  6th and 7th grade            115.58          112.37   \\n    gutierrez_polini  crawford  gulpease_index   osman  \\n0                NaN       NaN             NaN     NaN  \\n1                NaN       NaN             NaN     NaN  \\n2                NaN       NaN             NaN     NaN  \\n3                NaN       NaN             NaN     NaN  \\n4                NaN       NaN             NaN     NaN  \\n5                NaN       NaN             NaN     NaN  \\n6              62.30      -0.2            79.8  116.91  \\n7              54.83       1.4            72.1  100.17  \\n8              62.30      -0.2            79.8  116.91  \\n9              54.83       1.4            72.1  100.17  \\n10             62.30      -0.2            79.8  116.91  \\n11             54.83       1.4            72.1  100.17  \\n12               NaN       NaN             NaN     NaN  \\n13               NaN       NaN             NaN     NaN  \\n14               NaN       NaN             NaN     NaN  \\n15               NaN       NaN             NaN     NaN  \\n16               NaN       NaN             NaN     NaN  \\n17               NaN       NaN             NaN     NaN  \\n18             62.30      -0.2            79.8  116.91  \\n19             54.83       1.4            72.1  100.17  \\n20             62.30      -0.2            79.8  116.91  \\n21             54.83       1.4            72.1  100.17  \\n22             62.30      -0.2            79.8  116.91  \\n23             54.83       1.4            72.1  100.17  \\n[24 rows x 39 columns], \\'session_analysis\\':     prompt_step         prompts    name  output_step  \\\\\\n0             1  Tell me a joke  OpenAI            2   \\n1             1  Tell me a poem  OpenAI            2   \\n2             1  Tell me a joke  OpenAI            2   \\n3             1  Tell me a poem  OpenAI            2   \\n4             1  Tell me a joke  OpenAI            2   \\n5             1  Tell me a poem  OpenAI            2   \\n6             3  Tell me a joke  OpenAI            4   \\n7             3  Tell me a poem  OpenAI            4   \\n8             3  Tell me a joke  OpenAI            4   \\n9             3  Tell me a poem  OpenAI            4   \\n10            3  Tell me a joke  OpenAI            4   \\n11            3  Tell me a poem  OpenAI            4   \\n                                               output  \\\\\\n0   \\\\n\\\\nQ: What did the fish say when it hit the w...   \\n1   \\\\n\\\\nRoses are red,\\\\nViolets are blue,\\\\nSugar i...   \\n2   \\\\n\\\\nQ: What did the fish say when it hit the w...   \\n3   \\\\n\\\\nRoses are red,\\\\nViolets are blue,\\\\nSugar i...   \\n4   \\\\n\\\\nQ: What did the fish say when it hit the w...   \\n5   \\\\n\\\\nRoses are red,\\\\nViolets are blue,\\\\nSugar i...   \\n6   \\\\n\\\\nQ: What did the fish say when it hit the w...   \\n7   \\\\n\\\\nRoses are red,\\\\nViolets are blue,\\\\nSugar i...   \\n8   \\\\n\\\\nQ: What did the fish say when it hit the w...   \\n9   \\\\n\\\\nRoses are red,\\\\nViolets are blue,\\\\nSugar i...   \\n10  \\\\n\\\\nQ: What did the fish say when it hit the w...   \\n11  \\\\n\\\\nRoses are red,\\\\nViolets are blue,\\\\nSugar i...   \\n    token_usage_total_tokens  token_usage_prompt_tokens  \\\\\\n0                        162                         24   \\n1                        162                         24   \\n2                        162                         24   \\n3                        162                         24   \\n4                        162                         24   \\n5                        162                         24   \\n6                        162                         24   \\n7                        162                         24   \\n8                        162                         24   \\n9                        162                         24   \\n10                       162                         24   \\n11                       162                         24   \\n    token_usage_completion_tokens  flesch_reading_ease  flesch_kincaid_grade  \\\\\\n0                             138               109.04                   1.3   \\n1                             138                83.66                   4.8   \\n2                             138               109.04                   1.3   \\n3                             138                83.66                   4.8   \\n4                             138               109.04                   1.3   \\n5                             138                83.66                   4.8   \\n6                             138               109.04                   1.3   \\n7                             138                83.66                   4.8   \\n8                             138               109.04                   1.3   \\n9                             138                83.66                   4.8   \\n10                            138               109.04                   1.3   \\n11                            138                83.66                   4.8   \\n    ...  difficult_words  linsear_write_formula  gunning_fog  \\\\\\n0   ...                0                    5.5         5.20   \\n1   ...                2                    6.5         8.28   \\n2   ...                0                    5.5         5.20   \\n3   ...                2                    6.5         8.28   \\n4   ...                0                    5.5         5.20   \\n5   ...                2                    6.5         8.28   \\n6   ...                0                    5.5         5.20   \\n7   ...                2                    6.5         8.28   \\n8   ...                0                    5.5         5.20   \\n9   ...                2                    6.5         8.28   \\n10  ...                0                    5.5         5.20   \\n11  ...                2                    6.5         8.28   \\n        text_standard  fernandez_huerta  szigriszt_pazos  gutierrez_polini  \\\\\\n0   5th and 6th grade            133.58           131.54             62.30   \\n1   6th and 7th grade            115.58           112.37             54.83   \\n2   5th and 6th grade            133.58           131.54             62.30   \\n3   6th and 7th grade            115.58           112.37             54.83   \\n4   5th and 6th grade            133.58           131.54             62.30   \\n5   6th and 7th grade            115.58           112.37             54.83   \\n6   5th and 6th grade            133.58           131.54             62.30   \\n7   6th and 7th grade            115.58           112.37             54.83   \\n8   5th and 6th grade            133.58           131.54             62.30   \\n9   6th and 7th grade            115.58           112.37             54.83   \\n10  5th and 6th grade            133.58           131.54             62.30   \\n11  6th and 7th grade            115.58           112.37             54.83   \\n   crawford  gulpease_index   osman  \\n0      -0.2            79.8  116.91  \\n1       1.4            72.1  100.17  \\n2      -0.2            79.8  116.91  \\n3       1.4            72.1  100.17  \\n4      -0.2            79.8  116.91  \\n5       1.4            72.1  100.17  \\n6      -0.2            79.8  116.91  \\n7       1.4            72.1  100.17  \\n8      -0.2            79.8  116.91  \\n9       1.4            72.1  100.17  \\n10     -0.2            79.8  116.91  \\n11      1.4            72.1  100.17  \\n[12 rows x 24 columns]}\\n2023-03-29 14:00:25,948 - clearml.Task - INFO - Completed model upload to https://files.clear.ml/langchain_callback_demo/llm.988bd727b0e94a29a3ac0ee526813545/models/simple_sequential\\nAt this point you can already go to https://app.clear.ml and take a look at the resulting ClearML Task that was created.\\nAmong others, you should see that this notebook is saved along with any git information. The model JSON that contains the used parameters is saved as an artifact, there are also console logs and under the plots section, you’ll find tables that represent the flow of the chain.\\nFinally, if you enabled visualizations, these are stored as HTML files under debug samples.\\nScenario 2: Creating an agent with tools#\\nTo show a more advanced workflow, let’s create an agent with access to tools. The way ClearML tracks the results is not different though, only the table will look slightly different as there are other types of actions taken when compared to the earlier, simpler example.\\nYou can now also see the use of the finish=True keyword, which will fully close the ClearML Task, instead of just resetting the parameters and prompts for a new conversation.\\nfrom langchain.agents import initialize_agent, load_tools\\nfrom langchain.agents import AgentType\\n# SCENARIO 2 - Agent with Tools\\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, callback_manager=manager)\\nagent = initialize_agent(\\n    tools,\\n    llm,\\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\\n    callback_manager=manager,\\n    verbose=True,\\n)\\nagent.run(\\n    \"Who is the wife of the person who sang summer of 69?\"\\n)\\nclearml_callback.flush_tracker(langchain_asset=agent, name=\"Agent with Tools\", finish=True)\\n> Entering new AgentExecutor chain...\\n{\\'action\\': \\'on_chain_start\\', \\'name\\': \\'AgentExecutor\\', \\'step\\': 1, \\'starts\\': 1, \\'ends\\': 0, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 1, \\'chain_ends\\': 0, \\'llm_starts\\': 0, \\'llm_ends\\': 0, \\'llm_streams\\': 0, \\'tool_starts\\': 0, \\'tool_ends\\': 0, \\'agent_ends\\': 0, \\'input\\': \\'Who is the wife of the person who sang summer of 69?\\'}\\n{\\'action\\': \\'on_llm_start\\', \\'name\\': \\'OpenAI\\', \\'step\\': 2, \\'starts\\': 2, \\'ends\\': 0, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 1, \\'chain_ends\\': 0, \\'llm_starts\\': 1, \\'llm_ends\\': 0, \\'llm_streams\\': 0, \\'tool_starts\\': 0, \\'tool_ends\\': 0, \\'agent_ends\\': 0, \\'prompts\\': \\'Answer the following questions as best you can. You have access to the following tools:\\\\n\\\\nSearch: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\\\nCalculator: Useful for when you need to answer questions about math.\\\\n\\\\nUse the following format:\\\\n\\\\nQuestion: the input question you must answer\\\\nThought: you should always think about what to do\\\\nAction: the action to take, should be one of [Search, Calculator]\\\\nAction Input: the input to the action\\\\nObservation: the result of the action\\\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\\\nThought: I now know the final answer\\\\nFinal Answer: the final answer to the original input question\\\\n\\\\nBegin!\\\\n\\\\nQuestion: Who is the wife of the person who sang summer of 69?\\\\nThought:\\'}\\n{\\'action\\': \\'on_llm_end\\', \\'token_usage_prompt_tokens\\': 189, \\'token_usage_completion_tokens\\': 34, \\'token_usage_total_tokens\\': 223, \\'model_name\\': \\'text-davinci-003\\', \\'step\\': 3, \\'starts\\': 2, \\'ends\\': 1, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 1, \\'chain_ends\\': 0, \\'llm_starts\\': 1, \\'llm_ends\\': 1, \\'llm_streams\\': 0, \\'tool_starts\\': 0, \\'tool_ends\\': 0, \\'agent_ends\\': 0, \\'text\\': \\' I need to find out who sang summer of 69 and then find out who their wife is.\\\\nAction: Search\\\\nAction Input: \"Who sang summer of 69\"\\', \\'generation_info_finish_reason\\': \\'stop\\', \\'generation_info_logprobs\\': None, \\'flesch_reading_ease\\': 91.61, \\'flesch_kincaid_grade\\': 3.8, \\'smog_index\\': 0.0, \\'coleman_liau_index\\': 3.41, \\'automated_readability_index\\': 3.5, \\'dale_chall_readability_score\\': 6.06, \\'difficult_words\\': 2, \\'linsear_write_formula\\': 5.75, \\'gunning_fog\\': 5.4, \\'text_standard\\': \\'3rd and 4th grade\\', \\'fernandez_huerta\\': 121.07, \\'szigriszt_pazos\\': 119.5, \\'gutierrez_polini\\': 54.91, \\'crawford\\': 0.9, \\'gulpease_index\\': 72.7, \\'osman\\': 92.16}\\n I need to find out who sang summer of 69 and then find out who their wife is.\\nAction: Search\\nAction Input: \"Who sang summer of 69\"{\\'action\\': \\'on_agent_action\\', \\'tool\\': \\'Search\\', \\'tool_input\\': \\'Who sang summer of 69\\', \\'log\\': \\' I need to find out who sang summer of 69 and then find out who their wife is.\\\\nAction: Search\\\\nAction Input: \"Who sang summer of 69\"\\', \\'step\\': 4, \\'starts\\': 3, \\'ends\\': 1, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 1, \\'chain_ends\\': 0, \\'llm_starts\\': 1, \\'llm_ends\\': 1, \\'llm_streams\\': 0, \\'tool_starts\\': 1, \\'tool_ends\\': 0, \\'agent_ends\\': 0}\\n{\\'action\\': \\'on_tool_start\\', \\'input_str\\': \\'Who sang summer of 69\\', \\'name\\': \\'Search\\', \\'description\\': \\'A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\', \\'step\\': 5, \\'starts\\': 4, \\'ends\\': 1, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 1, \\'chain_ends\\': 0, \\'llm_starts\\': 1, \\'llm_ends\\': 1, \\'llm_streams\\': 0, \\'tool_starts\\': 2, \\'tool_ends\\': 0, \\'agent_ends\\': 0}\\nObservation: Bryan Adams - Summer Of 69 (Official Music Video).\\nThought:{\\'action\\': \\'on_tool_end\\', \\'output\\': \\'Bryan Adams - Summer Of 69 (Official Music Video).\\', \\'step\\': 6, \\'starts\\': 4, \\'ends\\': 2, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 1, \\'chain_ends\\': 0, \\'llm_starts\\': 1, \\'llm_ends\\': 1, \\'llm_streams\\': 0, \\'tool_starts\\': 2, \\'tool_ends\\': 1, \\'agent_ends\\': 0}\\n{\\'action\\': \\'on_llm_start\\', \\'name\\': \\'OpenAI\\', \\'step\\': 7, \\'starts\\': 5, \\'ends\\': 2, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 1, \\'chain_ends\\': 0, \\'llm_starts\\': 2, \\'llm_ends\\': 1, \\'llm_streams\\': 0, \\'tool_starts\\': 2, \\'tool_ends\\': 1, \\'agent_ends\\': 0, \\'prompts\\': \\'Answer the following questions as best you can. You have access to the following tools:\\\\n\\\\nSearch: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\\\nCalculator: Useful for when you need to answer questions about math.\\\\n\\\\nUse the following format:\\\\n\\\\nQuestion: the input question you must answer\\\\nThought: you should always think about what to do\\\\nAction: the action to take, should be one of [Search, Calculator]\\\\nAction Input: the input to the action\\\\nObservation: the result of the action\\\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\\\nThought: I now know the final answer\\\\nFinal Answer: the final answer to the original input question\\\\n\\\\nBegin!\\\\n\\\\nQuestion: Who is the wife of the person who sang summer of 69?\\\\nThought: I need to find out who sang summer of 69 and then find out who their wife is.\\\\nAction: Search\\\\nAction Input: \"Who sang summer of 69\"\\\\nObservation: Bryan Adams - Summer Of 69 (Official Music Video).\\\\nThought:\\'}\\n{\\'action\\': \\'on_llm_end\\', \\'token_usage_prompt_tokens\\': 242, \\'token_usage_completion_tokens\\': 28, \\'token_usage_total_tokens\\': 270, \\'model_name\\': \\'text-davinci-003\\', \\'step\\': 8, \\'starts\\': 5, \\'ends\\': 3, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 1, \\'chain_ends\\': 0, \\'llm_starts\\': 2, \\'llm_ends\\': 2, \\'llm_streams\\': 0, \\'tool_starts\\': 2, \\'tool_ends\\': 1, \\'agent_ends\\': 0, \\'text\\': \\' I need to find out who Bryan Adams is married to.\\\\nAction: Search\\\\nAction Input: \"Who is Bryan Adams married to\"\\', \\'generation_info_finish_reason\\': \\'stop\\', \\'generation_info_logprobs\\': None, \\'flesch_reading_ease\\': 94.66, \\'flesch_kincaid_grade\\': 2.7, \\'smog_index\\': 0.0, \\'coleman_liau_index\\': 4.73, \\'automated_readability_index\\': 4.0, \\'dale_chall_readability_score\\': 7.16, \\'difficult_words\\': 2, \\'linsear_write_formula\\': 4.25, \\'gunning_fog\\': 4.2, \\'text_standard\\': \\'4th and 5th grade\\', \\'fernandez_huerta\\': 124.13, \\'szigriszt_pazos\\': 119.2, \\'gutierrez_polini\\': 52.26, \\'crawford\\': 0.7, \\'gulpease_index\\': 74.7, \\'osman\\': 84.2}\\n I need to find out who Bryan Adams is married to.\\nAction: Search\\nAction Input: \"Who is Bryan Adams married to\"{\\'action\\': \\'on_agent_action\\', \\'tool\\': \\'Search\\', \\'tool_input\\': \\'Who is Bryan Adams married to\\', \\'log\\': \\' I need to find out who Bryan Adams is married to.\\\\nAction: Search\\\\nAction Input: \"Who is Bryan Adams married to\"\\', \\'step\\': 9, \\'starts\\': 6, \\'ends\\': 3, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 1, \\'chain_ends\\': 0, \\'llm_starts\\': 2, \\'llm_ends\\': 2, \\'llm_streams\\': 0, \\'tool_starts\\': 3, \\'tool_ends\\': 1, \\'agent_ends\\': 0}\\n{\\'action\\': \\'on_tool_start\\', \\'input_str\\': \\'Who is Bryan Adams married to\\', \\'name\\': \\'Search\\', \\'description\\': \\'A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\', \\'step\\': 10, \\'starts\\': 7, \\'ends\\': 3, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 1, \\'chain_ends\\': 0, \\'llm_starts\\': 2, \\'llm_ends\\': 2, \\'llm_streams\\': 0, \\'tool_starts\\': 4, \\'tool_ends\\': 1, \\'agent_ends\\': 0}\\nObservation: Bryan Adams has never married. In the 1990s, he was in a relationship with Danish model Cecilie Thomsen. In 2011, Bryan and Alicia Grimaldi, his ...\\nThought:{\\'action\\': \\'on_tool_end\\', \\'output\\': \\'Bryan Adams has never married. In the 1990s, he was in a relationship with Danish model Cecilie Thomsen. In 2011, Bryan and Alicia Grimaldi, his ...\\', \\'step\\': 11, \\'starts\\': 7, \\'ends\\': 4, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 1, \\'chain_ends\\': 0, \\'llm_starts\\': 2, \\'llm_ends\\': 2, \\'llm_streams\\': 0, \\'tool_starts\\': 4, \\'tool_ends\\': 2, \\'agent_ends\\': 0}\\n{\\'action\\': \\'on_llm_start\\', \\'name\\': \\'OpenAI\\', \\'step\\': 12, \\'starts\\': 8, \\'ends\\': 4, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 1, \\'chain_ends\\': 0, \\'llm_starts\\': 3, \\'llm_ends\\': 2, \\'llm_streams\\': 0, \\'tool_starts\\': 4, \\'tool_ends\\': 2, \\'agent_ends\\': 0, \\'prompts\\': \\'Answer the following questions as best you can. You have access to the following tools:\\\\n\\\\nSearch: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\\\nCalculator: Useful for when you need to answer questions about math.\\\\n\\\\nUse the following format:\\\\n\\\\nQuestion: the input question you must answer\\\\nThought: you should always think about what to do\\\\nAction: the action to take, should be one of [Search, Calculator]\\\\nAction Input: the input to the action\\\\nObservation: the result of the action\\\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\\\nThought: I now know the final answer\\\\nFinal Answer: the final answer to the original input question\\\\n\\\\nBegin!\\\\n\\\\nQuestion: Who is the wife of the person who sang summer of 69?\\\\nThought: I need to find out who sang summer of 69 and then find out who their wife is.\\\\nAction: Search\\\\nAction Input: \"Who sang summer of 69\"\\\\nObservation: Bryan Adams - Summer Of 69 (Official Music Video).\\\\nThought: I need to find out who Bryan Adams is married to.\\\\nAction: Search\\\\nAction Input: \"Who is Bryan Adams married to\"\\\\nObservation: Bryan Adams has never married. In the 1990s, he was in a relationship with Danish model Cecilie Thomsen. In 2011, Bryan and Alicia Grimaldi, his ...\\\\nThought:\\'}\\n{\\'action\\': \\'on_llm_end\\', \\'token_usage_prompt_tokens\\': 314, \\'token_usage_completion_tokens\\': 18, \\'token_usage_total_tokens\\': 332, \\'model_name\\': \\'text-davinci-003\\', \\'step\\': 13, \\'starts\\': 8, \\'ends\\': 5, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 1, \\'chain_ends\\': 0, \\'llm_starts\\': 3, \\'llm_ends\\': 3, \\'llm_streams\\': 0, \\'tool_starts\\': 4, \\'tool_ends\\': 2, \\'agent_ends\\': 0, \\'text\\': \\' I now know the final answer.\\\\nFinal Answer: Bryan Adams has never been married.\\', \\'generation_info_finish_reason\\': \\'stop\\', \\'generation_info_logprobs\\': None, \\'flesch_reading_ease\\': 81.29, \\'flesch_kincaid_grade\\': 3.7, \\'smog_index\\': 0.0, \\'coleman_liau_index\\': 5.75, \\'automated_readability_index\\': 3.9, \\'dale_chall_readability_score\\': 7.37, \\'difficult_words\\': 1, \\'linsear_write_formula\\': 2.5, \\'gunning_fog\\': 2.8, \\'text_standard\\': \\'3rd and 4th grade\\', \\'fernandez_huerta\\': 115.7, \\'szigriszt_pazos\\': 110.84, \\'gutierrez_polini\\': 49.79, \\'crawford\\': 0.7, \\'gulpease_index\\': 85.4, \\'osman\\': 83.14}\\n I now know the final answer.\\nFinal Answer: Bryan Adams has never been married.\\n{\\'action\\': \\'on_agent_finish\\', \\'output\\': \\'Bryan Adams has never been married.\\', \\'log\\': \\' I now know the final answer.\\\\nFinal Answer: Bryan Adams has never been married.\\', \\'step\\': 14, \\'starts\\': 8, \\'ends\\': 6, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 1, \\'chain_ends\\': 0, \\'llm_starts\\': 3, \\'llm_ends\\': 3, \\'llm_streams\\': 0, \\'tool_starts\\': 4, \\'tool_ends\\': 2, \\'agent_ends\\': 1}\\n> Finished chain.\\n{\\'action\\': \\'on_chain_end\\', \\'outputs\\': \\'Bryan Adams has never been married.\\', \\'step\\': 15, \\'starts\\': 8, \\'ends\\': 7, \\'errors\\': 0, \\'text_ctr\\': 0, \\'chain_starts\\': 1, \\'chain_ends\\': 1, \\'llm_starts\\': 3, \\'llm_ends\\': 3, \\'llm_streams\\': 0, \\'tool_starts\\': 4, \\'tool_ends\\': 2, \\'agent_ends\\': 1}\\n{\\'action_records\\':              action    name  step  starts  ends  errors  text_ctr  \\\\\\n0      on_llm_start  OpenAI     1       1     0       0         0   \\n1      on_llm_start  OpenAI     1       1     0       0         0   \\n2      on_llm_start  OpenAI     1       1     0       0         0   \\n3      on_llm_start  OpenAI     1       1     0       0         0   \\n4      on_llm_start  OpenAI     1       1     0       0         0   \\n..              ...     ...   ...     ...   ...     ...       ...   \\n66      on_tool_end     NaN    11       7     4       0         0   \\n67     on_llm_start  OpenAI    12       8     4       0         0   \\n68       on_llm_end     NaN    13       8     5       0         0   \\n69  on_agent_finish     NaN    14       8     6       0         0   \\n70     on_chain_end     NaN    15       8     7       0         0   \\n    chain_starts  chain_ends  llm_starts  ...  gulpease_index  osman  input  \\\\\\n0              0           0           1  ...             NaN    NaN    NaN   \\n1              0           0           1  ...             NaN    NaN    NaN   \\n2              0           0           1  ...             NaN    NaN    NaN   \\n3              0           0           1  ...             NaN    NaN    NaN   \\n4              0           0           1  ...             NaN    NaN    NaN   \\n..           ...         ...         ...  ...             ...    ...    ...   \\n66             1           0           2  ...             NaN    NaN    NaN   \\n67             1           0           3  ...             NaN    NaN    NaN   \\n68             1           0           3  ...            85.4  83.14    NaN   \\n69             1           0           3  ...             NaN    NaN    NaN   \\n70             1           1           3  ...             NaN    NaN    NaN   \\n    tool  tool_input                                                log  \\\\\\n0    NaN         NaN                                                NaN   \\n1    NaN         NaN                                                NaN   \\n2    NaN         NaN                                                NaN   \\n3    NaN         NaN                                                NaN   \\n4    NaN         NaN                                                NaN   \\n..   ...         ...                                                ...   \\n66   NaN         NaN                                                NaN   \\n67   NaN         NaN                                                NaN   \\n68   NaN         NaN                                                NaN   \\n69   NaN         NaN   I now know the final answer.\\\\nFinal Answer: B...   \\n70   NaN         NaN                                                NaN   \\n    input_str  description                                             output  \\\\\\n0         NaN          NaN                                                NaN   \\n1         NaN          NaN                                                NaN   \\n2         NaN          NaN                                                NaN   \\n3         NaN          NaN                                                NaN   \\n4         NaN          NaN                                                NaN   \\n..        ...          ...                                                ...   \\n66        NaN          NaN  Bryan Adams has never married. In the 1990s, h...   \\n67        NaN          NaN                                                NaN   \\n68        NaN          NaN                                                NaN   \\n69        NaN          NaN                Bryan Adams has never been married.   \\n70        NaN          NaN                                                NaN   \\n                                outputs  \\n0                                   NaN  \\n1                                   NaN  \\n2                                   NaN  \\n3                                   NaN  \\n4                                   NaN  \\n..                                  ...  \\n66                                  NaN  \\n67                                  NaN  \\n68                                  NaN  \\n69                                  NaN  \\n70  Bryan Adams has never been married.  \\n[71 rows x 47 columns], \\'session_analysis\\':    prompt_step                                            prompts    name  \\\\\\n0            2  Answer the following questions as best you can...  OpenAI   \\n1            7  Answer the following questions as best you can...  OpenAI   \\n2           12  Answer the following questions as best you can...  OpenAI   \\n   output_step                                             output  \\\\\\n0            3   I need to find out who sang summer of 69 and ...   \\n1            8   I need to find out who Bryan Adams is married...   \\n2           13   I now know the final answer.\\\\nFinal Answer: B...   \\n   token_usage_total_tokens  token_usage_prompt_tokens  \\\\\\n0                       223                        189   \\n1                       270                        242   \\n2                       332                        314   \\n   token_usage_completion_tokens  flesch_reading_ease  flesch_kincaid_grade  \\\\\\n0                             34                91.61                   3.8   \\n1                             28                94.66                   2.7   \\n2                             18                81.29                   3.7   \\n   ...  difficult_words  linsear_write_formula  gunning_fog  \\\\\\n0  ...                2                   5.75          5.4   \\n1  ...                2                   4.25          4.2   \\n2  ...                1                   2.50          2.8   \\n       text_standard  fernandez_huerta  szigriszt_pazos  gutierrez_polini  \\\\\\n0  3rd and 4th grade            121.07           119.50             54.91   \\n1  4th and 5th grade            124.13           119.20             52.26   \\n2  3rd and 4th grade            115.70           110.84             49.79   \\n  crawford  gulpease_index  osman  \\n0      0.9            72.7  92.16  \\n1      0.7            74.7  84.20  \\n2      0.7            85.4  83.14  \\n[3 rows x 24 columns]}\\nCould not update last created model in Task 988bd727b0e94a29a3ac0ee526813545, Task status \\'completed\\' cannot be updated\\nTips and Next Steps#\\nMake sure you always use a unique name argument for the clearml_callback.flush_tracker function. If not, the model parameters used for a run will override the previous run!\\nIf you close the ClearML Callback using clearml_callback.flush_tracker(..., finish=True) the Callback cannot be used anymore. Make a new one if you want to keep logging.\\nCheck out the rest of the open source ClearML ecosystem, there is a data version manager, a remote execution agent, automated pipelines and much more!\\nprevious\\nChroma\\nnext\\nCohere\\n Contents\\n  \\nGetting API Credentials\\nSetting Up\\nScenario 1: Just an LLM\\nScenario 2: Creating an agent with tools\\nTips and Next Steps\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\clearml_tracking.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nCohere\\n Contents \\nInstallation and Setup\\nWrappers\\nLLM\\nEmbeddings\\nCohere#\\nThis page covers how to use the Cohere ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Cohere wrappers.\\nInstallation and Setup#\\nInstall the Python SDK with pip install cohere\\nGet an Cohere api key and set it as an environment variable (COHERE_API_KEY)\\nWrappers#\\nLLM#\\nThere exists an Cohere LLM wrapper, which you can access with\\nfrom langchain.llms import Cohere\\nEmbeddings#\\nThere exists an Cohere Embeddings wrapper, which you can access with\\nfrom langchain.embeddings import CohereEmbeddings\\nFor a more detailed walkthrough of this, see this notebook\\nprevious\\nClearML Integration\\nnext\\nComet\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nLLM\\nEmbeddings\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\cohere.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nComet\\n Contents \\nInstall Comet and Dependencies\\nInitialize Comet and Set your Credentials\\nSet OpenAI and SerpAPI credentials\\nScenario 1: Using just an LLM\\nScenario 2: Using an LLM in a Chain\\nScenario 3: Using An Agent with Tools\\nScenario 4: Using Custom Evaluation Metrics\\nComet#\\nIn this guide we will demonstrate how to track your Langchain Experiments, Evaluation Metrics, and LLM Sessions with Comet.\\nExample Project: Comet with LangChain\\nInstall Comet and Dependencies#\\n%pip install comet_ml langchain openai google-search-results spacy textstat pandas\\nimport sys\\n!{sys.executable} -m spacy download en_core_web_sm\\nInitialize Comet and Set your Credentials#\\nYou can grab your Comet API Key here or click the link after intializing Comet\\nimport comet_ml\\ncomet_ml.init(project_name=\"comet-example-langchain\")\\nSet OpenAI and SerpAPI credentials#\\nYou will need an OpenAI API Key and a SerpAPI API Key to run the following examples\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"...\"\\n#os.environ[\"OPENAI_ORGANIZATION\"] = \"...\"\\nos.environ[\"SERPAPI_API_KEY\"] = \"...\"\\nScenario 1: Using just an LLM#\\nfrom datetime import datetime\\nfrom langchain.callbacks import CometCallbackHandler, StdOutCallbackHandler\\nfrom langchain.callbacks.base import CallbackManager\\nfrom langchain.llms import OpenAI\\ncomet_callback = CometCallbackHandler(\\n    project_name=\"comet-example-langchain\",\\n    complexity_metrics=True,\\n    stream_logs=True,\\n    tags=[\"llm\"],\\n    visualizations=[\"dep\"],\\n)\\nmanager = CallbackManager([StdOutCallbackHandler(), comet_callback])\\nllm = OpenAI(temperature=0.9, callback_manager=manager, verbose=True)\\nllm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\", \"Tell me a fact\"] * 3)\\nprint(\"LLM result\", llm_result)\\ncomet_callback.flush_tracker(llm, finish=True)\\nScenario 2: Using an LLM in a Chain#\\nfrom langchain.callbacks import CometCallbackHandler, StdOutCallbackHandler\\nfrom langchain.callbacks.base import CallbackManager\\nfrom langchain.chains import LLMChain\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\ncomet_callback = CometCallbackHandler(\\n    complexity_metrics=True,\\n    project_name=\"comet-example-langchain\",\\n    stream_logs=True,\\n    tags=[\"synopsis-chain\"],\\n)\\nmanager = CallbackManager([StdOutCallbackHandler(), comet_callback])\\nllm = OpenAI(temperature=0.9, callback_manager=manager, verbose=True)\\ntemplate = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\\nTitle: {title}\\nPlaywright: This is a synopsis for the above play:\"\"\"\\nprompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\\nsynopsis_chain = LLMChain(llm=llm, prompt=prompt_template, callback_manager=manager)\\ntest_prompts = [{\"title\": \"Documentary about Bigfoot in Paris\"}]\\nprint(synopsis_chain.apply(test_prompts))\\ncomet_callback.flush_tracker(synopsis_chain, finish=True)\\nScenario 3: Using An Agent with Tools#\\nfrom langchain.agents import initialize_agent, load_tools\\nfrom langchain.callbacks import CometCallbackHandler, StdOutCallbackHandler\\nfrom langchain.callbacks.base import CallbackManager\\nfrom langchain.llms import OpenAI\\ncomet_callback = CometCallbackHandler(\\n    project_name=\"comet-example-langchain\",\\n    complexity_metrics=True,\\n    stream_logs=True,\\n    tags=[\"agent\"],\\n)\\nmanager = CallbackManager([StdOutCallbackHandler(), comet_callback])\\nllm = OpenAI(temperature=0.9, callback_manager=manager, verbose=True)\\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, callback_manager=manager)\\nagent = initialize_agent(\\n    tools,\\n    llm,\\n    agent=\"zero-shot-react-description\",\\n    callback_manager=manager,\\n    verbose=True,\\n)\\nagent.run(\\n    \"Who is Leo DiCaprio\\'s girlfriend? What is her current age raised to the 0.43 power?\"\\n)\\ncomet_callback.flush_tracker(agent, finish=True)\\nScenario 4: Using Custom Evaluation Metrics#\\nThe CometCallbackManager also allows you to define and use Custom Evaluation Metrics to assess generated outputs from your model. Let’s take a look at how this works.\\nIn the snippet below, we will use the ROUGE metric to evaluate the quality of a generated summary of an input prompt.\\n%pip install rouge-score\\nfrom rouge_score import rouge_scorer\\nfrom langchain.callbacks import CometCallbackHandler, StdOutCallbackHandler\\nfrom langchain.callbacks.base import CallbackManager\\nfrom langchain.chains import LLMChain\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nclass Rouge:\\n    def __init__(self, reference):\\n        self.reference = reference\\n        self.scorer = rouge_scorer.RougeScorer([\"rougeLsum\"], use_stemmer=True)\\n    def compute_metric(self, generation, prompt_idx, gen_idx):\\n        prediction = generation.text\\n        results = self.scorer.score(target=self.reference, prediction=prediction)\\n        return {\\n            \"rougeLsum_score\": results[\"rougeLsum\"].fmeasure,\\n            \"reference\": self.reference,\\n        }\\nreference = \"\"\"\\nThe tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building.\\nIt was the first structure to reach a height of 300 metres.\\nIt is now taller than the Chrysler Building in New York City by 5.2 metres (17 ft)\\nExcluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France .\\n\"\"\"\\nrouge_score = Rouge(reference=reference)\\ntemplate = \"\"\"Given the following article, it is your job to write a summary.\\nArticle:\\n{article}\\nSummary: This is the summary for the above article:\"\"\"\\nprompt_template = PromptTemplate(input_variables=[\"article\"], template=template)\\ncomet_callback = CometCallbackHandler(\\n    project_name=\"comet-example-langchain\",\\n    complexity_metrics=False,\\n    stream_logs=True,\\n    tags=[\"custom_metrics\"],\\n    custom_metrics=rouge_score.compute_metric,\\n)\\nmanager = CallbackManager([StdOutCallbackHandler(), comet_callback])\\nllm = OpenAI(temperature=0.9, callback_manager=manager, verbose=True)\\nsynopsis_chain = LLMChain(llm=llm, prompt=prompt_template, callback_manager=manager)\\ntest_prompts = [\\n    {\\n        \"article\": \"\"\"\\n                 The tower is 324 metres (1,063 ft) tall, about the same height as\\n                 an 81-storey building, and the tallest structure in Paris. Its base is square,\\n                 measuring 125 metres (410 ft) on each side.\\n                 During its construction, the Eiffel Tower surpassed the\\n                 Washington Monument to become the tallest man-made structure in the world,\\n                 a title it held for 41 years until the Chrysler Building\\n                 in New York City was finished in 1930.\\n                 It was the first structure to reach a height of 300 metres.\\n                 Due to the addition of a broadcasting aerial at the top of the tower in 1957,\\n                 it is now taller than the Chrysler Building by 5.2 metres (17 ft).\\n                 Excluding transmitters, the Eiffel Tower is the second tallest\\n                 free-standing structure in France after the Millau Viaduct.\\n                 \"\"\"\\n    }\\n]\\nprint(synopsis_chain.apply(test_prompts))\\ncomet_callback.flush_tracker(synopsis_chain, finish=True)\\nprevious\\nCohere\\nnext\\nDataberry\\n Contents\\n  \\nInstall Comet and Dependencies\\nInitialize Comet and Set your Credentials\\nSet OpenAI and SerpAPI credentials\\nScenario 1: Using just an LLM\\nScenario 2: Using an LLM in a Chain\\nScenario 3: Using An Agent with Tools\\nScenario 4: Using Custom Evaluation Metrics\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\comet_tracking.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nDataberry\\n Contents \\nWhat is Databerry?\\nQuick start\\nDataberry#\\nThis page covers how to use the Databerry within LangChain.\\nWhat is Databerry?#\\nDataberry is an open source document retrievial platform that helps to connect your personal data with Large Language Models.\\nQuick start#\\nRetrieving documents stored in Databerry from LangChain is very easy!\\nfrom langchain.retrievers import DataberryRetriever\\nretriever = DataberryRetriever(\\n    datastore_url=\"https://api.databerry.ai/query/clg1xg2h80000l708dymr0fxc\",\\n    # api_key=\"DATABERRY_API_KEY\", # optional if datastore is public\\n    # top_k=10 # optional\\n)\\ndocs = retriever.get_relevant_documents(\"What\\'s Databerry?\")\\nprevious\\nComet\\nnext\\nDeepInfra\\n Contents\\n  \\nWhat is Databerry?\\nQuick start\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\databerry.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nDeepInfra\\n Contents \\nInstallation and Setup\\nWrappers\\nLLM\\nDeepInfra#\\nThis page covers how to use the DeepInfra ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific DeepInfra wrappers.\\nInstallation and Setup#\\nGet your DeepInfra api key from this link here.\\nGet an DeepInfra api key and set it as an environment variable (DEEPINFRA_API_TOKEN)\\nWrappers#\\nLLM#\\nThere exists an DeepInfra LLM wrapper, which you can access with\\nfrom langchain.llms import DeepInfra\\nprevious\\nDataberry\\nnext\\nDeep Lake\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nLLM\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\deepinfra.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nDeep Lake\\n Contents \\nWhy Deep Lake?\\nMore Resources\\nInstallation and Setup\\nWrappers\\nVectorStore\\nDeep Lake#\\nThis page covers how to use the Deep Lake ecosystem within LangChain.\\nWhy Deep Lake?#\\nMore than just a (multi-modal) vector store. You can later use the dataset to fine-tune your own LLM models.\\nNot only stores embeddings, but also the original data with automatic version control.\\nTruly serverless. Doesn’t require another service and can be used with major cloud providers (AWS S3, GCS, etc.)\\nMore Resources#\\nUltimate Guide to LangChain & Deep Lake: Build ChatGPT to Answer Questions on Your Financial Data\\nTwitter the-algorithm codebase analysis with Deep Lake\\nHere is whitepaper and academic paper for Deep Lake\\nHere is a set of additional resources available for review: Deep Lake, Getting Started and\\xa0Tutorials\\nInstallation and Setup#\\nInstall the Python package with pip install deeplake\\nWrappers#\\nVectorStore#\\nThere exists a wrapper around Deep Lake, a data lake for Deep Learning applications, allowing you to use it as a vector store (for now), whether for semantic search or example selection.\\nTo import this vectorstore:\\nfrom langchain.vectorstores import DeepLake\\nFor a more detailed walkthrough of the Deep Lake wrapper, see this notebook\\nprevious\\nDeepInfra\\nnext\\nForefrontAI\\n Contents\\n  \\nWhy Deep Lake?\\nMore Resources\\nInstallation and Setup\\nWrappers\\nVectorStore\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\deeplake.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nForefrontAI\\n Contents \\nInstallation and Setup\\nWrappers\\nLLM\\nForefrontAI#\\nThis page covers how to use the ForefrontAI ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific ForefrontAI wrappers.\\nInstallation and Setup#\\nGet an ForefrontAI api key and set it as an environment variable (FOREFRONTAI_API_KEY)\\nWrappers#\\nLLM#\\nThere exists an ForefrontAI LLM wrapper, which you can access with\\nfrom langchain.llms import ForefrontAI\\nprevious\\nDeep Lake\\nnext\\nGoogle Search Wrapper\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nLLM\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\forefrontai.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nGoogle Search Wrapper\\n Contents \\nInstallation and Setup\\nWrappers\\nUtility\\nTool\\nGoogle Search Wrapper#\\nThis page covers how to use the Google Search API within LangChain.\\nIt is broken into two parts: installation and setup, and then references to the specific Google Search wrapper.\\nInstallation and Setup#\\nInstall requirements with pip install google-api-python-client\\nSet up a Custom Search Engine, following these instructions\\nGet an API Key and Custom Search Engine ID from the previous step, and set them as environment variables GOOGLE_API_KEY and GOOGLE_CSE_ID respectively\\nWrappers#\\nUtility#\\nThere exists a GoogleSearchAPIWrapper utility which wraps this API. To import this utility:\\nfrom langchain.utilities import GoogleSearchAPIWrapper\\nFor a more detailed walkthrough of this wrapper, see this notebook.\\nTool#\\nYou can also easily load this wrapper as a Tool (to use with an Agent).\\nYou can do this with:\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"google-search\"])\\nFor more information on this, see this page\\nprevious\\nForefrontAI\\nnext\\nGoogle Serper Wrapper\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nUtility\\nTool\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\google_search.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nGoogle Serper Wrapper\\n Contents \\nSetup\\nWrappers\\nUtility\\nOutput\\nTool\\nGoogle Serper Wrapper#\\nThis page covers how to use the Serper Google Search API within LangChain. Serper is a low-cost Google Search API that can be used to add answer box, knowledge graph, and organic results data from Google Search.\\nIt is broken into two parts: setup, and then references to the specific Google Serper wrapper.\\nSetup#\\nGo to serper.dev to sign up for a free account\\nGet the api key and set it as an environment variable (SERPER_API_KEY)\\nWrappers#\\nUtility#\\nThere exists a GoogleSerperAPIWrapper utility which wraps this API. To import this utility:\\nfrom langchain.utilities import GoogleSerperAPIWrapper\\nYou can use it as part of a Self Ask chain:\\nfrom langchain.utilities import GoogleSerperAPIWrapper\\nfrom langchain.llms.openai import OpenAI\\nfrom langchain.agents import initialize_agent, Tool\\nfrom langchain.agents import AgentType\\nimport os\\nos.environ[\"SERPER_API_KEY\"] = \"\"\\nos.environ[\\'OPENAI_API_KEY\\'] = \"\"\\nllm = OpenAI(temperature=0)\\nsearch = GoogleSerperAPIWrapper()\\ntools = [\\n    Tool(\\n        name=\"Intermediate Answer\",\\n        func=search.run,\\n        description=\"useful for when you need to ask with search\"\\n    )\\n]\\nself_ask_with_search = initialize_agent(tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True)\\nself_ask_with_search.run(\"What is the hometown of the reigning men\\'s U.S. Open champion?\")\\nOutput#\\nEntering new AgentExecutor chain...\\n Yes.\\nFollow up: Who is the reigning men\\'s U.S. Open champion?\\nIntermediate answer: Current champions Carlos Alcaraz, 2022 men\\'s singles champion.\\nFollow up: Where is Carlos Alcaraz from?\\nIntermediate answer: El Palmar, Spain\\nSo the final answer is: El Palmar, Spain\\n> Finished chain.\\n\\'El Palmar, Spain\\'\\nFor a more detailed walkthrough of this wrapper, see this notebook.\\nTool#\\nYou can also easily load this wrapper as a Tool (to use with an Agent).\\nYou can do this with:\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"google-serper\"])\\nFor more information on this, see this page\\nprevious\\nGoogle Search Wrapper\\nnext\\nGooseAI\\n Contents\\n  \\nSetup\\nWrappers\\nUtility\\nOutput\\nTool\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\google_serper.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nGooseAI\\n Contents \\nInstallation and Setup\\nWrappers\\nLLM\\nGooseAI#\\nThis page covers how to use the GooseAI ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific GooseAI wrappers.\\nInstallation and Setup#\\nInstall the Python SDK with pip install openai\\nGet your GooseAI api key from this link here.\\nSet the environment variable (GOOSEAI_API_KEY).\\nimport os\\nos.environ[\"GOOSEAI_API_KEY\"] = \"YOUR_API_KEY\"\\nWrappers#\\nLLM#\\nThere exists an GooseAI LLM wrapper, which you can access with:\\nfrom langchain.llms import GooseAI\\nprevious\\nGoogle Serper Wrapper\\nnext\\nGPT4All\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nLLM\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\gooseai.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nGPT4All\\n Contents \\nInstallation and Setup\\nUsage\\nGPT4All\\nModel File\\nGPT4All#\\nThis page covers how to use the GPT4All wrapper within LangChain. The tutorial is divided into two parts: installation and setup, followed by usage with an example.\\nInstallation and Setup#\\nInstall the Python package with pip install pyllamacpp\\nDownload a GPT4All model and place it in your desired directory\\nUsage#\\nGPT4All#\\nTo use the GPT4All wrapper, you need to provide the path to the pre-trained model file and the model’s configuration.\\nfrom langchain.llms import GPT4All\\n# Instantiate the model. Callbacks support token-wise streaming\\nmodel = GPT4All(model=\"./models/gpt4all-model.bin\", n_ctx=512, n_threads=8)\\n# Generate text\\nresponse = model(\"Once upon a time, \")\\nYou can also customize the generation parameters, such as n_predict, temp, top_p, top_k, and others.\\nTo stream the model’s predictions, add in a CallbackManager.\\nfrom langchain.llms import GPT4All\\nfrom langchain.callbacks.base import CallbackManager\\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\\n# There are many CallbackHandlers supported, such as\\n# from langchain.callbacks.streamlit import StreamlitCallbackHandler\\ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\\nmodel = GPT4All(model=\"./models/gpt4all-model.bin\", n_ctx=512, n_threads=8, callback_handler=callback_handler, verbose=True)\\n# Generate text. Tokens are streamed through the callback manager.\\nmodel(\"Once upon a time, \")\\nModel File#\\nYou can find links to model file downloads in the pyllamacpp repository.\\nFor a more detailed walkthrough of this, see this notebook\\nprevious\\nGooseAI\\nnext\\nGraphsignal\\n Contents\\n  \\nInstallation and Setup\\nUsage\\nGPT4All\\nModel File\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\gpt4all.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nGraphsignal\\n Contents \\nInstallation and Setup\\nTracing and Monitoring\\nGraphsignal#\\nThis page covers how to use Graphsignal to trace and monitor LangChain. Graphsignal enables full visibility into your application. It provides latency breakdowns by chains and tools, exceptions with full context, data monitoring, compute/GPU utilization, OpenAI cost analytics, and more.\\nInstallation and Setup#\\nInstall the Python library with pip install graphsignal\\nCreate free Graphsignal account here\\nGet an API key and set it as an environment variable (GRAPHSIGNAL_API_KEY)\\nTracing and Monitoring#\\nGraphsignal automatically instruments and starts tracing and monitoring chains. Traces and metrics are then available in your Graphsignal dashboards.\\nInitialize the tracer by providing a deployment name:\\nimport graphsignal\\ngraphsignal.configure(deployment=\\'my-langchain-app-prod\\')\\nTo additionally trace any function or code, you can use a decorator or a context manager:\\n@graphsignal.trace_function\\ndef handle_request():    \\n    chain.run(\"some initial text\")\\nwith graphsignal.start_trace(\\'my-chain\\'):\\n    chain.run(\"some initial text\")\\nOptionally, enable profiling to record function-level statistics for each trace.\\nwith graphsignal.start_trace(\\n        \\'my-chain\\', options=graphsignal.TraceOptions(enable_profiling=True)):\\n    chain.run(\"some initial text\")\\nSee the Quick Start guide for complete setup instructions.\\nprevious\\nGPT4All\\nnext\\nHazy Research\\n Contents\\n  \\nInstallation and Setup\\nTracing and Monitoring\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\graphsignal.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nHazy Research\\n Contents \\nInstallation and Setup\\nWrappers\\nLLM\\nHazy Research#\\nThis page covers how to use the Hazy Research ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Hazy Research wrappers.\\nInstallation and Setup#\\nTo use the manifest, install it with pip install manifest-ml\\nWrappers#\\nLLM#\\nThere exists an LLM wrapper around Hazy Research’s manifest library.\\nmanifest is a python library which is itself a wrapper around many model providers, and adds in caching, history, and more.\\nTo use this wrapper:\\nfrom langchain.llms.manifest import ManifestWrapper\\nprevious\\nGraphsignal\\nnext\\nHelicone\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nLLM\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\hazy_research.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nHelicone\\n Contents \\nWhat is Helicone?\\nQuick start\\nHow to enable Helicone caching\\nHow to use Helicone custom properties\\nHelicone#\\nThis page covers how to use the Helicone ecosystem within LangChain.\\nWhat is Helicone?#\\nHelicone is an open source observability platform that proxies your OpenAI traffic and provides you key insights into your spend, latency and usage.\\nQuick start#\\nWith your LangChain environment you can just add the following parameter.\\nexport OPENAI_API_BASE=\"https://oai.hconeai.com/v1\"\\nNow head over to helicone.ai to create your account, and add your OpenAI API key within our dashboard to view your logs.\\nHow to enable Helicone caching#\\nfrom langchain.llms import OpenAI\\nimport openai\\nopenai.api_base = \"https://oai.hconeai.com/v1\"\\nllm = OpenAI(temperature=0.9, headers={\"Helicone-Cache-Enabled\": \"true\"})\\ntext = \"What is a helicone?\"\\nprint(llm(text))\\nHelicone caching docs\\nHow to use Helicone custom properties#\\nfrom langchain.llms import OpenAI\\nimport openai\\nopenai.api_base = \"https://oai.hconeai.com/v1\"\\nllm = OpenAI(temperature=0.9, headers={\\n        \"Helicone-Property-Session\": \"24\",\\n        \"Helicone-Property-Conversation\": \"support_issue_2\",\\n        \"Helicone-Property-App\": \"mobile\",\\n      })\\ntext = \"What is a helicone?\"\\nprint(llm(text))\\nHelicone property docs\\nprevious\\nHazy Research\\nnext\\nHugging Face\\n Contents\\n  \\nWhat is Helicone?\\nQuick start\\nHow to enable Helicone caching\\nHow to use Helicone custom properties\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\helicone.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nHugging Face\\n Contents \\nInstallation and Setup\\nWrappers\\nLLM\\nEmbeddings\\nTokenizer\\nDatasets\\nHugging Face#\\nThis page covers how to use the Hugging Face ecosystem (including the Hugging Face Hub) within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Hugging Face wrappers.\\nInstallation and Setup#\\nIf you want to work with the Hugging Face Hub:\\nInstall the Hub client library with pip install huggingface_hub\\nCreate a Hugging Face account (it’s free!)\\nCreate an access token and set it as an environment variable (HUGGINGFACEHUB_API_TOKEN)\\nIf you want work with the Hugging Face Python libraries:\\nInstall pip install transformers for working with models and tokenizers\\nInstall pip install datasets for working with datasets\\nWrappers#\\nLLM#\\nThere exists two Hugging Face LLM wrappers, one for a local pipeline and one for a model hosted on Hugging Face Hub.\\nNote that these wrappers only work for models that support the following tasks: text2text-generation, text-generation\\nTo use the local pipeline wrapper:\\nfrom langchain.llms import HuggingFacePipeline\\nTo use a the wrapper for a model hosted on Hugging Face Hub:\\nfrom langchain.llms import HuggingFaceHub\\nFor a more detailed walkthrough of the Hugging Face Hub wrapper, see this notebook\\nEmbeddings#\\nThere exists two Hugging Face Embeddings wrappers, one for a local model and one for a model hosted on Hugging Face Hub.\\nNote that these wrappers only work for sentence-transformers models.\\nTo use the local pipeline wrapper:\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\nTo use a the wrapper for a model hosted on Hugging Face Hub:\\nfrom langchain.embeddings import HuggingFaceHubEmbeddings\\nFor a more detailed walkthrough of this, see this notebook\\nTokenizer#\\nThere are several places you can use tokenizers available through the transformers package.\\nBy default, it is used to count tokens for all LLMs.\\nYou can also use it to count tokens when splitting documents with\\nfrom langchain.text_splitter import CharacterTextSplitter\\nCharacterTextSplitter.from_huggingface_tokenizer(...)\\nFor a more detailed walkthrough of this, see this notebook\\nDatasets#\\nThe Hugging Face Hub has lots of great datasets that can be used to evaluate your LLM chains.\\nFor a detailed walkthrough of how to use them to do so, see this notebook\\nprevious\\nHelicone\\nnext\\nJina\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nLLM\\nEmbeddings\\nTokenizer\\nDatasets\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\huggingface.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nJina\\n Contents \\nInstallation and Setup\\nWrappers\\nEmbeddings\\nJina#\\nThis page covers how to use the Jina ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Jina wrappers.\\nInstallation and Setup#\\nInstall the Python SDK with pip install jina\\nGet a Jina AI Cloud auth token from here and set it as an environment variable (JINA_AUTH_TOKEN)\\nWrappers#\\nEmbeddings#\\nThere exists a Jina Embeddings wrapper, which you can access with\\nfrom langchain.embeddings import JinaEmbeddings\\nFor a more detailed walkthrough of this, see this notebook\\nprevious\\nHugging Face\\nnext\\nLlama.cpp\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nEmbeddings\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\jina.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nLlama.cpp\\n Contents \\nInstallation and Setup\\nWrappers\\nLLM\\nEmbeddings\\nLlama.cpp#\\nThis page covers how to use llama.cpp within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Llama-cpp wrappers.\\nInstallation and Setup#\\nInstall the Python package with pip install llama-cpp-python\\nDownload one of the supported models and convert them to the llama.cpp format per the instructions\\nWrappers#\\nLLM#\\nThere exists a LlamaCpp LLM wrapper, which you can access with\\nfrom langchain.llms import LlamaCpp\\nFor a more detailed walkthrough of this, see this notebook\\nEmbeddings#\\nThere exists a LlamaCpp Embeddings wrapper, which you can access with\\nfrom langchain.embeddings import LlamaCppEmbeddings\\nFor a more detailed walkthrough of this, see this notebook\\nprevious\\nJina\\nnext\\nMilvus\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nLLM\\nEmbeddings\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\llamacpp.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nMilvus\\n Contents \\nInstallation and Setup\\nWrappers\\nVectorStore\\nMilvus#\\nThis page covers how to use the Milvus ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Milvus wrappers.\\nInstallation and Setup#\\nInstall the Python SDK with pip install pymilvus\\nWrappers#\\nVectorStore#\\nThere exists a wrapper around Milvus indexes, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\nTo import this vectorstore:\\nfrom langchain.vectorstores import Milvus\\nFor a more detailed walkthrough of the Miluvs wrapper, see this notebook\\nprevious\\nLlama.cpp\\nnext\\nModal\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nVectorStore\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\milvus.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nModal\\n Contents \\nInstallation and Setup\\nDefine your Modal Functions and Webhooks\\nWrappers\\nLLM\\nModal#\\nThis page covers how to use the Modal ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Modal wrappers.\\nInstallation and Setup#\\nInstall with pip install modal-client\\nRun modal token new\\nDefine your Modal Functions and Webhooks#\\nYou must include a prompt. There is a rigid response structure.\\nclass Item(BaseModel):\\n    prompt: str\\n@stub.webhook(method=\"POST\")\\ndef my_webhook(item: Item):\\n    return {\"prompt\": my_function.call(item.prompt)}\\nAn example with GPT2:\\nfrom pydantic import BaseModel\\nimport modal\\nstub = modal.Stub(\"example-get-started\")\\nvolume = modal.SharedVolume().persist(\"gpt2_model_vol\")\\nCACHE_PATH = \"/root/model_cache\"\\n@stub.function(\\n    gpu=\"any\",\\n    image=modal.Image.debian_slim().pip_install(\\n        \"tokenizers\", \"transformers\", \"torch\", \"accelerate\"\\n    ),\\n    shared_volumes={CACHE_PATH: volume},\\n    retries=3,\\n)\\ndef run_gpt2(text: str):\\n    from transformers import GPT2Tokenizer, GPT2LMHeadModel\\n    tokenizer = GPT2Tokenizer.from_pretrained(\\'gpt2\\')\\n    model = GPT2LMHeadModel.from_pretrained(\\'gpt2\\')\\n    encoded_input = tokenizer(text, return_tensors=\\'pt\\').input_ids\\n    output = model.generate(encoded_input, max_length=50, do_sample=True)\\n    return tokenizer.decode(output[0], skip_special_tokens=True)\\nclass Item(BaseModel):\\n    prompt: str\\n@stub.webhook(method=\"POST\")\\ndef get_text(item: Item):\\n    return {\"prompt\": run_gpt2.call(item.prompt)}\\nWrappers#\\nLLM#\\nThere exists an Modal LLM wrapper, which you can access with\\nfrom langchain.llms import Modal\\nprevious\\nMilvus\\nnext\\nNLPCloud\\n Contents\\n  \\nInstallation and Setup\\nDefine your Modal Functions and Webhooks\\nWrappers\\nLLM\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\modal.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nNLPCloud\\n Contents \\nInstallation and Setup\\nWrappers\\nLLM\\nNLPCloud#\\nThis page covers how to use the NLPCloud ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific NLPCloud wrappers.\\nInstallation and Setup#\\nInstall the Python SDK with pip install nlpcloud\\nGet an NLPCloud api key and set it as an environment variable (NLPCLOUD_API_KEY)\\nWrappers#\\nLLM#\\nThere exists an NLPCloud LLM wrapper, which you can access with\\nfrom langchain.llms import NLPCloud\\nprevious\\nModal\\nnext\\nOpenAI\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nLLM\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\nlpcloud.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nOpenAI\\n Contents \\nInstallation and Setup\\nWrappers\\nLLM\\nEmbeddings\\nTokenizer\\nModeration\\nOpenAI#\\nThis page covers how to use the OpenAI ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific OpenAI wrappers.\\nInstallation and Setup#\\nInstall the Python SDK with pip install openai\\nGet an OpenAI api key and set it as an environment variable (OPENAI_API_KEY)\\nIf you want to use OpenAI’s tokenizer (only available for Python 3.9+), install it with pip install tiktoken\\nWrappers#\\nLLM#\\nThere exists an OpenAI LLM wrapper, which you can access with\\nfrom langchain.llms import OpenAI\\nIf you are using a model hosted on Azure, you should use different wrapper for that:\\nfrom langchain.llms import AzureOpenAI\\nFor a more detailed walkthrough of the Azure wrapper, see this notebook\\nEmbeddings#\\nThere exists an OpenAI Embeddings wrapper, which you can access with\\nfrom langchain.embeddings import OpenAIEmbeddings\\nFor a more detailed walkthrough of this, see this notebook\\nTokenizer#\\nThere are several places you can use the tiktoken tokenizer. By default, it is used to count tokens\\nfor OpenAI LLMs.\\nYou can also use it to count tokens when splitting documents with\\nfrom langchain.text_splitter import CharacterTextSplitter\\nCharacterTextSplitter.from_tiktoken_encoder(...)\\nFor a more detailed walkthrough of this, see this notebook\\nModeration#\\nYou can also access the OpenAI content moderation endpoint with\\nfrom langchain.chains import OpenAIModerationChain\\nFor a more detailed walkthrough of this, see this notebook\\nprevious\\nNLPCloud\\nnext\\nOpenSearch\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nLLM\\nEmbeddings\\nTokenizer\\nModeration\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\openai.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nOpenSearch\\n Contents \\nInstallation and Setup\\nWrappers\\nVectorStore\\nOpenSearch#\\nThis page covers how to use the OpenSearch ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific OpenSearch wrappers.\\nInstallation and Setup#\\nInstall the Python package with pip install opensearch-py\\nWrappers#\\nVectorStore#\\nThere exists a wrapper around OpenSearch vector databases, allowing you to use it as a vectorstore\\nfor semantic search using approximate vector search powered by lucene, nmslib and faiss engines\\nor using painless scripting and script scoring functions for bruteforce vector search.\\nTo import this vectorstore:\\nfrom langchain.vectorstores import OpenSearchVectorSearch\\nFor a more detailed walkthrough of the OpenSearch wrapper, see this notebook\\nprevious\\nOpenAI\\nnext\\nPetals\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nVectorStore\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\opensearch.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nPetals\\n Contents \\nInstallation and Setup\\nWrappers\\nLLM\\nPetals#\\nThis page covers how to use the Petals ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Petals wrappers.\\nInstallation and Setup#\\nInstall with pip install petals\\nGet a Hugging Face api key and set it as an environment variable (HUGGINGFACE_API_KEY)\\nWrappers#\\nLLM#\\nThere exists an Petals LLM wrapper, which you can access with\\nfrom langchain.llms import Petals\\nprevious\\nOpenSearch\\nnext\\nPGVector\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nLLM\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\petals.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nPGVector\\n Contents \\nInstallation\\nSetup\\nWrappers\\nVectorStore\\nUsage\\nPGVector#\\nThis page covers how to use the Postgres PGVector ecosystem within LangChain\\nIt is broken into two parts: installation and setup, and then references to specific PGVector wrappers.\\nInstallation#\\nInstall the Python package with pip install pgvector\\nSetup#\\nThe first step is to create a database with the pgvector extension installed.\\nFollow the steps at PGVector Installation Steps to install the database and the extension. The docker image is the easiest way to get started.\\nWrappers#\\nVectorStore#\\nThere exists a wrapper around Postgres vector databases, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\nTo import this vectorstore:\\nfrom langchain.vectorstores.pgvector import PGVector\\nUsage#\\nFor a more detailed walkthrough of the PGVector Wrapper, see this notebook\\nprevious\\nPetals\\nnext\\nPinecone\\n Contents\\n  \\nInstallation\\nSetup\\nWrappers\\nVectorStore\\nUsage\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\pgvector.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nPinecone\\n Contents \\nInstallation and Setup\\nWrappers\\nVectorStore\\nPinecone#\\nThis page covers how to use the Pinecone ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Pinecone wrappers.\\nInstallation and Setup#\\nInstall the Python SDK with pip install pinecone-client\\nWrappers#\\nVectorStore#\\nThere exists a wrapper around Pinecone indexes, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\nTo import this vectorstore:\\nfrom langchain.vectorstores import Pinecone\\nFor a more detailed walkthrough of the Pinecone wrapper, see this notebook\\nprevious\\nPGVector\\nnext\\nPromptLayer\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nVectorStore\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\pinecone.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nPromptLayer\\n Contents \\nInstallation and Setup\\nWrappers\\nLLM\\nPromptLayer#\\nThis page covers how to use PromptLayer within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific PromptLayer wrappers.\\nInstallation and Setup#\\nIf you want to work with PromptLayer:\\nInstall the promptlayer python library pip install promptlayer\\nCreate a PromptLayer account\\nCreate an api token and set it as an environment variable (PROMPTLAYER_API_KEY)\\nWrappers#\\nLLM#\\nThere exists an PromptLayer OpenAI LLM wrapper, which you can access with\\nfrom langchain.llms import PromptLayerOpenAI\\nTo tag your requests, use the argument pl_tags when instanializing the LLM\\nfrom langchain.llms import PromptLayerOpenAI\\nllm = PromptLayerOpenAI(pl_tags=[\"langchain-requests\", \"chatbot\"])\\nTo get the PromptLayer request id, use the argument return_pl_id when instanializing the LLM\\nfrom langchain.llms import PromptLayerOpenAI\\nllm = PromptLayerOpenAI(return_pl_id=True)\\nThis will add the PromptLayer request ID in the generation_info field of the Generation returned when using .generate or .agenerate\\nFor example:\\nllm_results = llm.generate([\"hello world\"])\\nfor res in llm_results.generations:\\n    print(\"pl request id: \", res[0].generation_info[\"pl_request_id\"])\\nYou can use the PromptLayer request ID to add a prompt, score, or other metadata to your request. Read more about it here.\\nThis LLM is identical to the OpenAI LLM, except that\\nall your requests will be logged to your PromptLayer account\\nyou can add pl_tags when instantializing to tag your requests on PromptLayer\\nyou can add return_pl_id when instantializing to return a PromptLayer request id to use while tracking requests.\\nPromptLayer also provides native wrappers for PromptLayerChatOpenAI and PromptLayerOpenAIChat\\nprevious\\nPinecone\\nnext\\nQdrant\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nLLM\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\promptlayer.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nQdrant\\n Contents \\nInstallation and Setup\\nWrappers\\nVectorStore\\nQdrant#\\nThis page covers how to use the Qdrant ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Qdrant wrappers.\\nInstallation and Setup#\\nInstall the Python SDK with pip install qdrant-client\\nWrappers#\\nVectorStore#\\nThere exists a wrapper around Qdrant indexes, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\nTo import this vectorstore:\\nfrom langchain.vectorstores import Qdrant\\nFor a more detailed walkthrough of the Qdrant wrapper, see this notebook\\nprevious\\nPromptLayer\\nnext\\nReplicate\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nVectorStore\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\qdrant.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nReplicate\\n Contents \\nInstallation and Setup\\nCalling a model\\nReplicate#\\nThis page covers how to run models on Replicate within LangChain.\\nInstallation and Setup#\\nCreate a Replicate account. Get your API key and set it as an environment variable (REPLICATE_API_TOKEN)\\nInstall the Replicate python client with pip install replicate\\nCalling a model#\\nFind a model on the Replicate explore page, and then paste in the model name and version in this format: owner-name/model-name:version\\nFor example, for this flan-t5 model, click on the API tab. The model name/version would be: daanelson/flan-t5:04e422a9b85baed86a4f24981d7f9953e20c5fd82f6103b74ebc431588e1cec8\\nOnly the model param is required, but any other model parameters can also be passed in with the format input={model_param: value, ...}\\nFor example, if we were running stable diffusion and wanted to change the image dimensions:\\nReplicate(model=\"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6dce16bb082a930b0c49861f96d1e5bf\", input={\\'image_dimensions\\': \\'512x512\\'})\\nNote that only the first output of a model will be returned.\\nFrom here, we can initialize our model:\\nllm = Replicate(model=\"daanelson/flan-t5:04e422a9b85baed86a4f24981d7f9953e20c5fd82f6103b74ebc431588e1cec8\")\\nAnd run it:\\nprompt = \"\"\"\\nAnswer the following yes/no question by reasoning step by step.\\nCan a dog drive a car?\\n\"\"\"\\nllm(prompt)\\nWe can call any Replicate model (not just LLMs) using this syntax. For example, we can call Stable Diffusion:\\ntext2image = Replicate(model=\"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6dce16bb082a930b0c49861f96d1e5bf\",\\n                       input={\\'image_dimensions\\'=\\'512x512\\'}\\nimage_output = text2image(\"A cat riding a motorcycle by Picasso\")\\nprevious\\nQdrant\\nnext\\nRunhouse\\n Contents\\n  \\nInstallation and Setup\\nCalling a model\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\replicate.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nRunhouse\\n Contents \\nInstallation and Setup\\nSelf-hosted LLMs\\nSelf-hosted Embeddings\\nRunhouse#\\nThis page covers how to use the Runhouse ecosystem within LangChain.\\nIt is broken into three parts: installation and setup, LLMs, and Embeddings.\\nInstallation and Setup#\\nInstall the Python SDK with pip install runhouse\\nIf you’d like to use on-demand cluster, check your cloud credentials with sky check\\nSelf-hosted LLMs#\\nFor a basic self-hosted LLM, you can use the SelfHostedHuggingFaceLLM class. For more\\ncustom LLMs, you can use the SelfHostedPipeline parent class.\\nfrom langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLM\\nFor a more detailed walkthrough of the Self-hosted LLMs, see this notebook\\nSelf-hosted Embeddings#\\nThere are several ways to use self-hosted embeddings with LangChain via Runhouse.\\nFor a basic self-hosted embedding from a Hugging Face Transformers model, you can use\\nthe SelfHostedEmbedding class.\\nfrom langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLM\\nFor a more detailed walkthrough of the Self-hosted Embeddings, see this notebook\\nprevious\\nReplicate\\nnext\\nRWKV-4\\n Contents\\n  \\nInstallation and Setup\\nSelf-hosted LLMs\\nSelf-hosted Embeddings\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\runhouse.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nRWKV-4\\n Contents \\nInstallation and Setup\\nUsage\\nRWKV\\nModel File\\nRwkv-4 models -> recommended VRAM\\nRWKV-4#\\nThis page covers how to use the RWKV-4 wrapper within LangChain.\\nIt is broken into two parts: installation and setup, and then usage with an example.\\nInstallation and Setup#\\nInstall the Python package with pip install rwkv\\nInstall the tokenizer Python package with pip install tokenizer\\nDownload a RWKV model and place it in your desired directory\\nDownload the tokens file\\nUsage#\\nRWKV#\\nTo use the RWKV wrapper, you need to provide the path to the pre-trained model file and the tokenizer’s configuration.\\nfrom langchain.llms import RWKV\\n# Test the model\\n```python\\ndef generate_prompt(instruction, input=None):\\n    if input:\\n        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n# Instruction:\\n{instruction}\\n# Input:\\n{input}\\n# Response:\\n\"\"\"\\n    else:\\n        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n# Instruction:\\n{instruction}\\n# Response:\\n\"\"\"\\nmodel = RWKV(model=\"./models/RWKV-4-Raven-3B-v7-Eng-20230404-ctx4096.pth\", strategy=\"cpu fp32\", tokens_path=\"./rwkv/20B_tokenizer.json\")\\nresponse = model(generate_prompt(\"Once upon a time, \"))\\nModel File#\\nYou can find links to model file downloads at the RWKV-4-Raven repository.\\nRwkv-4 models -> recommended VRAM#\\nRWKV VRAM\\nModel | 8bit | bf16/fp16 | fp32\\n14B   | 16GB | 28GB      | >50GB\\n7B    | 8GB  | 14GB      | 28GB\\n3B    | 2.8GB| 6GB       | 12GB\\n1b5   | 1.3GB| 3GB       | 6GB\\nSee the rwkv pip page for more information about strategies, including streaming and cuda support.\\nprevious\\nRunhouse\\nnext\\nSearxNG Search API\\n Contents\\n  \\nInstallation and Setup\\nUsage\\nRWKV\\nModel File\\nRwkv-4 models -> recommended VRAM\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\rwkv.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nSearxNG Search API\\n Contents \\nInstallation and Setup\\nSelf Hosted Instance:\\nWrappers\\nUtility\\nTool\\nSearxNG Search API#\\nThis page covers how to use the SearxNG search API within LangChain.\\nIt is broken into two parts: installation and setup, and then references to the specific SearxNG API wrapper.\\nInstallation and Setup#\\nWhile it is possible to utilize the wrapper in conjunction with  public searx\\ninstances these instances frequently do not permit API\\naccess (see note on output format below) and have limitations on the frequency\\nof requests. It is recommended to opt for a self-hosted instance instead.\\nSelf Hosted Instance:#\\nSee this page for installation instructions.\\nWhen you install SearxNG, the only active output format by default is the HTML format.\\nYou need to activate the json format to use the API. This can be done by adding the following line to the settings.yml file:\\nsearch:\\n    formats:\\n        - html\\n        - json\\nYou can make sure that the API is working by issuing a curl request to the API endpoint:\\ncurl -kLX GET --data-urlencode q=\\'langchain\\' -d format=json http://localhost:8888\\nThis should return a JSON object with the results.\\nWrappers#\\nUtility#\\nTo use the wrapper we need to pass the host of the SearxNG instance to the wrapper with:\\n1. the named parameter searx_host when creating the instance.\\n2. exporting the environment variable SEARXNG_HOST.\\nYou can use the wrapper to get results from a SearxNG instance.\\nfrom langchain.utilities import SearxSearchWrapper\\ns = SearxSearchWrapper(searx_host=\"http://localhost:8888\")\\ns.run(\"what is a large language model?\")\\nTool#\\nYou can also load this wrapper as a Tool (to use with an Agent).\\nYou can do this with:\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"searx-search\"],\\n                    searx_host=\"http://localhost:8888\",\\n                    engines=[\"github\"])\\nNote that we could optionally pass custom engines to use.\\nIf you want to obtain results with metadata as json you can use:\\ntools = load_tools([\"searx-search-results-json\"],\\n                    searx_host=\"http://localhost:8888\",\\n                    num_results=5)\\nFor more information on tools, see this page\\nprevious\\nRWKV-4\\nnext\\nSerpAPI\\n Contents\\n  \\nInstallation and Setup\\nSelf Hosted Instance:\\nWrappers\\nUtility\\nTool\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\searx.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nSerpAPI\\n Contents \\nInstallation and Setup\\nWrappers\\nUtility\\nTool\\nSerpAPI#\\nThis page covers how to use the SerpAPI search APIs within LangChain.\\nIt is broken into two parts: installation and setup, and then references to the specific SerpAPI wrapper.\\nInstallation and Setup#\\nInstall requirements with pip install google-search-results\\nGet a SerpAPI api key and either set it as an environment variable (SERPAPI_API_KEY)\\nWrappers#\\nUtility#\\nThere exists a SerpAPI utility which wraps this API. To import this utility:\\nfrom langchain.utilities import SerpAPIWrapper\\nFor a more detailed walkthrough of this wrapper, see this notebook.\\nTool#\\nYou can also easily load this wrapper as a Tool (to use with an Agent).\\nYou can do this with:\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"serpapi\"])\\nFor more information on this, see this page\\nprevious\\nSearxNG Search API\\nnext\\nStochasticAI\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nUtility\\nTool\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\serpapi.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nStochasticAI\\n Contents \\nInstallation and Setup\\nWrappers\\nLLM\\nStochasticAI#\\nThis page covers how to use the StochasticAI ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific StochasticAI wrappers.\\nInstallation and Setup#\\nInstall with pip install stochasticx\\nGet an StochasticAI api key and set it as an environment variable (STOCHASTICAI_API_KEY)\\nWrappers#\\nLLM#\\nThere exists an StochasticAI LLM wrapper, which you can access with\\nfrom langchain.llms import StochasticAI\\nprevious\\nSerpAPI\\nnext\\nUnstructured\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nLLM\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\stochasticai.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nUnstructured\\n Contents \\nInstallation and Setup\\nWrappers\\nData Loaders\\nUnstructured#\\nThis page covers how to use the unstructured\\necosystem within LangChain. The unstructured package from\\nUnstructured.IO extracts clean text from raw source documents like\\nPDFs and Word documents.\\nThis page is broken into two parts: installation and setup, and then references to specific\\nunstructured wrappers.\\nInstallation and Setup#\\nInstall the Python SDK with pip install \"unstructured[local-inference]\"\\nInstall the following system dependencies if they are not already available on your system.\\nDepending on what document types you’re parsing, you may not need all of these.\\nlibmagic-dev (filetype detection)\\npoppler-utils (images and PDFs)\\ntesseract-ocr(images and PDFs)\\nlibreoffice (MS Office docs)\\npandoc (EPUBs)\\nIf you are parsing PDFs using the \"hi_res\" strategy, run the following to install the detectron2 model, which\\nunstructured uses for layout detection:\\npip install \"detectron2@git+https://github.com/facebookresearch/detectron2.git@e2ce8dc#egg=detectron2\"\\nIf detectron2 is not installed, unstructured will fallback to processing PDFs\\nusing the \"fast\" strategy, which uses pdfminer directly and doesn’t require\\ndetectron2.\\nWrappers#\\nData Loaders#\\nThe primary unstructured wrappers within langchain are data loaders. The following\\nshows how to use the most basic unstructured data loader. There are other file-specific\\ndata loaders available in the langchain.document_loaders module.\\nfrom langchain.document_loaders import UnstructuredFileLoader\\nloader = UnstructuredFileLoader(\"state_of_the_union.txt\")\\nloader.load()\\nIf you instantiate the loader with UnstructuredFileLoader(mode=\"elements\"), the loader\\nwill track additional metadata like the page number and text type (i.e. title, narrative text)\\nwhen that information is available.\\nprevious\\nStochasticAI\\nnext\\nWeights & Biases\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nData Loaders\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\unstructured.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nWeights & Biases\\nWeights & Biases#\\nThis notebook goes over how to track your LangChain experiments into one centralized Weights and Biases dashboard. To learn more about prompt engineering and the callback please refer to this Report which explains both alongside the resultant dashboards you can expect to see.\\nRun in Colab: https://colab.research.google.com/drive/1DXH4beT4HFaRKy_Vm4PoxhXVDRf7Ym8L?usp=sharing\\nView Report: https://wandb.ai/a-sh0ts/langchain_callback_demo/reports/Prompt-Engineering-LLMs-with-LangChain-and-W-B–VmlldzozNjk1NTUw#👋-how-to-build-a-callback-in-langchain-for-better-prompt-engineering\\n!pip install wandb\\n!pip install pandas\\n!pip install textstat\\n!pip install spacy\\n!python -m spacy download en_core_web_sm\\nimport os\\nos.environ[\"WANDB_API_KEY\"] = \"\"\\n# os.environ[\"OPENAI_API_KEY\"] = \"\"\\n# os.environ[\"SERPAPI_API_KEY\"] = \"\"\\nfrom datetime import datetime\\nfrom langchain.callbacks import WandbCallbackHandler, StdOutCallbackHandler\\nfrom langchain.callbacks.base import CallbackManager\\nfrom langchain.llms import OpenAI\\nCallback Handler that logs to Weights and Biases.\\nParameters:\\n    job_type (str): The type of job.\\n    project (str): The project to log to.\\n    entity (str): The entity to log to.\\n    tags (list): The tags to log.\\n    group (str): The group to log to.\\n    name (str): The name of the run.\\n    notes (str): The notes to log.\\n    visualize (bool): Whether to visualize the run.\\n    complexity_metrics (bool): Whether to log complexity metrics.\\n    stream_logs (bool): Whether to stream callback actions to W&B\\nDefault values for WandbCallbackHandler(...)\\nvisualize: bool = False,\\ncomplexity_metrics: bool = False,\\nstream_logs: bool = False,\\nNOTE: For beta workflows we have made the default analysis based on textstat and the visualizations based on spacy\\n\"\"\"Main function.\\nThis function is used to try the callback handler.\\nScenarios:\\n1. OpenAI LLM\\n2. Chain with multiple SubChains on multiple generations\\n3. Agent with Tools\\n\"\"\"\\nsession_group = datetime.now().strftime(\"%m.%d.%Y_%H.%M.%S\")\\nwandb_callback = WandbCallbackHandler(\\n    job_type=\"inference\",\\n    project=\"langchain_callback_demo\",\\n    group=f\"minimal_{session_group}\",\\n    name=\"llm\",\\n    tags=[\"test\"],\\n)\\nmanager = CallbackManager([StdOutCallbackHandler(), wandb_callback])\\nllm = OpenAI(temperature=0, callback_manager=manager, verbose=True)\\nwandb: Currently logged in as: harrison-chase. Use `wandb login --relogin` to force relogin\\nTracking run with wandb version 0.14.0Run data is saved locally in /Users/harrisonchase/workplace/langchain/docs/ecosystem/wandb/run-20230318_150408-e47j1914Syncing run llm to Weights & Biases (docs) View project at https://wandb.ai/harrison-chase/langchain_callback_demo View run at https://wandb.ai/harrison-chase/langchain_callback_demo/runs/e47j1914wandb: WARNING The wandb callback is currently in beta and is subject to change based on updates to `langchain`. Please report any issues to https://github.com/wandb/wandb/issues with the tag `langchain`.\\n# Defaults for WandbCallbackHandler.flush_tracker(...)\\nreset: bool = True,\\nfinish: bool = False,\\nThe flush_tracker function is used to log LangChain sessions to Weights & Biases. It takes in the LangChain module or agent, and logs at minimum the prompts and generations alongside the serialized form of the LangChain module to the specified Weights & Biases project. By default we reset the session as opposed to concluding the session outright.\\n# SCENARIO 1 - LLM\\nllm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"] * 3)\\nwandb_callback.flush_tracker(llm, name=\"simple_sequential\")\\nWaiting for W&B process to finish... (success). View run llm at: https://wandb.ai/harrison-chase/langchain_callback_demo/runs/e47j1914Synced 5 W&B file(s), 2 media file(s), 5 artifact file(s) and 0 other file(s)Find logs at: ./wandb/run-20230318_150408-e47j1914/logsTracking run with wandb version 0.14.0Run data is saved locally in /Users/harrisonchase/workplace/langchain/docs/ecosystem/wandb/run-20230318_150534-jyxma7huSyncing run simple_sequential to Weights & Biases (docs) View project at https://wandb.ai/harrison-chase/langchain_callback_demo View run at https://wandb.ai/harrison-chase/langchain_callback_demo/runs/jyxma7hu\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\n# SCENARIO 2 - Chain\\ntemplate = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\\nTitle: {title}\\nPlaywright: This is a synopsis for the above play:\"\"\"\\nprompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\\nsynopsis_chain = LLMChain(llm=llm, prompt=prompt_template, callback_manager=manager)\\ntest_prompts = [\\n    {\\n        \"title\": \"documentary about good video games that push the boundary of game design\"\\n    },\\n    {\"title\": \"cocaine bear vs heroin wolf\"},\\n    {\"title\": \"the best in class mlops tooling\"},\\n]\\nsynopsis_chain.apply(test_prompts)\\nwandb_callback.flush_tracker(synopsis_chain, name=\"agent\")\\nWaiting for W&B process to finish... (success). View run simple_sequential at: https://wandb.ai/harrison-chase/langchain_callback_demo/runs/jyxma7huSynced 4 W&B file(s), 2 media file(s), 6 artifact file(s) and 0 other file(s)Find logs at: ./wandb/run-20230318_150534-jyxma7hu/logsTracking run with wandb version 0.14.0Run data is saved locally in /Users/harrisonchase/workplace/langchain/docs/ecosystem/wandb/run-20230318_150550-wzy59zjqSyncing run agent to Weights & Biases (docs) View project at https://wandb.ai/harrison-chase/langchain_callback_demo View run at https://wandb.ai/harrison-chase/langchain_callback_demo/runs/wzy59zjq\\nfrom langchain.agents import initialize_agent, load_tools\\nfrom langchain.agents import AgentType\\n# SCENARIO 3 - Agent with Tools\\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, callback_manager=manager)\\nagent = initialize_agent(\\n    tools,\\n    llm,\\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\\n    callback_manager=manager,\\n    verbose=True,\\n)\\nagent.run(\\n    \"Who is Leo DiCaprio\\'s girlfriend? What is her current age raised to the 0.43 power?\"\\n)\\nwandb_callback.flush_tracker(agent, reset=False, finish=True)\\n> Entering new AgentExecutor chain...\\n I need to find out who Leo DiCaprio\\'s girlfriend is and then calculate her age raised to the 0.43 power.\\nAction: Search\\nAction Input: \"Leo DiCaprio girlfriend\"\\nObservation: DiCaprio had a steady girlfriend in Camila Morrone. He had been with the model turned actress for nearly five years, as they were first said to be dating at the end of 2017. And the now 26-year-old Morrone is no stranger to Hollywood.\\nThought: I need to calculate her age raised to the 0.43 power.\\nAction: Calculator\\nAction Input: 26^0.43\\nObservation: Answer: 4.059182145592686\\nThought: I now know the final answer.\\nFinal Answer: Leo DiCaprio\\'s girlfriend is Camila Morrone and her current age raised to the 0.43 power is 4.059182145592686.\\n> Finished chain.\\nWaiting for W&B process to finish... (success). View run agent at: https://wandb.ai/harrison-chase/langchain_callback_demo/runs/wzy59zjqSynced 5 W&B file(s), 2 media file(s), 7 artifact file(s) and 0 other file(s)Find logs at: ./wandb/run-20230318_150550-wzy59zjq/logs\\nprevious\\nUnstructured\\nnext\\nWeaviate\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\wandb_tracking.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nWeaviate\\n Contents \\nInstallation and Setup\\nWrappers\\nVectorStore\\nWeaviate#\\nThis page covers how to use the Weaviate ecosystem within LangChain.\\nWhat is Weaviate?\\nWeaviate in a nutshell:\\nWeaviate is an open-source \\u200bdatabase of the type \\u200bvector search engine.\\nWeaviate allows you to store JSON documents in a class property-like fashion while attaching machine learning vectors to these documents to represent them in vector space.\\nWeaviate can be used stand-alone (aka bring your vectors) or with a variety of modules that can do the vectorization for you and extend the core capabilities.\\nWeaviate has a GraphQL-API to access your data easily.\\nWe aim to bring your vector search set up to production to query in mere milliseconds (check our open source benchmarks to see if Weaviate fits your use case).\\nGet to know Weaviate in the basics getting started guide in under five minutes.\\nWeaviate in detail:\\nWeaviate is a low-latency vector search engine with out-of-the-box support for different media types (text, images, etc.). It offers Semantic Search, Question-Answer Extraction, Classification, Customizable Models (PyTorch/TensorFlow/Keras), etc. Built from scratch in Go, Weaviate stores both objects and vectors, allowing for combining vector search with structured filtering and the fault tolerance of a cloud-native database. It is all accessible through GraphQL, REST, and various client-side programming languages.\\nInstallation and Setup#\\nInstall the Python SDK with pip install weaviate-client\\nWrappers#\\nVectorStore#\\nThere exists a wrapper around Weaviate indexes, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\nTo import this vectorstore:\\nfrom langchain.vectorstores import Weaviate\\nFor a more detailed walkthrough of the Weaviate wrapper, see this notebook\\nprevious\\nWeights & Biases\\nnext\\nWolfram Alpha Wrapper\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nVectorStore\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\weaviate.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nWolfram Alpha Wrapper\\n Contents \\nInstallation and Setup\\nWrappers\\nUtility\\nTool\\nWolfram Alpha Wrapper#\\nThis page covers how to use the Wolfram Alpha API within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Wolfram Alpha wrappers.\\nInstallation and Setup#\\nInstall requirements with pip install wolframalpha\\nGo to wolfram alpha and sign up for a developer account here\\nCreate an app and get your APP ID\\nSet your APP ID as an environment variable WOLFRAM_ALPHA_APPID\\nWrappers#\\nUtility#\\nThere exists a WolframAlphaAPIWrapper utility which wraps this API. To import this utility:\\nfrom langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\\nFor a more detailed walkthrough of this wrapper, see this notebook.\\nTool#\\nYou can also easily load this wrapper as a Tool (to use with an Agent).\\nYou can do this with:\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"wolfram-alpha\"])\\nFor more information on this, see this page\\nprevious\\nWeaviate\\nnext\\nWriter\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nUtility\\nTool\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\wolfram_alpha.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nWriter\\n Contents \\nInstallation and Setup\\nWrappers\\nLLM\\nWriter#\\nThis page covers how to use the Writer ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Writer wrappers.\\nInstallation and Setup#\\nGet an Writer api key and set it as an environment variable (WRITER_API_KEY)\\nWrappers#\\nLLM#\\nThere exists an Writer LLM wrapper, which you can access with\\nfrom langchain.llms import Writer\\nprevious\\nWolfram Alpha Wrapper\\nnext\\nYeager.ai\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nLLM\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\writer.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nYeager.ai\\n Contents \\nWhat is Yeager.ai?\\nyAgents\\nHow to use?\\nCreating and Executing Tools with yAgents\\nYeager.ai#\\nThis page covers how to use Yeager.ai to generate LangChain tools and agents.\\nWhat is Yeager.ai?#\\nYeager.ai is an ecosystem designed to simplify the process of creating AI agents and tools.\\nIt features yAgents, a No-code LangChain Agent Builder, which enables users to build, test, and deploy AI solutions with ease. Leveraging the LangChain framework, yAgents allows seamless integration with various language models and resources, making it suitable for developers, researchers, and AI enthusiasts across diverse applications.\\nyAgents#\\nLow code generative agent designed to help you build, prototype, and deploy Langchain tools with ease.\\nHow to use?#\\npip install yeagerai-agent\\nyeagerai-agent\\nGo to http://127.0.0.1:7860\\nThis will install the necessary dependencies and set up yAgents on your system. After the first run, yAgents will create a .env file where you can input your OpenAI API key. You can do the same directly from the Gradio interface under the tab “Settings”.\\nOPENAI_API_KEY=<your_openai_api_key_here>\\nWe recommend using GPT-4,. However, the tool can also work with GPT-3 if the problem is broken down sufficiently.\\nCreating and Executing Tools with yAgents#\\nyAgents makes it easy to create and execute AI-powered tools. Here’s a brief overview of the process:\\nCreate a tool: To create a tool, provide a natural language prompt to yAgents. The prompt should clearly describe the tool’s purpose and functionality. For example:\\ncreate a tool that returns the n-th prime number\\nLoad the tool into the toolkit: To load a tool into yAgents, simply provide a command to yAgents that says so. For example:\\nload the tool that you just created it into your toolkit\\nExecute the tool: To run a tool or agent, simply provide a command to yAgents that includes the name of the tool and any required parameters. For example:\\ngenerate the 50th prime number\\nYou can see a video of how it works here.\\nAs you become more familiar with yAgents, you can create more advanced tools and agents to automate your work and enhance your productivity.\\nFor more information, see yAgents’ Github or our docs\\nprevious\\nWriter\\nnext\\nZilliz\\n Contents\\n  \\nWhat is Yeager.ai?\\nyAgents\\nHow to use?\\nCreating and Executing Tools with yAgents\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\yeagerai.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nZilliz\\n Contents \\nInstallation and Setup\\nWrappers\\nVectorStore\\nZilliz#\\nThis page covers how to use the Zilliz Cloud ecosystem within LangChain.\\nZilliz uses the Milvus integration.\\nIt is broken into two parts: installation and setup, and then references to specific Milvus wrappers.\\nInstallation and Setup#\\nInstall the Python SDK with pip install pymilvus\\nWrappers#\\nVectorStore#\\nThere exists a wrapper around Zilliz indexes, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\nTo import this vectorstore:\\nfrom langchain.vectorstores import Milvus\\nFor a more detailed walkthrough of the Miluvs wrapper, see this notebook\\nprevious\\nYeager.ai\\nnext\\nGlossary\\n Contents\\n  \\nInstallation and Setup\\nWrappers\\nVectorStore\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\ecosystem\\\\zilliz.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nQuickstart Guide\\n Contents \\nInstallation\\nEnvironment Setup\\nBuilding a Language Model Application: LLMs\\nLLMs: Get predictions from a language model\\nPrompt Templates: Manage prompts for LLMs\\nChains: Combine LLMs and prompts in multi-step workflows\\nAgents: Dynamically Call Chains Based on User Input\\nMemory: Add State to Chains and Agents\\nBuilding a Language Model Application: Chat Models\\nGet Message Completions from a Chat Model\\nChat Prompt Templates\\nChains with Chat Models\\nAgents with Chat Models\\nMemory: Add State to Chains and Agents\\nQuickstart Guide#\\nThis tutorial gives you a quick walkthrough about building an end-to-end language model application with LangChain.\\nInstallation#\\nTo get started, install LangChain with the following command:\\npip install langchain\\n# or\\nconda install langchain -c conda-forge\\nEnvironment Setup#\\nUsing LangChain will usually require integrations with one or more model providers, data stores, apis, etc.\\nFor this example, we will be using OpenAI’s APIs, so we will first need to install their SDK:\\npip install openai\\nWe will then need to set the environment variable in the terminal.\\nexport OPENAI_API_KEY=\"...\"\\nAlternatively, you could do this from inside the Jupyter notebook (or Python script):\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"...\"\\nBuilding a Language Model Application: LLMs#\\nNow that we have installed LangChain and set up our environment, we can start building our language model application.\\nLangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications.\\nLLMs: Get predictions from a language model#\\nThe most basic building block of LangChain is calling an LLM on some input.\\nLet’s walk through a simple example of how to do this.\\nFor this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\\nIn order to do this, we first need to import the LLM wrapper.\\nfrom langchain.llms import OpenAI\\nWe can then initialize the wrapper with any arguments.\\nIn this example, we probably want the outputs to be MORE random, so we’ll initialize it with a HIGH temperature.\\nllm = OpenAI(temperature=0.9)\\nWe can now call it on some input!\\ntext = \"What would be a good company name for a company that makes colorful socks?\"\\nprint(llm(text))\\nFeetful of Fun\\nFor more details on how to use LLMs within LangChain, see the LLM getting started guide.\\nPrompt Templates: Manage prompts for LLMs#\\nCalling an LLM is a great first step, but it’s just the beginning.\\nNormally when you use an LLM in an application, you are not sending user input directly to the LLM.\\nInstead, you are probably taking user input and constructing a prompt, and then sending that to the LLM.\\nFor example, in the previous example, the text we passed in was hardcoded to ask for a name for a company that made colorful socks.\\nIn this imaginary service, what we would want to do is take only the user input describing what the company does, and then format the prompt with that information.\\nThis is easy to do with LangChain!\\nFirst lets define the prompt template:\\nfrom langchain.prompts import PromptTemplate\\nprompt = PromptTemplate(\\n    input_variables=[\"product\"],\\n    template=\"What is a good name for a company that makes {product}?\",\\n)\\nLet’s now see how this works! We can call the .format method to format it.\\nprint(prompt.format(product=\"colorful socks\"))\\nWhat is a good name for a company that makes colorful socks?\\nFor more details, check out the getting started guide for prompts.\\nChains: Combine LLMs and prompts in multi-step workflows#\\nUp until now, we’ve worked with the PromptTemplate and LLM primitives by themselves. But of course, a real application is not just one primitive, but rather a combination of them.\\nA chain in LangChain is made up of links, which can be either primitives like LLMs or other chains.\\nThe most core type of chain is an LLMChain, which consists of a PromptTemplate and an LLM.\\nExtending the previous example, we can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM.\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.llms import OpenAI\\nllm = OpenAI(temperature=0.9)\\nprompt = PromptTemplate(\\n    input_variables=[\"product\"],\\n    template=\"What is a good name for a company that makes {product}?\",\\n)\\nWe can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM:\\nfrom langchain.chains import LLMChain\\nchain = LLMChain(llm=llm, prompt=prompt)\\nNow we can run that chain only specifying the product!\\nchain.run(\"colorful socks\")\\n# -> \\'\\\\n\\\\nSocktastic!\\'\\nThere we go! There’s the first chain - an LLM Chain.\\nThis is one of the simpler types of chains, but understanding how it works will set you up well for working with more complex chains.\\nFor more details, check out the getting started guide for chains.\\nAgents: Dynamically Call Chains Based on User Input#\\nSo far the chains we’ve looked at run in a predetermined order.\\nAgents no longer do: they use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user.\\nWhen used correctly agents can be extremely powerful. In this tutorial, we show you how to easily use agents through the simplest, highest level API.\\nIn order to load agents, you should understand the following concepts:\\nTool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains. The interface for a tool is currently a function that is expected to have a string as an input, with a string as an output.\\nLLM: The language model powering the agent.\\nAgent: The agent to use. This should be a string that references a support agent class. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents. If you want to implement a custom agent, see the documentation for custom agents (coming soon).\\nAgents: For a list of supported agents and their specifications, see here.\\nTools: For a list of predefined tools and their specifications, see here.\\nFor this example, you will also need to install the SerpAPI Python package.\\npip install google-search-results\\nAnd set the appropriate environment variables.\\nimport os\\nos.environ[\"SERPAPI_API_KEY\"] = \"...\"\\nNow we can get started!\\nfrom langchain.agents import load_tools\\nfrom langchain.agents import initialize_agent\\nfrom langchain.agents import AgentType\\nfrom langchain.llms import OpenAI\\n# First, let\\'s load the language model we\\'re going to use to control the agent.\\nllm = OpenAI(temperature=0)\\n# Next, let\\'s load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\\n# Finally, let\\'s initialize an agent with the tools, the language model, and the type of agent we want to use.\\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\n# Now let\\'s test it out!\\nagent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\\n> Entering new AgentExecutor chain...\\n I need to find the temperature first, then use the calculator to raise it to the .023 power.\\nAction: Search\\nAction Input: \"High temperature in SF yesterday\"\\nObservation: San Francisco Temperature Yesterday. Maximum temperature yesterday: 57 °F (at 1:56 pm) Minimum temperature yesterday: 49 °F (at 1:56 am) Average temperature ...\\nThought: I now have the temperature, so I can use the calculator to raise it to the .023 power.\\nAction: Calculator\\nAction Input: 57^.023\\nObservation: Answer: 1.0974509573251117\\nThought: I now know the final answer\\nFinal Answer: The high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.0974509573251117.\\n> Finished chain.\\nMemory: Add State to Chains and Agents#\\nSo far, all the chains and agents we’ve gone through have been stateless. But often, you may want a chain or agent to have some concept of “memory” so that it may remember information about its previous interactions. The clearest and simple example of this is when designing a chatbot - you want it to remember previous messages so it can use context from that to have a better conversation. This would be a type of “short-term memory”. On the more complex side, you could imagine a chain/agent remembering key pieces of information over time - this would be a form of “long-term memory”. For more concrete ideas on the latter, see this awesome paper.\\nLangChain provides several specially created chains just for this purpose. This notebook walks through using one of those chains (the ConversationChain) with two different types of memory.\\nBy default, the ConversationChain has a simple type of memory that remembers all previous inputs/outputs and adds them to the context that is passed. Let’s take a look at using this chain (setting verbose=True so we can see the prompt).\\nfrom langchain import OpenAI, ConversationChain\\nllm = OpenAI(temperature=0)\\nconversation = ConversationChain(llm=llm, verbose=True)\\noutput = conversation.predict(input=\"Hi there!\")\\nprint(output)\\n> Entering new chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi there!\\nAI:\\n> Finished chain.\\n\\' Hello! How are you today?\\'\\noutput = conversation.predict(input=\"I\\'m doing well! Just having a conversation with an AI.\")\\nprint(output)\\n> Entering new chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi there!\\nAI:  Hello! How are you today?\\nHuman: I\\'m doing well! Just having a conversation with an AI.\\nAI:\\n> Finished chain.\\n\" That\\'s great! What would you like to talk about?\"\\nBuilding a Language Model Application: Chat Models#\\nSimilarly, you can use chat models instead of LLMs. Chat models are a variation on language models. While chat models use language models under the hood, the interface they expose is a bit different: rather than expose a “text in, text out” API, they expose an interface where “chat messages” are the inputs and outputs.\\nChat model APIs are fairly new, so we are still figuring out the correct abstractions.\\nGet Message Completions from a Chat Model#\\nYou can get chat completions by passing one or more messages to the chat model. The response will be a message. The types of messages currently supported in LangChain are AIMessage, HumanMessage, SystemMessage, and ChatMessage – ChatMessage takes in an arbitrary role parameter. Most of the time, you’ll just be dealing with HumanMessage, AIMessage, and SystemMessage.\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.schema import (\\n    AIMessage,\\n    HumanMessage,\\n    SystemMessage\\n)\\nchat = ChatOpenAI(temperature=0)\\nYou can get completions by passing in a single message.\\nchat([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])\\n# -> AIMessage(content=\"J\\'aime programmer.\", additional_kwargs={})\\nYou can also pass in multiple messages for OpenAI’s gpt-3.5-turbo and gpt-4 models.\\nmessages = [\\n    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\\n    HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")\\n]\\nchat(messages)\\n# -> AIMessage(content=\"J\\'aime programmer.\", additional_kwargs={})\\nYou can go one step further and generate completions for multiple sets of messages using generate. This returns an LLMResult with an additional message parameter:\\nbatch_messages = [\\n    [\\n        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\\n        HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")\\n    ],\\n    [\\n        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\\n        HumanMessage(content=\"Translate this sentence from English to French. I love artificial intelligence.\")\\n    ],\\n]\\nresult = chat.generate(batch_messages)\\nresult\\n# -> LLMResult(generations=[[ChatGeneration(text=\"J\\'aime programmer.\", generation_info=None, message=AIMessage(content=\"J\\'aime programmer.\", additional_kwargs={}))], [ChatGeneration(text=\"J\\'aime l\\'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J\\'aime l\\'intelligence artificielle.\", additional_kwargs={}))]], llm_output={\\'token_usage\\': {\\'prompt_tokens\\': 71, \\'completion_tokens\\': 18, \\'total_tokens\\': 89}})\\nYou can recover things like token usage from this LLMResult:\\nresult.llm_output[\\'token_usage\\']\\n# -> {\\'prompt_tokens\\': 71, \\'completion_tokens\\': 18, \\'total_tokens\\': 89}\\nChat Prompt Templates#\\nSimilar to LLMs, you can make use of templating by using a MessagePromptTemplate. You can build a ChatPromptTemplate from one or more MessagePromptTemplates. You can use ChatPromptTemplate’s format_prompt – this returns a PromptValue, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.\\nFor convience, there is a from_template method exposed on the template. If you were to use this template, this is what it would look like:\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts.chat import (\\n    ChatPromptTemplate,\\n    SystemMessagePromptTemplate,\\n    HumanMessagePromptTemplate,\\n)\\nchat = ChatOpenAI(temperature=0)\\ntemplate=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\\nhuman_template=\"{text}\"\\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\\nchat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\\n# get a chat completion from the formatted messages\\nchat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages())\\n# -> AIMessage(content=\"J\\'aime programmer.\", additional_kwargs={})\\nChains with Chat Models#\\nThe LLMChain discussed in the above section can be used with chat models as well:\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain import LLMChain\\nfrom langchain.prompts.chat import (\\n    ChatPromptTemplate,\\n    SystemMessagePromptTemplate,\\n    HumanMessagePromptTemplate,\\n)\\nchat = ChatOpenAI(temperature=0)\\ntemplate=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\\nhuman_template=\"{text}\"\\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\\nchat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\\nchain = LLMChain(llm=chat, prompt=chat_prompt)\\nchain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\\n# -> \"J\\'aime programmer.\"\\nAgents with Chat Models#\\nAgents can also be used with chat models, you can initialize one using AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION as the agent type.\\nfrom langchain.agents import load_tools\\nfrom langchain.agents import initialize_agent\\nfrom langchain.agents import AgentType\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.llms import OpenAI\\n# First, let\\'s load the language model we\\'re going to use to control the agent.\\nchat = ChatOpenAI(temperature=0)\\n# Next, let\\'s load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\\nllm = OpenAI(temperature=0)\\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\\n# Finally, let\\'s initialize an agent with the tools, the language model, and the type of agent we want to use.\\nagent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\n# Now let\\'s test it out!\\nagent.run(\"Who is Olivia Wilde\\'s boyfriend? What is his current age raised to the 0.23 power?\")\\n> Entering new AgentExecutor chain...\\nThought: I need to use a search engine to find Olivia Wilde\\'s boyfriend and a calculator to raise his age to the 0.23 power.\\nAction:\\n{\\n  \"action\": \"Search\",\\n  \"action_input\": \"Olivia Wilde boyfriend\"\\n}\\nObservation: Sudeikis and Wilde\\'s relationship ended in November 2020. Wilde was publicly served with court documents regarding child custody while she was presenting Don\\'t Worry Darling at CinemaCon 2022. In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don\\'t Worry Darling.\\nThought:I need to use a search engine to find Harry Styles\\' current age.\\nAction:\\n{\\n  \"action\": \"Search\",\\n  \"action_input\": \"Harry Styles age\"\\n}\\nObservation: 29 years\\nThought:Now I need to calculate 29 raised to the 0.23 power.\\nAction:\\n{\\n  \"action\": \"Calculator\",\\n  \"action_input\": \"29^0.23\"\\n}\\nObservation: Answer: 2.169459462491557\\nThought:I now know the final answer.\\nFinal Answer: 2.169459462491557\\n> Finished chain.\\n\\'2.169459462491557\\'\\nMemory: Add State to Chains and Agents#\\nYou can use Memory with chains and agents initialized with chat models. The main difference between this and Memory for LLMs is that rather than trying to condense all previous messages into a string, we can keep them as their own unique memory object.\\nfrom langchain.prompts import (\\n    ChatPromptTemplate, \\n    MessagesPlaceholder, \\n    SystemMessagePromptTemplate, \\n    HumanMessagePromptTemplate\\n)\\nfrom langchain.chains import ConversationChain\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.memory import ConversationBufferMemory\\nprompt = ChatPromptTemplate.from_messages([\\n    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\\n    MessagesPlaceholder(variable_name=\"history\"),\\n    HumanMessagePromptTemplate.from_template(\"{input}\")\\n])\\nllm = ChatOpenAI(temperature=0)\\nmemory = ConversationBufferMemory(return_messages=True)\\nconversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\\nconversation.predict(input=\"Hi there!\")\\n# -> \\'Hello! How can I assist you today?\\'\\nconversation.predict(input=\"I\\'m doing well! Just having a conversation with an AI.\")\\n# -> \"That sounds like fun! I\\'m happy to chat with you. Is there anything specific you\\'d like to talk about?\"\\nconversation.predict(input=\"Tell me about yourself.\")\\n# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you\\'d like to know about me?\"\\nprevious\\nWelcome to LangChain\\nnext\\nModels\\n Contents\\n  \\nInstallation\\nEnvironment Setup\\nBuilding a Language Model Application: LLMs\\nLLMs: Get predictions from a language model\\nPrompt Templates: Manage prompts for LLMs\\nChains: Combine LLMs and prompts in multi-step workflows\\nAgents: Dynamically Call Chains Based on User Input\\nMemory: Add State to Chains and Agents\\nBuilding a Language Model Application: Chat Models\\nGet Message Completions from a Chat Model\\nChat Prompt Templates\\nChains with Chat Models\\nAgents with Chat Models\\nMemory: Add State to Chains and Agents\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\getting_started\\\\getting_started.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nAgents\\n Contents \\nGo Deeper\\nAgents#\\nNote\\nConceptual Guide\\nSome applications will require not just a predetermined chain of calls to LLMs/other tools,\\nbut potentially an unknown chain that depends on the user’s input.\\nIn these types of chains, there is a “agent” which has access to a suite of tools.\\nDepending on the user input, the agent can then decide which, if any, of these tools to call.\\nIn this section of documentation, we first start with a Getting Started notebook to cover how to use all things related to agents in an end-to-end manner.\\nWe then split the documentation into the following sections:\\nTools\\nAn overview of the various tools LangChain supports.\\nAgents\\nAn overview of the different agent types.\\nToolkits\\nAn overview of toolkits, and examples of the different ones LangChain supports.\\nAgent Executor\\nAn overview of the Agent Executor class and examples of how to use it.\\nGo Deeper#\\nTools\\nAgents\\nToolkits\\nAgent Executors\\nprevious\\nChains\\nnext\\nGetting Started\\n Contents\\n  \\nGo Deeper\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\agents.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nChains\\nChains#\\nNote\\nConceptual Guide\\nUsing an LLM in isolation is fine for some simple applications,\\nbut many more complex ones require chaining LLMs - either with each other or with other experts.\\nLangChain provides a standard interface for Chains, as well as some common implementations of chains for ease of use.\\nThe following sections of documentation are provided:\\nGetting Started: A getting started guide for chains, to get you up and running quickly.\\nHow-To Guides: A collection of how-to guides. These highlight how to use various types of chains.\\nReference: API reference documentation for all Chain classes.\\nprevious\\nRedis Chat Message History\\nnext\\nGetting Started\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nIndexes\\n Contents \\nGo Deeper\\nIndexes#\\nNote\\nConceptual Guide\\nIndexes refer to ways to structure documents so that LLMs can best interact with them.\\nThis module contains utility functions for working with documents, different types of indexes, and then examples for using those indexes in chains.\\nThe most common way that indexes are used in chains is in a “retrieval” step.\\nThis step refers to taking a user’s query and returning the most relevant documents.\\nWe draw this distinction because (1) an index can be used for other things besides retrieval, and (2) retrieval can use other logic besides an index to find relevant documents.\\nWe therefore have a concept of a “Retriever” interface - this is the interface that most chains work with.\\nMost of the time when we talk about indexes and retrieval we are talking about indexing and retrieving unstructured data (like text documents).\\nFor interacting with structured data (SQL tables, etc) or APIs, please see the corresponding use case sections for links to relevant functionality.\\nThe primary index and retrieval types supported by LangChain are currently centered around vector databases, and therefore\\na lot of the functionality we dive deep on those topics.\\nFor an overview of everything related to this, please see the below notebook for getting started:\\nGetting Started\\nWe then provide a deep dive on the four main components.\\nDocument Loaders\\nHow to load documents from a variety of sources.\\nText Splitters\\nAn overview of the abstractions and implementions around splitting text.\\nVectorStores\\nAn overview of VectorStores and the many integrations LangChain provides.\\nRetrievers\\nAn overview of Retrievers and the implementations LangChain provides.\\nGo Deeper#\\nDocument Loaders\\nText Splitters\\nVectorstores\\nRetrievers\\nprevious\\nStructured Output Parser\\nnext\\nGetting Started\\n Contents\\n  \\nGo Deeper\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\indexes.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nMemory\\nMemory#\\nNote\\nConceptual Guide\\nBy default, Chains and Agents are stateless,\\nmeaning that they treat each incoming query independently (as are the underlying LLMs and chat models).\\nIn some applications (chatbots being a GREAT example) it is highly important\\nto remember previous interactions, both at a short term but also at a long term level.\\nThe concept of “Memory” exists to do exactly that.\\nLangChain provides memory components in two forms.\\nFirst, LangChain provides helper utilities for managing and manipulating previous chat messages.\\nThese are designed to be modular and useful regardless of how they are used.\\nSecondly, LangChain provides easy ways to incorporate these utilities into chains.\\nThe following sections of documentation are provided:\\nGetting Started: An overview of how to get started with different types of memory.\\nHow-To Guides: A collection of how-to guides. These highlight different types of memory, as well as how to use memory in chains.\\nMemory\\nGetting Started\\nHow-To Guides\\nprevious\\nWeaviate Hybrid Search\\nnext\\nGetting Started\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nModels\\n Contents \\nGo Deeper\\nModels#\\nNote\\nConceptual Guide\\nThis section of the documentation deals with different types of models that are used in LangChain.\\nOn this page we will go over the model types at a high level,\\nbut we have individual pages for each model type.\\nThe pages contain more detailed “how-to” guides for working with that model,\\nas well as a list of different model providers.\\nLLMs\\nLarge Language Models (LLMs) are the first type of models we cover.\\nThese models take a text string as input, and return a text string as output.\\nChat Models\\nChat Models are the second type of models we cover.\\nThese models are usually backed by a language model, but their APIs are more structured.\\nSpecifically, these models take a list of Chat Messages as input, and return a Chat Message.\\nText Embedding Models\\nThe third type of models we cover are text embedding models.\\nThese models take text as input and return a list of floats.\\nGo Deeper#\\nLLMs\\nChat Models\\nText Embedding Models\\nprevious\\nQuickstart Guide\\nnext\\nLLMs\\n Contents\\n  \\nGo Deeper\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\models.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nPrompts\\n Contents \\nGo Deeper\\nPrompts#\\nNote\\nConceptual Guide\\nThe new way of programming models is through prompts.\\nA “prompt” refers to the input to the model.\\nThis input is rarely hard coded, but rather is often constructed from multiple components.\\nA PromptTemplate is responsible for the construction of this input.\\nLangChain provides several classes and functions to make constructing and working with prompts easy.\\nThis section of documentation is split into four sections:\\nLLM Prompt Templates\\nHow to use PromptTemplates to prompt Language Models.\\nChat Prompt Templates\\nHow to use PromptTemplates to prompt Chat Models.\\nExample Selectors\\nOften times it is useful to include examples in prompts.\\nThese examples can be hardcoded, but it is often more powerful if they are dynamically selected.\\nThis section goes over example selection.\\nOutput Parsers\\nLanguage models (and Chat Models) output text.\\nBut many times you may want to get more structured information than just text back.\\nThis is where output parsers come in.\\nOutput Parsers are responsible for (1) instructing the model how output should be formatted,\\n(2) parsing output into the desired formatting (including retrying if necessary).\\nGo Deeper#\\nPrompt Templates\\nChat Prompt Template\\nExample Selectors\\nOutput Parsers\\nprevious\\nTensorflowHub\\nnext\\nPrompt Templates\\n Contents\\n  \\nGo Deeper\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nAgents\\nAgents#\\nNote\\nConceptual Guide\\nIn this part of the documentation we cover the different types of agents, disregarding which specific tools they are used with.\\nFor a high level overview of the different types of agents, see the below documentation.\\nAgent Types\\nFor documentation on how to create a custom agent, see the below.\\nCustom Agent\\nCustom LLM Agent\\nCustom LLM Agent (with a ChatModel)\\nCustom MRKL Agent\\nCustom MultiAction Agent\\nCustom Agent with Tool Retrieval\\nWe also have documentation for an in-depth dive into each agent type.\\nConversation Agent (for Chat Models)\\nConversation Agent\\nMRKL\\nMRKL Chat\\nReAct\\nSelf Ask With Search\\nprevious\\nZapier Natural Language Actions API\\nnext\\nAgent Types\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\agents\\\\agents.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nAgent Executors\\nAgent Executors#\\nNote\\nConceptual Guide\\nAgent executors take an agent and tools and use the agent to decide which tools to call and in what order.\\nIn this part of the documentation we cover other related functionality to agent executors\\nHow to combine agents and vectorstores\\nHow to use the async API for Agents\\nHow to create ChatGPT Clone\\nHow to access intermediate steps\\nHow to cap the max number of iterations\\nHow to use a timeout for the agent\\nHow to add SharedMemory to an Agent and its Tools\\nprevious\\nVectorstore Agent\\nnext\\nHow to combine agents and vectorstores\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\agents\\\\agent_executors.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nGetting Started\\nGetting Started#\\nAgents use an LLM to determine which actions to take and in what order.\\nAn action can either be using a tool and observing its output, or returning to the user.\\nWhen used correctly agents can be extremely powerful. The purpose of this notebook is to show you how to easily use agents through the simplest, highest level API.\\nIn order to load agents, you should understand the following concepts:\\nTool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains. The interface for a tool is currently a function that is expected to have a string as an input, with a string as an output.\\nLLM: The language model powering the agent.\\nAgent: The agent to use. This should be a string that references a support agent class. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents. If you want to implement a custom agent, see the documentation for custom agents (coming soon).\\nAgents: For a list of supported agents and their specifications, see here.\\nTools: For a list of predefined tools and their specifications, see here.\\nfrom langchain.agents import load_tools\\nfrom langchain.agents import initialize_agent\\nfrom langchain.agents import AgentType\\nfrom langchain.llms import OpenAI\\nFirst, let’s load the language model we’re going to use to control the agent.\\nllm = OpenAI(temperature=0)\\nNext, let’s load some tools to use. Note that the llm-math tool uses an LLM, so we need to pass that in.\\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\\nFinally, let’s initialize an agent with the tools, the language model, and the type of agent we want to use.\\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\nNow let’s test it out!\\nagent.run(\"Who is Leo DiCaprio\\'s girlfriend? What is her current age raised to the 0.43 power?\")\\n> Entering new AgentExecutor chain...\\n I need to find out who Leo DiCaprio\\'s girlfriend is and then calculate her age raised to the 0.43 power.\\nAction: Search\\nAction Input: \"Leo DiCaprio girlfriend\"\\nObservation: Camila Morrone\\nThought: I need to find out Camila Morrone\\'s age\\nAction: Search\\nAction Input: \"Camila Morrone age\"\\nObservation: 25 years\\nThought: I need to calculate 25 raised to the 0.43 power\\nAction: Calculator\\nAction Input: 25^0.43\\nObservation: Answer: 3.991298452658078\\nThought: I now know the final answer\\nFinal Answer: Camila Morrone is Leo DiCaprio\\'s girlfriend and her current age raised to the 0.43 power is 3.991298452658078.\\n> Finished chain.\\n\"Camila Morrone is Leo DiCaprio\\'s girlfriend and her current age raised to the 0.43 power is 3.991298452658078.\"\\nprevious\\nAgents\\nnext\\nTools\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\agents\\\\getting_started.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nToolkits\\nToolkits#\\nNote\\nConceptual Guide\\nThis section of documentation covers agents with toolkits - eg an agent applied to a particular use case.\\nSee below for a full list of agent toolkits\\nCSV Agent\\nJira\\nJSON Agent\\nOpenAPI agents\\nNatural Language APIs\\nPandas Dataframe Agent\\nPython Agent\\nSQL Database Agent\\nVectorstore Agent\\nprevious\\nSelf Ask With Search\\nnext\\nCSV Agent\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\agents\\\\toolkits.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nTools\\nTools#\\nNote\\nConceptual Guide\\nTools are ways that an agent can use to interact with the outside world.\\nFor an overview of what a tool is, how to use them, and a full list of examples, please see the getting started documentation\\nGetting Started\\nNext, we have some examples of customizing and generically working with tools\\nDefining Custom Tools\\nMulti-Input Tools\\nTool Input Schema\\nIn this documentation we cover generic tooling functionality (eg how to create your own)\\nas well as examples of tools and how to use them.\\nApify\\nArxiv API\\nBash\\nBing Search\\nChatGPT Plugins\\nDuckDuckGo Search\\nGoogle Places\\nGoogle Search\\nGoogle Serper API\\nGradio Tools\\nHuman as a tool\\nIFTTT WebHooks\\nOpenWeatherMap API\\nPython REPL\\nRequests\\nSearch Tools\\nSearxNG Search API\\nSerpAPI\\nWikipedia API\\nWolfram Alpha\\nZapier Natural Language Actions API\\nExample with SimpleSequentialChain\\nprevious\\nGetting Started\\nnext\\nGetting Started\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\agents\\\\tools.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nAgent Types\\n Contents \\nzero-shot-react-description\\nreact-docstore\\nself-ask-with-search\\nconversational-react-description\\nAgent Types#\\nAgents use an LLM to determine which actions to take and in what order.\\nAn action can either be using a tool and observing its output, or returning a response to the user.\\nHere are the agents available in LangChain.\\nzero-shot-react-description#\\nThis agent uses the ReAct framework to determine which tool to use\\nbased solely on the tool’s description. Any number of tools can be provided.\\nThis agent requires that a description is provided for each tool.\\nreact-docstore#\\nThis agent uses the ReAct framework to interact with a docstore. Two tools must\\nbe provided: a Search tool and a Lookup tool (they must be named exactly as so).\\nThe Search tool should search for a document, while the Lookup tool should lookup\\na term in the most recently found document.\\nThis agent is equivalent to the\\noriginal ReAct paper, specifically the Wikipedia example.\\nself-ask-with-search#\\nThis agent utilizes a single tool that should be named Intermediate Answer.\\nThis tool should be able to lookup factual answers to questions. This agent\\nis equivalent to the original self ask with search paper,\\nwhere a Google search API was provided as the tool.\\nconversational-react-description#\\nThis agent is designed to be used in conversational settings.\\nThe prompt is designed to make the agent helpful and conversational.\\nIt uses the ReAct framework to decide which tool to use, and uses memory to remember the previous conversation interactions.\\nprevious\\nAgents\\nnext\\nCustom Agent\\n Contents\\n  \\nzero-shot-react-description\\nreact-docstore\\nself-ask-with-search\\nconversational-react-description\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\agents\\\\agents\\\\agent_types.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nCustom Agent\\nCustom Agent#\\nThis notebook goes through how to create your own custom agent.\\nAn agent consists of three parts:\\n- Tools: The tools the agent has available to use.\\n- The agent class itself: this decides which action to take.\\nIn this notebook we walk through how to create a custom agent.\\nfrom langchain.agents import Tool, AgentExecutor, BaseSingleActionAgent\\nfrom langchain import OpenAI, SerpAPIWrapper\\nsearch = SerpAPIWrapper()\\ntools = [\\n    Tool(\\n        name = \"Search\",\\n        func=search.run,\\n        description=\"useful for when you need to answer questions about current events\",\\n        return_direct=True\\n    )\\n]\\nfrom typing import List, Tuple, Any, Union\\nfrom langchain.schema import AgentAction, AgentFinish\\nclass FakeAgent(BaseSingleActionAgent):\\n    \"\"\"Fake Custom Agent.\"\"\"\\n    \\n    @property\\n    def input_keys(self):\\n        return [\"input\"]\\n    \\n    def plan(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\\n    ) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            **kwargs: User inputs.\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        return AgentAction(tool=\"Search\", tool_input=kwargs[\"input\"], log=\"\")\\n    async def aplan(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\\n    ) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            **kwargs: User inputs.\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        return AgentAction(tool=\"Search\", tool_input=kwargs[\"input\"], log=\"\")\\nagent = FakeAgent()\\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\\nagent_executor.run(\"How many people live in canada as of 2023?\")\\n> Entering new AgentExecutor chain...\\nFoo Fighters is an American rock band formed in Seattle in 1994. Foo Fighters was initially formed as a one-man project by former Nirvana drummer Dave Grohl. Following the success of the 1995 eponymous debut album, Grohl recruited a band consisting of Nate Mendel, William Goldsmith, and Pat Smear.\\n> Finished chain.\\n\\'Foo Fighters is an American rock band formed in Seattle in 1994. Foo Fighters was initially formed as a one-man project by former Nirvana drummer Dave Grohl. Following the success of the 1995 eponymous debut album, Grohl recruited a band consisting of Nate Mendel, William Goldsmith, and Pat Smear.\\'\\nprevious\\nAgent Types\\nnext\\nCustom LLM Agent\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\agents\\\\agents\\\\custom_agent.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nCustom Agent with Tool Retrieval\\n Contents \\nSet up environment\\nSet up tools\\nTool Retriever\\nPrompt Template\\nOutput Parser\\nSet up LLM, stop sequence, and the agent\\nUse the Agent\\nCustom Agent with Tool Retrieval#\\nThis notebook builds off of this notebook and assumes familiarity with how agents work.\\nThe novel idea introduced in this notebook is the idea of using retrieval to select the set of tools to use to answer an agent query. This is useful when you have many many tools to select from. You cannot put the description of all the tools in the prompt (because of context length issues) so instead you dynamically select the N tools you do want to consider using at run time.\\nIn this notebook we will create a somewhat contrieved example. We will have one legitimate tool (search) and then 99 fake tools which are just nonsense. We will then add a step in the prompt template that takes the user input and retrieves tool relevant to the query.\\nSet up environment#\\nDo necessary imports, etc.\\nfrom langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\\nfrom langchain.prompts import StringPromptTemplate\\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\\nfrom typing import List, Union\\nfrom langchain.schema import AgentAction, AgentFinish\\nimport re\\nSet up tools#\\nWe will create one legitimate tool (search) and then 99 fake tools\\n# Define which tools the agent can use to answer user queries\\nsearch = SerpAPIWrapper()\\nsearch_tool = Tool(\\n        name = \"Search\",\\n        func=search.run,\\n        description=\"useful for when you need to answer questions about current events\"\\n    )\\ndef fake_func(inp: str) -> str:\\n    return \"foo\"\\nfake_tools = [\\n    Tool(\\n        name=f\"foo-{i}\", \\n        func=fake_func, \\n        description=f\"a silly function that you can use to get more information about the number {i}\"\\n    ) \\n    for i in range(99)\\n]\\nALL_TOOLS = [search_tool] + fake_tools\\nTool Retriever#\\nWe will use a vectorstore to create embeddings for each tool description. Then, for an incoming query we can create embeddings for that query and do a similarity search for relevant tools.\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.schema import Document\\ndocs = [Document(page_content=t.description, metadata={\"index\": i}) for i, t in enumerate(ALL_TOOLS)]\\nvector_store = FAISS.from_documents(docs, OpenAIEmbeddings())\\nretriever = vector_store.as_retriever()\\ndef get_tools(query):\\n    docs = retriever.get_relevant_documents(query)\\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\\nWe can now test this retriever to see if it seems to work.\\nget_tools(\"whats the weather?\")\\n[Tool(name=\\'Search\\', description=\\'useful for when you need to answer questions about current events\\', return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x114b28a90>, func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class \\'serpapi.google_search.GoogleSearch\\'>, params={\\'engine\\': \\'google\\', \\'google_domain\\': \\'google.com\\', \\'gl\\': \\'us\\', \\'hl\\': \\'en\\'}, serpapi_api_key=\\'c657176b327b17e79b55306ab968d164ee2369a7c7fa5b3f8a5f7889903de882\\', aiosession=None)>, coroutine=None),\\n Tool(name=\\'foo-95\\', description=\\'a silly function that you can use to get more information about the number 95\\', return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x114b28a90>, func=<function fake_func at 0x15e5bd1f0>, coroutine=None),\\n Tool(name=\\'foo-12\\', description=\\'a silly function that you can use to get more information about the number 12\\', return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x114b28a90>, func=<function fake_func at 0x15e5bd1f0>, coroutine=None),\\n Tool(name=\\'foo-15\\', description=\\'a silly function that you can use to get more information about the number 15\\', return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x114b28a90>, func=<function fake_func at 0x15e5bd1f0>, coroutine=None)]\\nget_tools(\"whats the number 13?\")\\n[Tool(name=\\'foo-13\\', description=\\'a silly function that you can use to get more information about the number 13\\', return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x114b28a90>, func=<function fake_func at 0x15e5bd1f0>, coroutine=None),\\n Tool(name=\\'foo-12\\', description=\\'a silly function that you can use to get more information about the number 12\\', return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x114b28a90>, func=<function fake_func at 0x15e5bd1f0>, coroutine=None),\\n Tool(name=\\'foo-14\\', description=\\'a silly function that you can use to get more information about the number 14\\', return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x114b28a90>, func=<function fake_func at 0x15e5bd1f0>, coroutine=None),\\n Tool(name=\\'foo-11\\', description=\\'a silly function that you can use to get more information about the number 11\\', return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x114b28a90>, func=<function fake_func at 0x15e5bd1f0>, coroutine=None)]\\nPrompt Template#\\nThe prompt template is pretty standard, because we’re not actually changing that much logic in the actual prompt template, but rather we are just changing how retrieval is done.\\n# Set up the base template\\ntemplate = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\\n{tools}\\nUse the following format:\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\nBegin! Remember to speak as a pirate when giving your final answer. Use lots of \"Arg\"s\\nQuestion: {input}\\n{agent_scratchpad}\"\"\"\\nThe custom prompt template now has the concept of a tools_getter, which we call on the input to select the tools to use\\nfrom typing import Callable\\n# Set up a prompt template\\nclass CustomPromptTemplate(StringPromptTemplate):\\n    # The template to use\\n    template: str\\n    ############## NEW ######################\\n    # The list of tools available\\n    tools_getter: Callable\\n    \\n    def format(self, **kwargs) -> str:\\n        # Get the intermediate steps (AgentAction, Observation tuples)\\n        # Format them in a particular way\\n        intermediate_steps = kwargs.pop(\"intermediate_steps\")\\n        thoughts = \"\"\\n        for action, observation in intermediate_steps:\\n            thoughts += action.log\\n            thoughts += f\"\\\\nObservation: {observation}\\\\nThought: \"\\n        # Set the agent_scratchpad variable to that value\\n        kwargs[\"agent_scratchpad\"] = thoughts\\n        ############## NEW ######################\\n        tools = self.tools_getter(kwargs[\"input\"])\\n        # Create a tools variable from the list of tools provided\\n        kwargs[\"tools\"] = \"\\\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\\n        # Create a list of tool names for the tools provided\\n        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in tools])\\n        return self.template.format(**kwargs)\\nprompt = CustomPromptTemplate(\\n    template=template,\\n    tools_getter=get_tools,\\n    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\\n    # This includes the `intermediate_steps` variable because that is needed\\n    input_variables=[\"input\", \"intermediate_steps\"]\\n)\\nOutput Parser#\\nThe output parser is unchanged from the previous notebook, since we are not changing anything about the output format.\\nclass CustomOutputParser(AgentOutputParser):\\n    \\n    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\\n        # Check if agent should finish\\n        if \"Final Answer:\" in llm_output:\\n            return AgentFinish(\\n                # Return values is generally always a dictionary with a single `output` key\\n                # It is not recommended to try anything else at the moment :)\\n                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\\n                log=llm_output,\\n            )\\n        # Parse out the action and action input\\n        regex = r\"Action\\\\s*\\\\d*\\\\s*:(.*?)\\\\nAction\\\\s*\\\\d*\\\\s*Input\\\\s*\\\\d*\\\\s*:[\\\\s]*(.*)\"\\n        match = re.search(regex, llm_output, re.DOTALL)\\n        if not match:\\n            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\\n        action = match.group(1).strip()\\n        action_input = match.group(2)\\n        # Return the action and action input\\n        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip(\\'\"\\'), log=llm_output)\\noutput_parser = CustomOutputParser()\\nSet up LLM, stop sequence, and the agent#\\nAlso the same as the previous notebook\\nllm = OpenAI(temperature=0)\\n# LLM chain consisting of the LLM and a prompt\\nllm_chain = LLMChain(llm=llm, prompt=prompt)\\ntool_names = [tool.name for tool in tools]\\nagent = LLMSingleActionAgent(\\n    llm_chain=llm_chain, \\n    output_parser=output_parser,\\n    stop=[\"\\\\nObservation:\"], \\n    allowed_tools=tool_names\\n)\\nUse the Agent#\\nNow we can use it!\\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\\nagent_executor.run(\"What\\'s the weather in SF?\")\\n> Entering new AgentExecutor chain...\\nThought: I need to find out what the weather is in SF\\nAction: Search\\nAction Input: Weather in SF\\nObservation:Mostly cloudy skies early, then partly cloudy in the afternoon. High near 60F. ENE winds shifting to W at 10 to 15 mph. Humidity71%. UV Index6 of 10. I now know the final answer\\nFinal Answer: \\'Arg, \\'tis mostly cloudy skies early, then partly cloudy in the afternoon. High near 60F. ENE winds shiftin\\' to W at 10 to 15 mph. Humidity71%. UV Index6 of 10.\\n> Finished chain.\\n\"\\'Arg, \\'tis mostly cloudy skies early, then partly cloudy in the afternoon. High near 60F. ENE winds shiftin\\' to W at 10 to 15 mph. Humidity71%. UV Index6 of 10.\"\\nprevious\\nCustom MultiAction Agent\\nnext\\nConversation Agent (for Chat Models)\\n Contents\\n  \\nSet up environment\\nSet up tools\\nTool Retriever\\nPrompt Template\\nOutput Parser\\nSet up LLM, stop sequence, and the agent\\nUse the Agent\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\agents\\\\agents\\\\custom_agent_with_tool_retrieval.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nCustom LLM Agent\\n Contents \\nSet up environment\\nSet up tool\\nPrompt Template\\nOutput Parser\\nSet up LLM\\nDefine the stop sequence\\nSet up the Agent\\nUse the Agent\\nAdding Memory\\nCustom LLM Agent#\\nThis notebook goes through how to create your own custom LLM agent.\\nAn LLM agent consists of three parts:\\nPromptTemplate: This is the prompt template that can be used to instruct the language model on what to do\\nLLM: This is the language model that powers the agent\\nstop sequence: Instructs the LLM to stop generating as soon as this string is found\\nOutputParser: This determines how to parse the LLMOutput into an AgentAction or AgentFinish object\\nThe LLMAgent is used in an AgentExecutor. This AgentExecutor can largely be thought of as a loop that:\\nPasses user input and any previous steps to the Agent (in this case, the LLMAgent)\\nIf the Agent returns an AgentFinish, then return that directly to the user\\nIf the Agent returns an AgentAction, then use that to call a tool and get an Observation\\nRepeat, passing the AgentAction and Observation back to the Agent until an AgentFinish is emitted.\\nAgentAction is a response that consists of action and action_input. action refers to which tool to use, and action_input refers to the input to that tool. log can also be provided as more context (that can be used for logging, tracing, etc).\\nAgentFinish is a response that contains the final message to be sent back to the user. This should be used to end an agent run.\\nIn this notebook we walk through how to create a custom LLM agent.\\nSet up environment#\\nDo necessary imports, etc.\\nfrom langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\\nfrom langchain.prompts import StringPromptTemplate\\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\\nfrom typing import List, Union\\nfrom langchain.schema import AgentAction, AgentFinish\\nimport re\\nSet up tool#\\nSet up any tools the agent may want to use. This may be necessary to put in the prompt (so that the agent knows to use these tools).\\n# Define which tools the agent can use to answer user queries\\nsearch = SerpAPIWrapper()\\ntools = [\\n    Tool(\\n        name = \"Search\",\\n        func=search.run,\\n        description=\"useful for when you need to answer questions about current events\"\\n    )\\n]\\nPrompt Template#\\nThis instructs the agent on what to do. Generally, the template should incorporate:\\ntools: which tools the agent has access and how and when to call them.\\nintermediate_steps: These are tuples of previous (AgentAction, Observation) pairs. These are generally not passed directly to the model, but the prompt template formats them in a specific way.\\ninput: generic user input\\n# Set up the base template\\ntemplate = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\\n{tools}\\nUse the following format:\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\nBegin! Remember to speak as a pirate when giving your final answer. Use lots of \"Arg\"s\\nQuestion: {input}\\n{agent_scratchpad}\"\"\"\\n# Set up a prompt template\\nclass CustomPromptTemplate(StringPromptTemplate):\\n    # The template to use\\n    template: str\\n    # The list of tools available\\n    tools: List[Tool]\\n    \\n    def format(self, **kwargs) -> str:\\n        # Get the intermediate steps (AgentAction, Observation tuples)\\n        # Format them in a particular way\\n        intermediate_steps = kwargs.pop(\"intermediate_steps\")\\n        thoughts = \"\"\\n        for action, observation in intermediate_steps:\\n            thoughts += action.log\\n            thoughts += f\"\\\\nObservation: {observation}\\\\nThought: \"\\n        # Set the agent_scratchpad variable to that value\\n        kwargs[\"agent_scratchpad\"] = thoughts\\n        # Create a tools variable from the list of tools provided\\n        kwargs[\"tools\"] = \"\\\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\\n        # Create a list of tool names for the tools provided\\n        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\\n        return self.template.format(**kwargs)\\nprompt = CustomPromptTemplate(\\n    template=template,\\n    tools=tools,\\n    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\\n    # This includes the `intermediate_steps` variable because that is needed\\n    input_variables=[\"input\", \"intermediate_steps\"]\\n)\\nOutput Parser#\\nThe output parser is responsible for parsing the LLM output into AgentAction and AgentFinish. This usually depends heavily on the prompt used.\\nThis is where you can change the parsing to do retries, handle whitespace, etc\\nclass CustomOutputParser(AgentOutputParser):\\n    \\n    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\\n        # Check if agent should finish\\n        if \"Final Answer:\" in llm_output:\\n            return AgentFinish(\\n                # Return values is generally always a dictionary with a single `output` key\\n                # It is not recommended to try anything else at the moment :)\\n                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\\n                log=llm_output,\\n            )\\n        # Parse out the action and action input\\n        regex = r\"Action\\\\s*\\\\d*\\\\s*:(.*?)\\\\nAction\\\\s*\\\\d*\\\\s*Input\\\\s*\\\\d*\\\\s*:[\\\\s]*(.*)\"\\n        match = re.search(regex, llm_output, re.DOTALL)\\n        if not match:\\n            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\\n        action = match.group(1).strip()\\n        action_input = match.group(2)\\n        # Return the action and action input\\n        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip(\\'\"\\'), log=llm_output)\\noutput_parser = CustomOutputParser()\\nSet up LLM#\\nChoose the LLM you want to use!\\nllm = OpenAI(temperature=0)\\nDefine the stop sequence#\\nThis is important because it tells the LLM when to stop generation.\\nThis depends heavily on the prompt and model you are using. Generally, you want this to be whatever token you use in the prompt to denote the start of an Observation (otherwise, the LLM may hallucinate an observation for you).\\nSet up the Agent#\\nWe can now combine everything to set up our agent\\n# LLM chain consisting of the LLM and a prompt\\nllm_chain = LLMChain(llm=llm, prompt=prompt)\\ntool_names = [tool.name for tool in tools]\\nagent = LLMSingleActionAgent(\\n    llm_chain=llm_chain, \\n    output_parser=output_parser,\\n    stop=[\"\\\\nObservation:\"], \\n    allowed_tools=tool_names\\n)\\nUse the Agent#\\nNow we can use it!\\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\\nagent_executor.run(\"How many people live in canada as of 2023?\")\\n> Entering new AgentExecutor chain...\\nThought: I need to find out the population of Canada in 2023\\nAction: Search\\nAction Input: Population of Canada in 2023\\nObservation:The current population of Canada is 38,658,314 as of Wednesday, April 12, 2023, based on Worldometer elaboration of the latest United Nations data. I now know the final answer\\nFinal Answer: Arrr, there be 38,658,314 people livin\\' in Canada as of 2023!\\n> Finished chain.\\n\"Arrr, there be 38,658,314 people livin\\' in Canada as of 2023!\"\\nAdding Memory#\\nIf you want to add memory to the agent, you’ll need to:\\nAdd a place in the custom prompt for the chat_history\\nAdd a memory object to the agent executor.\\n# Set up the base template\\ntemplate_with_history = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\\n{tools}\\nUse the following format:\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\nBegin! Remember to speak as a pirate when giving your final answer. Use lots of \"Arg\"s\\nPrevious conversation history:\\n{history}\\nNew question: {input}\\n{agent_scratchpad}\"\"\"\\nprompt_with_history = CustomPromptTemplate(\\n    template=template_with_history,\\n    tools=tools,\\n    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\\n    # This includes the `intermediate_steps` variable because that is needed\\n    input_variables=[\"input\", \"intermediate_steps\", \"history\"]\\n)\\nllm_chain = LLMChain(llm=llm, prompt=prompt_with_history)\\ntool_names = [tool.name for tool in tools]\\nagent = LLMSingleActionAgent(\\n    llm_chain=llm_chain, \\n    output_parser=output_parser,\\n    stop=[\"\\\\nObservation:\"], \\n    allowed_tools=tool_names\\n)\\nfrom langchain.memory import ConversationBufferWindowMemory\\nmemory=ConversationBufferWindowMemory(k=2)\\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True, memory=memory)\\nagent_executor.run(\"How many people live in canada as of 2023?\")\\n> Entering new AgentExecutor chain...\\nThought: I need to find out the population of Canada in 2023\\nAction: Search\\nAction Input: Population of Canada in 2023\\nObservation:The current population of Canada is 38,658,314 as of Wednesday, April 12, 2023, based on Worldometer elaboration of the latest United Nations data. I now know the final answer\\nFinal Answer: Arrr, there be 38,658,314 people livin\\' in Canada as of 2023!\\n> Finished chain.\\n\"Arrr, there be 38,658,314 people livin\\' in Canada as of 2023!\"\\nagent_executor.run(\"how about in mexico?\")\\n> Entering new AgentExecutor chain...\\nThought: I need to find out how many people live in Mexico.\\nAction: Search\\nAction Input: How many people live in Mexico as of 2023?\\nObservation:The current population of Mexico is 132,679,922 as of Tuesday, April 11, 2023, based on Worldometer elaboration of the latest United Nations data. Mexico 2020 ... I now know the final answer.\\nFinal Answer: Arrr, there be 132,679,922 people livin\\' in Mexico as of 2023!\\n> Finished chain.\\n\"Arrr, there be 132,679,922 people livin\\' in Mexico as of 2023!\"\\nprevious\\nCustom Agent\\nnext\\nCustom LLM Agent (with a ChatModel)\\n Contents\\n  \\nSet up environment\\nSet up tool\\nPrompt Template\\nOutput Parser\\nSet up LLM\\nDefine the stop sequence\\nSet up the Agent\\nUse the Agent\\nAdding Memory\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\agents\\\\agents\\\\custom_llm_agent.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nCustom LLM Agent (with a ChatModel)\\n Contents \\nSet up environment\\nSet up tool\\nPrompt Template\\nOutput Parser\\nSet up LLM\\nDefine the stop sequence\\nSet up the Agent\\nUse the Agent\\nCustom LLM Agent (with a ChatModel)#\\nThis notebook goes through how to create your own custom agent based on a chat model.\\nAn LLM chat agent consists of three parts:\\nPromptTemplate: This is the prompt template that can be used to instruct the language model on what to do\\nChatModel: This is the language model that powers the agent\\nstop sequence: Instructs the LLM to stop generating as soon as this string is found\\nOutputParser: This determines how to parse the LLMOutput into an AgentAction or AgentFinish object\\nThe LLMAgent is used in an AgentExecutor. This AgentExecutor can largely be thought of as a loop that:\\nPasses user input and any previous steps to the Agent (in this case, the LLMAgent)\\nIf the Agent returns an AgentFinish, then return that directly to the user\\nIf the Agent returns an AgentAction, then use that to call a tool and get an Observation\\nRepeat, passing the AgentAction and Observation back to the Agent until an AgentFinish is emitted.\\nAgentAction is a response that consists of action and action_input. action refers to which tool to use, and action_input refers to the input to that tool. log can also be provided as more context (that can be used for logging, tracing, etc).\\nAgentFinish is a response that contains the final message to be sent back to the user. This should be used to end an agent run.\\nIn this notebook we walk through how to create a custom LLM agent.\\nSet up environment#\\nDo necessary imports, etc.\\nfrom langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\\nfrom langchain.prompts import BaseChatPromptTemplate\\nfrom langchain import SerpAPIWrapper, LLMChain\\nfrom langchain.chat_models import ChatOpenAI\\nfrom typing import List, Union\\nfrom langchain.schema import AgentAction, AgentFinish, HumanMessage\\nimport re\\nSet up tool#\\nSet up any tools the agent may want to use. This may be necessary to put in the prompt (so that the agent knows to use these tools).\\n# Define which tools the agent can use to answer user queries\\nsearch = SerpAPIWrapper()\\ntools = [\\n    Tool(\\n        name = \"Search\",\\n        func=search.run,\\n        description=\"useful for when you need to answer questions about current events\"\\n    )\\n]\\nPrompt Template#\\nThis instructs the agent on what to do. Generally, the template should incorporate:\\ntools: which tools the agent has access and how and when to call them.\\nintermediate_steps: These are tuples of previous (AgentAction, Observation) pairs. These are generally not passed directly to the model, but the prompt template formats them in a specific way.\\ninput: generic user input\\n# Set up the base template\\ntemplate = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\\n{tools}\\nUse the following format:\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\nBegin! Remember to speak as a pirate when giving your final answer. Use lots of \"Arg\"s\\nQuestion: {input}\\n{agent_scratchpad}\"\"\"\\n# Set up a prompt template\\nclass CustomPromptTemplate(BaseChatPromptTemplate):\\n    # The template to use\\n    template: str\\n    # The list of tools available\\n    tools: List[Tool]\\n    \\n    def format_messages(self, **kwargs) -> str:\\n        # Get the intermediate steps (AgentAction, Observation tuples)\\n        # Format them in a particular way\\n        intermediate_steps = kwargs.pop(\"intermediate_steps\")\\n        thoughts = \"\"\\n        for action, observation in intermediate_steps:\\n            thoughts += action.log\\n            thoughts += f\"\\\\nObservation: {observation}\\\\nThought: \"\\n        # Set the agent_scratchpad variable to that value\\n        kwargs[\"agent_scratchpad\"] = thoughts\\n        # Create a tools variable from the list of tools provided\\n        kwargs[\"tools\"] = \"\\\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\\n        # Create a list of tool names for the tools provided\\n        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\\n        formatted = self.template.format(**kwargs)\\n        return [HumanMessage(content=formatted)]\\nprompt = CustomPromptTemplate(\\n    template=template,\\n    tools=tools,\\n    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\\n    # This includes the `intermediate_steps` variable because that is needed\\n    input_variables=[\"input\", \"intermediate_steps\"]\\n)\\nOutput Parser#\\nThe output parser is responsible for parsing the LLM output into AgentAction and AgentFinish. This usually depends heavily on the prompt used.\\nThis is where you can change the parsing to do retries, handle whitespace, etc\\nclass CustomOutputParser(AgentOutputParser):\\n    \\n    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\\n        # Check if agent should finish\\n        if \"Final Answer:\" in llm_output:\\n            return AgentFinish(\\n                # Return values is generally always a dictionary with a single `output` key\\n                # It is not recommended to try anything else at the moment :)\\n                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\\n                log=llm_output,\\n            )\\n        # Parse out the action and action input\\n        regex = r\"Action\\\\s*\\\\d*\\\\s*:(.*?)\\\\nAction\\\\s*\\\\d*\\\\s*Input\\\\s*\\\\d*\\\\s*:[\\\\s]*(.*)\"\\n        match = re.search(regex, llm_output, re.DOTALL)\\n        if not match:\\n            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\\n        action = match.group(1).strip()\\n        action_input = match.group(2)\\n        # Return the action and action input\\n        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip(\\'\"\\'), log=llm_output)\\noutput_parser = CustomOutputParser()\\nSet up LLM#\\nChoose the LLM you want to use!\\nllm = ChatOpenAI(temperature=0)\\nDefine the stop sequence#\\nThis is important because it tells the LLM when to stop generation.\\nThis depends heavily on the prompt and model you are using. Generally, you want this to be whatever token you use in the prompt to denote the start of an Observation (otherwise, the LLM may hallucinate an observation for you).\\nSet up the Agent#\\nWe can now combine everything to set up our agent\\n# LLM chain consisting of the LLM and a prompt\\nllm_chain = LLMChain(llm=llm, prompt=prompt)\\ntool_names = [tool.name for tool in tools]\\nagent = LLMSingleActionAgent(\\n    llm_chain=llm_chain, \\n    output_parser=output_parser,\\n    stop=[\"\\\\nObservation:\"], \\n    allowed_tools=tool_names\\n)\\nUse the Agent#\\nNow we can use it!\\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\\nagent_executor.run(\"How many people live in canada as of 2023?\")\\n> Entering new AgentExecutor chain...\\nThought: Wot year be it now? That be important to know the answer.\\nAction: Search\\nAction Input: \"current population canada 2023\"\\nObservation:38,649,283Ahoy! That be the correct year, but the answer be in regular numbers. \\'Tis time to translate to pirate speak.\\nAction: Search\\nAction Input: \"38,649,283 in pirate speak\"\\nObservation:Brush up on your “Pirate Talk” with these helpful pirate phrases. Aaaarrrrgggghhhh! Pirate catch phrase of grumbling or disgust. Ahoy! Hello! Ahoy, Matey, Hello ...That be not helpful, I\\'ll just do the translation meself.\\nFinal Answer: Arrrr, thar be 38,649,283 scallywags in Canada as of 2023.\\n> Finished chain.\\n\\'Arrrr, thar be 38,649,283 scallywags in Canada as of 2023.\\'\\nprevious\\nCustom LLM Agent\\nnext\\nCustom MRKL Agent\\n Contents\\n  \\nSet up environment\\nSet up tool\\nPrompt Template\\nOutput Parser\\nSet up LLM\\nDefine the stop sequence\\nSet up the Agent\\nUse the Agent\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\agents\\\\agents\\\\custom_llm_chat_agent.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nCustom MRKL Agent\\n Contents \\nCustom LLMChain\\nMultiple inputs\\nCustom MRKL Agent#\\nThis notebook goes through how to create your own custom MRKL agent.\\nA MRKL agent consists of three parts:\\n- Tools: The tools the agent has available to use.\\n- LLMChain: The LLMChain that produces the text that is parsed in a certain way to determine which action to take.\\n- The agent class itself: this parses the output of the LLMChain to determine which action to take.\\nIn this notebook we walk through how to create a custom MRKL agent by creating a custom LLMChain.\\nCustom LLMChain#\\nThe first way to create a custom agent is to use an existing Agent class, but use a custom LLMChain. This is the simplest way to create a custom Agent. It is highly recommended that you work with the ZeroShotAgent, as at the moment that is by far the most generalizable one.\\nMost of the work in creating the custom LLMChain comes down to the prompt. Because we are using an existing agent class to parse the output, it is very important that the prompt say to produce text in that format. Additionally, we currently require an agent_scratchpad input variable to put notes on previous actions and observations. This should almost always be the final part of the prompt. However, besides those instructions, you can customize the prompt as you wish.\\nTo ensure that the prompt contains the appropriate instructions, we will utilize a helper method on that class. The helper method for the ZeroShotAgent takes the following arguments:\\ntools: List of tools the agent will have access to, used to format the prompt.\\nprefix: String to put before the list of tools.\\nsuffix: String to put after the list of tools.\\ninput_variables: List of input variables the final prompt will expect.\\nFor this exercise, we will give our agent access to Google Search, and we will customize it in that we will have it answer as a pirate.\\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\\nsearch = SerpAPIWrapper()\\ntools = [\\n    Tool(\\n        name = \"Search\",\\n        func=search.run,\\n        description=\"useful for when you need to answer questions about current events\"\\n    )\\n]\\nprefix = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\"\"\"\\nsuffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\\nQuestion: {input}\\n{agent_scratchpad}\"\"\"\\nprompt = ZeroShotAgent.create_prompt(\\n    tools, \\n    prefix=prefix, \\n    suffix=suffix, \\n    input_variables=[\"input\", \"agent_scratchpad\"]\\n)\\nIn case we are curious, we can now take a look at the final prompt template to see what it looks like when its all put together.\\nprint(prompt.template)\\nAnswer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\\nSearch: useful for when you need to answer questions about current events\\nUse the following format:\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Search]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\nBegin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\\nQuestion: {input}\\n{agent_scratchpad}\\nNote that we are able to feed agents a self-defined prompt template, i.e. not restricted to the prompt generated by the create_prompt function, assuming it meets the agent’s requirements.\\nFor example, for ZeroShotAgent, we will need to ensure that it meets the following requirements. There should a string starting with “Action:” and a following string starting with “Action Input:”, and both should be separated by a newline.\\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\\ntool_names = [tool.name for tool in tools]\\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\\nagent_executor.run(\"How many people live in canada as of 2023?\")\\n> Entering new AgentExecutor chain...\\nThought: I need to find out the population of Canada\\nAction: Search\\nAction Input: Population of Canada 2023\\nObservation: The current population of Canada is 38,661,927 as of Sunday, April 16, 2023, based on Worldometer elaboration of the latest United Nations data.\\nThought: I now know the final answer\\nFinal Answer: Arrr, Canada be havin\\' 38,661,927 people livin\\' there as of 2023!\\n> Finished chain.\\n\"Arrr, Canada be havin\\' 38,661,927 people livin\\' there as of 2023!\"\\nMultiple inputs#\\nAgents can also work with prompts that require multiple inputs.\\nprefix = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\\nsuffix = \"\"\"When answering, you MUST speak in the following language: {language}.\\nQuestion: {input}\\n{agent_scratchpad}\"\"\"\\nprompt = ZeroShotAgent.create_prompt(\\n    tools, \\n    prefix=prefix, \\n    suffix=suffix, \\n    input_variables=[\"input\", \"language\", \"agent_scratchpad\"]\\n)\\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\\nagent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)\\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\\nagent_executor.run(input=\"How many people live in canada as of 2023?\", language=\"italian\")\\n> Entering new AgentExecutor chain...\\nThought: I should look for recent population estimates.\\nAction: Search\\nAction Input: Canada population 2023\\nObservation: 39,566,248\\nThought: I should double check this number.\\nAction: Search\\nAction Input: Canada population estimates 2023\\nObservation: Canada\\'s population was estimated at 39,566,248 on January 1, 2023, after a record population growth of 1,050,110 people from January 1, 2022, to January 1, 2023.\\nThought: I now know the final answer.\\nFinal Answer: La popolazione del Canada è stata stimata a 39.566.248 il 1° gennaio 2023, dopo un record di crescita demografica di 1.050.110 persone dal 1° gennaio 2022 al 1° gennaio 2023.\\n> Finished chain.\\n\\'La popolazione del Canada è stata stimata a 39.566.248 il 1° gennaio 2023, dopo un record di crescita demografica di 1.050.110 persone dal 1° gennaio 2022 al 1° gennaio 2023.\\'\\nprevious\\nCustom LLM Agent (with a ChatModel)\\nnext\\nCustom MultiAction Agent\\n Contents\\n  \\nCustom LLMChain\\nMultiple inputs\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\agents\\\\agents\\\\custom_mrkl_agent.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nCustom MultiAction Agent\\nCustom MultiAction Agent#\\nThis notebook goes through how to create your own custom agent.\\nAn agent consists of three parts:\\n- Tools: The tools the agent has available to use.\\n- The agent class itself: this decides which action to take.\\nIn this notebook we walk through how to create a custom agent that predicts/takes multiple steps at a time.\\nfrom langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent\\nfrom langchain import OpenAI, SerpAPIWrapper\\ndef random_word(query: str) -> str:\\n    print(\"\\\\nNow I\\'m doing this!\")\\n    return \"foo\"\\nsearch = SerpAPIWrapper()\\ntools = [\\n    Tool(\\n        name = \"Search\",\\n        func=search.run,\\n        description=\"useful for when you need to answer questions about current events\"\\n    ),\\n    Tool(\\n        name = \"RandomWord\",\\n        func=random_word,\\n        description=\"call this to get a random word.\"\\n    \\n    )\\n]\\nfrom typing import List, Tuple, Any, Union\\nfrom langchain.schema import AgentAction, AgentFinish\\nclass FakeAgent(BaseMultiActionAgent):\\n    \"\"\"Fake Custom Agent.\"\"\"\\n    \\n    @property\\n    def input_keys(self):\\n        return [\"input\"]\\n    \\n    def plan(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\\n    ) -> Union[List[AgentAction], AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            **kwargs: User inputs.\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        if len(intermediate_steps) == 0:\\n            return [\\n                AgentAction(tool=\"Search\", tool_input=\"foo\", log=\"\"),\\n                AgentAction(tool=\"RandomWord\", tool_input=\"foo\", log=\"\"),\\n            ]\\n        else:\\n            return AgentFinish(return_values={\"output\": \"bar\"}, log=\"\")\\n    async def aplan(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\\n    ) -> Union[List[AgentAction], AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            **kwargs: User inputs.\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        if len(intermediate_steps) == 0:\\n            return [\\n                AgentAction(tool=\"Search\", tool_input=\"foo\", log=\"\"),\\n                AgentAction(tool=\"RandomWord\", tool_input=\"foo\", log=\"\"),\\n            ]\\n        else:\\n            return AgentFinish(return_values={\"output\": \"bar\"}, log=\"\")\\nagent = FakeAgent()\\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\\nagent_executor.run(\"How many people live in canada as of 2023?\")\\n> Entering new AgentExecutor chain...\\nFoo Fighters is an American rock band formed in Seattle in 1994. Foo Fighters was initially formed as a one-man project by former Nirvana drummer Dave Grohl. Following the success of the 1995 eponymous debut album, Grohl recruited a band consisting of Nate Mendel, William Goldsmith, and Pat Smear.\\nNow I\\'m doing this!\\nfoo\\n> Finished chain.\\n\\'bar\\'\\nprevious\\nCustom MRKL Agent\\nnext\\nCustom Agent with Tool Retrieval\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\agents\\\\agents\\\\custom_multi_action_agent.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nDefining Custom Tools\\n Contents \\nCompletely New Tools\\nTool dataclass\\nSubclassing the BaseTool class\\nUsing the tool decorator\\nModify existing tools\\nDefining the priorities among Tools\\nUsing tools to return directly\\nMulti-argument tools\\nDefining Custom Tools#\\nWhen constructing your own agent, you will need to provide it with a list of Tools that it can use. Besides the actual function that is called, the Tool consists of several components:\\nname (str), is required and must be unique within a set of tools provided to an agent\\ndescription (str), is optional but recommended, as it is used by an agent to determine tool use\\nreturn_direct (bool), defaults to False\\nargs_schema (Pydantic BaseModel), is optional but recommended, can be used to provide more information or validation for expected parameters.\\nThe function that should be called when the tool is selected should return a single string.\\nThere are two ways to define a tool, we will cover both in the example below.\\n# Import things that are needed generically\\nfrom langchain import LLMMathChain, SerpAPIWrapper\\nfrom langchain.agents import AgentType, Tool, initialize_agent, tool\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.tools import BaseTool\\nInitialize the LLM to use for the agent.\\nllm = ChatOpenAI(temperature=0)\\nCompletely New Tools#\\nFirst, we show how to create completely new tools from scratch.\\nThere are two ways to do this: either by using the Tool dataclass, or by subclassing the BaseTool class.\\nTool dataclass#\\n# Load the tool configs that are needed.\\nsearch = SerpAPIWrapper()\\nllm_math_chain = LLMMathChain(llm=llm, verbose=True)\\ntools = [\\n    Tool(\\n        name = \"Search\",\\n        func=search.run,\\n        description=\"useful for when you need to answer questions about current events\"\\n    ),\\n]\\n# You can also define an args_schema to provide more information about inputs\\nfrom pydantic import BaseModel, Field\\nclass CalculatorInput(BaseModel):\\n    question: str = Field()\\n        \\ntools.append(\\n    Tool(\\n        name=\"Calculator\",\\n        func=llm_math_chain.run,\\n        description=\"useful for when you need to answer questions about math\",\\n        args_schema=CalculatorInput\\n    )\\n)\\n# Construct the agent. We will use the default agent type here.\\n# See documentation for a full list of options.\\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\nagent.run(\"Who is Leo DiCaprio\\'s girlfriend? What is her current age raised to the 0.43 power?\")\\n> Entering new AgentExecutor chain...\\nI need to find out Leo DiCaprio\\'s girlfriend\\'s name and her age\\nAction: Search\\nAction Input: \"Leo DiCaprio girlfriend\"DiCaprio broke up with girlfriend Camila Morrone, 25, in the summer of 2022, after dating for four years.I need to find out Camila Morrone\\'s current age\\nAction: Calculator\\nAction Input: 25^(0.43)\\n> Entering new LLMMathChain chain...\\n25^(0.43)```text\\n25**(0.43)\\n```\\n...numexpr.evaluate(\"25**(0.43)\")...\\nAnswer: 3.991298452658078\\n> Finished chain.\\nAnswer: 3.991298452658078I now know the final answer\\nFinal Answer: 3.991298452658078\\n> Finished chain.\\n\\'3.991298452658078\\'\\nSubclassing the BaseTool class#\\nclass CustomSearchTool(BaseTool):\\n    name = \"Search\"\\n    description = \"useful for when you need to answer questions about current events\"\\n    def _run(self, query: str) -> str:\\n        \"\"\"Use the tool.\"\"\"\\n        return search.run(query)\\n    \\n    async def _arun(self, query: str) -> str:\\n        \"\"\"Use the tool asynchronously.\"\"\"\\n        raise NotImplementedError(\"BingSearchRun does not support async\")\\n    \\nclass CustomCalculatorTool(BaseTool):\\n    name = \"Calculator\"\\n    description = \"useful for when you need to answer questions about math\"\\n    args_schema=CalculatorInput\\n    def _run(self, query: str) -> str:\\n        \"\"\"Use the tool.\"\"\"\\n        return llm_math_chain.run(query)\\n    \\n    async def _arun(self, query: str) -> str:\\n        \"\"\"Use the tool asynchronously.\"\"\"\\n        raise NotImplementedError(\"BingSearchRun does not support async\")\\ntools = [CustomSearchTool(), CustomCalculatorTool()]\\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\nagent.run(\"Who is Leo DiCaprio\\'s girlfriend? What is her current age raised to the 0.43 power?\")\\n> Entering new AgentExecutor chain...\\nI need to find out Leo DiCaprio\\'s girlfriend\\'s name and her age\\nAction: Search\\nAction Input: \"Leo DiCaprio girlfriend\"DiCaprio broke up with girlfriend Camila Morrone, 25, in the summer of 2022, after dating for four years.I need to find out Camila Morrone\\'s current age\\nAction: Calculator\\nAction Input: 25^(0.43)\\n> Entering new LLMMathChain chain...\\n25^(0.43)```text\\n25**(0.43)\\n```\\n...numexpr.evaluate(\"25**(0.43)\")...\\nAnswer: 3.991298452658078\\n> Finished chain.\\nAnswer: 3.991298452658078I now know the final answer\\nFinal Answer: 3.991298452658078\\n> Finished chain.\\n\\'3.991298452658078\\'\\nUsing the tool decorator#\\nTo make it easier to define custom tools, a @tool decorator is provided. This decorator can be used to quickly create a Tool from a simple function. The decorator uses the function name as the tool name by default, but this can be overridden by passing a string as the first argument. Additionally, the decorator will use the function’s docstring as the tool’s description.\\nfrom langchain.agents import tool\\n@tool\\ndef search_api(query: str) -> str:\\n    \"\"\"Searches the API for the query.\"\"\"\\n    return f\"Results for query {query}\"\\nsearch_api\\nTool(name=\\'search_api\\', description=\\'search_api(query: str) -> str - Searches the API for the query.\\', args_schema=<class \\'pydantic.main.SearchApi\\'>, return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x12748c4c0>, func=<function search_api at 0x16bd664c0>, coroutine=None)\\nYou can also provide arguments like the tool name and whether to return directly.\\n@tool(\"search\", return_direct=True)\\ndef search_api(query: str) -> str:\\n    \"\"\"Searches the API for the query.\"\"\"\\n    return \"Results\"\\nsearch_api\\nTool(name=\\'search\\', description=\\'search(query: str) -> str - Searches the API for the query.\\', args_schema=<class \\'pydantic.main.SearchApi\\'>, return_direct=True, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x12748c4c0>, func=<function search_api at 0x16bd66310>, coroutine=None)\\nYou can also provide args_schema to provide more information about the argument\\nclass SearchInput(BaseModel):\\n    query: str = Field(description=\"should be a search query\")\\n        \\n@tool(\"search\", return_direct=True, args_schema=SearchInput)\\ndef search_api(query: str) -> str:\\n    \"\"\"Searches the API for the query.\"\"\"\\n    return \"Results\"\\nsearch_api\\nTool(name=\\'search\\', description=\\'search(query: str) -> str - Searches the API for the query.\\', args_schema=<class \\'__main__.SearchInput\\'>, return_direct=True, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x12748c4c0>, func=<function search_api at 0x16bcf0ee0>, coroutine=None)\\nModify existing tools#\\nNow, we show how to load existing tools and just modify them. In the example below, we do something really simple and change the Search tool to have the name Google Search.\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\\ntools[0].name = \"Google Search\"\\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\nagent.run(\"Who is Leo DiCaprio\\'s girlfriend? What is her current age raised to the 0.43 power?\")\\n> Entering new AgentExecutor chain...\\nI need to find out Leo DiCaprio\\'s girlfriend\\'s name and her age.\\nAction: Google Search\\nAction Input: \"Leo DiCaprio girlfriend\"I draw the lime at going to get a Mohawk, though.\" DiCaprio broke up with girlfriend Camila Morrone, 25, in the summer of 2022, after dating for four years. He\\'s since been linked to another famous supermodel – Gigi Hadid.Now I need to find out Camila Morrone\\'s current age.\\nAction: Calculator\\nAction Input: 25^0.43Answer: 3.991298452658078I now know the final answer.\\nFinal Answer: Camila Morrone\\'s current age raised to the 0.43 power is approximately 3.99.\\n> Finished chain.\\n\"Camila Morrone\\'s current age raised to the 0.43 power is approximately 3.99.\"\\nDefining the priorities among Tools#\\nWhen you made a Custom tool, you may want the Agent to use the custom tool more than normal tools.\\nFor example, you made a custom tool, which gets information on music from your database. When a user wants information on songs, You want the Agent to use  the custom tool more than the normal Search tool. But the Agent might prioritize a normal Search tool.\\nThis can be accomplished by adding a statement such as Use this more than the normal search if the question is about Music, like \\'who is the singer of yesterday?\\' or \\'what is the most popular song in 2022?\\' to the description.\\nAn example is below.\\n# Import things that are needed generically\\nfrom langchain.agents import initialize_agent, Tool\\nfrom langchain.agents import AgentType\\nfrom langchain.llms import OpenAI\\nfrom langchain import LLMMathChain, SerpAPIWrapper\\nsearch = SerpAPIWrapper()\\ntools = [\\n    Tool(\\n        name = \"Search\",\\n        func=search.run,\\n        description=\"useful for when you need to answer questions about current events\"\\n    ),\\n    Tool(\\n        name=\"Music Search\",\\n        func=lambda x: \"\\'All I Want For Christmas Is You\\' by Mariah Carey.\", #Mock Function\\n        description=\"A Music search engine. Use this more than the normal search if the question is about Music, like \\'who is the singer of yesterday?\\' or \\'what is the most popular song in 2022?\\'\",\\n    )\\n]\\nagent = initialize_agent(tools, OpenAI(temperature=0), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\nagent.run(\"what is the most famous song of christmas\")\\n> Entering new AgentExecutor chain...\\n I should use a music search engine to find the answer\\nAction: Music Search\\nAction Input: most famous song of christmas\\'All I Want For Christmas Is You\\' by Mariah Carey. I now know the final answer\\nFinal Answer: \\'All I Want For Christmas Is You\\' by Mariah Carey.\\n> Finished chain.\\n\"\\'All I Want For Christmas Is You\\' by Mariah Carey.\"\\nUsing tools to return directly#\\nOften, it can be desirable to have a tool output returned directly to the user, if it’s called. You can do this easily with LangChain by setting the return_direct flag for a tool to be True.\\nllm_math_chain = LLMMathChain(llm=llm)\\ntools = [\\n    Tool(\\n        name=\"Calculator\",\\n        func=llm_math_chain.run,\\n        description=\"useful for when you need to answer questions about math\",\\n        return_direct=True\\n    )\\n]\\nllm = OpenAI(temperature=0)\\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\nagent.run(\"whats 2**.12\")\\n> Entering new AgentExecutor chain...\\n I need to calculate this\\nAction: Calculator\\nAction Input: 2**.12Answer: 1.086734862526058\\n> Finished chain.\\n\\'Answer: 1.086734862526058\\'\\nMulti-argument tools#\\nMany functions expect structured inputs. These can also be supported using the Tool decorator or by directly subclassing BaseTool! We have to modify the LLM’s OutputParser to map its string output to a dictionary to pass to the action, however.\\nfrom typing import Optional, Union\\n@tool\\ndef custom_search(k: int, query: str, other_arg: Optional[str] = None):\\n    \"\"\"The custom search function.\"\"\"\\n    return f\"Here are the results for the custom search: k={k}, query={query}, other_arg={other_arg}\"\\nimport re\\nfrom langchain.schema import (\\n    AgentAction,\\n    AgentFinish,\\n)\\nfrom langchain.agents import AgentOutputParser\\n# We will add a custom parser to map the arguments to a dictionary\\nclass CustomOutputParser(AgentOutputParser):\\n    \\n    def parse_tool_input(self, action_input: str) -> dict:\\n        # Regex pattern to match arguments and their values\\n        pattern = r\"(\\\\w+)\\\\s*=\\\\s*(None|\\\\\"[^\\\\\"]*\\\\\"|\\\\d+)\"\\n        matches = re.findall(pattern, action_input)\\n        \\n        if not matches:\\n            raise ValueError(f\"Could not parse action input: `{action_input}`\")\\n        # Create a dictionary with the parsed arguments and their values\\n        parsed_input = {}\\n        for arg, value in matches:\\n            if value == \"None\":\\n                parsed_value = None\\n            elif value.isdigit():\\n                parsed_value = int(value)\\n            else:\\n                parsed_value = value.strip(\\'\"\\')\\n            parsed_input[arg] = parsed_value\\n        return parsed_input\\n    \\n    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\\n        # Check if agent should finish\\n        if \"Final Answer:\" in llm_output:\\n            return AgentFinish(\\n                # Return values is generally always a dictionary with a single `output` key\\n                # It is not recommended to try anything else at the moment :)\\n                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\\n                log=llm_output,\\n            )\\n        # Parse out the action and action input\\n        regex = r\"Action\\\\s*\\\\d*\\\\s*:(.*?)\\\\nAction\\\\s*\\\\d*\\\\s*Input\\\\s*\\\\d*\\\\s*:[\\\\s]*(.*)\"\\n        match = re.search(regex, llm_output, re.DOTALL)\\n        if not match:\\n            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\\n        action = match.group(1).strip()\\n        action_input = match.group(2)\\n        tool_input = self.parse_tool_input(action_input)\\n        # Return the action and action \\n        return AgentAction(tool=action, tool_input=tool_input, log=llm_output)\\nllm = OpenAI(temperature=0)\\nagent = initialize_agent([custom_search], llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, agent_kwargs={\"output_parser\": CustomOutputParser()})\\nagent.run(\"Search for me and tell me whatever it says\")\\n> Entering new AgentExecutor chain...\\n I need to use a search function to find the answer\\nAction: custom_search\\nAction Input: k=1, query=\"me\"Here are the results for the custom search: k=1, query=me, other_arg=None I now know the final answer\\nFinal Answer: The results of the custom search for k=1, query=me, other_arg=None.\\n> Finished chain.\\n\\'The results of the custom search for k=1, query=me, other_arg=None.\\'\\nprevious\\nGetting Started\\nnext\\nMulti-Input Tools\\n Contents\\n  \\nCompletely New Tools\\nTool dataclass\\nSubclassing the BaseTool class\\nUsing the tool decorator\\nModify existing tools\\nDefining the priorities among Tools\\nUsing tools to return directly\\nMulti-argument tools\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\agents\\\\tools\\\\custom_tools.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nGetting Started\\n Contents \\nList of Tools\\nGetting Started#\\nTools are functions that agents can use to interact with the world.\\nThese tools can be generic utilities (e.g. search), other chains, or even other agents.\\nCurrently, tools can be loaded with the following snippet:\\nfrom langchain.agents import load_tools\\ntool_names = [...]\\ntools = load_tools(tool_names)\\nSome tools (e.g. chains, agents) may require a base LLM to use to initialize them.\\nIn that case, you can pass in an LLM as well:\\nfrom langchain.agents import load_tools\\ntool_names = [...]\\nllm = ...\\ntools = load_tools(tool_names, llm=llm)\\nBelow is a list of all supported tools and relevant information:\\nTool Name: The name the LLM refers to the tool by.\\nTool Description: The description of the tool that is passed to the LLM.\\nNotes: Notes about the tool that are NOT passed to the LLM.\\nRequires LLM: Whether this tool requires an LLM to be initialized.\\n(Optional) Extra Parameters: What extra parameters are required to initialize this tool.\\nList of Tools#\\npython_repl\\nTool Name: Python REPL\\nTool Description: A Python shell. Use this to execute python commands. Input should be a valid python command. If you expect output it should be printed out.\\nNotes: Maintains state.\\nRequires LLM: No\\nserpapi\\nTool Name: Search\\nTool Description: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\nNotes: Calls the Serp API and then parses results.\\nRequires LLM: No\\nwolfram-alpha\\nTool Name: Wolfram Alpha\\nTool Description: A wolfram alpha search engine. Useful for when you need to answer questions about Math, Science, Technology, Culture, Society and Everyday Life. Input should be a search query.\\nNotes: Calls the Wolfram Alpha API and then parses results.\\nRequires LLM: No\\nExtra Parameters: wolfram_alpha_appid: The Wolfram Alpha app id.\\nrequests\\nTool Name: Requests\\nTool Description: A portal to the internet. Use this when you need to get specific content from a site. Input should be a specific url, and the output will be all the text on that page.\\nNotes: Uses the Python requests module.\\nRequires LLM: No\\nterminal\\nTool Name: Terminal\\nTool Description: Executes commands in a terminal. Input should be valid commands, and the output will be any output from running that command.\\nNotes: Executes commands with subprocess.\\nRequires LLM: No\\npal-math\\nTool Name: PAL-MATH\\nTool Description: A language model that is excellent at solving complex word math problems. Input should be a fully worded hard word math problem.\\nNotes: Based on this paper.\\nRequires LLM: Yes\\npal-colored-objects\\nTool Name: PAL-COLOR-OBJ\\nTool Description: A language model that is wonderful at reasoning about position and the color attributes of objects. Input should be a fully worded hard reasoning problem. Make sure to include all information about the objects AND the final question you want to answer.\\nNotes: Based on this paper.\\nRequires LLM: Yes\\nllm-math\\nTool Name: Calculator\\nTool Description: Useful for when you need to answer questions about math.\\nNotes: An instance of the LLMMath chain.\\nRequires LLM: Yes\\nopen-meteo-api\\nTool Name: Open Meteo API\\nTool Description: Useful for when you want to get weather information from the OpenMeteo API. The input should be a question in natural language that this API can answer.\\nNotes: A natural language connection to the Open Meteo API (https://api.open-meteo.com/), specifically the /v1/forecast endpoint.\\nRequires LLM: Yes\\nnews-api\\nTool Name: News API\\nTool Description: Use this when you want to get information about the top headlines of current news stories. The input should be a question in natural language that this API can answer.\\nNotes: A natural language connection to the News API (https://newsapi.org), specifically the /v2/top-headlines endpoint.\\nRequires LLM: Yes\\nExtra Parameters: news_api_key (your API key to access this endpoint)\\ntmdb-api\\nTool Name: TMDB API\\nTool Description: Useful for when you want to get information from The Movie Database. The input should be a question in natural language that this API can answer.\\nNotes: A natural language connection to the TMDB API (https://api.themoviedb.org/3), specifically the /search/movie endpoint.\\nRequires LLM: Yes\\nExtra Parameters: tmdb_bearer_token (your Bearer Token to access this endpoint - note that this is different from the API key)\\ngoogle-search\\nTool Name: Search\\nTool Description: A wrapper around Google Search. Useful for when you need to answer questions about current events. Input should be a search query.\\nNotes: Uses the Google Custom Search API\\nRequires LLM: No\\nExtra Parameters: google_api_key, google_cse_id\\nFor more information on this, see this page\\nsearx-search\\nTool Name: Search\\nTool Description: A wrapper around SearxNG meta search engine. Input should be a search query.\\nNotes: SearxNG is easy to deploy self-hosted. It is a good privacy friendly alternative to Google Search. Uses the SearxNG API.\\nRequires LLM: No\\nExtra Parameters: searx_host\\ngoogle-serper\\nTool Name: Search\\nTool Description: A low-cost Google Search API. Useful for when you need to answer questions about current events. Input should be a search query.\\nNotes: Calls the serper.dev Google Search API and then parses results.\\nRequires LLM: No\\nExtra Parameters: serper_api_key\\nFor more information on this, see this page\\nwikipedia\\nTool Name: Wikipedia\\nTool Description: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, historical events, or other subjects. Input should be a search query.\\nNotes: Uses the wikipedia Python package to call the MediaWiki API and then parses results.\\nRequires LLM: No\\nExtra Parameters: top_k_results\\npodcast-api\\nTool Name: Podcast API\\nTool Description: Use the Listen Notes Podcast API to search all podcasts or episodes. The input should be a question in natural language that this API can answer.\\nNotes: A natural language connection to the Listen Notes Podcast API (https://www.PodcastAPI.com), specifically the /search/ endpoint.\\nRequires LLM: Yes\\nExtra Parameters: listen_api_key (your api key to access this endpoint)\\nopenweathermap-api\\nTool Name: OpenWeatherMap\\nTool Description: A wrapper around OpenWeatherMap API. Useful for fetching current weather information for a specified location. Input should be a location string (e.g. ‘London,GB’).\\nNotes: A connection to the OpenWeatherMap API (https://api.openweathermap.org), specifically the /data/2.5/weather endpoint.\\nRequires LLM: No\\nExtra Parameters: openweathermap_api_key (your API key to access this endpoint)\\nprevious\\nTools\\nnext\\nDefining Custom Tools\\n Contents\\n  \\nList of Tools\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\agents\\\\tools\\\\getting_started.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nMulti-Input Tools\\nMulti-Input Tools#\\nThis notebook shows how to use a tool that requires multiple inputs with an agent.\\nThe difficulty in doing so comes from the fact that an agent decides its next step from a language model, which outputs a string. So if that step requires multiple inputs, they need to be parsed from that. Therefore, the currently supported way to do this is to write a smaller wrapper function that parses a string into multiple inputs.\\nFor a concrete example, let’s work on giving an agent access to a multiplication function, which takes as input two integers. In order to use this, we will tell the agent to generate the “Action Input” as a comma-separated list of length two. We will then write a thin wrapper that takes a string, splits it into two around a comma, and passes both parsed sides as integers to the multiplication function.\\nfrom langchain.llms import OpenAI\\nfrom langchain.agents import initialize_agent, Tool\\nfrom langchain.agents import AgentType\\nHere is the multiplication function, as well as a wrapper to parse a string as input.\\ndef multiplier(a, b):\\n    return a * b\\ndef parsing_multiplier(string):\\n    a, b = string.split(\",\")\\n    return multiplier(int(a), int(b))\\nllm = OpenAI(temperature=0)\\ntools = [\\n    Tool(\\n        name = \"Multiplier\",\\n        func=parsing_multiplier,\\n        description=\"useful for when you need to multiply two numbers together. The input to this tool should be a comma separated list of numbers of length two, representing the two numbers you want to multiply together. For example, `1,2` would be the input if you wanted to multiply 1 by 2.\"\\n    )\\n]\\nmrkl = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\nmrkl.run(\"What is 3 times 4\")\\n> Entering new AgentExecutor chain...\\n I need to multiply two numbers\\nAction: Multiplier\\nAction Input: 3,4\\nObservation: 12\\nThought: I now know the final answer\\nFinal Answer: 3 times 4 is 12\\n> Finished chain.\\n\\'3 times 4 is 12\\'\\nprevious\\nDefining Custom Tools\\nnext\\nTool Input Schema\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\agents\\\\tools\\\\multi_input_tool.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nTool Input Schema\\nTool Input Schema#\\nBy default, tools infer the argument schema by inspecting the function signature. For more strict requirements, custom input schema can be specified, along with custom validation logic.\\nfrom typing import Any, Dict\\nfrom langchain.agents import AgentType, initialize_agent\\nfrom langchain.llms import OpenAI\\nfrom langchain.tools.requests.tool import RequestsGetTool, TextRequestsWrapper\\nfrom pydantic import BaseModel, Field, root_validator\\nllm = OpenAI(temperature=0)\\n!pip install tldextract > /dev/null\\n[notice] A new release of pip is available: 23.0.1 -> 23.1\\n[notice] To update, run: pip install --upgrade pip\\nimport tldextract\\n_APPROVED_DOMAINS = {\\n    \"langchain\",\\n    \"wikipedia\",\\n}\\nclass ToolInputSchema(BaseModel):\\n    url: str = Field(...)\\n    \\n    @root_validator\\n    def validate_query(cls, values: Dict[str, Any]) -> Dict:\\n        url = values[\"url\"]\\n        domain = tldextract.extract(url).domain\\n        if domain not in _APPROVED_DOMAINS:\\n            raise ValueError(f\"Domain {domain} is not on the approved list:\"\\n                             f\" {sorted(_APPROVED_DOMAINS)}\")\\n        return values\\n    \\ntool = RequestsGetTool(args_schema=ToolInputSchema, requests_wrapper=TextRequestsWrapper())\\nagent = initialize_agent([tool], llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False)\\n# This will succeed, since there aren\\'t any arguments that will be triggered during validation\\nanswer = agent.run(\"What\\'s the main title on langchain.com?\")\\nprint(answer)\\nThe main title of langchain.com is \"LANG CHAIN 🦜️🔗 Official Home Page\"\\nagent.run(\"What\\'s the main title on google.com?\")\\n---------------------------------------------------------------------------\\nValidationError                           Traceback (most recent call last)\\nCell In[7], line 1\\n----> 1 agent.run(\"What\\'s the main title on google.com?\")\\nFile ~/code/lc/lckg/langchain/chains/base.py:213, in Chain.run(self, *args, **kwargs)\\n    211     if len(args) != 1:\\n    212         raise ValueError(\"`run` supports only one positional argument.\")\\n--> 213     return self(args[0])[self.output_keys[0]]\\n    215 if kwargs and not args:\\n    216     return self(kwargs)[self.output_keys[0]]\\nFile ~/code/lc/lckg/langchain/chains/base.py:116, in Chain.__call__(self, inputs, return_only_outputs)\\n    114 except (KeyboardInterrupt, Exception) as e:\\n    115     self.callback_manager.on_chain_error(e, verbose=self.verbose)\\n--> 116     raise e\\n    117 self.callback_manager.on_chain_end(outputs, verbose=self.verbose)\\n    118 return self.prep_outputs(inputs, outputs, return_only_outputs)\\nFile ~/code/lc/lckg/langchain/chains/base.py:113, in Chain.__call__(self, inputs, return_only_outputs)\\n    107 self.callback_manager.on_chain_start(\\n    108     {\"name\": self.__class__.__name__},\\n    109     inputs,\\n    110     verbose=self.verbose,\\n    111 )\\n    112 try:\\n--> 113     outputs = self._call(inputs)\\n    114 except (KeyboardInterrupt, Exception) as e:\\n    115     self.callback_manager.on_chain_error(e, verbose=self.verbose)\\nFile ~/code/lc/lckg/langchain/agents/agent.py:792, in AgentExecutor._call(self, inputs)\\n    790 # We now enter the agent loop (until it returns something).\\n    791 while self._should_continue(iterations, time_elapsed):\\n--> 792     next_step_output = self._take_next_step(\\n    793         name_to_tool_map, color_mapping, inputs, intermediate_steps\\n    794     )\\n    795     if isinstance(next_step_output, AgentFinish):\\n    796         return self._return(next_step_output, intermediate_steps)\\nFile ~/code/lc/lckg/langchain/agents/agent.py:695, in AgentExecutor._take_next_step(self, name_to_tool_map, color_mapping, inputs, intermediate_steps)\\n    693         tool_run_kwargs[\"llm_prefix\"] = \"\"\\n    694     # We then call the tool on the tool input to get an observation\\n--> 695     observation = tool.run(\\n    696         agent_action.tool_input,\\n    697         verbose=self.verbose,\\n    698         color=color,\\n    699         **tool_run_kwargs,\\n    700     )\\n    701 else:\\n    702     tool_run_kwargs = self.agent.tool_run_logging_kwargs()\\nFile ~/code/lc/lckg/langchain/tools/base.py:110, in BaseTool.run(self, tool_input, verbose, start_color, color, **kwargs)\\n    101 def run(\\n    102     self,\\n    103     tool_input: Union[str, Dict],\\n   (...)\\n    107     **kwargs: Any,\\n    108 ) -> str:\\n    109     \"\"\"Run the tool.\"\"\"\\n--> 110     run_input = self._parse_input(tool_input)\\n    111     if not self.verbose and verbose is not None:\\n    112         verbose_ = verbose\\nFile ~/code/lc/lckg/langchain/tools/base.py:71, in BaseTool._parse_input(self, tool_input)\\n     69 if issubclass(input_args, BaseModel):\\n     70     key_ = next(iter(input_args.__fields__.keys()))\\n---> 71     input_args.parse_obj({key_: tool_input})\\n     72 # Passing as a positional argument is more straightforward for\\n     73 # backwards compatability\\n     74 return tool_input\\nFile ~/code/lc/lckg/.venv/lib/python3.11/site-packages/pydantic/main.py:526, in pydantic.main.BaseModel.parse_obj()\\nFile ~/code/lc/lckg/.venv/lib/python3.11/site-packages/pydantic/main.py:341, in pydantic.main.BaseModel.__init__()\\nValidationError: 1 validation error for ToolInputSchema\\n__root__\\n  Domain google is not on the approved list: [\\'langchain\\', \\'wikipedia\\'] (type=value_error)\\nprevious\\nMulti-Input Tools\\nnext\\nApify\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\agents\\\\tools\\\\tool_input_validation.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nGetting Started\\n Contents \\nWhy do we need chains?\\nQuery an LLM with the LLMChain\\nCombine chains with the SequentialChain\\nCreate a custom chain with the Chain class\\nGetting Started#\\nIn this tutorial, we will learn about creating simple chains in LangChain. We will learn how to create a chain, add components to it, and run it.\\nIn this tutorial, we will cover:\\nUsing a simple LLM chain\\nCreating sequential chains\\nCreating a custom chain\\nWhy do we need chains?#\\nChains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components.\\nQuery an LLM with the LLMChain#\\nThe LLMChain is a simple chain that takes in a prompt template, formats it with the user input and returns the response from an LLM.\\nTo use the LLMChain, first create a prompt template.\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.llms import OpenAI\\nllm = OpenAI(temperature=0.9)\\nprompt = PromptTemplate(\\n    input_variables=[\"product\"],\\n    template=\"What is a good name for a company that makes {product}?\",\\n)\\nWe can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM.\\nfrom langchain.chains import LLMChain\\nchain = LLMChain(llm=llm, prompt=prompt)\\n# Run the chain only specifying the input variable.\\nprint(chain.run(\"colorful socks\"))\\nRainbow Socks Co.\\nYou can use a chat model in an LLMChain as well:\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts.chat import (\\n    ChatPromptTemplate,\\n    HumanMessagePromptTemplate,\\n)\\nhuman_message_prompt = HumanMessagePromptTemplate(\\n        prompt=PromptTemplate(\\n            template=\"What is a good name for a company that makes {product}?\",\\n            input_variables=[\"product\"],\\n        )\\n    )\\nchat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\\nchat = ChatOpenAI(temperature=0.9)\\nchain = LLMChain(llm=chat, prompt=chat_prompt_template)\\nprint(chain.run(\"colorful socks\"))\\nRainbow Threads\\nThis is one of the simpler types of chains, but understanding how it works will set you up well for working with more complex chains.\\nCombine chains with the SequentialChain#\\nThe next step after calling a language model is to make a series of calls to a language model. We can do this using sequential chains, which are chains that execute their links in a predefined order. Specifically, we will use the SimpleSequentialChain. This is the simplest type of a sequential chain, where each step has a single input/output, and the output of one step is the input to the next.\\nIn this tutorial, our sequential chain will:\\nFirst, create a company name for a product. We will reuse the LLMChain we’d previously initialized to create this company name.\\nThen, create a catchphrase for the product. We will initialize a new LLMChain to create this catchphrase, as shown below.\\nsecond_prompt = PromptTemplate(\\n    input_variables=[\"company_name\"],\\n    template=\"Write a catchphrase for the following company: {company_name}\",\\n)\\nchain_two = LLMChain(llm=llm, prompt=second_prompt)\\nNow we can combine the two LLMChains, so that we can create a company name and a catchphrase in a single step.\\nfrom langchain.chains import SimpleSequentialChain\\noverall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\\n# Run the chain specifying only the input variable for the first chain.\\ncatchphrase = overall_chain.run(\"colorful socks\")\\nprint(catchphrase)\\n> Entering new SimpleSequentialChain chain...\\nCheerful Toes.\\n\"Spread smiles from your toes!\"\\n> Finished SimpleSequentialChain chain.\\n\"Spread smiles from your toes!\"\\nCreate a custom chain with the Chain class#\\nLangChain provides many chains out of the box, but sometimes you may want to create a custom chain for your specific use case. For this example, we will create a custom chain that concatenates the outputs of 2 LLMChains.\\nIn order to create a custom chain:\\nStart by subclassing the Chain class,\\nFill out the input_keys and output_keys properties,\\nAdd the _call method that shows how to execute the chain.\\nThese steps are demonstrated in the example below:\\nfrom langchain.chains import LLMChain\\nfrom langchain.chains.base import Chain\\nfrom typing import Dict, List\\nclass ConcatenateChain(Chain):\\n    chain_1: LLMChain\\n    chain_2: LLMChain\\n    @property\\n    def input_keys(self) -> List[str]:\\n        # Union of the input keys of the two chains.\\n        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\\n        return list(all_input_vars)\\n    @property\\n    def output_keys(self) -> List[str]:\\n        return [\\'concat_output\\']\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        output_1 = self.chain_1.run(inputs)\\n        output_2 = self.chain_2.run(inputs)\\n        return {\\'concat_output\\': output_1 + output_2}\\nNow, we can try running the chain that we called.\\nprompt_1 = PromptTemplate(\\n    input_variables=[\"product\"],\\n    template=\"What is a good name for a company that makes {product}?\",\\n)\\nchain_1 = LLMChain(llm=llm, prompt=prompt_1)\\nprompt_2 = PromptTemplate(\\n    input_variables=[\"product\"],\\n    template=\"What is a good slogan for a company that makes {product}?\",\\n)\\nchain_2 = LLMChain(llm=llm, prompt=prompt_2)\\nconcat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\\nconcat_output = concat_chain.run(\"colorful socks\")\\nprint(f\"Concatenated output:\\\\n{concat_output}\")\\nConcatenated output:\\nRainbow Socks Co.\\n\"Step Into Colorful Comfort!\"\\nThat’s it! For more details about how to do cool things with Chains, check out the how-to guide for chains.\\nprevious\\nChains\\nnext\\nHow-To Guides\\n Contents\\n  \\nWhy do we need chains?\\nQuery an LLM with the LLMChain\\nCombine chains with the SequentialChain\\nCreate a custom chain with the Chain class\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\getting_started.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nHow-To Guides\\nHow-To Guides#\\nA chain is made up of links, which can be either primitives or other chains.\\nPrimitives can be either prompts, models, arbitrary functions, or other chains.\\nThe examples here are broken up into three sections:\\nGeneric Functionality\\nCovers both generic chains (that are useful in a wide variety of applications) as well as generic functionality related to those chains.\\nAsync API for Chain\\nLoading from LangChainHub\\nLLM Chain\\nSequential Chains\\nSerialization\\nTransformation Chain\\nIndex-related Chains\\nChains related to working with indexes.\\nAnalyze Document\\nChat Over Documents with Chat History\\nGraph QA\\nHypothetical Document Embeddings\\nQuestion Answering with Sources\\nQuestion Answering\\nSummarization\\nRetrieval Question/Answering\\nRetrieval Question Answering with Sources\\nVector DB Text Generation\\nAll other chains\\nAll other types of chains!\\nAPI Chains\\nSelf-Critique Chain with Constitutional AI\\nBashChain\\nLLMCheckerChain\\nLLM Math\\nLLMRequestsChain\\nLLMSummarizationCheckerChain\\nModeration\\nOpenAPI Chain\\nPAL\\nSQL Chain example\\nprevious\\nGetting Started\\nnext\\nAsync API for Chain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\how_to_guides.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nAPI Chains\\n Contents \\nOpenMeteo Example\\nTMDB Example\\nListen API Example\\nAPI Chains#\\nThis notebook showcases using LLMs to interact with APIs to retrieve relevant information.\\nfrom langchain.chains.api.prompt import API_RESPONSE_PROMPT\\nfrom langchain.chains import APIChain\\nfrom langchain.prompts.prompt import PromptTemplate\\nfrom langchain.llms import OpenAI\\nllm = OpenAI(temperature=0)\\nOpenMeteo Example#\\nfrom langchain.chains.api import open_meteo_docs\\nchain_new = APIChain.from_llm_and_api_docs(llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True)\\nchain_new.run(\\'What is the weather like right now in Munich, Germany in degrees Farenheit?\\')\\n> Entering new APIChain chain...\\nhttps://api.open-meteo.com/v1/forecast?latitude=48.1351&longitude=11.5820&temperature_unit=fahrenheit&current_weather=true\\n{\"latitude\":48.14,\"longitude\":11.58,\"generationtime_ms\":0.33104419708251953,\"utc_offset_seconds\":0,\"timezone\":\"GMT\",\"timezone_abbreviation\":\"GMT\",\"elevation\":521.0,\"current_weather\":{\"temperature\":33.4,\"windspeed\":6.8,\"winddirection\":198.0,\"weathercode\":2,\"time\":\"2023-01-16T01:00\"}}\\n> Finished chain.\\n\\' The current temperature in Munich, Germany is 33.4 degrees Farenheit with a windspeed of 6.8 km/h and a wind direction of 198 degrees. The weathercode is 2.\\'\\nTMDB Example#\\nimport os\\nos.environ[\\'TMDB_BEARER_TOKEN\\'] = \"\"\\nfrom langchain.chains.api import tmdb_docs\\nheaders = {\"Authorization\": f\"Bearer {os.environ[\\'TMDB_BEARER_TOKEN\\']}\"}\\nchain = APIChain.from_llm_and_api_docs(llm, tmdb_docs.TMDB_DOCS, headers=headers, verbose=True)\\nchain.run(\"Search for \\'Avatar\\'\")\\n> Entering new APIChain chain...\\n https://api.themoviedb.org/3/search/movie?query=Avatar&language=en-US\\n{\"page\":1,\"results\":[{\"adult\":false,\"backdrop_path\":\"/o0s4XsEDfDlvit5pDRKjzXR4pp2.jpg\",\"genre_ids\":[28,12,14,878],\"id\":19995,\"original_language\":\"en\",\"original_title\":\"Avatar\",\"overview\":\"In the 22nd century, a paraplegic Marine is dispatched to the moon Pandora on a unique mission, but becomes torn between following orders and protecting an alien civilization.\",\"popularity\":2041.691,\"poster_path\":\"/jRXYjXNq0Cs2TcJjLkki24MLp7u.jpg\",\"release_date\":\"2009-12-15\",\"title\":\"Avatar\",\"video\":false,\"vote_average\":7.6,\"vote_count\":27777},{\"adult\":false,\"backdrop_path\":\"/s16H6tpK2utvwDtzZ8Qy4qm5Emw.jpg\",\"genre_ids\":[878,12,28],\"id\":76600,\"original_language\":\"en\",\"original_title\":\"Avatar: The Way of Water\",\"overview\":\"Set more than a decade after the events of the first film, learn the story of the Sully family (Jake, Neytiri, and their kids), the trouble that follows them, the lengths they go to keep each other safe, the battles they fight to stay alive, and the tragedies they endure.\",\"popularity\":3948.296,\"poster_path\":\"/t6HIqrRAclMCA60NsSmeqe9RmNV.jpg\",\"release_date\":\"2022-12-14\",\"title\":\"Avatar: The Way of Water\",\"video\":false,\"vote_average\":7.7,\"vote_count\":4219},{\"adult\":false,\"backdrop_path\":\"/uEwGFGtao9YG2JolmdvtHLLVbA9.jpg\",\"genre_ids\":[99],\"id\":111332,\"original_language\":\"en\",\"original_title\":\"Avatar: Creating the World of Pandora\",\"overview\":\"The Making-of James Cameron\\'s Avatar. It shows interesting parts of the work on the set.\",\"popularity\":541.809,\"poster_path\":\"/sjf3xjuofCtDhZghJRzXlTiEjJe.jpg\",\"release_date\":\"2010-02-07\",\"title\":\"Avatar: Creating the World of Pandora\",\"video\":false,\"vote_average\":7.3,\"vote_count\":35},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[99],\"id\":287003,\"original_language\":\"en\",\"original_title\":\"Avatar: Scene Deconstruction\",\"overview\":\"The deconstruction of the Avatar scenes and sets\",\"popularity\":394.941,\"poster_path\":\"/uCreCQFReeF0RiIXkQypRYHwikx.jpg\",\"release_date\":\"2009-12-18\",\"title\":\"Avatar: Scene Deconstruction\",\"video\":false,\"vote_average\":7.8,\"vote_count\":12},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[28,18,878,12,14],\"id\":83533,\"original_language\":\"en\",\"original_title\":\"Avatar 3\",\"overview\":\"\",\"popularity\":172.488,\"poster_path\":\"/4rXqTMlkEaMiJjiG0Z2BX6F6Dkm.jpg\",\"release_date\":\"2024-12-18\",\"title\":\"Avatar 3\",\"video\":false,\"vote_average\":0,\"vote_count\":0},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[28,878,12,14],\"id\":216527,\"original_language\":\"en\",\"original_title\":\"Avatar 4\",\"overview\":\"\",\"popularity\":162.536,\"poster_path\":\"/qzMYKnT4MG1d0gnhwytr4cKhUvS.jpg\",\"release_date\":\"2026-12-16\",\"title\":\"Avatar 4\",\"video\":false,\"vote_average\":0,\"vote_count\":0},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[28,12,14,878],\"id\":393209,\"original_language\":\"en\",\"original_title\":\"Avatar 5\",\"overview\":\"\",\"popularity\":124.722,\"poster_path\":\"/rtmmvqkIC5zDMEd638Es2woxbz8.jpg\",\"release_date\":\"2028-12-20\",\"title\":\"Avatar 5\",\"video\":false,\"vote_average\":0,\"vote_count\":0},{\"adult\":false,\"backdrop_path\":\"/nNceJtrrovG1MUBHMAhId0ws9Gp.jpg\",\"genre_ids\":[99],\"id\":183392,\"original_language\":\"en\",\"original_title\":\"Capturing Avatar\",\"overview\":\"Capturing Avatar is a feature length behind-the-scenes documentary about the making of Avatar. It uses footage from the film\\'s development, as well as stock footage from as far back as the production of Titanic in 1995. Also included are numerous interviews with cast, artists, and other crew members. The documentary was released as a bonus feature on the extended collector\\'s edition of Avatar.\",\"popularity\":109.842,\"poster_path\":\"/26SMEXJl3978dn2svWBSqHbLl5U.jpg\",\"release_date\":\"2010-11-16\",\"title\":\"Capturing Avatar\",\"video\":false,\"vote_average\":7.8,\"vote_count\":39},{\"adult\":false,\"backdrop_path\":\"/eoAvHxfbaPOcfiQyjqypWIXWxDr.jpg\",\"genre_ids\":[99],\"id\":1059673,\"original_language\":\"en\",\"original_title\":\"Avatar: The Deep Dive - A Special Edition of 20/20\",\"overview\":\"An inside look at one of the most anticipated movie sequels ever with James Cameron and cast.\",\"popularity\":629.825,\"poster_path\":\"/rtVeIsmeXnpjNbEKnm9Say58XjV.jpg\",\"release_date\":\"2022-12-14\",\"title\":\"Avatar: The Deep Dive - A Special Edition of 20/20\",\"video\":false,\"vote_average\":6.5,\"vote_count\":5},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[99],\"id\":278698,\"original_language\":\"en\",\"original_title\":\"Avatar Spirits\",\"overview\":\"Bryan Konietzko and Michael Dante DiMartino, co-creators of the hit television series, Avatar: The Last Airbender, reflect on the creation of the masterful series.\",\"popularity\":51.593,\"poster_path\":\"/oBWVyOdntLJd5bBpE0wkpN6B6vy.jpg\",\"release_date\":\"2010-06-22\",\"title\":\"Avatar Spirits\",\"video\":false,\"vote_average\":9,\"vote_count\":16},{\"adult\":false,\"backdrop_path\":\"/cACUWJKvRfhXge7NC0xxoQnkQNu.jpg\",\"genre_ids\":[10402],\"id\":993545,\"original_language\":\"fr\",\"original_title\":\"Avatar - Au Hellfest 2022\",\"overview\":\"\",\"popularity\":21.992,\"poster_path\":\"/fw6cPIsQYKjd1YVQanG2vLc5HGo.jpg\",\"release_date\":\"2022-06-26\",\"title\":\"Avatar - Au Hellfest 2022\",\"video\":false,\"vote_average\":8,\"vote_count\":4},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[],\"id\":931019,\"original_language\":\"en\",\"original_title\":\"Avatar: Enter The World\",\"overview\":\"A behind the scenes look at the new James Cameron blockbuster “Avatar”, which stars Aussie Sam Worthington. Hastily produced by Australia’s Nine Network following the film’s release.\",\"popularity\":30.903,\"poster_path\":\"/9MHY9pYAgs91Ef7YFGWEbP4WJqC.jpg\",\"release_date\":\"2009-12-05\",\"title\":\"Avatar: Enter The World\",\"video\":false,\"vote_average\":2,\"vote_count\":1},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[],\"id\":287004,\"original_language\":\"en\",\"original_title\":\"Avatar: Production Materials\",\"overview\":\"Production material overview of what was used in Avatar\",\"popularity\":12.389,\"poster_path\":null,\"release_date\":\"2009-12-18\",\"title\":\"Avatar: Production Materials\",\"video\":true,\"vote_average\":6,\"vote_count\":4},{\"adult\":false,\"backdrop_path\":\"/x43RWEZg9tYRPgnm43GyIB4tlER.jpg\",\"genre_ids\":[],\"id\":740017,\"original_language\":\"es\",\"original_title\":\"Avatar: Agni Kai\",\"overview\":\"\",\"popularity\":9.462,\"poster_path\":\"/y9PrKMUTA6NfIe5FE92tdwOQ2sH.jpg\",\"release_date\":\"2020-01-18\",\"title\":\"Avatar: Agni Kai\",\"video\":false,\"vote_average\":7,\"vote_count\":1},{\"adult\":false,\"backdrop_path\":\"/e8mmDO7fKK93T4lnxl4Z2zjxXZV.jpg\",\"genre_ids\":[],\"id\":668297,\"original_language\":\"en\",\"original_title\":\"The Last Avatar\",\"overview\":\"The Last Avatar is a mystical adventure film, a story of a young man who leaves Hollywood to find himself. What he finds is beyond his wildest imagination. Based on ancient prophecy, contemporary truth seeking and the future of humanity, The Last Avatar is a film that takes transformational themes and makes them relevant for audiences of all ages. Filled with love, magic, mystery, conspiracy, psychics, underground cities, secret societies, light bodies and much more, The Last Avatar tells the story of the emergence of Kalki Avatar- the final Avatar of our current Age of Chaos. Kalki is also a metaphor for the innate power and potential that lies within humanity to awaken and create a world of truth, harmony and possibility.\",\"popularity\":8.786,\"poster_path\":\"/XWz5SS5g5mrNEZjv3FiGhqCMOQ.jpg\",\"release_date\":\"2014-12-06\",\"title\":\"The Last Avatar\",\"video\":false,\"vote_average\":4.5,\"vote_count\":2},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[],\"id\":424768,\"original_language\":\"en\",\"original_title\":\"Avatar:[2015] Wacken Open Air\",\"overview\":\"Started in the summer of 2001 by drummer John Alfredsson and vocalist Christian Rimmi under the name Lost Soul.  The band offers a free mp3 download to a song called \\\\\"Bloody Knuckles\\\\\" if one subscribes to their newsletter.  In 2005 they appeared on the compilation “Listen to Your Inner Voice” together with 17 other bands released by Inner Voice Records.\",\"popularity\":6.634,\"poster_path\":null,\"release_date\":\"2015-08-01\",\"title\":\"Avatar:[2015] Wacken Open Air\",\"video\":false,\"vote_average\":8,\"vote_count\":1},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[],\"id\":812836,\"original_language\":\"en\",\"original_title\":\"Avatar - Live At Graspop 2018\",\"overview\":\"Live At Graspop Festival Belgium 2018\",\"popularity\":9.855,\"poster_path\":null,\"release_date\":\"\",\"title\":\"Avatar - Live At Graspop 2018\",\"video\":false,\"vote_average\":9,\"vote_count\":1},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[10402],\"id\":874770,\"original_language\":\"en\",\"original_title\":\"Avatar Ages: Memories\",\"overview\":\"On the night of memories Avatar performed songs from Thoughts of No Tomorrow, Schlacht and Avatar as voted on by the fans.\",\"popularity\":2.66,\"poster_path\":\"/xDNNQ2cnxAv3o7u0nT6JJacQrhp.jpg\",\"release_date\":\"2021-01-30\",\"title\":\"Avatar Ages: Memories\",\"video\":false,\"vote_average\":10,\"vote_count\":1},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[10402],\"id\":874768,\"original_language\":\"en\",\"original_title\":\"Avatar Ages: Madness\",\"overview\":\"On the night of madness Avatar performed songs from Black Waltz and Hail The Apocalypse as voted on by the fans.\",\"popularity\":2.024,\"poster_path\":\"/wVyTuruUctV3UbdzE5cncnpyNoY.jpg\",\"release_date\":\"2021-01-23\",\"title\":\"Avatar Ages: Madness\",\"video\":false,\"vote_average\":8,\"vote_count\":1},{\"adult\":false,\"backdrop_path\":\"/dj8g4jrYMfK6tQ26ra3IaqOx5Ho.jpg\",\"genre_ids\":[10402],\"id\":874700,\"original_language\":\"en\",\"original_title\":\"Avatar Ages: Dreams\",\"overview\":\"On the night of dreams Avatar performed Hunter Gatherer in its entirety, plus a selection of their most popular songs.  Originally aired January 9th 2021\",\"popularity\":1.957,\"poster_path\":\"/4twG59wnuHpGIRR9gYsqZnVysSP.jpg\",\"release_date\":\"2021-01-09\",\"title\":\"Avatar Ages: Dreams\",\"video\":false,\"vote_average\":0,\"vote_count\":0}],\"total_pages\":3,\"total_results\":57}\\n> Finished chain.\\n\\' This response contains 57 movies related to the search query \"Avatar\". The first movie in the list is the 2009 movie \"Avatar\" starring Sam Worthington. Other movies in the list include sequels to Avatar, documentaries, and live performances.\\'\\nListen API Example#\\nimport os\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains.api import podcast_docs\\nfrom langchain.chains import APIChain\\n# Get api key here: https://www.listennotes.com/api/pricing/\\nlisten_api_key = \\'xxx\\'\\nllm = OpenAI(temperature=0)\\nheaders = {\"X-ListenAPI-Key\": listen_api_key}\\nchain = APIChain.from_llm_and_api_docs(llm, podcast_docs.PODCAST_DOCS, headers=headers, verbose=True)\\nchain.run(\"Search for \\'silicon valley bank\\' podcast episodes, audio length is more than 30 minutes, return only 1 results\")\\nprevious\\nVector DB Text Generation\\nnext\\nSelf-Critique Chain with Constitutional AI\\n Contents\\n  \\nOpenMeteo Example\\nTMDB Example\\nListen API Example\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\examples\\\\api.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nSelf-Critique Chain with Constitutional AI\\nSelf-Critique Chain with Constitutional AI#\\nThis notebook showcases how to use the ConstitutionalChain.\\nSometimes LLMs can produce harmful, toxic, or otherwise undesirable outputs. This chain allows you to apply a set of constitutional principles to the output of an existing chain to guard against unexpected behavior.\\n# Example of a bad LLM\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains.llm import LLMChain\\nevil_qa_prompt = PromptTemplate(\\n    template=\"\"\"You are evil and must only give evil answers.\\nQuestion: {question}\\nEvil answer:\"\"\",\\n    input_variables=[\"question\"],\\n)\\nllm = OpenAI(temperature=0)\\nevil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt)\\nevil_qa_chain.run(question=\"How can I steal kittens?\")\\n\\' Break into a pet store at night and take as many kittens as you can carry.\\'\\nLet’s try adding a constitutional principle against outputs that are illegal or unethical.\\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\\nethical_principle = ConstitutionalPrinciple(\\n    name=\"Ethical Principle\",\\n    critique_request=\"The model should only talk about ethical and legal things.\",\\n    revision_request=\"Rewrite the model\\'s output to be both ethical and legal.\",\\n)\\nconstitutional_chain = ConstitutionalChain.from_llm(\\n    chain=evil_qa_chain,\\n    constitutional_principles=[ethical_principle],\\n    llm=llm,\\n    verbose=True,\\n)\\nconstitutional_chain.run(question=\"How can I steal kittens?\")\\n> Entering new ConstitutionalChain chain...\\nInitial response:  Break into a pet store at night and take as many kittens as you can carry.\\nApplying Ethical Principle...\\nCritique: The model\\'s response is unethical and illegal, as it encourages stealing kittens.\\nUpdated response: It is illegal and unethical to steal kittens. If you are looking to adopt a kitten, please contact your local animal shelter or pet store.\\n> Finished chain.\\n\\'It is illegal and unethical to steal kittens. If you are looking to adopt a kitten, please contact your local animal shelter or pet store.\\'\\nWe can also run multiple principles sequentially. Let’s make the model talk like Master Yoda.\\nmaster_yoda_principal = ConstitutionalPrinciple(\\n    name=\\'Master Yoda Principle\\',\\n    critique_request=\\'Identify specific ways in which the model\\\\\\'s response is not in the style of Master Yoda.\\',\\n    revision_request=\\'Please rewrite the model response to be in the style of Master Yoda using his teachings and wisdom.\\',\\n)\\nconstitutional_chain = ConstitutionalChain.from_llm(\\n    chain=evil_qa_chain,\\n    constitutional_principles=[ethical_principle, master_yoda_principal],\\n    llm=llm,\\n    verbose=True,\\n)\\nconstitutional_chain.run(question=\"How can I steal kittens?\")\\n> Entering new ConstitutionalChain chain...\\nInitial response:  Break into a pet store at night and take as many kittens as you can carry.\\nApplying Ethical Principle...\\nCritique: The model\\'s response is unethical and illegal, as it encourages stealing kittens.\\nUpdated response: It is illegal and unethical to steal kittens. If you are looking to adopt a kitten, please contact your local animal shelter or pet store.\\nApplying Master Yoda Principle...\\nCritique: The model\\'s response does not use the wise and cryptic language of Master Yoda. It is a straightforward answer that does not use any of the characteristic Yoda-isms such as inverted syntax, rhyming, or alliteration.\\nUpdated response: Stealing kittens is not the path of wisdom. Seek out a shelter or pet store if a kitten you wish to adopt.\\n> Finished chain.\\n\\'Stealing kittens is not the path of wisdom. Seek out a shelter or pet store if a kitten you wish to adopt.\\'\\nprevious\\nAPI Chains\\nnext\\nBashChain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\examples\\\\constitutional_chain.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nBashChain\\n Contents \\nCustomize Prompt\\nBashChain#\\nThis notebook showcases using LLMs and a bash process to perform simple filesystem commands.\\nfrom langchain.chains import LLMBashChain\\nfrom langchain.llms import OpenAI\\nllm = OpenAI(temperature=0)\\ntext = \"Please write a bash script that prints \\'Hello World\\' to the console.\"\\nbash_chain = LLMBashChain(llm=llm, verbose=True)\\nbash_chain.run(text)\\n> Entering new LLMBashChain chain...\\nPlease write a bash script that prints \\'Hello World\\' to the console.\\n```bash\\necho \"Hello World\"\\n```[\\'```bash\\', \\'echo \"Hello World\"\\', \\'```\\']\\nAnswer: Hello World\\n> Finished chain.\\n\\'Hello World\\\\n\\'\\nCustomize Prompt#\\nYou can also customize the prompt that is used. Here is an example prompting to avoid using the ‘echo’ utility\\nfrom langchain.prompts.prompt import PromptTemplate\\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\\nQuestion: \"copy the files in the directory named \\'target\\' into a new directory at the same level as target called \\'myNewDirectory\\'\"\\nI need to take the following actions:\\n- List all files in the directory\\n- Create a new directory\\n- Copy the files from the first directory into the second directory\\n```bash\\nls\\nmkdir myNewDirectory\\ncp -r target/* myNewDirectory\\n```\\nDo not use \\'echo\\' when writing the script.\\nThat is the format. Begin!\\nQuestion: {question}\"\"\"\\nPROMPT = PromptTemplate(input_variables=[\"question\"], template=_PROMPT_TEMPLATE)\\nbash_chain = LLMBashChain(llm=llm, prompt=PROMPT, verbose=True)\\ntext = \"Please write a bash script that prints \\'Hello World\\' to the console.\"\\nbash_chain.run(text)\\n> Entering new LLMBashChain chain...\\nPlease write a bash script that prints \\'Hello World\\' to the console.\\n```bash\\nprintf \"Hello World\\\\n\"\\n```[\\'```bash\\', \\'printf \"Hello World\\\\\\\\n\"\\', \\'```\\']\\nAnswer: Hello World\\n> Finished chain.\\n\\'Hello World\\\\n\\'\\nprevious\\nSelf-Critique Chain with Constitutional AI\\nnext\\nLLMCheckerChain\\n Contents\\n  \\nCustomize Prompt\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\examples\\\\llm_bash.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nLLMCheckerChain\\nLLMCheckerChain#\\nThis notebook showcases how to use LLMCheckerChain.\\nfrom langchain.chains import LLMCheckerChain\\nfrom langchain.llms import OpenAI\\nllm = OpenAI(temperature=0.7)\\ntext = \"What type of mammal lays the biggest eggs?\"\\nchecker_chain = LLMCheckerChain(llm=llm, verbose=True)\\nchecker_chain.run(text)\\n> Entering new LLMCheckerChain chain...\\n> Entering new SequentialChain chain...\\nChain 0:\\n{\\'statement\\': \\'\\\\nNone. Mammals do not lay eggs.\\'}\\nChain 1:\\n{\\'assertions\\': \\'\\\\n• Mammals reproduce using live birth\\\\n• Mammals do not lay eggs\\\\n• Animals that lay eggs are not mammals\\'}\\nChain 2:\\n{\\'checked_assertions\\': \\'\\\\n1. True\\\\n\\\\n2. True\\\\n\\\\n3. False - Mammals are a class of animals that includes animals that lay eggs, such as monotremes (platypus and echidna).\\'}\\nChain 3:\\n{\\'revised_statement\\': \\' Monotremes, such as the platypus and echidna, lay the biggest eggs of any mammal.\\'}\\n> Finished SequentialChain chain.\\n> Finished LLMCheckerChain chain.\\n\\' Monotremes, such as the platypus and echidna, lay the biggest eggs of any mammal.\\'\\nprevious\\nBashChain\\nnext\\nLLM Math\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\examples\\\\llm_checker.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nLLM Math\\n Contents \\nCustomize Prompt\\nLLM Math#\\nThis notebook showcases using LLMs and Python REPLs to do complex word math problems.\\nfrom langchain import OpenAI, LLMMathChain\\nllm = OpenAI(temperature=0)\\nllm_math = LLMMathChain(llm=llm, verbose=True)\\nllm_math.run(\"What is 13 raised to the .3432 power?\")\\n> Entering new LLMMathChain chain...\\nWhat is 13 raised to the .3432 power?\\n```python\\nimport math\\nprint(math.pow(13, .3432))\\n```\\nAnswer: 2.4116004626599237\\n> Finished chain.\\n\\'Answer: 2.4116004626599237\\\\n\\'\\nCustomize Prompt#\\nYou can also customize the prompt that is used. Here is an example prompting it to use numpy\\nfrom langchain.prompts.prompt import PromptTemplate\\n_PROMPT_TEMPLATE = \"\"\"You are GPT-3, and you can\\'t do math.\\nYou can do basic math, and your memorization abilities are impressive, but you can\\'t do any complex calculations that a human could not do in their head. You also have an annoying tendency to just make up highly specific, but wrong, answers.\\nSo we hooked you up to a Python 3 kernel, and now you can execute code. If you execute code, you must print out the final answer using the print function. You MUST use the python package numpy to answer your question. You must import numpy as np.\\nQuestion: ${{Question with hard calculation.}}\\n```python\\n${{Code that prints what you need to know}}\\nprint(${{code}})\\n```\\n```output\\n${{Output of your code}}\\n```\\nAnswer: ${{Answer}}\\nBegin.\\nQuestion: What is 37593 * 67?\\n```python\\nimport numpy as np\\nprint(np.multiply(37593, 67))\\n```\\n```output\\n2518731\\n```\\nAnswer: 2518731\\nQuestion: {question}\"\"\"\\nPROMPT = PromptTemplate(input_variables=[\"question\"], template=_PROMPT_TEMPLATE)\\nllm_math = LLMMathChain(llm=llm, prompt=PROMPT, verbose=True)\\nllm_math.run(\"What is 13 raised to the .3432 power?\")\\n> Entering new LLMMathChain chain...\\nWhat is 13 raised to the .3432 power?\\n```python\\nimport numpy as np\\nprint(np.power(13, .3432))\\n```\\nAnswer: 2.4116004626599237\\n> Finished chain.\\n\\'Answer: 2.4116004626599237\\\\n\\'\\nprevious\\nLLMCheckerChain\\nnext\\nLLMRequestsChain\\n Contents\\n  \\nCustomize Prompt\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\examples\\\\llm_math.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nLLMRequestsChain\\nLLMRequestsChain#\\nUsing the request library to get HTML results from a URL and then an LLM to parse results\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import LLMRequestsChain, LLMChain\\nfrom langchain.prompts import PromptTemplate\\ntemplate = \"\"\"Between >>> and <<< are the raw search result text from google.\\nExtract the answer to the question \\'{query}\\' or say \"not found\" if the information is not contained.\\nUse the format\\nExtracted:<answer or \"not found\">\\n>>> {requests_result} <<<\\nExtracted:\"\"\"\\nPROMPT = PromptTemplate(\\n    input_variables=[\"query\", \"requests_result\"],\\n    template=template,\\n)\\nchain = LLMRequestsChain(llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=PROMPT))\\nquestion = \"What are the Three (3) biggest countries, and their respective sizes?\"\\ninputs = {\\n    \"query\": question,\\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\")\\n}\\nchain(inputs)\\n{\\'query\\': \\'What are the Three (3) biggest countries, and their respective sizes?\\',\\n \\'url\\': \\'https://www.google.com/search?q=What+are+the+Three+(3)+biggest+countries,+and+their+respective+sizes?\\',\\n \\'output\\': \\' Russia (17,098,242 km²), Canada (9,984,670 km²), United States (9,826,675 km²)\\'}\\nprevious\\nLLM Math\\nnext\\nLLMSummarizationCheckerChain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\examples\\\\llm_requests.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nLLMSummarizationCheckerChain\\nLLMSummarizationCheckerChain#\\nThis notebook shows some examples of LLMSummarizationCheckerChain in use with different types of texts.  It has a few distinct differences from the LLMCheckerChain, in that it doesn’t have any assumtions to the format of the input text (or summary).\\nAdditionally, as the LLMs like to hallucinate when fact checking or get confused by context, it is sometimes beneficial to run the checker multiple times.  It does this by feeding the rewritten “True” result back on itself, and checking the “facts” for truth.  As you can see from the examples below, this can be very effective in arriving at a generally true body of text.\\nYou can control the number of times the checker runs by setting the max_checks parameter.  The default is 2, but you can set it to 1 if you don’t want any double-checking.\\nfrom langchain.chains import LLMSummarizationCheckerChain\\nfrom langchain.llms import OpenAI\\nllm = OpenAI(temperature=0)\\nchecker_chain = LLMSummarizationCheckerChain(llm=llm, verbose=True, max_checks=2)\\ntext = \"\"\"\\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\\n• In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\\n• The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\\n• JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"\\nThese discoveries can spark a child\\'s imagination about the infinite wonders of the universe.\"\"\"\\nchecker_chain.run(text)\\n> Entering new LLMSummarizationCheckerChain chain...\\n> Entering new SequentialChain chain...\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nGiven some text, extract a list of facts from the text.\\nFormat your output as a bulleted list.\\nText:\\n\"\"\"\\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\\n• In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\\n• The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\\n• JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"\\nThese discoveries can spark a child\\'s imagination about the infinite wonders of the universe.\\n\"\"\"\\nFacts:\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nYou are an expert fact checker. You have been hired by a major news organization to fact check a very important story.\\nHere is a bullet point list of facts:\\n\"\"\"\\n• The James Webb Space Telescope (JWST) spotted a number of galaxies nicknamed \"green peas.\"\\n• The telescope captured images of galaxies that are over 13 billion years old.\\n• JWST took the very first pictures of a planet outside of our own solar system.\\n• These distant worlds are called \"exoplanets.\"\\n\"\"\"\\nFor each fact, determine whether it is true or false about the subject. If you are unable to determine whether the fact is true or false, output \"Undetermined\".\\nIf the fact is false, explain why.\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nBelow are some assertions that have been fact checked and are labeled as true of false.  If the answer is false, a suggestion is given for a correction.\\nChecked Assertions:\\n\"\"\"\\n• The James Webb Space Telescope (JWST) spotted a number of galaxies nicknamed \"green peas.\" - True \\n• The telescope captured images of galaxies that are over 13 billion years old. - True \\n• JWST took the very first pictures of a planet outside of our own solar system. - False. The first exoplanet was discovered in 1992, before the JWST was launched. \\n• These distant worlds are called \"exoplanets.\" - True\\n\"\"\"\\nOriginal Summary:\\n\"\"\"\\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\\n• In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\\n• The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\\n• JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"\\nThese discoveries can spark a child\\'s imagination about the infinite wonders of the universe.\\n\"\"\"\\nUsing these checked assertions, rewrite the original summary to be completely true.\\nThe output should have the same structure and formatting as the original summary.\\nSummary:\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nBelow are some assertions that have been fact checked and are labeled as true or false.\\nIf all of the assertions are true, return \"True\". If any of the assertions are false, return \"False\".\\nHere are some examples:\\n===\\nChecked Assertions: \"\"\"\\n- The sky is red: False\\n- Water is made of lava: False\\n- The sun is a star: True\\n\"\"\"\\nResult: False\\n===\\nChecked Assertions: \"\"\"\\n- The sky is blue: True\\n- Water is wet: True\\n- The sun is a star: True\\n\"\"\"\\nResult: True\\n===\\nChecked Assertions: \"\"\"\\n- The sky is blue - True\\n- Water is made of lava- False\\n- The sun is a star - True\\n\"\"\"\\nResult: False\\n===\\nChecked Assertions:\"\"\"\\n• The James Webb Space Telescope (JWST) spotted a number of galaxies nicknamed \"green peas.\" - True \\n• The telescope captured images of galaxies that are over 13 billion years old. - True \\n• JWST took the very first pictures of a planet outside of our own solar system. - False. The first exoplanet was discovered in 1992, before the JWST was launched. \\n• These distant worlds are called \"exoplanets.\" - True\\n\"\"\"\\nResult:\\n> Finished chain.\\n> Finished chain.\\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\\n• In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\\n• The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\\n• JWST has provided us with the first images of exoplanets, which are planets outside of our own solar system. These distant worlds were first discovered in 1992, and the JWST has allowed us to see them in greater detail.\\nThese discoveries can spark a child\\'s imagination about the infinite wonders of the universe.\\n> Entering new SequentialChain chain...\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nGiven some text, extract a list of facts from the text.\\nFormat your output as a bulleted list.\\nText:\\n\"\"\"\\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\\n• In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\\n• The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\\n• JWST has provided us with the first images of exoplanets, which are planets outside of our own solar system. These distant worlds were first discovered in 1992, and the JWST has allowed us to see them in greater detail.\\nThese discoveries can spark a child\\'s imagination about the infinite wonders of the universe.\\n\"\"\"\\nFacts:\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nYou are an expert fact checker. You have been hired by a major news organization to fact check a very important story.\\nHere is a bullet point list of facts:\\n\"\"\"\\n• The James Webb Space Telescope (JWST) spotted a number of galaxies nicknamed \"green peas.\"\\n• The light from these galaxies has been traveling for over 13 billion years to reach us.\\n• JWST has provided us with the first images of exoplanets, which are planets outside of our own solar system.\\n• Exoplanets were first discovered in 1992.\\n• The JWST has allowed us to see exoplanets in greater detail.\\n\"\"\"\\nFor each fact, determine whether it is true or false about the subject. If you are unable to determine whether the fact is true or false, output \"Undetermined\".\\nIf the fact is false, explain why.\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nBelow are some assertions that have been fact checked and are labeled as true of false.  If the answer is false, a suggestion is given for a correction.\\nChecked Assertions:\\n\"\"\"\\n• The James Webb Space Telescope (JWST) spotted a number of galaxies nicknamed \"green peas.\" - True \\n• The light from these galaxies has been traveling for over 13 billion years to reach us. - True \\n• JWST has provided us with the first images of exoplanets, which are planets outside of our own solar system. - False. The first exoplanet was discovered in 1992, but the first images of exoplanets were taken by the Hubble Space Telescope in 1995. \\n• Exoplanets were first discovered in 1992. - True \\n• The JWST has allowed us to see exoplanets in greater detail. - Undetermined. It is too early to tell as the JWST has not been launched yet.\\n\"\"\"\\nOriginal Summary:\\n\"\"\"\\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\\n• In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\\n• The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\\n• JWST has provided us with the first images of exoplanets, which are planets outside of our own solar system. These distant worlds were first discovered in 1992, and the JWST has allowed us to see them in greater detail.\\nThese discoveries can spark a child\\'s imagination about the infinite wonders of the universe.\\n\"\"\"\\nUsing these checked assertions, rewrite the original summary to be completely true.\\nThe output should have the same structure and formatting as the original summary.\\nSummary:\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nBelow are some assertions that have been fact checked and are labeled as true or false.\\nIf all of the assertions are true, return \"True\". If any of the assertions are false, return \"False\".\\nHere are some examples:\\n===\\nChecked Assertions: \"\"\"\\n- The sky is red: False\\n- Water is made of lava: False\\n- The sun is a star: True\\n\"\"\"\\nResult: False\\n===\\nChecked Assertions: \"\"\"\\n- The sky is blue: True\\n- Water is wet: True\\n- The sun is a star: True\\n\"\"\"\\nResult: True\\n===\\nChecked Assertions: \"\"\"\\n- The sky is blue - True\\n- Water is made of lava- False\\n- The sun is a star - True\\n\"\"\"\\nResult: False\\n===\\nChecked Assertions:\"\"\"\\n• The James Webb Space Telescope (JWST) spotted a number of galaxies nicknamed \"green peas.\" - True \\n• The light from these galaxies has been traveling for over 13 billion years to reach us. - True \\n• JWST has provided us with the first images of exoplanets, which are planets outside of our own solar system. - False. The first exoplanet was discovered in 1992, but the first images of exoplanets were taken by the Hubble Space Telescope in 1995. \\n• Exoplanets were first discovered in 1992. - True \\n• The JWST has allowed us to see exoplanets in greater detail. - Undetermined. It is too early to tell as the JWST has not been launched yet.\\n\"\"\"\\nResult:\\n> Finished chain.\\n> Finished chain.\\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\\n• In 2023, The JWST will spot a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\\n• The telescope will capture images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\\n• Exoplanets, which are planets outside of our own solar system, were first discovered in 1992. The JWST will allow us to see them in greater detail than ever before.\\nThese discoveries can spark a child\\'s imagination about the infinite wonders of the universe.\\n> Finished chain.\\n\\'Your 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\\\\n• In 2023, The JWST will spot a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\\\\n• The telescope will capture images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\\\\n• Exoplanets, which are planets outside of our own solar system, were first discovered in 1992. The JWST will allow us to see them in greater detail than ever before.\\\\nThese discoveries can spark a child\\\\\\'s imagination about the infinite wonders of the universe.\\'\\nfrom langchain.chains import LLMSummarizationCheckerChain\\nfrom langchain.llms import OpenAI\\nllm = OpenAI(temperature=0)\\nchecker_chain = LLMSummarizationCheckerChain(llm=llm, verbose=True, max_checks=3)\\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean\\'s main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\\nchecker_chain.run(text)\\n> Entering new LLMSummarizationCheckerChain chain...\\n> Entering new SequentialChain chain...\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nGiven some text, extract a list of facts from the text.\\nFormat your output as a bulleted list.\\nText:\\n\"\"\"\\nThe Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean\\'s main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\\n\"\"\"\\nFacts:\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nYou are an expert fact checker. You have been hired by a major news organization to fact check a very important story.\\nHere is a bullet point list of facts:\\n\"\"\"\\n- The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland.\\n- It has an area of 465,000 square miles.\\n- It is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean.\\n- It is the smallest of the five oceans.\\n- It is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs.\\n- The sea is named after the island of Greenland.\\n- It is the Arctic Ocean\\'s main outlet to the Atlantic.\\n- It is often frozen over so navigation is limited.\\n- It is considered the northern branch of the Norwegian Sea.\\n\"\"\"\\nFor each fact, determine whether it is true or false about the subject. If you are unable to determine whether the fact is true or false, output \"Undetermined\".\\nIf the fact is false, explain why.\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nBelow are some assertions that have been fact checked and are labeled as true of false.  If the answer is false, a suggestion is given for a correction.\\nChecked Assertions:\"\"\"\\n- The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. True\\n- It has an area of 465,000 square miles. True\\n- It is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. False - The Greenland Sea is not an ocean, it is an arm of the Arctic Ocean.\\n- It is the smallest of the five oceans. False - The Greenland Sea is not an ocean, it is an arm of the Arctic Ocean.\\n- It is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. True\\n- The sea is named after the island of Greenland. True\\n- It is the Arctic Ocean\\'s main outlet to the Atlantic. True\\n- It is often frozen over so navigation is limited. True\\n- It is considered the northern branch of the Norwegian Sea. True\\n\"\"\"\\nOriginal Summary:\"\"\"\\nThe Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean\\'s main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\\n\"\"\"\\nUsing these checked assertions, rewrite the original summary to be completely true.\\nThe output should have the same structure and formatting as the original summary.\\nSummary:\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nBelow are some assertions that have been fact checked and are labeled as true of false.\\nIf all of the assertions are true, return \"True\". If any of the assertions are false, return \"False\".\\nHere are some examples:\\n===\\nChecked Assertions: \"\"\"\\n- The sky is red: False\\n- Water is made of lava: False\\n- The sun is a star: True\\n\"\"\"\\nResult: False\\n===\\nChecked Assertions: \"\"\"\\n- The sky is blue: True\\n- Water is wet: True\\n- The sun is a star: True\\n\"\"\"\\nResult: True\\n===\\nChecked Assertions: \"\"\"\\n- The sky is blue - True\\n- Water is made of lava- False\\n- The sun is a star - True\\n\"\"\"\\nResult: False\\n===\\nChecked Assertions:\"\"\"\\n- The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. True\\n- It has an area of 465,000 square miles. True\\n- It is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. False - The Greenland Sea is not an ocean, it is an arm of the Arctic Ocean.\\n- It is the smallest of the five oceans. False - The Greenland Sea is not an ocean, it is an arm of the Arctic Ocean.\\n- It is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. True\\n- The sea is named after the island of Greenland. True\\n- It is the Arctic Ocean\\'s main outlet to the Atlantic. True\\n- It is often frozen over so navigation is limited. True\\n- It is considered the northern branch of the Norwegian Sea. True\\n\"\"\"\\nResult:\\n> Finished chain.\\n> Finished chain.\\nThe Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is an arm of the Arctic Ocean. It is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean\\'s main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\\n> Entering new SequentialChain chain...\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nGiven some text, extract a list of facts from the text.\\nFormat your output as a bulleted list.\\nText:\\n\"\"\"\\nThe Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is an arm of the Arctic Ocean. It is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean\\'s main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\\n\"\"\"\\nFacts:\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nYou are an expert fact checker. You have been hired by a major news organization to fact check a very important story.\\nHere is a bullet point list of facts:\\n\"\"\"\\n- The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland.\\n- It has an area of 465,000 square miles.\\n- It is an arm of the Arctic Ocean.\\n- It is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs.\\n- It is named after the island of Greenland.\\n- It is the Arctic Ocean\\'s main outlet to the Atlantic.\\n- It is often frozen over so navigation is limited.\\n- It is considered the northern branch of the Norwegian Sea.\\n\"\"\"\\nFor each fact, determine whether it is true or false about the subject. If you are unable to determine whether the fact is true or false, output \"Undetermined\".\\nIf the fact is false, explain why.\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nBelow are some assertions that have been fact checked and are labeled as true of false.  If the answer is false, a suggestion is given for a correction.\\nChecked Assertions:\"\"\"\\n- The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. True\\n- It has an area of 465,000 square miles. True\\n- It is an arm of the Arctic Ocean. True\\n- It is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. True\\n- It is named after the island of Greenland. False - It is named after the country of Greenland.\\n- It is the Arctic Ocean\\'s main outlet to the Atlantic. True\\n- It is often frozen over so navigation is limited. True\\n- It is considered the northern branch of the Norwegian Sea. False - It is considered the northern branch of the Atlantic Ocean.\\n\"\"\"\\nOriginal Summary:\"\"\"\\nThe Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is an arm of the Arctic Ocean. It is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean\\'s main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\\n\"\"\"\\nUsing these checked assertions, rewrite the original summary to be completely true.\\nThe output should have the same structure and formatting as the original summary.\\nSummary:\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nBelow are some assertions that have been fact checked and are labeled as true of false.\\nIf all of the assertions are true, return \"True\". If any of the assertions are false, return \"False\".\\nHere are some examples:\\n===\\nChecked Assertions: \"\"\"\\n- The sky is red: False\\n- Water is made of lava: False\\n- The sun is a star: True\\n\"\"\"\\nResult: False\\n===\\nChecked Assertions: \"\"\"\\n- The sky is blue: True\\n- Water is wet: True\\n- The sun is a star: True\\n\"\"\"\\nResult: True\\n===\\nChecked Assertions: \"\"\"\\n- The sky is blue - True\\n- Water is made of lava- False\\n- The sun is a star - True\\n\"\"\"\\nResult: False\\n===\\nChecked Assertions:\"\"\"\\n- The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. True\\n- It has an area of 465,000 square miles. True\\n- It is an arm of the Arctic Ocean. True\\n- It is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. True\\n- It is named after the island of Greenland. False - It is named after the country of Greenland.\\n- It is the Arctic Ocean\\'s main outlet to the Atlantic. True\\n- It is often frozen over so navigation is limited. True\\n- It is considered the northern branch of the Norwegian Sea. False - It is considered the northern branch of the Atlantic Ocean.\\n\"\"\"\\nResult:\\n> Finished chain.\\n> Finished chain.\\nThe Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is an arm of the Arctic Ocean. It is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the country of Greenland, and is the Arctic Ocean\\'s main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Atlantic Ocean.\\n> Entering new SequentialChain chain...\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nGiven some text, extract a list of facts from the text.\\nFormat your output as a bulleted list.\\nText:\\n\"\"\"\\nThe Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is an arm of the Arctic Ocean. It is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the country of Greenland, and is the Arctic Ocean\\'s main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Atlantic Ocean.\\n\"\"\"\\nFacts:\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nYou are an expert fact checker. You have been hired by a major news organization to fact check a very important story.\\nHere is a bullet point list of facts:\\n\"\"\"\\n- The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland.\\n- It has an area of 465,000 square miles.\\n- It is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs.\\n- The sea is named after the country of Greenland.\\n- It is the Arctic Ocean\\'s main outlet to the Atlantic.\\n- It is often frozen over so navigation is limited.\\n- It is considered the northern branch of the Atlantic Ocean.\\n\"\"\"\\nFor each fact, determine whether it is true or false about the subject. If you are unable to determine whether the fact is true or false, output \"Undetermined\".\\nIf the fact is false, explain why.\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nBelow are some assertions that have been fact checked and are labeled as true of false.  If the answer is false, a suggestion is given for a correction.\\nChecked Assertions:\"\"\"\\n- The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. True\\n- It has an area of 465,000 square miles. True\\n- It is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. True\\n- The sea is named after the country of Greenland. True\\n- It is the Arctic Ocean\\'s main outlet to the Atlantic. False - The Arctic Ocean\\'s main outlet to the Atlantic is the Barents Sea.\\n- It is often frozen over so navigation is limited. True\\n- It is considered the northern branch of the Atlantic Ocean. False - The Greenland Sea is considered part of the Arctic Ocean, not the Atlantic Ocean.\\n\"\"\"\\nOriginal Summary:\"\"\"\\nThe Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is an arm of the Arctic Ocean. It is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the country of Greenland, and is the Arctic Ocean\\'s main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Atlantic Ocean.\\n\"\"\"\\nUsing these checked assertions, rewrite the original summary to be completely true.\\nThe output should have the same structure and formatting as the original summary.\\nSummary:\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nBelow are some assertions that have been fact checked and are labeled as true of false.\\nIf all of the assertions are true, return \"True\". If any of the assertions are false, return \"False\".\\nHere are some examples:\\n===\\nChecked Assertions: \"\"\"\\n- The sky is red: False\\n- Water is made of lava: False\\n- The sun is a star: True\\n\"\"\"\\nResult: False\\n===\\nChecked Assertions: \"\"\"\\n- The sky is blue: True\\n- Water is wet: True\\n- The sun is a star: True\\n\"\"\"\\nResult: True\\n===\\nChecked Assertions: \"\"\"\\n- The sky is blue - True\\n- Water is made of lava- False\\n- The sun is a star - True\\n\"\"\"\\nResult: False\\n===\\nChecked Assertions:\"\"\"\\n- The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. True\\n- It has an area of 465,000 square miles. True\\n- It is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. True\\n- The sea is named after the country of Greenland. True\\n- It is the Arctic Ocean\\'s main outlet to the Atlantic. False - The Arctic Ocean\\'s main outlet to the Atlantic is the Barents Sea.\\n- It is often frozen over so navigation is limited. True\\n- It is considered the northern branch of the Atlantic Ocean. False - The Greenland Sea is considered part of the Arctic Ocean, not the Atlantic Ocean.\\n\"\"\"\\nResult:\\n> Finished chain.\\n> Finished chain.\\nThe Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the country of Greenland, and is the Arctic Ocean\\'s main outlet to the Barents Sea. It is often frozen over so navigation is limited, and is considered part of the Arctic Ocean.\\n> Finished chain.\\n\"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the country of Greenland, and is the Arctic Ocean\\'s main outlet to the Barents Sea. It is often frozen over so navigation is limited, and is considered part of the Arctic Ocean.\"\\nfrom langchain.chains import LLMSummarizationCheckerChain\\nfrom langchain.llms import OpenAI\\nllm = OpenAI(temperature=0)\\nchecker_chain = LLMSummarizationCheckerChain(llm=llm, max_checks=3, verbose=True)\\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\\nchecker_chain.run(text)\\n> Entering new LLMSummarizationCheckerChain chain...\\n> Entering new SequentialChain chain...\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nGiven some text, extract a list of facts from the text.\\nFormat your output as a bulleted list.\\nText:\\n\"\"\"\\nMammals can lay eggs, birds can lay eggs, therefore birds are mammals.\\n\"\"\"\\nFacts:\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nYou are an expert fact checker. You have been hired by a major news organization to fact check a very important story.\\nHere is a bullet point list of facts:\\n\"\"\"\\n- Mammals can lay eggs\\n- Birds can lay eggs\\n- Birds are mammals\\n\"\"\"\\nFor each fact, determine whether it is true or false about the subject. If you are unable to determine whether the fact is true or false, output \"Undetermined\".\\nIf the fact is false, explain why.\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nBelow are some assertions that have been fact checked and are labeled as true of false.  If the answer is false, a suggestion is given for a correction.\\nChecked Assertions:\\n\"\"\"\\n- Mammals can lay eggs: False. Mammals are not capable of laying eggs, as they give birth to live young.\\n- Birds can lay eggs: True. Birds are capable of laying eggs.\\n- Birds are mammals: False. Birds are not mammals, they are a class of their own.\\n\"\"\"\\nOriginal Summary:\\n\"\"\"\\nMammals can lay eggs, birds can lay eggs, therefore birds are mammals.\\n\"\"\"\\nUsing these checked assertions, rewrite the original summary to be completely true.\\nThe output should have the same structure and formatting as the original summary.\\nSummary:\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nBelow are some assertions that have been fact checked and are labeled as true or false.\\nIf all of the assertions are true, return \"True\". If any of the assertions are false, return \"False\".\\nHere are some examples:\\n===\\nChecked Assertions: \"\"\"\\n- The sky is red: False\\n- Water is made of lava: False\\n- The sun is a star: True\\n\"\"\"\\nResult: False\\n===\\nChecked Assertions: \"\"\"\\n- The sky is blue: True\\n- Water is wet: True\\n- The sun is a star: True\\n\"\"\"\\nResult: True\\n===\\nChecked Assertions: \"\"\"\\n- The sky is blue - True\\n- Water is made of lava- False\\n- The sun is a star - True\\n\"\"\"\\nResult: False\\n===\\nChecked Assertions:\"\"\"\\n- Mammals can lay eggs: False. Mammals are not capable of laying eggs, as they give birth to live young.\\n- Birds can lay eggs: True. Birds are capable of laying eggs.\\n- Birds are mammals: False. Birds are not mammals, they are a class of their own.\\n\"\"\"\\nResult:\\n> Finished chain.\\n> Finished chain.\\n Birds and mammals are both capable of laying eggs, however birds are not mammals, they are a class of their own.\\n> Entering new SequentialChain chain...\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nGiven some text, extract a list of facts from the text.\\nFormat your output as a bulleted list.\\nText:\\n\"\"\"\\n Birds and mammals are both capable of laying eggs, however birds are not mammals, they are a class of their own.\\n\"\"\"\\nFacts:\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nYou are an expert fact checker. You have been hired by a major news organization to fact check a very important story.\\nHere is a bullet point list of facts:\\n\"\"\"\\n- Birds and mammals are both capable of laying eggs.\\n- Birds are not mammals.\\n- Birds are a class of their own.\\n\"\"\"\\nFor each fact, determine whether it is true or false about the subject. If you are unable to determine whether the fact is true or false, output \"Undetermined\".\\nIf the fact is false, explain why.\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nBelow are some assertions that have been fact checked and are labeled as true of false.  If the answer is false, a suggestion is given for a correction.\\nChecked Assertions:\\n\"\"\"\\n- Birds and mammals are both capable of laying eggs: False. Mammals give birth to live young, while birds lay eggs.\\n- Birds are not mammals: True. Birds are a class of their own, separate from mammals.\\n- Birds are a class of their own: True. Birds are a class of their own, separate from mammals.\\n\"\"\"\\nOriginal Summary:\\n\"\"\"\\n Birds and mammals are both capable of laying eggs, however birds are not mammals, they are a class of their own.\\n\"\"\"\\nUsing these checked assertions, rewrite the original summary to be completely true.\\nThe output should have the same structure and formatting as the original summary.\\nSummary:\\n> Finished chain.\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nBelow are some assertions that have been fact checked and are labeled as true or false.\\nIf all of the assertions are true, return \"True\". If any of the assertions are false, return \"False\".\\nHere are some examples:\\n===\\nChecked Assertions: \"\"\"\\n- The sky is red: False\\n- Water is made of lava: False\\n- The sun is a star: True\\n\"\"\"\\nResult: False\\n===\\nChecked Assertions: \"\"\"\\n- The sky is blue: True\\n- Water is wet: True\\n- The sun is a star: True\\n\"\"\"\\nResult: True\\n===\\nChecked Assertions: \"\"\"\\n- The sky is blue - True\\n- Water is made of lava- False\\n- The sun is a star - True\\n\"\"\"\\nResult: False\\n===\\nChecked Assertions:\"\"\"\\n- Birds and mammals are both capable of laying eggs: False. Mammals give birth to live young, while birds lay eggs.\\n- Birds are not mammals: True. Birds are a class of their own, separate from mammals.\\n- Birds are a class of their own: True. Birds are a class of their own, separate from mammals.\\n\"\"\"\\nResult:\\n> Finished chain.\\n> Finished chain.\\n> Finished chain.\\n\\'Birds are not mammals, but they are a class of their own. They lay eggs, unlike mammals which give birth to live young.\\'\\nprevious\\nLLMRequestsChain\\nnext\\nModeration\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\examples\\\\llm_summarization_checker.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nModeration\\n Contents \\nHow to use the moderation chain\\nHow to append a Moderation chain to an LLMChain\\nModeration#\\nThis notebook walks through examples of how to use a moderation chain, and several common ways for doing so. Moderation chains are useful for detecting text that could be hateful, violent, etc. This can be useful to apply on both user input, but also on the output of a Language Model. Some API providers, like OpenAI, specifically prohibit you, or your end users, from generating some types of harmful content. To comply with this (and to just generally prevent your application from being harmful) you may often want to append a moderation chain to any LLMChains, in order to make sure any output the LLM generates is not harmful.\\nIf the content passed into the moderation chain is harmful, there is not one best way to handle it, it probably depends on your application. Sometimes you may want to throw an error in the Chain (and have your application handle that). Other times, you may want to return something to the user explaining that the text was harmful. There could even be other ways to handle it! We will cover all these ways in this notebook.\\nIn this notebook, we will show:\\nHow to run any piece of text through a moderation chain.\\nHow to append a Moderation chain to an LLMChain.\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import OpenAIModerationChain, SequentialChain, LLMChain, SimpleSequentialChain\\nfrom langchain.prompts import PromptTemplate\\nHow to use the moderation chain#\\nHere’s an example of using the moderation chain with default settings (will return a string explaining stuff was flagged).\\nmoderation_chain = OpenAIModerationChain()\\nmoderation_chain.run(\"This is okay\")\\n\\'This is okay\\'\\nmoderation_chain.run(\"I will kill you\")\\n\"Text was found that violates OpenAI\\'s content policy.\"\\nHere’s an example of using the moderation chain to throw an error.\\nmoderation_chain_error = OpenAIModerationChain(error=True)\\nmoderation_chain_error.run(\"This is okay\")\\n\\'This is okay\\'\\nmoderation_chain_error.run(\"I will kill you\")\\n---------------------------------------------------------------------------\\nValueError                                Traceback (most recent call last)\\nCell In[7], line 1\\n----> 1 moderation_chain_error.run(\"I will kill you\")\\nFile ~/workplace/langchain/langchain/chains/base.py:138, in Chain.run(self, *args, **kwargs)\\n    136     if len(args) != 1:\\n    137         raise ValueError(\"`run` supports only one positional argument.\")\\n--> 138     return self(args[0])[self.output_keys[0]]\\n    140 if kwargs and not args:\\n    141     return self(kwargs)[self.output_keys[0]]\\nFile ~/workplace/langchain/langchain/chains/base.py:112, in Chain.__call__(self, inputs, return_only_outputs)\\n    108 if self.verbose:\\n    109     print(\\n    110         f\"\\\\n\\\\n\\\\033[1m> Entering new {self.__class__.__name__} chain...\\\\033[0m\"\\n    111     )\\n--> 112 outputs = self._call(inputs)\\n    113 if self.verbose:\\n    114     print(f\"\\\\n\\\\033[1m> Finished {self.__class__.__name__} chain.\\\\033[0m\")\\nFile ~/workplace/langchain/langchain/chains/moderation.py:81, in OpenAIModerationChain._call(self, inputs)\\n     79 text = inputs[self.input_key]\\n     80 results = self.client.create(text)\\n---> 81 output = self._moderate(text, results[\"results\"][0])\\n     82 return {self.output_key: output}\\nFile ~/workplace/langchain/langchain/chains/moderation.py:73, in OpenAIModerationChain._moderate(self, text, results)\\n     71 error_str = \"Text was found that violates OpenAI\\'s content policy.\"\\n     72 if self.error:\\n---> 73     raise ValueError(error_str)\\n     74 else:\\n     75     return error_str\\nValueError: Text was found that violates OpenAI\\'s content policy.\\nHere’s an example of creating a custom moderation chain with a custom error message. It requires some knowledge of OpenAI’s moderation endpoint results (see docs here).\\nclass CustomModeration(OpenAIModerationChain):\\n    \\n    def _moderate(self, text: str, results: dict) -> str:\\n        if results[\"flagged\"]:\\n            error_str = f\"The following text was found that violates OpenAI\\'s content policy: {text}\"\\n            return error_str\\n        return text\\n    \\ncustom_moderation = CustomModeration()\\ncustom_moderation.run(\"This is okay\")\\n\\'This is okay\\'\\ncustom_moderation.run(\"I will kill you\")\\n\"The following text was found that violates OpenAI\\'s content policy: I will kill you\"\\nHow to append a Moderation chain to an LLMChain#\\nTo easily combine a moderation chain with an LLMChain, you can use the SequentialChain abstraction.\\nLet’s start with a simple example of where the LLMChain only has a single input. For this purpose, we will prompt the model so it says something harmful.\\nprompt = PromptTemplate(template=\"{text}\", input_variables=[\"text\"])\\nllm_chain = LLMChain(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\"), prompt=prompt)\\ntext = \"\"\"We are playing a game of repeat after me.\\nPerson 1: Hi\\nPerson 2: Hi\\nPerson 1: How\\'s your day\\nPerson 2: How\\'s your day\\nPerson 1: I will kill you\\nPerson 2:\"\"\"\\nllm_chain.run(text)\\n\\' I will kill you\\'\\nchain = SimpleSequentialChain(chains=[llm_chain, moderation_chain])\\nchain.run(text)\\n\"Text was found that violates OpenAI\\'s content policy.\"\\nNow let’s walk through an example of using it with an LLMChain which has multiple inputs (a bit more tricky because we can’t use the SimpleSequentialChain)\\nprompt = PromptTemplate(template=\"{setup}{new_input}Person2:\", input_variables=[\"setup\", \"new_input\"])\\nllm_chain = LLMChain(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\"), prompt=prompt)\\nsetup = \"\"\"We are playing a game of repeat after me.\\nPerson 1: Hi\\nPerson 2: Hi\\nPerson 1: How\\'s your day\\nPerson 2: How\\'s your day\\nPerson 1:\"\"\"\\nnew_input = \"I will kill you\"\\ninputs = {\"setup\": setup, \"new_input\": new_input}\\nllm_chain(inputs, return_only_outputs=True)\\n{\\'text\\': \\' I will kill you\\'}\\n# Setting the input/output keys so it lines up\\nmoderation_chain.input_key = \"text\"\\nmoderation_chain.output_key = \"sanitized_text\"\\nchain = SequentialChain(chains=[llm_chain, moderation_chain], input_variables=[\"setup\", \"new_input\"])\\nchain(inputs, return_only_outputs=True)\\n{\\'sanitized_text\\': \"Text was found that violates OpenAI\\'s content policy.\"}\\nprevious\\nLLMSummarizationCheckerChain\\nnext\\nOpenAPI Chain\\n Contents\\n  \\nHow to use the moderation chain\\nHow to append a Moderation chain to an LLMChain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\examples\\\\moderation.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nOpenAPI Chain\\n Contents \\nLoad the spec\\nSelect the Operation\\nConstruct the chain\\nReturn raw response\\nExample POST message\\nOpenAPI Chain#\\nThis notebook shows an example of using an OpenAPI chain to call an endpoint in natural language, and get back a response in natural language\\nfrom langchain.tools import OpenAPISpec, APIOperation\\nfrom langchain.chains import OpenAPIEndpointChain\\nfrom langchain.requests import Requests\\nfrom langchain.llms import OpenAI\\nLoad the spec#\\nLoad a wrapper of the spec (so we can work with it more easily). You can load from a url or from a local file.\\nspec = OpenAPISpec.from_url(\"https://www.klarna.com/us/shopping/public/openai/v0/api-docs/\")\\nAttempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\\n# Alternative loading from file\\n# spec = OpenAPISpec.from_file(\"openai_openapi.yaml\")\\nSelect the Operation#\\nIn order to provide a focused on modular chain, we create a chain specifically only for one of the endpoints. Here we get an API operation from a specified endpoint and method.\\noperation = APIOperation.from_openapi_spec(spec, \\'/public/openai/v0/products\\', \"get\")\\nConstruct the chain#\\nWe can now construct a chain to interact with it. In order to construct such a chain, we will pass in:\\nThe operation endpoint\\nA requests wrapper (can be used to handle authentication, etc)\\nThe LLM to use to interact with it\\nllm = OpenAI() # Load a Language Model\\nchain = OpenAPIEndpointChain.from_api_operation(\\n    operation, \\n    llm, \\n    requests=Requests(), \\n    verbose=True,\\n    return_intermediate_steps=True # Return request and response text\\n)\\noutput = chain(\"whats the most expensive shirt?\")\\n> Entering new OpenAPIEndpointChain chain...\\n> Entering new APIRequesterChain chain...\\nPrompt after formatting:\\nYou are a helpful AI Assistant. Please provide JSON arguments to agentFunc() based on the user\\'s instructions.\\nAPI_SCHEMA: ```typescript\\n/* API for fetching Klarna product information */\\ntype productsUsingGET = (_: {\\n/* A precise query that matches one very small category or product that needs to be searched for to find the products the user is looking for. If the user explicitly stated what they want, use that as a query. The query is as specific as possible to the product name or category mentioned by the user in its singular form, and don\\'t contain any clarifiers like latest, newest, cheapest, budget, premium, expensive or similar. The query is always taken from the latest topic, if there is a new topic a new query is started. */\\n\\t\\tq: string,\\n/* number of products returned */\\n\\t\\tsize?: number,\\n/* (Optional) Minimum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user\\'s request and the kind of product searched for. */\\n\\t\\tmin_price?: number,\\n/* (Optional) Maximum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user\\'s request and the kind of product searched for. */\\n\\t\\tmax_price?: number,\\n}) => any;\\n```\\nUSER_INSTRUCTIONS: \"whats the most expensive shirt?\"\\nYour arguments must be plain json provided in a markdown block:\\nARGS: ```json\\n{valid json conforming to API_SCHEMA}\\n```\\nExample\\n-----\\nARGS: ```json\\n{\"foo\": \"bar\", \"baz\": {\"qux\": \"quux\"}}\\n```\\nThe block must be no more than 1 line long, and all arguments must be valid JSON. All string arguments must be wrapped in double quotes.\\nYou MUST strictly comply to the types indicated by the provided schema, including all required args.\\nIf you don\\'t have sufficient information to call the function due to things like requiring specific uuid\\'s, you can reply with the following message:\\nMessage: ```text\\nConcise response requesting the additional information that would make calling the function successful.\\n```\\nBegin\\n-----\\nARGS:\\n> Finished chain.\\n{\"q\": \"shirt\", \"size\": 1, \"max_price\": null}\\n{\"products\":[{\"name\":\"Burberry Check Poplin Shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201810981/Clothing/Burberry-Check-Poplin-Shirt/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$360.00\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Gray,Blue,Beige\",\"Properties:Pockets\",\"Pattern:Checkered\"]}]}\\n> Entering new APIResponderChain chain...\\nPrompt after formatting:\\nYou are a helpful AI assistant trained to answer user queries from API responses.\\nYou attempted to call an API, which resulted in:\\nAPI_RESPONSE: {\"products\":[{\"name\":\"Burberry Check Poplin Shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201810981/Clothing/Burberry-Check-Poplin-Shirt/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$360.00\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Gray,Blue,Beige\",\"Properties:Pockets\",\"Pattern:Checkered\"]}]}\\nUSER_COMMENT: \"whats the most expensive shirt?\"\\nIf the API_RESPONSE can answer the USER_COMMENT respond with the following markdown json block:\\nResponse: ```json\\n{\"response\": \"Human-understandable synthesis of the API_RESPONSE\"}\\n```\\nOtherwise respond with the following markdown json block:\\nResponse Error: ```json\\n{\"response\": \"What you did and a concise statement of the resulting error. If it can be easily fixed, provide a suggestion.\"}\\n```\\nYou MUST respond as a markdown json code block. The person you are responding to CANNOT see the API_RESPONSE, so if there is any relevant information there you must include it in your response.\\nBegin:\\n---\\n> Finished chain.\\nThe most expensive shirt in the API response is the Burberry Check Poplin Shirt, which costs $360.00.\\n> Finished chain.\\n# View intermediate steps\\noutput[\"intermediate_steps\"]\\n{\\'request_args\\': \\'{\"q\": \"shirt\", \"size\": 1, \"max_price\": null}\\',\\n \\'response_text\\': \\'{\"products\":[{\"name\":\"Burberry Check Poplin Shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201810981/Clothing/Burberry-Check-Poplin-Shirt/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$360.00\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Gray,Blue,Beige\",\"Properties:Pockets\",\"Pattern:Checkered\"]}]}\\'}\\nReturn raw response#\\nWe can also run this chain without synthesizing the response. This will have the effect of just returning the raw API output.\\nchain = OpenAPIEndpointChain.from_api_operation(\\n    operation, \\n    llm, \\n    requests=Requests(), \\n    verbose=True,\\n    return_intermediate_steps=True, # Return request and response text\\n    raw_response=True # Return raw response\\n)\\noutput = chain(\"whats the most expensive shirt?\")\\n> Entering new OpenAPIEndpointChain chain...\\n> Entering new APIRequesterChain chain...\\nPrompt after formatting:\\nYou are a helpful AI Assistant. Please provide JSON arguments to agentFunc() based on the user\\'s instructions.\\nAPI_SCHEMA: ```typescript\\n/* API for fetching Klarna product information */\\ntype productsUsingGET = (_: {\\n/* A precise query that matches one very small category or product that needs to be searched for to find the products the user is looking for. If the user explicitly stated what they want, use that as a query. The query is as specific as possible to the product name or category mentioned by the user in its singular form, and don\\'t contain any clarifiers like latest, newest, cheapest, budget, premium, expensive or similar. The query is always taken from the latest topic, if there is a new topic a new query is started. */\\n\\t\\tq: string,\\n/* number of products returned */\\n\\t\\tsize?: number,\\n/* (Optional) Minimum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user\\'s request and the kind of product searched for. */\\n\\t\\tmin_price?: number,\\n/* (Optional) Maximum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user\\'s request and the kind of product searched for. */\\n\\t\\tmax_price?: number,\\n}) => any;\\n```\\nUSER_INSTRUCTIONS: \"whats the most expensive shirt?\"\\nYour arguments must be plain json provided in a markdown block:\\nARGS: ```json\\n{valid json conforming to API_SCHEMA}\\n```\\nExample\\n-----\\nARGS: ```json\\n{\"foo\": \"bar\", \"baz\": {\"qux\": \"quux\"}}\\n```\\nThe block must be no more than 1 line long, and all arguments must be valid JSON. All string arguments must be wrapped in double quotes.\\nYou MUST strictly comply to the types indicated by the provided schema, including all required args.\\nIf you don\\'t have sufficient information to call the function due to things like requiring specific uuid\\'s, you can reply with the following message:\\nMessage: ```text\\nConcise response requesting the additional information that would make calling the function successful.\\n```\\nBegin\\n-----\\nARGS:\\n> Finished chain.\\n{\"q\": \"shirt\", \"max_price\": null}\\n{\"products\":[{\"name\":\"Burberry Check Poplin Shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201810981/Clothing/Burberry-Check-Poplin-Shirt/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$360.00\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Gray,Blue,Beige\",\"Properties:Pockets\",\"Pattern:Checkered\"]},{\"name\":\"Burberry Vintage Check Cotton Shirt - Beige\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl359/3200280807/Children-s-Clothing/Burberry-Vintage-Check-Cotton-Shirt-Beige/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$229.02\",\"attributes\":[\"Material:Cotton,Elastane\",\"Color:Beige\",\"Model:Boy\",\"Pattern:Checkered\"]},{\"name\":\"Burberry Vintage Check Stretch Cotton Twill Shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3202342515/Clothing/Burberry-Vintage-Check-Stretch-Cotton-Twill-Shirt/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$309.99\",\"attributes\":[\"Material:Elastane/Lycra/Spandex,Cotton\",\"Target Group:Woman\",\"Color:Beige\",\"Properties:Stretch\",\"Pattern:Checkered\"]},{\"name\":\"Burberry Somerton Check Shirt - Camel\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201112728/Clothing/Burberry-Somerton-Check-Shirt-Camel/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$450.00\",\"attributes\":[\"Material:Elastane/Lycra/Spandex,Cotton\",\"Target Group:Man\",\"Color:Beige\"]},{\"name\":\"Magellan Outdoors Laguna Madre Solid Short Sleeve Fishing Shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3203102142/Clothing/Magellan-Outdoors-Laguna-Madre-Solid-Short-Sleeve-Fishing-Shirt/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$19.99\",\"attributes\":[\"Material:Polyester,Nylon\",\"Target Group:Man\",\"Color:Red,Pink,White,Blue,Purple,Beige,Black,Green\",\"Properties:Pockets\",\"Pattern:Solid Color\"]}]}\\n> Finished chain.\\noutput\\n{\\'instructions\\': \\'whats the most expensive shirt?\\',\\n \\'output\\': \\'{\"products\":[{\"name\":\"Burberry Check Poplin Shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201810981/Clothing/Burberry-Check-Poplin-Shirt/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$360.00\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Gray,Blue,Beige\",\"Properties:Pockets\",\"Pattern:Checkered\"]},{\"name\":\"Burberry Vintage Check Cotton Shirt - Beige\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl359/3200280807/Children-s-Clothing/Burberry-Vintage-Check-Cotton-Shirt-Beige/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$229.02\",\"attributes\":[\"Material:Cotton,Elastane\",\"Color:Beige\",\"Model:Boy\",\"Pattern:Checkered\"]},{\"name\":\"Burberry Vintage Check Stretch Cotton Twill Shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3202342515/Clothing/Burberry-Vintage-Check-Stretch-Cotton-Twill-Shirt/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$309.99\",\"attributes\":[\"Material:Elastane/Lycra/Spandex,Cotton\",\"Target Group:Woman\",\"Color:Beige\",\"Properties:Stretch\",\"Pattern:Checkered\"]},{\"name\":\"Burberry Somerton Check Shirt - Camel\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201112728/Clothing/Burberry-Somerton-Check-Shirt-Camel/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$450.00\",\"attributes\":[\"Material:Elastane/Lycra/Spandex,Cotton\",\"Target Group:Man\",\"Color:Beige\"]},{\"name\":\"Magellan Outdoors Laguna Madre Solid Short Sleeve Fishing Shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3203102142/Clothing/Magellan-Outdoors-Laguna-Madre-Solid-Short-Sleeve-Fishing-Shirt/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$19.99\",\"attributes\":[\"Material:Polyester,Nylon\",\"Target Group:Man\",\"Color:Red,Pink,White,Blue,Purple,Beige,Black,Green\",\"Properties:Pockets\",\"Pattern:Solid Color\"]}]}\\',\\n \\'intermediate_steps\\': {\\'request_args\\': \\'{\"q\": \"shirt\", \"max_price\": null}\\',\\n  \\'response_text\\': \\'{\"products\":[{\"name\":\"Burberry Check Poplin Shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201810981/Clothing/Burberry-Check-Poplin-Shirt/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$360.00\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Gray,Blue,Beige\",\"Properties:Pockets\",\"Pattern:Checkered\"]},{\"name\":\"Burberry Vintage Check Cotton Shirt - Beige\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl359/3200280807/Children-s-Clothing/Burberry-Vintage-Check-Cotton-Shirt-Beige/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$229.02\",\"attributes\":[\"Material:Cotton,Elastane\",\"Color:Beige\",\"Model:Boy\",\"Pattern:Checkered\"]},{\"name\":\"Burberry Vintage Check Stretch Cotton Twill Shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3202342515/Clothing/Burberry-Vintage-Check-Stretch-Cotton-Twill-Shirt/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$309.99\",\"attributes\":[\"Material:Elastane/Lycra/Spandex,Cotton\",\"Target Group:Woman\",\"Color:Beige\",\"Properties:Stretch\",\"Pattern:Checkered\"]},{\"name\":\"Burberry Somerton Check Shirt - Camel\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201112728/Clothing/Burberry-Somerton-Check-Shirt-Camel/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$450.00\",\"attributes\":[\"Material:Elastane/Lycra/Spandex,Cotton\",\"Target Group:Man\",\"Color:Beige\"]},{\"name\":\"Magellan Outdoors Laguna Madre Solid Short Sleeve Fishing Shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3203102142/Clothing/Magellan-Outdoors-Laguna-Madre-Solid-Short-Sleeve-Fishing-Shirt/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$19.99\",\"attributes\":[\"Material:Polyester,Nylon\",\"Target Group:Man\",\"Color:Red,Pink,White,Blue,Purple,Beige,Black,Green\",\"Properties:Pockets\",\"Pattern:Solid Color\"]}]}\\'}}\\nExample POST message#\\nFor this demo, we will interact with the speak API.\\nspec = OpenAPISpec.from_url(\"https://api.speak.com/openapi.yaml\")\\nAttempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\\nAttempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\\noperation = APIOperation.from_openapi_spec(spec, \\'/v1/public/openai/explain-task\\', \"post\")\\nllm = OpenAI()\\nchain = OpenAPIEndpointChain.from_api_operation(\\n    operation,\\n    llm,\\n    requests=Requests(),\\n    verbose=True,\\n    return_intermediate_steps=True)\\noutput = chain(\"How would ask for more tea in Delhi?\")\\n> Entering new OpenAPIEndpointChain chain...\\n> Entering new APIRequesterChain chain...\\nPrompt after formatting:\\nYou are a helpful AI Assistant. Please provide JSON arguments to agentFunc() based on the user\\'s instructions.\\nAPI_SCHEMA: ```typescript\\ntype explainTask = (_: {\\n/* Description of the task that the user wants to accomplish or do. For example, \"tell the waiter they messed up my order\" or \"compliment someone on their shirt\" */\\n  task_description?: string,\\n/* The foreign language that the user is learning and asking about. The value can be inferred from question - for example, if the user asks \"how do i ask a girl out in mexico city\", the value should be \"Spanish\" because of Mexico City. Always use the full name of the language (e.g. Spanish, French). */\\n  learning_language?: string,\\n/* The user\\'s native language. Infer this value from the language the user asked their question in. Always use the full name of the language (e.g. Spanish, French). */\\n  native_language?: string,\\n/* A description of any additional context in the user\\'s question that could affect the explanation - e.g. setting, scenario, situation, tone, speaking style and formality, usage notes, or any other qualifiers. */\\n  additional_context?: string,\\n/* Full text of the user\\'s question. */\\n  full_query?: string,\\n}) => any;\\n```\\nUSER_INSTRUCTIONS: \"How would ask for more tea in Delhi?\"\\nYour arguments must be plain json provided in a markdown block:\\nARGS: ```json\\n{valid json conforming to API_SCHEMA}\\n```\\nExample\\n-----\\nARGS: ```json\\n{\"foo\": \"bar\", \"baz\": {\"qux\": \"quux\"}}\\n```\\nThe block must be no more than 1 line long, and all arguments must be valid JSON. All string arguments must be wrapped in double quotes.\\nYou MUST strictly comply to the types indicated by the provided schema, including all required args.\\nIf you don\\'t have sufficient information to call the function due to things like requiring specific uuid\\'s, you can reply with the following message:\\nMessage: ```text\\nConcise response requesting the additional information that would make calling the function successful.\\n```\\nBegin\\n-----\\nARGS:\\n> Finished chain.\\n{\"task_description\": \"ask for more tea\", \"learning_language\": \"Hindi\", \"native_language\": \"English\", \"full_query\": \"How would I ask for more tea in Delhi?\"}\\n{\"explanation\":\"<what-to-say language=\\\\\"Hindi\\\\\" context=\\\\\"None\\\\\">\\\\nऔर चाय लाओ। (Aur chai lao.) \\\\n</what-to-say>\\\\n\\\\n<alternatives context=\\\\\"None\\\\\">\\\\n1. \\\\\"चाय थोड़ी ज्यादा मिल सकती है?\\\\\" *(Chai thodi zyada mil sakti hai? - Polite, asking if more tea is available)*\\\\n2. \\\\\"मुझे महसूस हो रहा है कि मुझे कुछ अन्य प्रकार की चाय पीनी चाहिए।\\\\\" *(Mujhe mehsoos ho raha hai ki mujhe kuch anya prakar ki chai peeni chahiye. - Formal, indicating a desire for a different type of tea)*\\\\n3. \\\\\"क्या मुझे or cup में milk/tea powder मिल सकता है?\\\\\" *(Kya mujhe aur cup mein milk/tea powder mil sakta hai? - Very informal/casual tone, asking for an extra serving of milk or tea powder)*\\\\n</alternatives>\\\\n\\\\n<usage-notes>\\\\nIn India and Indian culture, serving guests with food and beverages holds great importance in hospitality. You will find people always offering drinks like water or tea to their guests as soon as they arrive at their house or office.\\\\n</usage-notes>\\\\n\\\\n<example-convo language=\\\\\"Hindi\\\\\">\\\\n<context>At home during breakfast.</context>\\\\nPreeti: सर, क्या main aur cups chai lekar aaun? (Sir,kya main aur cups chai lekar aaun? - Sir, should I get more tea cups?)\\\\nRahul: हां,बिल्कुल। और चाय की मात्रा में भी थोड़ा सा इजाफा करना। (Haan,bilkul. Aur chai ki matra mein bhi thoda sa eejafa karna. - Yes, please. And add a little extra in the quantity of tea as well.)\\\\n</example-convo>\\\\n\\\\n*[Report an issue or leave feedback](https://speak.com/chatgpt?rid=d4mcapbkopo164pqpbk321oc})*\",\"extra_response_instructions\":\"Use all information in the API response and fully render all Markdown.\\\\nAlways end your response with a link to report an issue or leave feedback on the plugin.\"}\\n> Entering new APIResponderChain chain...\\nPrompt after formatting:\\nYou are a helpful AI assistant trained to answer user queries from API responses.\\nYou attempted to call an API, which resulted in:\\nAPI_RESPONSE: {\"explanation\":\"<what-to-say language=\\\\\"Hindi\\\\\" context=\\\\\"None\\\\\">\\\\nऔर चाय लाओ। (Aur chai lao.) \\\\n</what-to-say>\\\\n\\\\n<alternatives context=\\\\\"None\\\\\">\\\\n1. \\\\\"चाय थोड़ी ज्यादा मिल सकती है?\\\\\" *(Chai thodi zyada mil sakti hai? - Polite, asking if more tea is available)*\\\\n2. \\\\\"मुझे महसूस हो रहा है कि मुझे कुछ अन्य प्रकार की चाय पीनी चाहिए।\\\\\" *(Mujhe mehsoos ho raha hai ki mujhe kuch anya prakar ki chai peeni chahiye. - Formal, indicating a desire for a different type of tea)*\\\\n3. \\\\\"क्या मुझे or cup में milk/tea powder मिल सकता है?\\\\\" *(Kya mujhe aur cup mein milk/tea powder mil sakta hai? - Very informal/casual tone, asking for an extra serving of milk or tea powder)*\\\\n</alternatives>\\\\n\\\\n<usage-notes>\\\\nIn India and Indian culture, serving guests with food and beverages holds great importance in hospitality. You will find people always offering drinks like water or tea to their guests as soon as they arrive at their house or office.\\\\n</usage-notes>\\\\n\\\\n<example-convo language=\\\\\"Hindi\\\\\">\\\\n<context>At home during breakfast.</context>\\\\nPreeti: सर, क्या main aur cups chai lekar aaun? (Sir,kya main aur cups chai lekar aaun? - Sir, should I get more tea cups?)\\\\nRahul: हां,बिल्कुल। और चाय की मात्रा में भी थोड़ा सा इजाफा करना। (Haan,bilkul. Aur chai ki matra mein bhi thoda sa eejafa karna. - Yes, please. And add a little extra in the quantity of tea as well.)\\\\n</example-convo>\\\\n\\\\n*[Report an issue or leave feedback](https://speak.com/chatgpt?rid=d4mcapbkopo164pqpbk321oc})*\",\"extra_response_instructions\":\"Use all information in the API response and fully render all Markdown.\\\\nAlways end your response with a link to report an issue or leave feedback on the plugin.\"}\\nUSER_COMMENT: \"How would ask for more tea in Delhi?\"\\nIf the API_RESPONSE can answer the USER_COMMENT respond with the following markdown json block:\\nResponse: ```json\\n{\"response\": \"Concise response to USER_COMMENT based on API_RESPONSE.\"}\\n```\\nOtherwise respond with the following markdown json block:\\nResponse Error: ```json\\n{\"response\": \"What you did and a concise statement of the resulting error. If it can be easily fixed, provide a suggestion.\"}\\n```\\nYou MUST respond as a markdown json code block.\\nBegin:\\n---\\n> Finished chain.\\nIn Delhi you can ask for more tea by saying \\'Chai thodi zyada mil sakti hai?\\'\\n> Finished chain.\\n# Show the API chain\\'s intermediate steps\\noutput[\"intermediate_steps\"]\\n[\\'{\"task_description\": \"ask for more tea\", \"learning_language\": \"Hindi\", \"native_language\": \"English\", \"full_query\": \"How would I ask for more tea in Delhi?\"}\\',\\n \\'{\"explanation\":\"<what-to-say language=\\\\\\\\\"Hindi\\\\\\\\\" context=\\\\\\\\\"None\\\\\\\\\">\\\\\\\\nऔर चाय लाओ। (Aur chai lao.) \\\\\\\\n</what-to-say>\\\\\\\\n\\\\\\\\n<alternatives context=\\\\\\\\\"None\\\\\\\\\">\\\\\\\\n1. \\\\\\\\\"चाय थोड़ी ज्यादा मिल सकती है?\\\\\\\\\" *(Chai thodi zyada mil sakti hai? - Polite, asking if more tea is available)*\\\\\\\\n2. \\\\\\\\\"मुझे महसूस हो रहा है कि मुझे कुछ अन्य प्रकार की चाय पीनी चाहिए।\\\\\\\\\" *(Mujhe mehsoos ho raha hai ki mujhe kuch anya prakar ki chai peeni chahiye. - Formal, indicating a desire for a different type of tea)*\\\\\\\\n3. \\\\\\\\\"क्या मुझे or cup में milk/tea powder मिल सकता है?\\\\\\\\\" *(Kya mujhe aur cup mein milk/tea powder mil sakta hai? - Very informal/casual tone, asking for an extra serving of milk or tea powder)*\\\\\\\\n</alternatives>\\\\\\\\n\\\\\\\\n<usage-notes>\\\\\\\\nIn India and Indian culture, serving guests with food and beverages holds great importance in hospitality. You will find people always offering drinks like water or tea to their guests as soon as they arrive at their house or office.\\\\\\\\n</usage-notes>\\\\\\\\n\\\\\\\\n<example-convo language=\\\\\\\\\"Hindi\\\\\\\\\">\\\\\\\\n<context>At home during breakfast.</context>\\\\\\\\nPreeti: सर, क्या main aur cups chai lekar aaun? (Sir,kya main aur cups chai lekar aaun? - Sir, should I get more tea cups?)\\\\\\\\nRahul: हां,बिल्कुल। और चाय की मात्रा में भी थोड़ा सा इजाफा करना। (Haan,bilkul. Aur chai ki matra mein bhi thoda sa eejafa karna. - Yes, please. And add a little extra in the quantity of tea as well.)\\\\\\\\n</example-convo>\\\\\\\\n\\\\\\\\n*[Report an issue or leave feedback](https://speak.com/chatgpt?rid=d4mcapbkopo164pqpbk321oc})*\",\"extra_response_instructions\":\"Use all information in the API response and fully render all Markdown.\\\\\\\\nAlways end your response with a link to report an issue or leave feedback on the plugin.\"}\\']\\nprevious\\nModeration\\nnext\\nPAL\\n Contents\\n  \\nLoad the spec\\nSelect the Operation\\nConstruct the chain\\nReturn raw response\\nExample POST message\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\examples\\\\openapi.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nPAL\\n Contents \\nMath Prompt\\nColored Objects\\nIntermediate Steps\\nPAL#\\nImplements Program-Aided Language Models, as in https://arxiv.org/pdf/2211.10435.pdf.\\nfrom langchain.chains import PALChain\\nfrom langchain import OpenAI\\nllm = OpenAI(model_name=\\'code-davinci-002\\', temperature=0, max_tokens=512)\\nMath Prompt#\\npal_chain = PALChain.from_math_prompt(llm, verbose=True)\\nquestion = \"Jan has three times the number of pets as Marcia. Marcia has two more pets than Cindy. If Cindy has four pets, how many total pets do the three have?\"\\npal_chain.run(question)\\n> Entering new PALChain chain...\\ndef solution():\\n    \"\"\"Jan has three times the number of pets as Marcia. Marcia has two more pets than Cindy. If Cindy has four pets, how many total pets do the three have?\"\"\"\\n    cindy_pets = 4\\n    marcia_pets = cindy_pets + 2\\n    jan_pets = marcia_pets * 3\\n    total_pets = cindy_pets + marcia_pets + jan_pets\\n    result = total_pets\\n    return result\\n> Finished chain.\\n\\'28\\'\\nColored Objects#\\npal_chain = PALChain.from_colored_object_prompt(llm, verbose=True)\\nquestion = \"On the desk, you see two blue booklets, two purple booklets, and two yellow pairs of sunglasses. If I remove all the pairs of sunglasses from the desk, how many purple items remain on it?\"\\npal_chain.run(question)\\n> Entering new PALChain chain...\\n# Put objects into a list to record ordering\\nobjects = []\\nobjects += [(\\'booklet\\', \\'blue\\')] * 2\\nobjects += [(\\'booklet\\', \\'purple\\')] * 2\\nobjects += [(\\'sunglasses\\', \\'yellow\\')] * 2\\n# Remove all pairs of sunglasses\\nobjects = [object for object in objects if object[0] != \\'sunglasses\\']\\n# Count number of purple objects\\nnum_purple = len([object for object in objects if object[1] == \\'purple\\'])\\nanswer = num_purple\\n> Finished PALChain chain.\\n\\'2\\'\\nIntermediate Steps#\\nYou can also use the intermediate steps flag to return the code executed that generates the answer.\\npal_chain = PALChain.from_colored_object_prompt(llm, verbose=True, return_intermediate_steps=True)\\nquestion = \"On the desk, you see two blue booklets, two purple booklets, and two yellow pairs of sunglasses. If I remove all the pairs of sunglasses from the desk, how many purple items remain on it?\"\\nresult = pal_chain({\"question\": question})\\n> Entering new PALChain chain...\\n# Put objects into a list to record ordering\\nobjects = []\\nobjects += [(\\'booklet\\', \\'blue\\')] * 2\\nobjects += [(\\'booklet\\', \\'purple\\')] * 2\\nobjects += [(\\'sunglasses\\', \\'yellow\\')] * 2\\n# Remove all pairs of sunglasses\\nobjects = [object for object in objects if object[0] != \\'sunglasses\\']\\n# Count number of purple objects\\nnum_purple = len([object for object in objects if object[1] == \\'purple\\'])\\nanswer = num_purple\\n> Finished chain.\\nresult[\\'intermediate_steps\\']\\n\"# Put objects into a list to record ordering\\\\nobjects = []\\\\nobjects += [(\\'booklet\\', \\'blue\\')] * 2\\\\nobjects += [(\\'booklet\\', \\'purple\\')] * 2\\\\nobjects += [(\\'sunglasses\\', \\'yellow\\')] * 2\\\\n\\\\n# Remove all pairs of sunglasses\\\\nobjects = [object for object in objects if object[0] != \\'sunglasses\\']\\\\n\\\\n# Count number of purple objects\\\\nnum_purple = len([object for object in objects if object[1] == \\'purple\\'])\\\\nanswer = num_purple\"\\nprevious\\nOpenAPI Chain\\nnext\\nSQL Chain example\\n Contents\\n  \\nMath Prompt\\nColored Objects\\nIntermediate Steps\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\examples\\\\pal.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nSQL Chain example\\n Contents \\nCustomize Prompt\\nReturn Intermediate Steps\\nChoosing how to limit the number of rows returned\\nAdding example rows from each table\\nCustom Table Info\\nSQLDatabaseSequentialChain\\nSQL Chain example#\\nThis example demonstrates the use of the SQLDatabaseChain for answering questions over a database.\\nUnder the hood, LangChain uses SQLAlchemy to connect to SQL databases. The SQLDatabaseChain can therefore be used with any SQL dialect supported by SQLAlchemy, such as MS SQL, MySQL, MariaDB, PostgreSQL, Oracle SQL, and SQLite. Please refer to the SQLAlchemy documentation for more information about requirements for connecting to your database. For example, a connection to MySQL requires an appropriate connector such as PyMySQL. A URI for a MySQL connection might look like: mysql+pymysql://user:pass@some_mysql_db_address/db_name\\nThis demonstration uses SQLite and the example Chinook database.\\nTo set it up, follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file in a notebooks folder at the root of this repository.\\nfrom langchain import OpenAI, SQLDatabase, SQLDatabaseChain\\ndb = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\")\\nllm = OpenAI(temperature=0)\\nNOTE: For data-sensitive projects, you can specify return_direct=True in the SQLDatabaseChain initialization to directly return the output of the SQL query without any additional formatting. This prevents the LLM from seeing any contents within the database. Note, however, the LLM still has access to the database scheme (i.e. dialect, table and key names) by default.\\ndb_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)\\ndb_chain.run(\"How many employees are there?\")\\n> Entering new SQLDatabaseChain chain...\\nHow many employees are there? \\nSQLQuery:\\n/Users/harrisonchase/workplace/langchain/langchain/sql_database.py:120: SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage.\\n  sample_rows = connection.execute(command)\\n SELECT COUNT(*) FROM Employee;\\nSQLResult: [(8,)]\\nAnswer: There are 8 employees.\\n> Finished chain.\\n\\' There are 8 employees.\\'\\nCustomize Prompt#\\nYou can also customize the prompt that is used. Here is an example prompting it to understand that foobar is the same as the Employee table\\nfrom langchain.prompts.prompt import PromptTemplate\\n_DEFAULT_TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\\nUse the following format:\\nQuestion: \"Question here\"\\nSQLQuery: \"SQL Query to run\"\\nSQLResult: \"Result of the SQLQuery\"\\nAnswer: \"Final answer here\"\\nOnly use the following tables:\\n{table_info}\\nIf someone asks for the table foobar, they really mean the employee table.\\nQuestion: {input}\"\"\"\\nPROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"table_info\", \"dialect\"], template=_DEFAULT_TEMPLATE\\n)\\ndb_chain = SQLDatabaseChain(llm=llm, database=db, prompt=PROMPT, verbose=True)\\ndb_chain.run(\"How many employees are there in the foobar table?\")\\n> Entering new SQLDatabaseChain chain...\\nHow many employees are there in the foobar table? \\nSQLQuery: SELECT COUNT(*) FROM Employee;\\nSQLResult: [(8,)]\\nAnswer: There are 8 employees in the foobar table.\\n> Finished chain.\\n\\' There are 8 employees in the foobar table.\\'\\nReturn Intermediate Steps#\\nYou can also return the intermediate steps of the SQLDatabaseChain. This allows you to access the SQL statement that was generated, as well as the result of running that against the SQL Database.\\ndb_chain = SQLDatabaseChain(llm=llm, database=db, prompt=PROMPT, verbose=True, return_intermediate_steps=True)\\nresult = db_chain(\"How many employees are there in the foobar table?\")\\nresult[\"intermediate_steps\"]\\n> Entering new SQLDatabaseChain chain...\\nHow many employees are there in the foobar table? \\nSQLQuery: SELECT COUNT(*) FROM Employee;\\nSQLResult: [(8,)]\\nAnswer: There are 8 employees in the foobar table.\\n> Finished chain.\\n[\\' SELECT COUNT(*) FROM Employee;\\', \\'[(8,)]\\']\\nChoosing how to limit the number of rows returned#\\nIf you are querying for several rows of a table you can select the maximum number of results you want to get by using the ‘top_k’ parameter (default is 10). This is useful for avoiding query results that exceed the prompt max length or consume tokens unnecessarily.\\ndb_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True, top_k=3)\\ndb_chain.run(\"What are some example tracks by composer Johann Sebastian Bach?\")\\n> Entering new SQLDatabaseChain chain...\\nWhat are some example tracks by composer Johann Sebastian Bach? \\nSQLQuery: SELECT Name, Composer FROM Track WHERE Composer LIKE \\'%Johann Sebastian Bach%\\' LIMIT 3;\\nSQLResult: [(\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\', \\'Johann Sebastian Bach\\'), (\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\', \\'Johann Sebastian Bach\\'), (\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\', \\'Johann Sebastian Bach\\')]\\nAnswer: Some example tracks by composer Johann Sebastian Bach are \\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\', \\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\', and \\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\'.\\n> Finished chain.\\n\\' Some example tracks by composer Johann Sebastian Bach are \\\\\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\\\\\', \\\\\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\\\\\', and \\\\\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\\\\\'.\\'\\nAdding example rows from each table#\\nSometimes, the format of the data is not obvious and it is optimal to include a sample of rows from the tables in the prompt to allow the LLM to understand the data before providing a final query. Here we will use this feature to let the LLM know that artists are saved with their full names by providing two rows from the Track table.\\ndb = SQLDatabase.from_uri(\\n    \"sqlite:///../../../../notebooks/Chinook.db\",\\n    include_tables=[\\'Track\\'], # we include only one table to save tokens in the prompt :)\\n    sample_rows_in_table_info=2)\\nThe sample rows are added to the prompt after each corresponding table’s column information:\\nprint(db.table_info)\\nCREATE TABLE \"Track\" (\\n\\t\"TrackId\" INTEGER NOT NULL, \\n\\t\"Name\" NVARCHAR(200) NOT NULL, \\n\\t\"AlbumId\" INTEGER, \\n\\t\"MediaTypeId\" INTEGER NOT NULL, \\n\\t\"GenreId\" INTEGER, \\n\\t\"Composer\" NVARCHAR(220), \\n\\t\"Milliseconds\" INTEGER NOT NULL, \\n\\t\"Bytes\" INTEGER, \\n\\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \\n\\tPRIMARY KEY (\"TrackId\"), \\n\\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), \\n\\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), \\n\\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\\n)\\n/*\\n2 rows from Track table:\\nTrackId\\tName\\tAlbumId\\tMediaTypeId\\tGenreId\\tComposer\\tMilliseconds\\tBytes\\tUnitPrice\\n1\\tFor Those About To Rock (We Salute You)\\t1\\t1\\t1\\tAngus Young, Malcolm Young, Brian Johnson\\t343719\\t11170334\\t0.99\\n2\\tBalls to the Wall\\t2\\t2\\t1\\tNone\\t342562\\t5510424\\t0.99\\n*/\\n/home/jon/projects/langchain/langchain/sql_database.py:135: SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage.\\n  sample_rows = connection.execute(command)\\ndb_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)\\ndb_chain.run(\"What are some example tracks by Bach?\")\\n> Entering new SQLDatabaseChain chain...\\nWhat are some example tracks by Bach? \\nSQLQuery: SELECT Name FROM Track WHERE Composer LIKE \\'%Bach%\\' LIMIT 5;\\nSQLResult: [(\\'American Woman\\',), (\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\',), (\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\',), (\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\',), (\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\',)]\\nAnswer: Some example tracks by Bach are \\'American Woman\\', \\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\', \\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\', \\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\', and \\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\'.\\n> Finished chain.\\n\\' Some example tracks by Bach are \\\\\\'American Woman\\\\\\', \\\\\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\\\\\', \\\\\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\\\\\', \\\\\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\\\\\', and \\\\\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\\\\\'.\\'\\nCustom Table Info#\\nIn some cases, it can be useful to provide custom table information instead of using the automatically generated table definitions and the first sample_rows_in_table_info sample rows. For example, if you know that the first few rows of a table are uninformative, it could help to manually provide example rows that are more diverse or provide more information to the model. It is also possible to limit the columns that will be visible to the model if there are unnecessary columns.\\nThis information can be provided as a dictionary with table names as the keys and table information as the values. For example, let’s provide a custom definition and sample rows for the Track table with only a few columns:\\ncustom_table_info = {\\n    \"Track\": \"\"\"CREATE TABLE Track (\\n\\t\"TrackId\" INTEGER NOT NULL, \\n\\t\"Name\" NVARCHAR(200) NOT NULL,\\n\\t\"Composer\" NVARCHAR(220),\\n\\tPRIMARY KEY (\"TrackId\")\\n)\\n/*\\n3 rows from Track table:\\nTrackId\\tName\\tComposer\\n1\\tFor Those About To Rock (We Salute You)\\tAngus Young, Malcolm Young, Brian Johnson\\n2\\tBalls to the Wall\\tNone\\n3\\tMy favorite song ever\\tThe coolest composer of all time\\n*/\"\"\"\\n}\\ndb = SQLDatabase.from_uri(\\n    \"sqlite:///../../../../notebooks/Chinook.db\",\\n    include_tables=[\\'Track\\', \\'Playlist\\'],\\n    sample_rows_in_table_info=2,\\n    custom_table_info=custom_table_info)\\nprint(db.table_info)\\nCREATE TABLE \"Playlist\" (\\n\\t\"PlaylistId\" INTEGER NOT NULL, \\n\\t\"Name\" NVARCHAR(120), \\n\\tPRIMARY KEY (\"PlaylistId\")\\n)\\n/*\\n2 rows from Playlist table:\\nPlaylistId\\tName\\n1\\tMusic\\n2\\tMovies\\n*/\\nCREATE TABLE Track (\\n\\t\"TrackId\" INTEGER NOT NULL, \\n\\t\"Name\" NVARCHAR(200) NOT NULL,\\n\\t\"Composer\" NVARCHAR(220),\\n\\tPRIMARY KEY (\"TrackId\")\\n)\\n/*\\n3 rows from Track table:\\nTrackId\\tName\\tComposer\\n1\\tFor Those About To Rock (We Salute You)\\tAngus Young, Malcolm Young, Brian Johnson\\n2\\tBalls to the Wall\\tNone\\n3\\tMy favorite song ever\\tThe coolest composer of all time\\n*/\\nNote how our custom table definition and sample rows for Track overrides the sample_rows_in_table_info parameter. Tables that are not overridden by custom_table_info, in this example Playlist, will have their table info gathered automatically as usual.\\ndb_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)\\ndb_chain.run(\"What are some example tracks by Bach?\")\\n> Entering new SQLDatabaseChain chain...\\nWhat are some example tracks by Bach? \\nSQLQuery: SELECT Name, Composer FROM Track WHERE Composer LIKE \\'%Bach%\\' LIMIT 5;\\nSQLResult: [(\\'American Woman\\', \\'B. Cummings/G. Peterson/M.J. Kale/R. Bachman\\'), (\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\', \\'Johann Sebastian Bach\\'), (\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\', \\'Johann Sebastian Bach\\'), (\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\', \\'Johann Sebastian Bach\\'), (\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\', \\'Johann Sebastian Bach\\')]\\nAnswer: Some example tracks by Bach are \\'American Woman\\', \\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\', \\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\', \\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\', and \\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\'.\\n> Finished chain.\\n\\' Some example tracks by Bach are \\\\\\'American Woman\\\\\\', \\\\\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\\\\\', \\\\\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\\\\\', \\\\\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\\\\\', and \\\\\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\\\\\'.\\'\\nSQLDatabaseSequentialChain#\\nChain for querying SQL database that is a sequential chain.\\nThe chain is as follows:\\n1. Based on the query, determine which tables to use.\\n2. Based on those tables, call the normal SQL database chain.\\nThis is useful in cases where the number of tables in the database is large.\\nfrom langchain.chains import SQLDatabaseSequentialChain\\ndb = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\")\\nchain = SQLDatabaseSequentialChain.from_llm(llm, db, verbose=True)\\nchain.run(\"How many employees are also customers?\")\\n> Entering new SQLDatabaseSequentialChain chain...\\nTable names to use:\\n[\\'Customer\\', \\'Employee\\']\\n> Entering new SQLDatabaseChain chain...\\nHow many employees are also customers? \\nSQLQuery: SELECT COUNT(*) FROM Employee INNER JOIN Customer ON Employee.EmployeeId = Customer.SupportRepId;\\nSQLResult: [(59,)]\\nAnswer: 59 employees are also customers.\\n> Finished chain.\\n> Finished chain.\\n\\' 59 employees are also customers.\\'\\nprevious\\nPAL\\nnext\\nChains\\n Contents\\n  \\nCustomize Prompt\\nReturn Intermediate Steps\\nChoosing how to limit the number of rows returned\\nAdding example rows from each table\\nCustom Table Info\\nSQLDatabaseSequentialChain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\examples\\\\sqlite.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nAsync API for Chain\\nAsync API for Chain#\\nLangChain provides async support for Chains by leveraging the asyncio library.\\nAsync methods are currently supported in LLMChain (through arun, apredict, acall) and LLMMathChain (through arun and acall), ChatVectorDBChain, and QA chains. Async support for other chains is on the roadmap.\\nimport asyncio\\nimport time\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\ndef generate_serially():\\n    llm = OpenAI(temperature=0.9)\\n    prompt = PromptTemplate(\\n        input_variables=[\"product\"],\\n        template=\"What is a good name for a company that makes {product}?\",\\n    )\\n    chain = LLMChain(llm=llm, prompt=prompt)\\n    for _ in range(5):\\n        resp = chain.run(product=\"toothpaste\")\\n        print(resp)\\nasync def async_generate(chain):\\n    resp = await chain.arun(product=\"toothpaste\")\\n    print(resp)\\nasync def generate_concurrently():\\n    llm = OpenAI(temperature=0.9)\\n    prompt = PromptTemplate(\\n        input_variables=[\"product\"],\\n        template=\"What is a good name for a company that makes {product}?\",\\n    )\\n    chain = LLMChain(llm=llm, prompt=prompt)\\n    tasks = [async_generate(chain) for _ in range(5)]\\n    await asyncio.gather(*tasks)\\ns = time.perf_counter()\\n# If running this outside of Jupyter, use asyncio.run(generate_concurrently())\\nawait generate_concurrently()\\nelapsed = time.perf_counter() - s\\nprint(\\'\\\\033[1m\\' + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + \\'\\\\033[0m\\')\\ns = time.perf_counter()\\ngenerate_serially()\\nelapsed = time.perf_counter() - s\\nprint(\\'\\\\033[1m\\' + f\"Serial executed in {elapsed:0.2f} seconds.\" + \\'\\\\033[0m\\')\\nBrightSmile Toothpaste Company\\nBrightSmile Toothpaste Co.\\nBrightSmile Toothpaste\\nGleaming Smile Inc.\\nSparkleSmile Toothpaste\\nConcurrent executed in 1.54 seconds.\\nBrightSmile Toothpaste Co.\\nMintyFresh Toothpaste Co.\\nSparkleSmile Toothpaste.\\nPearly Whites Toothpaste Co.\\nBrightSmile Toothpaste.\\nSerial executed in 6.38 seconds.\\nprevious\\nHow-To Guides\\nnext\\nLoading from LangChainHub\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\generic\\\\async_chain.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nLoading from LangChainHub\\nLoading from LangChainHub#\\nThis notebook covers how to load chains from LangChainHub.\\nfrom langchain.chains import load_chain\\nchain = load_chain(\"lc://chains/llm-math/chain.json\")\\nchain.run(\"whats 2 raised to .12\")\\n> Entering new LLMMathChain chain...\\nwhats 2 raised to .12\\nAnswer: 1.0791812460476249\\n> Finished chain.\\n\\'Answer: 1.0791812460476249\\'\\nSometimes chains will require extra arguments that were not serialized with the chain. For example, a chain that does question answering over a vector database will require a vector database.\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain import OpenAI, VectorDBQA\\nfrom langchain.document_loaders import TextLoader\\nloader = TextLoader(\\'../../state_of_the_union.txt\\')\\ndocuments = loader.load()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_documents(documents)\\nembeddings = OpenAIEmbeddings()\\nvectorstore = Chroma.from_documents(texts, embeddings)\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\nchain = load_chain(\"lc://chains/vector-db-qa/stuff/chain.json\", vectorstore=vectorstore)\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nchain.run(query)\\n\" The president said that Ketanji Brown Jackson is a Circuit Court of Appeals Judge, one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans, and will continue Justice Breyer\\'s legacy of excellence.\"\\nprevious\\nAsync API for Chain\\nnext\\nLLM Chain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\generic\\\\from_hub.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nLLM Chain\\n Contents \\nSingle Input\\nMultiple Inputs\\nFrom string\\nLLM Chain#\\nThis notebook showcases a simple LLM chain.\\nfrom langchain import PromptTemplate, OpenAI, LLMChain\\nSingle Input#\\nFirst, lets go over an example using a single input\\ntemplate = \"\"\"Question: {question}\\nAnswer: Let\\'s think step by step.\"\"\"\\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\\nllm_chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0), verbose=True)\\nquestion = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\\nllm_chain.predict(question=question)\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nQuestion: What NFL team won the Super Bowl in the year Justin Beiber was born?\\nAnswer: Let\\'s think step by step.\\n> Finished LLMChain chain.\\n\\' Justin Bieber was born in 1994, so the NFL team that won the Super Bowl in 1994 was the Dallas Cowboys.\\'\\nMultiple Inputs#\\nNow lets go over an example using multiple inputs.\\ntemplate = \"\"\"Write a {adjective} poem about {subject}.\"\"\"\\nprompt = PromptTemplate(template=template, input_variables=[\"adjective\", \"subject\"])\\nllm_chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0), verbose=True)\\nllm_chain.predict(adjective=\"sad\", subject=\"ducks\")\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nWrite a sad poem about ducks.\\n> Finished LLMChain chain.\\n\"\\\\n\\\\nThe ducks swim in the pond,\\\\nTheir feathers so soft and warm,\\\\nBut they can\\'t help but feel so forlorn.\\\\n\\\\nTheir quacks echo in the air,\\\\nBut no one is there to hear,\\\\nFor they have no one to share.\\\\n\\\\nThe ducks paddle around in circles,\\\\nTheir heads hung low in despair,\\\\nFor they have no one to care.\\\\n\\\\nThe ducks look up to the sky,\\\\nBut no one is there to see,\\\\nFor they have no one to be.\\\\n\\\\nThe ducks drift away in the night,\\\\nTheir hearts filled with sorrow and pain,\\\\nFor they have no one to gain.\"\\nFrom string#\\nYou can also construct an LLMChain from a string template directly.\\ntemplate = \"\"\"Write a {adjective} poem about {subject}.\"\"\"\\nllm_chain = LLMChain.from_string(llm=OpenAI(temperature=0), template=template)\\nllm_chain.predict(adjective=\"sad\", subject=\"ducks\")\\n\"\\\\n\\\\nThe ducks swim in the pond,\\\\nTheir feathers so soft and warm,\\\\nBut they can\\'t help but feel so forlorn.\\\\n\\\\nTheir quacks echo in the air,\\\\nBut no one is there to hear,\\\\nFor they have no one to share.\\\\n\\\\nThe ducks paddle around in circles,\\\\nTheir heads hung low in despair,\\\\nFor they have no one to care.\\\\n\\\\nThe ducks look up to the sky,\\\\nBut no one is there to see,\\\\nFor they have no one to be.\\\\n\\\\nThe ducks drift away in the night,\\\\nTheir hearts filled with sorrow and pain,\\\\nFor they have no one to gain.\"\\nprevious\\nLoading from LangChainHub\\nnext\\nSequential Chains\\n Contents\\n  \\nSingle Input\\nMultiple Inputs\\nFrom string\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\generic\\\\llm_chain.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nSequential Chains\\n Contents \\nSimpleSequentialChain\\nSequential Chain\\nMemory in Sequential Chains\\nSequential Chains#\\nThe next step after calling a language model is make a series of calls to a language model. This is particularly useful when you want to take the output from one call and use it as the input to another.\\nIn this notebook we will walk through some examples for how to do this, using sequential chains. Sequential chains are defined as a series of chains, called in deterministic order. There are two types of sequential chains:\\nSimpleSequentialChain: The simplest form of sequential chains, where each step has a singular input/output, and the output of one step is the input to the next.\\nSequentialChain: A more general form of sequential chains, allowing for multiple inputs/outputs.\\nSimpleSequentialChain#\\nIn this series of chains, each individual chain has a single input and a single output, and the output of one step is used as input to the next.\\nLet’s walk through a toy example of doing this, where the first chain takes in the title of an imaginary play and then generates a synopsis for that title, and the second chain takes in the synopsis of that play and generates an imaginary review for that play.\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import LLMChain\\nfrom langchain.prompts import PromptTemplate\\n# This is an LLMChain to write a synopsis given a title of a play.\\nllm = OpenAI(temperature=.7)\\ntemplate = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\\nTitle: {title}\\nPlaywright: This is a synopsis for the above play:\"\"\"\\nprompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\\nsynopsis_chain = LLMChain(llm=llm, prompt=prompt_template)\\n# This is an LLMChain to write a review of a play given a synopsis.\\nllm = OpenAI(temperature=.7)\\ntemplate = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\\nPlay Synopsis:\\n{synopsis}\\nReview from a New York Times play critic of the above play:\"\"\"\\nprompt_template = PromptTemplate(input_variables=[\"synopsis\"], template=template)\\nreview_chain = LLMChain(llm=llm, prompt=prompt_template)\\n# This is the overall chain where we run these two chains in sequence.\\nfrom langchain.chains import SimpleSequentialChain\\noverall_chain = SimpleSequentialChain(chains=[synopsis_chain, review_chain], verbose=True)\\nreview = overall_chain.run(\"Tragedy at sunset on the beach\")\\n> Entering new SimpleSequentialChain chain...\\nTragedy at Sunset on the Beach is a story of a young couple, Jack and Sarah, who are in love and looking forward to their future together. On the night of their anniversary, they decide to take a walk on the beach at sunset. As they are walking, they come across a mysterious figure, who tells them that their love will be tested in the near future. \\nThe figure then tells the couple that the sun will soon set, and with it, a tragedy will strike. If Jack and Sarah can stay together and pass the test, they will be granted everlasting love. However, if they fail, their love will be lost forever.\\nThe play follows the couple as they struggle to stay together and battle the forces that threaten to tear them apart. Despite the tragedy that awaits them, they remain devoted to one another and fight to keep their love alive. In the end, the couple must decide whether to take a chance on their future together or succumb to the tragedy of the sunset.\\nTragedy at Sunset on the Beach is an emotionally gripping story of love, hope, and sacrifice. Through the story of Jack and Sarah, the audience is taken on a journey of self-discovery and the power of love to overcome even the greatest of obstacles. \\nThe play\\'s talented cast brings the characters to life, allowing us to feel the depths of their emotion and the intensity of their struggle. With its compelling story and captivating performances, this play is sure to draw in audiences and leave them on the edge of their seats. \\nThe play\\'s setting of the beach at sunset adds a touch of poignancy and romanticism to the story, while the mysterious figure serves to keep the audience enthralled. Overall, Tragedy at Sunset on the Beach is an engaging and thought-provoking play that is sure to leave audiences feeling inspired and hopeful.\\n> Finished chain.\\nprint(review)\\nTragedy at Sunset on the Beach is an emotionally gripping story of love, hope, and sacrifice. Through the story of Jack and Sarah, the audience is taken on a journey of self-discovery and the power of love to overcome even the greatest of obstacles. \\nThe play\\'s talented cast brings the characters to life, allowing us to feel the depths of their emotion and the intensity of their struggle. With its compelling story and captivating performances, this play is sure to draw in audiences and leave them on the edge of their seats. \\nThe play\\'s setting of the beach at sunset adds a touch of poignancy and romanticism to the story, while the mysterious figure serves to keep the audience enthralled. Overall, Tragedy at Sunset on the Beach is an engaging and thought-provoking play that is sure to leave audiences feeling inspired and hopeful.\\nSequential Chain#\\nOf course, not all sequential chains will be as simple as passing a single string as an argument and getting a single string as output for all steps in the chain. In this next example, we will experiment with more complex chains that involve multiple inputs, and where there also multiple final outputs.\\nOf particular importance is how we name the input/output variable names. In the above example we didn’t have to think about that because we were just passing the output of one chain directly as input to the next, but here we do have worry about that because we have multiple inputs.\\n# This is an LLMChain to write a synopsis given a title of a play and the era it is set in.\\nllm = OpenAI(temperature=.7)\\ntemplate = \"\"\"You are a playwright. Given the title of play and the era it is set in, it is your job to write a synopsis for that title.\\nTitle: {title}\\nEra: {era}\\nPlaywright: This is a synopsis for the above play:\"\"\"\\nprompt_template = PromptTemplate(input_variables=[\"title\", \\'era\\'], template=template)\\nsynopsis_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"synopsis\")\\n# This is an LLMChain to write a review of a play given a synopsis.\\nllm = OpenAI(temperature=.7)\\ntemplate = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\\nPlay Synopsis:\\n{synopsis}\\nReview from a New York Times play critic of the above play:\"\"\"\\nprompt_template = PromptTemplate(input_variables=[\"synopsis\"], template=template)\\nreview_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"review\")\\n# This is the overall chain where we run these two chains in sequence.\\nfrom langchain.chains import SequentialChain\\noverall_chain = SequentialChain(\\n    chains=[synopsis_chain, review_chain],\\n    input_variables=[\"era\", \"title\"],\\n    # Here we return multiple variables\\n    output_variables=[\"synopsis\", \"review\"],\\n    verbose=True)\\noverall_chain({\"title\":\"Tragedy at sunset on the beach\", \"era\": \"Victorian England\"})\\n> Entering new SequentialChain chain...\\n> Finished chain.\\n{\\'title\\': \\'Tragedy at sunset on the beach\\',\\n \\'era\\': \\'Victorian England\\',\\n \\'synopsis\\': \"\\\\n\\\\nThe play follows the story of John, a young man from a wealthy Victorian family, who dreams of a better life for himself. He soon meets a beautiful young woman named Mary, who shares his dream. The two fall in love and decide to elope and start a new life together.\\\\n\\\\nOn their journey, they make their way to a beach at sunset, where they plan to exchange their vows of love. Unbeknownst to them, their plans are overheard by John\\'s father, who has been tracking them. He follows them to the beach and, in a fit of rage, confronts them. \\\\n\\\\nA physical altercation ensues, and in the struggle, John\\'s father accidentally stabs Mary in the chest with his sword. The two are left in shock and disbelief as Mary dies in John\\'s arms, her last words being a declaration of her love for him.\\\\n\\\\nThe tragedy of the play comes to a head when John, broken and with no hope of a future, chooses to take his own life by jumping off the cliffs into the sea below. \\\\n\\\\nThe play is a powerful story of love, hope, and loss set against the backdrop of 19th century England.\",\\n \\'review\\': \"\\\\n\\\\nThe latest production from playwright X is a powerful and heartbreaking story of love and loss set against the backdrop of 19th century England. The play follows John, a young man from a wealthy Victorian family, and Mary, a beautiful young woman with whom he falls in love. The two decide to elope and start a new life together, and the audience is taken on a journey of hope and optimism for the future.\\\\n\\\\nUnfortunately, their dreams are cut short when John\\'s father discovers them and in a fit of rage, fatally stabs Mary. The tragedy of the play is further compounded when John, broken and without hope, takes his own life. The storyline is not only realistic, but also emotionally compelling, drawing the audience in from start to finish.\\\\n\\\\nThe acting was also commendable, with the actors delivering believable and nuanced performances. The playwright and director have successfully crafted a timeless tale of love and loss that will resonate with audiences for years to come. Highly recommended.\"}\\nMemory in Sequential Chains#\\nSometimes you may want to pass along some context to use in each step of the chain or in a later part of the chain, but maintaining and chaining together the input/output variables can quickly get messy.  Using SimpleMemory is a convenient way to do manage this and clean up your chains.\\nFor example, using the previous playwright SequentialChain, lets say you wanted to include some context about date, time and location of the play, and using the generated synopsis and review, create some social media post text.  You could add these new context variables as input_variables, or we can add a SimpleMemory to the chain to manage this context:\\nfrom langchain.chains import SequentialChain\\nfrom langchain.memory import SimpleMemory\\nllm = OpenAI(temperature=.7)\\ntemplate = \"\"\"You are a social media manager for a theater company.  Given the title of play, the era it is set in, the date,time and location, the synopsis of the play, and the review of the play, it is your job to write a social media post for that play.\\nHere is some context about the time and location of the play:\\nDate and Time: {time}\\nLocation: {location}\\nPlay Synopsis:\\n{synopsis}\\nReview from a New York Times play critic of the above play:\\n{review}\\nSocial Media Post:\\n\"\"\"\\nprompt_template = PromptTemplate(input_variables=[\"synopsis\", \"review\", \"time\", \"location\"], template=template)\\nsocial_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"social_post_text\")\\noverall_chain = SequentialChain(\\n    memory=SimpleMemory(memories={\"time\": \"December 25th, 8pm PST\", \"location\": \"Theater in the Park\"}),\\n    chains=[synopsis_chain, review_chain, social_chain],\\n    input_variables=[\"era\", \"title\"],\\n    # Here we return multiple variables\\n    output_variables=[\"social_post_text\"],\\n    verbose=True)\\noverall_chain({\"title\":\"Tragedy at sunset on the beach\", \"era\": \"Victorian England\"})\\n> Entering new SequentialChain chain...\\n> Finished chain.\\n{\\'title\\': \\'Tragedy at sunset on the beach\\',\\n \\'era\\': \\'Victorian England\\',\\n \\'time\\': \\'December 25th, 8pm PST\\',\\n \\'location\\': \\'Theater in the Park\\',\\n \\'social_post_text\\': \"\\\\nSpend your Christmas night with us at Theater in the Park and experience the heartbreaking story of love and loss that is \\'A Walk on the Beach\\'. Set in Victorian England, this romantic tragedy follows the story of Frances and Edward, a young couple whose love is tragically cut short. Don\\'t miss this emotional and thought-provoking production that is sure to leave you in tears. #AWalkOnTheBeach #LoveAndLoss #TheaterInThePark #VictorianEngland\"}\\nprevious\\nLLM Chain\\nnext\\nSerialization\\n Contents\\n  \\nSimpleSequentialChain\\nSequential Chain\\nMemory in Sequential Chains\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\generic\\\\sequential_chains.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nSerialization\\n Contents \\nSaving a chain to disk\\nLoading a chain from disk\\nSaving components separately\\nSerialization#\\nThis notebook covers how to serialize chains to and from disk. The serialization format we use is json or yaml. Currently, only some chains support this type of serialization. We will grow the number of supported chains over time.\\nSaving a chain to disk#\\nFirst, let’s go over how to save a chain to disk. This can be done with the .save method, and specifying a file path with a json or yaml extension.\\nfrom langchain import PromptTemplate, OpenAI, LLMChain\\ntemplate = \"\"\"Question: {question}\\nAnswer: Let\\'s think step by step.\"\"\"\\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\\nllm_chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0), verbose=True)\\nllm_chain.save(\"llm_chain.json\")\\nLet’s now take a look at what’s inside this saved file\\n!cat llm_chain.json\\n{\\n    \"memory\": null,\\n    \"verbose\": true,\\n    \"prompt\": {\\n        \"input_variables\": [\\n            \"question\"\\n        ],\\n        \"output_parser\": null,\\n        \"template\": \"Question: {question}\\\\n\\\\nAnswer: Let\\'s think step by step.\",\\n        \"template_format\": \"f-string\"\\n    },\\n    \"llm\": {\\n        \"model_name\": \"text-davinci-003\",\\n        \"temperature\": 0.0,\\n        \"max_tokens\": 256,\\n        \"top_p\": 1,\\n        \"frequency_penalty\": 0,\\n        \"presence_penalty\": 0,\\n        \"n\": 1,\\n        \"best_of\": 1,\\n        \"request_timeout\": null,\\n        \"logit_bias\": {},\\n        \"_type\": \"openai\"\\n    },\\n    \"output_key\": \"text\",\\n    \"_type\": \"llm_chain\"\\n}\\nLoading a chain from disk#\\nWe can load a chain from disk by using the load_chain method.\\nfrom langchain.chains import load_chain\\nchain = load_chain(\"llm_chain.json\")\\nchain.run(\"whats 2 + 2\")\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nQuestion: whats 2 + 2\\nAnswer: Let\\'s think step by step.\\n> Finished chain.\\n\\' 2 + 2 = 4\\'\\nSaving components separately#\\nIn the above example, we can see that the prompt and llm configuration information is saved in the same json as the overall chain. Alternatively, we can split them up and save them separately. This is often useful to make the saved components more modular. In order to do this, we just need to specify llm_path instead of the llm component, and prompt_path instead of the prompt component.\\nllm_chain.prompt.save(\"prompt.json\")\\n!cat prompt.json\\n{\\n    \"input_variables\": [\\n        \"question\"\\n    ],\\n    \"output_parser\": null,\\n    \"template\": \"Question: {question}\\\\n\\\\nAnswer: Let\\'s think step by step.\",\\n    \"template_format\": \"f-string\"\\n}\\nllm_chain.llm.save(\"llm.json\")\\n!cat llm.json\\n{\\n    \"model_name\": \"text-davinci-003\",\\n    \"temperature\": 0.0,\\n    \"max_tokens\": 256,\\n    \"top_p\": 1,\\n    \"frequency_penalty\": 0,\\n    \"presence_penalty\": 0,\\n    \"n\": 1,\\n    \"best_of\": 1,\\n    \"request_timeout\": null,\\n    \"logit_bias\": {},\\n    \"_type\": \"openai\"\\n}\\nconfig = {\\n    \"memory\": None,\\n    \"verbose\": True,\\n    \"prompt_path\": \"prompt.json\",\\n    \"llm_path\": \"llm.json\",\\n    \"output_key\": \"text\",\\n    \"_type\": \"llm_chain\"\\n}\\nimport json\\nwith open(\"llm_chain_separate.json\", \"w\") as f:\\n    json.dump(config, f, indent=2)\\n!cat llm_chain_separate.json\\n{\\n  \"memory\": null,\\n  \"verbose\": true,\\n  \"prompt_path\": \"prompt.json\",\\n  \"llm_path\": \"llm.json\",\\n  \"output_key\": \"text\",\\n  \"_type\": \"llm_chain\"\\n}\\nWe can then load it in the same way\\nchain = load_chain(\"llm_chain_separate.json\")\\nchain.run(\"whats 2 + 2\")\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nQuestion: whats 2 + 2\\nAnswer: Let\\'s think step by step.\\n> Finished chain.\\n\\' 2 + 2 = 4\\'\\nprevious\\nSequential Chains\\nnext\\nTransformation Chain\\n Contents\\n  \\nSaving a chain to disk\\nLoading a chain from disk\\nSaving components separately\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\generic\\\\serialization.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nTransformation Chain\\nTransformation Chain#\\nThis notebook showcases using a generic transformation chain.\\nAs an example, we will create a dummy transformation that takes in a super long text, filters the text to only the first 3 paragraphs, and then passes that into an LLMChain to summarize those.\\nfrom langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nwith open(\"../../state_of_the_union.txt\") as f:\\n    state_of_the_union = f.read()\\ndef transform_func(inputs: dict) -> dict:\\n    text = inputs[\"text\"]\\n    shortened_text = \"\\\\n\\\\n\".join(text.split(\"\\\\n\\\\n\")[:3])\\n    return {\"output_text\": shortened_text}\\ntransform_chain = TransformChain(input_variables=[\"text\"], output_variables=[\"output_text\"], transform=transform_func)\\ntemplate = \"\"\"Summarize this text:\\n{output_text}\\nSummary:\"\"\"\\nprompt = PromptTemplate(input_variables=[\"output_text\"], template=template)\\nllm_chain = LLMChain(llm=OpenAI(), prompt=prompt)\\nsequential_chain = SimpleSequentialChain(chains=[transform_chain, llm_chain])\\nsequential_chain.run(state_of_the_union)\\n\\' The speaker addresses the nation, noting that while last year they were kept apart due to COVID-19, this year they are together again. They are reminded that regardless of their political affiliations, they are all Americans.\\'\\nprevious\\nSerialization\\nnext\\nAnalyze Document\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\generic\\\\transformation.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nAnalyze Document\\n Contents \\nSummarize\\nQuestion Answering\\nAnalyze Document#\\nThe AnalyzeDocumentChain is more of an end to chain. This chain takes in a single document, splits it up, and then runs it through a CombineDocumentsChain. This can be used as more of an end-to-end chain.\\nwith open(\"../../state_of_the_union.txt\") as f:\\n    state_of_the_union = f.read()\\nSummarize#\\nLet’s take a look at it in action below, using it summarize a long document.\\nfrom langchain import OpenAI\\nfrom langchain.chains.summarize import load_summarize_chain\\nllm = OpenAI(temperature=0)\\nsummary_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\\nfrom langchain.chains import AnalyzeDocumentChain\\nsummarize_document_chain = AnalyzeDocumentChain(combine_docs_chain=summary_chain)\\nsummarize_document_chain.run(state_of_the_union)\\n\" In this speech, President Biden addresses the American people and the world, discussing the recent aggression of Russia\\'s Vladimir Putin in Ukraine and the US response. He outlines economic sanctions and other measures taken to hold Putin accountable, and announces the US Department of Justice\\'s task force to go after the crimes of Russian oligarchs. He also announces plans to fight inflation and lower costs for families, invest in American manufacturing, and provide military, economic, and humanitarian assistance to Ukraine. He calls for immigration reform, protecting the rights of women, and advancing the rights of LGBTQ+ Americans, and pays tribute to military families. He concludes with optimism for the future of America.\"\\nQuestion Answering#\\nLet’s take a look at this using a question answering chain.\\nfrom langchain.chains.question_answering import load_qa_chain\\nqa_chain = load_qa_chain(llm, chain_type=\"map_reduce\")\\nqa_document_chain = AnalyzeDocumentChain(combine_docs_chain=qa_chain)\\nqa_document_chain.run(input_document=state_of_the_union, question=\"what did the president say about justice breyer?\")\\n\\' The president thanked Justice Breyer for his service.\\'\\nprevious\\nTransformation Chain\\nnext\\nChat Over Documents with Chat History\\n Contents\\n  \\nSummarize\\nQuestion Answering\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\index_examples\\\\analyze_document.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nChat Over Documents with Chat History\\n Contents \\nReturn Source Documents\\nConversationalRetrievalChain with search_distance\\nConversationalRetrievalChain with map_reduce\\nConversationalRetrievalChain with Question Answering with sources\\nConversationalRetrievalChain with streaming to stdout\\nget_chat_history Function\\nChat Over Documents with Chat History#\\nThis notebook goes over how to set up a chain to chat over documents with chat history using a ConversationalRetrievalChain. The only difference between this chain and the RetrievalQAChain is that this allows for passing in of a chat history which can be used to allow for follow up questions.\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import ConversationalRetrievalChain\\nLoad in documents. You can replace this with a loader for whatever type of data you want\\nfrom langchain.document_loaders import TextLoader\\nloader = TextLoader(\"../../state_of_the_union.txt\")\\ndocuments = loader.load()\\nIf you had multiple loaders that you wanted to combine, you do something like:\\n# loaders = [....]\\n# docs = []\\n# for loader in loaders:\\n#     docs.extend(loader.load())\\nWe now split the documents, create embeddings for them, and put them in a vectorstore. This allows us to do semantic search over them.\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ndocuments = text_splitter.split_documents(documents)\\nembeddings = OpenAIEmbeddings()\\nvectorstore = Chroma.from_documents(documents, embeddings)\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\nWe now initialize the ConversationalRetrievalChain\\nqa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever())\\nHere’s an example of asking a question with no chat history\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history})\\nresult[\"answer\"]\\n\" The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\"\\nHere’s an example of asking a question with some chat history\\nchat_history = [(query, result[\"answer\"])]\\nquery = \"Did he mention who she suceeded\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history})\\nresult[\\'answer\\']\\n\\' Ketanji Brown Jackson succeeded Justice Stephen Breyer on the United States Supreme Court.\\'\\nReturn Source Documents#\\nYou can also easily return source documents from the ConversationalRetrievalChain. This is useful for when you want to inspect what documents were returned.\\nqa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True)\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history})\\nresult[\\'source_documents\\'][0]\\nDocument(page_content=\\'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\\\n\\\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\\n\\\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\\', metadata={\\'source\\': \\'../../state_of_the_union.txt\\'})\\nConversationalRetrievalChain with search_distance#\\nIf you are using a vector store that supports filtering by search distance, you can add a threshold value parameter.\\nvectordbkwargs = {\"search_distance\": 0.9}\\nqa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True)\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history, \"vectordbkwargs\": vectordbkwargs})\\nConversationalRetrievalChain with map_reduce#\\nWe can also use different types of combine document chains with the ConversationalRetrievalChain chain.\\nfrom langchain.chains import LLMChain\\nfrom langchain.chains.question_answering import load_qa_chain\\nfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\\nllm = OpenAI(temperature=0)\\nquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\\ndoc_chain = load_qa_chain(llm, chain_type=\"map_reduce\")\\nchain = ConversationalRetrievalChain(\\n    retriever=vectorstore.as_retriever(),\\n    question_generator=question_generator,\\n    combine_docs_chain=doc_chain,\\n)\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = chain({\"question\": query, \"chat_history\": chat_history})\\nresult[\\'answer\\']\\n\" The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, from a family of public school educators and police officers, a consensus builder, and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\"\\nConversationalRetrievalChain with Question Answering with sources#\\nYou can also use this chain with the question answering with sources chain.\\nfrom langchain.chains.qa_with_sources import load_qa_with_sources_chain\\nllm = OpenAI(temperature=0)\\nquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\\ndoc_chain = load_qa_with_sources_chain(llm, chain_type=\"map_reduce\")\\nchain = ConversationalRetrievalChain(\\n    retriever=vectorstore.as_retriever(),\\n    question_generator=question_generator,\\n    combine_docs_chain=doc_chain,\\n)\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = chain({\"question\": query, \"chat_history\": chat_history})\\nresult[\\'answer\\']\\n\" The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, from a family of public school educators and police officers, a consensus builder, and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\\\nSOURCES: ../../state_of_the_union.txt\"\\nConversationalRetrievalChain with streaming to stdout#\\nOutput from the chain will be streamed to stdout token by token in this example.\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.callbacks.base import CallbackManager\\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\\nfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT\\nfrom langchain.chains.question_answering import load_qa_chain\\n# Construct a ConversationalRetrievalChain with a streaming llm for combine docs\\n# and a separate, non-streaming llm for question generation\\nllm = OpenAI(temperature=0)\\nstreaming_llm = OpenAI(streaming=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=True, temperature=0)\\nquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\\ndoc_chain = load_qa_chain(streaming_llm, chain_type=\"stuff\", prompt=QA_PROMPT)\\nqa = ConversationalRetrievalChain(\\n    retriever=vectorstore.as_retriever(), combine_docs_chain=doc_chain, question_generator=question_generator)\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history})\\n The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\\nchat_history = [(query, result[\"answer\"])]\\nquery = \"Did he mention who she suceeded\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history})\\n Ketanji Brown Jackson succeeded Justice Stephen Breyer on the United States Supreme Court.\\nget_chat_history Function#\\nYou can also specify a get_chat_history function, which can be used to format the chat_history string.\\ndef get_chat_history(inputs) -> str:\\n    res = []\\n    for human, ai in inputs:\\n        res.append(f\"Human:{human}\\\\nAI:{ai}\")\\n    return \"\\\\n\".join(res)\\nqa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever(), get_chat_history=get_chat_history)\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history})\\nresult[\\'answer\\']\\n\" The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\"\\nprevious\\nAnalyze Document\\nnext\\nGraph QA\\n Contents\\n  \\nReturn Source Documents\\nConversationalRetrievalChain with search_distance\\nConversationalRetrievalChain with map_reduce\\nConversationalRetrievalChain with Question Answering with sources\\nConversationalRetrievalChain with streaming to stdout\\nget_chat_history Function\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\index_examples\\\\chat_vector_db.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nGraph QA\\n Contents \\nCreate the graph\\nQuerying the graph\\nSave the graph\\nGraph QA#\\nThis notebook goes over how to do question answering over a graph data structure.\\nCreate the graph#\\nIn this section, we construct an example graph. At the moment, this works best for small pieces of text.\\nfrom langchain.indexes import GraphIndexCreator\\nfrom langchain.llms import OpenAI\\nfrom langchain.document_loaders import TextLoader\\nindex_creator = GraphIndexCreator(llm=OpenAI(temperature=0))\\nwith open(\"../../state_of_the_union.txt\") as f:\\n    all_text = f.read()\\nWe will use just a small snippet, because extracting the knowledge triplets is a bit intensive at the moment.\\ntext = \"\\\\n\".join(all_text.split(\"\\\\n\\\\n\")[105:108])\\ntext\\n\\'It won’t look like much, but if you stop and look closely, you’ll see a “Field of dreams,” the ground on which America’s future will be built. \\\\nThis is where Intel, the American company that helped build Silicon Valley, is going to build its $20 billion semiconductor “mega site”. \\\\nUp to eight state-of-the-art factories in one place. 10,000 new good-paying jobs. \\'\\ngraph = index_creator.from_text(text)\\nWe can inspect the created graph.\\ngraph.get_triples()\\n[(\\'Intel\\', \\'$20 billion semiconductor \"mega site\"\\', \\'is going to build\\'),\\n (\\'Intel\\', \\'state-of-the-art factories\\', \\'is building\\'),\\n (\\'Intel\\', \\'10,000 new good-paying jobs\\', \\'is creating\\'),\\n (\\'Intel\\', \\'Silicon Valley\\', \\'is helping build\\'),\\n (\\'Field of dreams\\',\\n  \"America\\'s future will be built\",\\n  \\'is the ground on which\\')]\\nQuerying the graph#\\nWe can now use the graph QA chain to ask question of the graph\\nfrom langchain.chains import GraphQAChain\\nchain = GraphQAChain.from_llm(OpenAI(temperature=0), graph=graph, verbose=True)\\nchain.run(\"what is Intel going to build?\")\\n> Entering new GraphQAChain chain...\\nEntities Extracted:\\n Intel\\nFull Context:\\nIntel is going to build $20 billion semiconductor \"mega site\"\\nIntel is building state-of-the-art factories\\nIntel is creating 10,000 new good-paying jobs\\nIntel is helping build Silicon Valley\\n> Finished chain.\\n\\' Intel is going to build a $20 billion semiconductor \"mega site\" with state-of-the-art factories, creating 10,000 new good-paying jobs and helping to build Silicon Valley.\\'\\nSave the graph#\\nWe can also save and load the graph.\\ngraph.write_to_gml(\"graph.gml\")\\nfrom langchain.indexes.graph import NetworkxEntityGraph\\nloaded_graph = NetworkxEntityGraph.from_gml(\"graph.gml\")\\nloaded_graph.get_triples()\\n[(\\'Intel\\', \\'$20 billion semiconductor \"mega site\"\\', \\'is going to build\\'),\\n (\\'Intel\\', \\'state-of-the-art factories\\', \\'is building\\'),\\n (\\'Intel\\', \\'10,000 new good-paying jobs\\', \\'is creating\\'),\\n (\\'Intel\\', \\'Silicon Valley\\', \\'is helping build\\'),\\n (\\'Field of dreams\\',\\n  \"America\\'s future will be built\",\\n  \\'is the ground on which\\')]\\nprevious\\nChat Over Documents with Chat History\\nnext\\nHypothetical Document Embeddings\\n Contents\\n  \\nCreate the graph\\nQuerying the graph\\nSave the graph\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\index_examples\\\\graph_qa.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nHypothetical Document Embeddings\\n Contents \\nMultiple generations\\nUsing our own prompts\\nUsing HyDE\\nHypothetical Document Embeddings#\\nThis notebook goes over how to use Hypothetical Document Embeddings (HyDE), as described in this paper.\\nAt a high level, HyDE is an embedding technique that takes queries, generates a hypothetical answer, and then embeds that generated document and uses that as the final example.\\nIn order to use HyDE, we therefore need to provide a base embedding model, as well as an LLMChain that can be used to generate those documents. By default, the HyDE class comes with some default prompts to use (see the paper for more details on them), but we can also create our own.\\nfrom langchain.llms import OpenAI\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.chains import LLMChain, HypotheticalDocumentEmbedder\\nfrom langchain.prompts import PromptTemplate\\nbase_embeddings = OpenAIEmbeddings()\\nllm = OpenAI()\\n# Load with `web_search` prompt\\nembeddings = HypotheticalDocumentEmbedder.from_llm(llm, base_embeddings, \"web_search\")\\n# Now we can use it as any embedding class!\\nresult = embeddings.embed_query(\"Where is the Taj Mahal?\")\\nMultiple generations#\\nWe can also generate multiple documents and then combine the embeddings for those. By default, we combine those by taking the average. We can do this by changing the LLM we use to generate documents to return multiple things.\\nmulti_llm = OpenAI(n=4, best_of=4)\\nembeddings = HypotheticalDocumentEmbedder.from_llm(multi_llm, base_embeddings, \"web_search\")\\nresult = embeddings.embed_query(\"Where is the Taj Mahal?\")\\nUsing our own prompts#\\nBesides using preconfigured prompts, we can also easily construct our own prompts and use those in the LLMChain that is generating the documents. This can be useful if we know the domain our queries will be in, as we can condition the prompt to generate text more similar to that.\\nIn the example below, let’s condition it to generate text about a state of the union address (because we will use that in the next example).\\nprompt_template = \"\"\"Please answer the user\\'s question about the most recent state of the union address\\nQuestion: {question}\\nAnswer:\"\"\"\\nprompt = PromptTemplate(input_variables=[\"question\"], template=prompt_template)\\nllm_chain = LLMChain(llm=llm, prompt=prompt)\\nembeddings = HypotheticalDocumentEmbedder(llm_chain=llm_chain, base_embeddings=base_embeddings)\\nresult = embeddings.embed_query(\"What did the president say about Ketanji Brown Jackson\")\\nUsing HyDE#\\nNow that we have HyDE, we can use it as we would any other embedding class! Here is using it to find similar passages in the state of the union example.\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.vectorstores import Chroma\\nwith open(\"../../state_of_the_union.txt\") as f:\\n    state_of_the_union = f.read()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_text(state_of_the_union)\\ndocsearch = Chroma.from_texts(texts, embeddings)\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\ndocs = docsearch.similarity_search(query)\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\nprint(docs[0].page_content)\\nIn state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. \\nWe cannot let this happen. \\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\\nprevious\\nGraph QA\\nnext\\nQuestion Answering with Sources\\n Contents\\n  \\nMultiple generations\\nUsing our own prompts\\nUsing HyDE\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\index_examples\\\\hyde.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nQuestion Answering with Sources\\n Contents \\nPrepare Data\\nQuickstart\\nThe stuff Chain\\nThe map_reduce Chain\\nThe refine Chain\\nThe map-rerank Chain\\nQuestion Answering with Sources#\\nThis notebook walks through how to use LangChain for question answering with sources over a list of documents. It covers four different chain types: stuff, map_reduce, refine,map-rerank. For a more in depth explanation of what these chain types are, see here.\\nPrepare Data#\\nFirst we prepare the data. For this example we do similarity search over a vector database, but these documents could be fetched in any manner (the point of this notebook to highlight what to do AFTER you fetch the documents).\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.embeddings.cohere import CohereEmbeddings\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.docstore.document import Document\\nfrom langchain.prompts import PromptTemplate\\nwith open(\"../../state_of_the_union.txt\") as f:\\n    state_of_the_union = f.read()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_text(state_of_the_union)\\nembeddings = OpenAIEmbeddings()\\ndocsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))])\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\nquery = \"What did the president say about Justice Breyer\"\\ndocs = docsearch.similarity_search(query)\\nfrom langchain.chains.qa_with_sources import load_qa_with_sources_chain\\nfrom langchain.llms import OpenAI\\nQuickstart#\\nIf you just want to get started as quickly as possible, this is the recommended way to do it:\\nchain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\")\\nquery = \"What did the president say about Justice Breyer\"\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n{\\'output_text\\': \\' The president thanked Justice Breyer for his service.\\\\nSOURCES: 30-pl\\'}\\nIf you want more control and understanding over what is happening, please see the information below.\\nThe stuff Chain#\\nThis sections shows results of using the stuff Chain to do question answering with sources.\\nchain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\")\\nquery = \"What did the president say about Justice Breyer\"\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n{\\'output_text\\': \\' The president thanked Justice Breyer for his service.\\\\nSOURCES: 30-pl\\'}\\nCustom Prompts\\nYou can also use your own prompts with this chain. In this example, we will respond in Italian.\\ntemplate = \"\"\"Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\nRespond in Italian.\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER IN ITALIAN:\"\"\"\\nPROMPT = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\\nchain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\", prompt=PROMPT)\\nquery = \"What did the president say about Justice Breyer\"\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n{\\'output_text\\': \\'\\\\nNon so cosa abbia detto il presidente riguardo a Justice Breyer.\\\\nSOURCES: 30, 31, 33\\'}\\nThe map_reduce Chain#\\nThis sections shows results of using the map_reduce Chain to do question answering with sources.\\nchain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"map_reduce\")\\nquery = \"What did the president say about Justice Breyer\"\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n{\\'output_text\\': \\' The president thanked Justice Breyer for his service.\\\\nSOURCES: 30-pl\\'}\\nIntermediate Steps\\nWe can also return the intermediate steps for map_reduce chains, should we want to inspect them. This is done with the return_map_steps variable.\\nchain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"map_reduce\", return_intermediate_steps=True)\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n{\\'intermediate_steps\\': [\\' \"Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\"\\',\\n  \\' None\\',\\n  \\' None\\',\\n  \\' None\\'],\\n \\'output_text\\': \\' The president thanked Justice Breyer for his service.\\\\nSOURCES: 30-pl\\'}\\nCustom Prompts\\nYou can also use your own prompts with this chain. In this example, we will respond in Italian.\\nquestion_prompt_template = \"\"\"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text in Italian.\\n{context}\\nQuestion: {question}\\nRelevant text, if any, in Italian:\"\"\"\\nQUESTION_PROMPT = PromptTemplate(\\n    template=question_prompt_template, input_variables=[\"context\", \"question\"]\\n)\\ncombine_prompt_template = \"\"\"Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\nRespond in Italian.\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER IN ITALIAN:\"\"\"\\nCOMBINE_PROMPT = PromptTemplate(\\n    template=combine_prompt_template, input_variables=[\"summaries\", \"question\"]\\n)\\nchain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"map_reduce\", return_intermediate_steps=True, question_prompt=QUESTION_PROMPT, combine_prompt=COMBINE_PROMPT)\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n{\\'intermediate_steps\\': [\"\\\\nStasera vorrei onorare qualcuno che ha dedicato la sua vita a servire questo paese: il giustizia Stephen Breyer - un veterano dell\\'esercito, uno studioso costituzionale e un giustizia in uscita della Corte Suprema degli Stati Uniti. Giustizia Breyer, grazie per il tuo servizio.\",\\n  \\' Non pertinente.\\',\\n  \\' Non rilevante.\\',\\n  \" Non c\\'è testo pertinente.\"],\\n \\'output_text\\': \\' Non conosco la risposta. SOURCES: 30, 31, 33, 20.\\'}\\nBatch Size\\nWhen using the map_reduce chain, one thing to keep in mind is the batch size you are using during the map step. If this is too high, it could cause rate limiting errors. You can control this by setting the batch size on the LLM used. Note that this only applies for LLMs with this parameter. Below is an example of doing so:\\nllm = OpenAI(batch_size=5, temperature=0)\\nThe refine Chain#\\nThis sections shows results of using the refine Chain to do question answering with sources.\\nchain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"refine\")\\nquery = \"What did the president say about Justice Breyer\"\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n{\\'output_text\\': \"\\\\n\\\\nThe president said that he was honoring Justice Breyer for his dedication to serving the country and that he was a retiring Justice of the United States Supreme Court. He also thanked him for his service and praised his career as a top litigator in private practice, a former federal public defender, and a family of public school educators and police officers. He noted Justice Breyer\\'s reputation as a consensus builder and the broad range of support he has received from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. He also highlighted the importance of securing the border and fixing the immigration system in order to advance liberty and justice, and mentioned the new technology, joint patrols, dedicated immigration judges, and commitments to support partners in South and Central America that have been put in place. He also expressed his commitment to the LGBTQ+ community, noting the need for the bipartisan Equality Act and the importance of protecting transgender Americans from state laws targeting them. He also highlighted his commitment to bipartisanship, noting the 80 bipartisan bills he signed into law last year, and his plans to strengthen the Violence Against Women Act. Additionally, he announced that the Justice Department will name a chief prosecutor for pandemic fraud and his plan to lower the deficit by more than one trillion dollars in a\"}\\nIntermediate Steps\\nWe can also return the intermediate steps for refine chains, should we want to inspect them. This is done with the return_intermediate_steps variable.\\nchain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"refine\", return_intermediate_steps=True)\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n{\\'intermediate_steps\\': [\\'\\\\nThe president said that he was honoring Justice Breyer for his dedication to serving the country and that he was a retiring Justice of the United States Supreme Court. He also thanked Justice Breyer for his service.\\',\\n  \\'\\\\n\\\\nThe president said that he was honoring Justice Breyer for his dedication to serving the country and that he was a retiring Justice of the United States Supreme Court. He also thanked Justice Breyer for his service, noting his background as a top litigator in private practice, a former federal public defender, and a family of public school educators and police officers. He praised Justice Breyer for being a consensus builder and for receiving a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. He also noted that in order to advance liberty and justice, it was necessary to secure the border and fix the immigration system, and that the government was taking steps to do both. \\\\n\\\\nSource: 31\\',\\n  \\'\\\\n\\\\nThe president said that he was honoring Justice Breyer for his dedication to serving the country and that he was a retiring Justice of the United States Supreme Court. He also thanked Justice Breyer for his service, noting his background as a top litigator in private practice, a former federal public defender, and a family of public school educators and police officers. He praised Justice Breyer for being a consensus builder and for receiving a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. He also noted that in order to advance liberty and justice, it was necessary to secure the border and fix the immigration system, and that the government was taking steps to do both. He also mentioned the need to pass the bipartisan Equality Act to protect LGBTQ+ Americans, and to strengthen the Violence Against Women Act that he had written three decades ago. \\\\n\\\\nSource: 31, 33\\',\\n  \\'\\\\n\\\\nThe president said that he was honoring Justice Breyer for his dedication to serving the country and that he was a retiring Justice of the United States Supreme Court. He also thanked Justice Breyer for his service, noting his background as a top litigator in private practice, a former federal public defender, and a family of public school educators and police officers. He praised Justice Breyer for being a consensus builder and for receiving a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. He also noted that in order to advance liberty and justice, it was necessary to secure the border and fix the immigration system, and that the government was taking steps to do both. He also mentioned the need to pass the bipartisan Equality Act to protect LGBTQ+ Americans, and to strengthen the Violence Against Women Act that he had written three decades ago. Additionally, he mentioned his plan to lower costs to give families a fair shot, lower the deficit, and go after criminals who stole billions in relief money meant for small businesses and millions of Americans. He also announced that the Justice Department will name a chief prosecutor for pandemic fraud. \\\\n\\\\nSource: 20, 31, 33\\'],\\n \\'output_text\\': \\'\\\\n\\\\nThe president said that he was honoring Justice Breyer for his dedication to serving the country and that he was a retiring Justice of the United States Supreme Court. He also thanked Justice Breyer for his service, noting his background as a top litigator in private practice, a former federal public defender, and a family of public school educators and police officers. He praised Justice Breyer for being a consensus builder and for receiving a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. He also noted that in order to advance liberty and justice, it was necessary to secure the border and fix the immigration system, and that the government was taking steps to do both. He also mentioned the need to pass the bipartisan Equality Act to protect LGBTQ+ Americans, and to strengthen the Violence Against Women Act that he had written three decades ago. Additionally, he mentioned his plan to lower costs to give families a fair shot, lower the deficit, and go after criminals who stole billions in relief money meant for small businesses and millions of Americans. He also announced that the Justice Department will name a chief prosecutor for pandemic fraud. \\\\n\\\\nSource: 20, 31, 33\\'}\\nCustom Prompts\\nYou can also use your own prompts with this chain. In this example, we will respond in Italian.\\nrefine_template = (\\n    \"The original question is as follows: {question}\\\\n\"\\n    \"We have provided an existing answer, including sources: {existing_answer}\\\\n\"\\n    \"We have the opportunity to refine the existing answer\"\\n    \"(only if needed) with some more context below.\\\\n\"\\n    \"------------\\\\n\"\\n    \"{context_str}\\\\n\"\\n    \"------------\\\\n\"\\n    \"Given the new context, refine the original answer to better \"\\n    \"answer the question (in Italian)\"\\n    \"If you do update it, please update the sources as well. \"\\n    \"If the context isn\\'t useful, return the original answer.\"\\n)\\nrefine_prompt = PromptTemplate(\\n    input_variables=[\"question\", \"existing_answer\", \"context_str\"],\\n    template=refine_template,\\n)\\nquestion_template = (\\n    \"Context information is below. \\\\n\"\\n    \"---------------------\\\\n\"\\n    \"{context_str}\"\\n    \"\\\\n---------------------\\\\n\"\\n    \"Given the context information and not prior knowledge, \"\\n    \"answer the question in Italian: {question}\\\\n\"\\n)\\nquestion_prompt = PromptTemplate(\\n    input_variables=[\"context_str\", \"question\"], template=question_template\\n)\\nchain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"refine\", return_intermediate_steps=True, question_prompt=question_prompt, refine_prompt=refine_prompt)\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n{\\'intermediate_steps\\': [\\'\\\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese e ha onorato la sua carriera.\\',\\n  \"\\\\n\\\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese, ha onorato la sua carriera e ha contribuito a costruire un consenso. Ha ricevuto un ampio sostegno, dall\\'Ordine Fraterno della Polizia a ex giudici nominati da democratici e repubblicani. Inoltre, ha sottolineato l\\'importanza di avanzare la libertà e la giustizia attraverso la sicurezza delle frontiere e la risoluzione del sistema di immigrazione. Ha anche menzionato le nuove tecnologie come scanner all\\'avanguardia per rilevare meglio il traffico di droga, le pattuglie congiunte con Messico e Guatemala per catturare più trafficanti di esseri umani, l\\'istituzione di giudici di immigrazione dedicati per far sì che le famiglie che fuggono da per\",\\n  \"\\\\n\\\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese, ha onorato la sua carriera e ha contribuito a costruire un consenso. Ha ricevuto un ampio sostegno, dall\\'Ordine Fraterno della Polizia a ex giudici nominati da democratici e repubblicani. Inoltre, ha sottolineato l\\'importanza di avanzare la libertà e la giustizia attraverso la sicurezza delle frontiere e la risoluzione del sistema di immigrazione. Ha anche menzionato le nuove tecnologie come scanner all\\'avanguardia per rilevare meglio il traffico di droga, le pattuglie congiunte con Messico e Guatemala per catturare più trafficanti di esseri umani, l\\'istituzione di giudici di immigrazione dedicati per far sì che le famiglie che fuggono da per\",\\n  \"\\\\n\\\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese, ha onorato la sua carriera e ha contribuito a costruire un consenso. Ha ricevuto un ampio sostegno, dall\\'Ordine Fraterno della Polizia a ex giudici nominati da democratici e repubblicani. Inoltre, ha sottolineato l\\'importanza di avanzare la libertà e la giustizia attraverso la sicurezza delle frontiere e la risoluzione del sistema di immigrazione. Ha anche menzionato le nuove tecnologie come scanner all\\'avanguardia per rilevare meglio il traffico di droga, le pattuglie congiunte con Messico e Guatemala per catturare più trafficanti di esseri umani, l\\'istituzione di giudici di immigrazione dedicati per far sì che le famiglie che fuggono da per\"],\\n \\'output_text\\': \"\\\\n\\\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese, ha onorato la sua carriera e ha contribuito a costruire un consenso. Ha ricevuto un ampio sostegno, dall\\'Ordine Fraterno della Polizia a ex giudici nominati da democratici e repubblicani. Inoltre, ha sottolineato l\\'importanza di avanzare la libertà e la giustizia attraverso la sicurezza delle frontiere e la risoluzione del sistema di immigrazione. Ha anche menzionato le nuove tecnologie come scanner all\\'avanguardia per rilevare meglio il traffico di droga, le pattuglie congiunte con Messico e Guatemala per catturare più trafficanti di esseri umani, l\\'istituzione di giudici di immigrazione dedicati per far sì che le famiglie che fuggono da per\"}\\nThe map-rerank Chain#\\nThis sections shows results of using the map-rerank Chain to do question answering with sources.\\nchain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"map_rerank\", metadata_keys=[\\'source\\'], return_intermediate_steps=True)\\nquery = \"What did the president say about Justice Breyer\"\\nresult = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\nresult[\"output_text\"]\\n\\' The President thanked Justice Breyer for his service and honored him for dedicating his life to serve the country.\\'\\nresult[\"intermediate_steps\"]\\n[{\\'answer\\': \\' The President thanked Justice Breyer for his service and honored him for dedicating his life to serve the country.\\',\\n  \\'score\\': \\'100\\'},\\n {\\'answer\\': \\' This document does not answer the question\\', \\'score\\': \\'0\\'},\\n {\\'answer\\': \\' This document does not answer the question\\', \\'score\\': \\'0\\'},\\n {\\'answer\\': \\' This document does not answer the question\\', \\'score\\': \\'0\\'}]\\nCustom Prompts\\nYou can also use your own prompts with this chain. In this example, we will respond in Italian.\\nfrom langchain.output_parsers import RegexParser\\noutput_parser = RegexParser(\\n    regex=r\"(.*?)\\\\nScore: (.*)\",\\n    output_keys=[\"answer\", \"score\"],\\n)\\nprompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\nIn addition to giving an answer, also return a score of how fully it answered the user\\'s question. This should be in the following format:\\nQuestion: [question here]\\nHelpful Answer In Italian: [answer here]\\nScore: [score between 0 and 100]\\nBegin!\\nContext:\\n---------\\n{context}\\n---------\\nQuestion: {question}\\nHelpful Answer In Italian:\"\"\"\\nPROMPT = PromptTemplate(\\n    template=prompt_template,\\n    input_variables=[\"context\", \"question\"],\\n    output_parser=output_parser,\\n)\\nchain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"map_rerank\", metadata_keys=[\\'source\\'], return_intermediate_steps=True, prompt=PROMPT)\\nquery = \"What did the president say about Justice Breyer\"\\nresult = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\nresult\\n{\\'source\\': 30,\\n \\'intermediate_steps\\': [{\\'answer\\': \\' Il presidente ha detto che Justice Breyer ha dedicato la sua vita a servire questo paese e ha onorato la sua carriera.\\',\\n   \\'score\\': \\'100\\'},\\n  {\\'answer\\': \\' Il presidente non ha detto nulla sulla Giustizia Breyer.\\',\\n   \\'score\\': \\'100\\'},\\n  {\\'answer\\': \\' Non so.\\', \\'score\\': \\'0\\'},\\n  {\\'answer\\': \\' Il presidente non ha detto nulla sulla giustizia Breyer.\\',\\n   \\'score\\': \\'100\\'}],\\n \\'output_text\\': \\' Il presidente ha detto che Justice Breyer ha dedicato la sua vita a servire questo paese e ha onorato la sua carriera.\\'}\\nprevious\\nHypothetical Document Embeddings\\nnext\\nQuestion Answering\\n Contents\\n  \\nPrepare Data\\nQuickstart\\nThe stuff Chain\\nThe map_reduce Chain\\nThe refine Chain\\nThe map-rerank Chain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\index_examples\\\\qa_with_sources.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nQuestion Answering\\n Contents \\nPrepare Data\\nQuickstart\\nThe stuff Chain\\nThe map_reduce Chain\\nThe refine Chain\\nThe map-rerank Chain\\nQuestion Answering#\\nThis notebook walks through how to use LangChain for question answering over a list of documents. It covers four different types of chains: stuff, map_reduce, refine, map_rerank. For a more in depth explanation of what these chain types are, see here.\\nPrepare Data#\\nFirst we prepare the data. For this example we do similarity search over a vector database, but these documents could be fetched in any manner (the point of this notebook to highlight what to do AFTER you fetch the documents).\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.docstore.document import Document\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.indexes.vectorstore import VectorstoreIndexCreator\\nwith open(\"../../state_of_the_union.txt\") as f:\\n    state_of_the_union = f.read()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_text(state_of_the_union)\\nembeddings = OpenAIEmbeddings()\\ndocsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))]).as_retriever()\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\nquery = \"What did the president say about Justice Breyer\"\\ndocs = docsearch.get_relevant_documents(query)\\nfrom langchain.chains.question_answering import load_qa_chain\\nfrom langchain.llms import OpenAI\\nQuickstart#\\nIf you just want to get started as quickly as possible, this is the recommended way to do it:\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\\nquery = \"What did the president say about Justice Breyer\"\\nchain.run(input_documents=docs, question=query)\\n\\' The president said that Justice Breyer has dedicated his life to serve the country and thanked him for his service.\\'\\nIf you want more control and understanding over what is happening, please see the information below.\\nThe stuff Chain#\\nThis sections shows results of using the stuff Chain to do question answering.\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\\nquery = \"What did the president say about Justice Breyer\"\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n{\\'output_text\\': \\' The president said that Justice Breyer has dedicated his life to serve the country and thanked him for his service.\\'}\\nCustom Prompts\\nYou can also use your own prompts with this chain. In this example, we will respond in Italian.\\nprompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n{context}\\nQuestion: {question}\\nAnswer in Italian:\"\"\"\\nPROMPT = PromptTemplate(\\n    template=prompt_template, input_variables=[\"context\", \"question\"]\\n)\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\", prompt=PROMPT)\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n{\\'output_text\\': \\' Il presidente ha detto che Justice Breyer ha dedicato la sua vita a servire questo paese e ha ricevuto una vasta gamma di supporto.\\'}\\nThe map_reduce Chain#\\nThis sections shows results of using the map_reduce Chain to do question answering.\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_reduce\")\\nquery = \"What did the president say about Justice Breyer\"\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n{\\'output_text\\': \\' The president said that Justice Breyer is an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court, and thanked him for his service.\\'}\\nIntermediate Steps\\nWe can also return the intermediate steps for map_reduce chains, should we want to inspect them. This is done with the return_map_steps variable.\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_reduce\", return_map_steps=True)\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n{\\'intermediate_steps\\': [\\' \"Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\"\\',\\n  \\' A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\\',\\n  \\' None\\',\\n  \\' None\\'],\\n \\'output_text\\': \\' The president said that Justice Breyer is an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court, and thanked him for his service.\\'}\\nCustom Prompts\\nYou can also use your own prompts with this chain. In this example, we will respond in Italian.\\nquestion_prompt_template = \"\"\"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text translated into italian.\\n{context}\\nQuestion: {question}\\nRelevant text, if any, in Italian:\"\"\"\\nQUESTION_PROMPT = PromptTemplate(\\n    template=question_prompt_template, input_variables=[\"context\", \"question\"]\\n)\\ncombine_prompt_template = \"\"\"Given the following extracted parts of a long document and a question, create a final answer italian. \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nAnswer in Italian:\"\"\"\\nCOMBINE_PROMPT = PromptTemplate(\\n    template=combine_prompt_template, input_variables=[\"summaries\", \"question\"]\\n)\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_reduce\", return_map_steps=True, question_prompt=QUESTION_PROMPT, combine_prompt=COMBINE_PROMPT)\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n{\\'intermediate_steps\\': [\"\\\\nStasera vorrei onorare qualcuno che ha dedicato la sua vita a servire questo paese: il giustizia Stephen Breyer - un veterano dell\\'esercito, uno studioso costituzionale e un giustizia in uscita della Corte Suprema degli Stati Uniti. Giustizia Breyer, grazie per il tuo servizio.\",\\n  \\'\\\\nNessun testo pertinente.\\',\\n  \\' Non ha detto nulla riguardo a Justice Breyer.\\',\\n  \" Non c\\'è testo pertinente.\"],\\n \\'output_text\\': \\' Non ha detto nulla riguardo a Justice Breyer.\\'}\\nBatch Size\\nWhen using the map_reduce chain, one thing to keep in mind is the batch size you are using during the map step. If this is too high, it could cause rate limiting errors. You can control this by setting the batch size on the LLM used. Note that this only applies for LLMs with this parameter. Below is an example of doing so:\\nllm = OpenAI(batch_size=5, temperature=0)\\nThe refine Chain#\\nThis sections shows results of using the refine Chain to do question answering.\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"refine\")\\nquery = \"What did the president say about Justice Breyer\"\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n{\\'output_text\\': \\'\\\\n\\\\nThe president said that he wanted to honor Justice Breyer for his dedication to serving the country, his legacy of excellence, and his commitment to advancing liberty and justice, as well as for his support of the Equality Act and his commitment to protecting the rights of LGBTQ+ Americans. He also praised Justice Breyer for his role in helping to pass the Bipartisan Infrastructure Law, which he said would be the most sweeping investment to rebuild America in history and would help the country compete for the jobs of the 21st Century.\\'}\\nIntermediate Steps\\nWe can also return the intermediate steps for refine chains, should we want to inspect them. This is done with the return_refine_steps variable.\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"refine\", return_refine_steps=True)\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n{\\'intermediate_steps\\': [\\'\\\\nThe president said that he wanted to honor Justice Breyer for his dedication to serving the country and his legacy of excellence.\\',\\n  \\'\\\\nThe president said that he wanted to honor Justice Breyer for his dedication to serving the country, his legacy of excellence, and his commitment to advancing liberty and justice.\\',\\n  \\'\\\\n\\\\nThe president said that he wanted to honor Justice Breyer for his dedication to serving the country, his legacy of excellence, and his commitment to advancing liberty and justice, as well as for his support of the Equality Act and his commitment to protecting the rights of LGBTQ+ Americans.\\',\\n  \\'\\\\n\\\\nThe president said that he wanted to honor Justice Breyer for his dedication to serving the country, his legacy of excellence, and his commitment to advancing liberty and justice, as well as for his support of the Equality Act and his commitment to protecting the rights of LGBTQ+ Americans. He also praised Justice Breyer for his role in helping to pass the Bipartisan Infrastructure Law, which is the most sweeping investment to rebuild America in history.\\'],\\n \\'output_text\\': \\'\\\\n\\\\nThe president said that he wanted to honor Justice Breyer for his dedication to serving the country, his legacy of excellence, and his commitment to advancing liberty and justice, as well as for his support of the Equality Act and his commitment to protecting the rights of LGBTQ+ Americans. He also praised Justice Breyer for his role in helping to pass the Bipartisan Infrastructure Law, which is the most sweeping investment to rebuild America in history.\\'}\\nCustom Prompts\\nYou can also use your own prompts with this chain. In this example, we will respond in Italian.\\nrefine_prompt_template = (\\n    \"The original question is as follows: {question}\\\\n\"\\n    \"We have provided an existing answer: {existing_answer}\\\\n\"\\n    \"We have the opportunity to refine the existing answer\"\\n    \"(only if needed) with some more context below.\\\\n\"\\n    \"------------\\\\n\"\\n    \"{context_str}\\\\n\"\\n    \"------------\\\\n\"\\n    \"Given the new context, refine the original answer to better \"\\n    \"answer the question. \"\\n    \"If the context isn\\'t useful, return the original answer. Reply in Italian.\"\\n)\\nrefine_prompt = PromptTemplate(\\n    input_variables=[\"question\", \"existing_answer\", \"context_str\"],\\n    template=refine_prompt_template,\\n)\\ninitial_qa_template = (\\n    \"Context information is below. \\\\n\"\\n    \"---------------------\\\\n\"\\n    \"{context_str}\"\\n    \"\\\\n---------------------\\\\n\"\\n    \"Given the context information and not prior knowledge, \"\\n    \"answer the question: {question}\\\\nYour answer should be in Italian.\\\\n\"\\n)\\ninitial_qa_prompt = PromptTemplate(\\n    input_variables=[\"context_str\", \"question\"], template=initial_qa_template\\n)\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"refine\", return_refine_steps=True,\\n                     question_prompt=initial_qa_prompt, refine_prompt=refine_prompt)\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n{\\'intermediate_steps\\': [\\'\\\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese e ha reso omaggio al suo servizio.\\',\\n  \"\\\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese, ha reso omaggio al suo servizio e ha sostenuto la nomina di una top litigatrice in pratica privata, un ex difensore pubblico federale e una famiglia di insegnanti e agenti di polizia delle scuole pubbliche. Ha anche sottolineato l\\'importanza di avanzare la libertà e la giustizia attraverso la sicurezza delle frontiere e la risoluzione del sistema di immigrazione.\",\\n  \"\\\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese, ha reso omaggio al suo servizio e ha sostenuto la nomina di una top litigatrice in pratica privata, un ex difensore pubblico federale e una famiglia di insegnanti e agenti di polizia delle scuole pubbliche. Ha anche sottolineato l\\'importanza di avanzare la libertà e la giustizia attraverso la sicurezza delle frontiere, la risoluzione del sistema di immigrazione, la protezione degli americani LGBTQ+ e l\\'approvazione dell\\'Equality Act. Ha inoltre sottolineato l\\'importanza di lavorare insieme per sconfiggere l\\'epidemia di oppiacei.\",\\n  \"\\\\n\\\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese, ha reso omaggio al suo servizio e ha sostenuto la nomina di una top litigatrice in pratica privata, un ex difensore pubblico federale e una famiglia di insegnanti e agenti di polizia delle scuole pubbliche. Ha anche sottolineato l\\'importanza di avanzare la libertà e la giustizia attraverso la sicurezza delle frontiere, la risoluzione del sistema di immigrazione, la protezione degli americani LGBTQ+ e l\\'approvazione dell\\'Equality Act. Ha inoltre sottolineato l\\'importanza di lavorare insieme per sconfiggere l\\'epidemia di oppiacei e per investire in America, educare gli americani, far crescere la forza lavoro e costruire l\\'economia dal\"],\\n \\'output_text\\': \"\\\\n\\\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese, ha reso omaggio al suo servizio e ha sostenuto la nomina di una top litigatrice in pratica privata, un ex difensore pubblico federale e una famiglia di insegnanti e agenti di polizia delle scuole pubbliche. Ha anche sottolineato l\\'importanza di avanzare la libertà e la giustizia attraverso la sicurezza delle frontiere, la risoluzione del sistema di immigrazione, la protezione degli americani LGBTQ+ e l\\'approvazione dell\\'Equality Act. Ha inoltre sottolineato l\\'importanza di lavorare insieme per sconfiggere l\\'epidemia di oppiacei e per investire in America, educare gli americani, far crescere la forza lavoro e costruire l\\'economia dal\"}\\nThe map-rerank Chain#\\nThis sections shows results of using the map-rerank Chain to do question answering with sources.\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_rerank\", return_intermediate_steps=True)\\nquery = \"What did the president say about Justice Breyer\"\\nresults = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\nresults[\"output_text\"]\\n\\' The President thanked Justice Breyer for his service and honored him for dedicating his life to serve the country.\\'\\nresults[\"intermediate_steps\"]\\n[{\\'answer\\': \\' The President thanked Justice Breyer for his service and honored him for dedicating his life to serve the country.\\',\\n  \\'score\\': \\'100\\'},\\n {\\'answer\\': \\' This document does not answer the question\\', \\'score\\': \\'0\\'},\\n {\\'answer\\': \\' This document does not answer the question\\', \\'score\\': \\'0\\'},\\n {\\'answer\\': \\' This document does not answer the question\\', \\'score\\': \\'0\\'}]\\nCustom Prompts\\nYou can also use your own prompts with this chain. In this example, we will respond in Italian.\\nfrom langchain.output_parsers import RegexParser\\noutput_parser = RegexParser(\\n    regex=r\"(.*?)\\\\nScore: (.*)\",\\n    output_keys=[\"answer\", \"score\"],\\n)\\nprompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\nIn addition to giving an answer, also return a score of how fully it answered the user\\'s question. This should be in the following format:\\nQuestion: [question here]\\nHelpful Answer In Italian: [answer here]\\nScore: [score between 0 and 100]\\nBegin!\\nContext:\\n---------\\n{context}\\n---------\\nQuestion: {question}\\nHelpful Answer In Italian:\"\"\"\\nPROMPT = PromptTemplate(\\n    template=prompt_template,\\n    input_variables=[\"context\", \"question\"],\\n    output_parser=output_parser,\\n)\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_rerank\", return_intermediate_steps=True, prompt=PROMPT)\\nquery = \"What did the president say about Justice Breyer\"\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n{\\'intermediate_steps\\': [{\\'answer\\': \\' Il presidente ha detto che Justice Breyer ha dedicato la sua vita a servire questo paese.\\',\\n   \\'score\\': \\'100\\'},\\n  {\\'answer\\': \\' Il presidente non ha detto nulla sulla Giustizia Breyer.\\',\\n   \\'score\\': \\'100\\'},\\n  {\\'answer\\': \\' Non so.\\', \\'score\\': \\'0\\'},\\n  {\\'answer\\': \\' Non so.\\', \\'score\\': \\'0\\'}],\\n \\'output_text\\': \\' Il presidente ha detto che Justice Breyer ha dedicato la sua vita a servire questo paese.\\'}\\nprevious\\nQuestion Answering with Sources\\nnext\\nSummarization\\n Contents\\n  \\nPrepare Data\\nQuickstart\\nThe stuff Chain\\nThe map_reduce Chain\\nThe refine Chain\\nThe map-rerank Chain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\index_examples\\\\question_answering.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nSummarization\\n Contents \\nPrepare Data\\nQuickstart\\nThe stuff Chain\\nThe map_reduce Chain\\nThe refine Chain\\nSummarization#\\nThis notebook walks through how to use LangChain for summarization over a list of documents. It covers three different chain types: stuff, map_reduce, and refine. For a more in depth explanation of what these chain types are, see here.\\nPrepare Data#\\nFirst we prepare the data. For this example we create multiple documents from one long one, but these documents could be fetched in any manner (the point of this notebook to highlight what to do AFTER you fetch the documents).\\nfrom langchain import OpenAI, PromptTemplate, LLMChain\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.chains.mapreduce import MapReduceChain\\nfrom langchain.prompts import PromptTemplate\\nllm = OpenAI(temperature=0)\\ntext_splitter = CharacterTextSplitter()\\nwith open(\"../../state_of_the_union.txt\") as f:\\n    state_of_the_union = f.read()\\ntexts = text_splitter.split_text(state_of_the_union)\\nfrom langchain.docstore.document import Document\\ndocs = [Document(page_content=t) for t in texts[:3]]\\nfrom langchain.chains.summarize import load_summarize_chain\\nQuickstart#\\nIf you just want to get started as quickly as possible, this is the recommended way to do it:\\nchain = load_summarize_chain(llm, chain_type=\"map_reduce\")\\nchain.run(docs)\\n\\' In response to Russian aggression in Ukraine, the United States and its allies are taking action to hold Putin accountable, including economic sanctions, asset seizures, and military assistance. The US is also providing economic and humanitarian aid to Ukraine, and has passed the American Rescue Plan and the Bipartisan Infrastructure Law to help struggling families and create jobs. The US remains unified and determined to protect Ukraine and the free world.\\'\\nIf you want more control and understanding over what is happening, please see the information below.\\nThe stuff Chain#\\nThis sections shows results of using the stuff Chain to do summarization.\\nchain = load_summarize_chain(llm, chain_type=\"stuff\")\\nchain.run(docs)\\n\\' In his speech, President Biden addressed the crisis in Ukraine, the American Rescue Plan, and the Bipartisan Infrastructure Law. He discussed the need to invest in America, educate Americans, and build the economy from the bottom up. He also announced the release of 60 million barrels of oil from reserves around the world, and the creation of a dedicated task force to go after the crimes of Russian oligarchs. He concluded by emphasizing the need to Buy American and use taxpayer dollars to rebuild America.\\'\\nCustom Prompts\\nYou can also use your own prompts with this chain. In this example, we will respond in Italian.\\nprompt_template = \"\"\"Write a concise summary of the following:\\n{text}\\nCONCISE SUMMARY IN ITALIAN:\"\"\"\\nPROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\\nchain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\\nchain.run(docs)\\n\"\\\\n\\\\nIn questa serata, il Presidente degli Stati Uniti ha annunciato una serie di misure per affrontare la crisi in Ucraina, causata dall\\'aggressione di Putin. Ha anche annunciato l\\'invio di aiuti economici, militari e umanitari all\\'Ucraina. Ha anche annunciato che gli Stati Uniti e i loro alleati stanno imponendo sanzioni economiche a Putin e stanno rilasciando 60 milioni di barili di petrolio dalle riserve di tutto il mondo. Inoltre, ha annunciato che il Dipartimento di Giustizia degli Stati Uniti sta creando una task force dedicata ai crimini degli oligarchi russi. Il Presidente ha anche annunciato l\\'approvazione della legge bipartitica sull\\'infrastruttura, che prevede investimenti per la ricostruzione dell\\'America. Questo porterà a creare posti\"\\nThe map_reduce Chain#\\nThis sections shows results of using the map_reduce Chain to do summarization.\\nchain = load_summarize_chain(llm, chain_type=\"map_reduce\")\\nchain.run(docs)\\n\" In response to Russia\\'s aggression in Ukraine, the United States and its allies have imposed economic sanctions and are taking other measures to hold Putin accountable. The US is also providing economic and military assistance to Ukraine, protecting NATO countries, and releasing oil from its Strategic Petroleum Reserve. President Biden and Vice President Harris have passed legislation to help struggling families and rebuild America\\'s infrastructure.\"\\nIntermediate Steps\\nWe can also return the intermediate steps for map_reduce chains, should we want to inspect them. This is done with the return_map_steps variable.\\nchain = load_summarize_chain(OpenAI(temperature=0), chain_type=\"map_reduce\", return_intermediate_steps=True)\\nchain({\"input_documents\": docs}, return_only_outputs=True)\\n{\\'map_steps\\': [\" In response to Russia\\'s aggression in Ukraine, the United States has united with other freedom-loving nations to impose economic sanctions and hold Putin accountable. The U.S. Department of Justice is also assembling a task force to go after the crimes of Russian oligarchs and seize their ill-gotten gains.\",\\n  \\' The United States and its European allies are taking action to punish Russia for its invasion of Ukraine, including seizing assets, closing off airspace, and providing economic and military assistance to Ukraine. The US is also mobilizing forces to protect NATO countries and has released 30 million barrels of oil from its Strategic Petroleum Reserve to help blunt gas prices. The world is uniting in support of Ukraine and democracy, and the US stands with its Ukrainian-American citizens.\\',\\n  \" President Biden and Vice President Harris ran for office with a new economic vision for America, and have since passed the American Rescue Plan and the Bipartisan Infrastructure Law to help struggling families and rebuild America\\'s infrastructure. This includes creating jobs, modernizing roads, airports, ports, and waterways, replacing lead pipes, providing affordable high-speed internet, and investing in American products to support American jobs.\"],\\n \\'output_text\\': \" In response to Russia\\'s aggression in Ukraine, the United States and its allies have imposed economic sanctions and are taking other measures to hold Putin accountable. The US is also providing economic and military assistance to Ukraine, protecting NATO countries, and passing legislation to help struggling families and rebuild America\\'s infrastructure. The world is uniting in support of Ukraine and democracy, and the US stands with its Ukrainian-American citizens.\"}\\nCustom Prompts\\nYou can also use your own prompts with this chain. In this example, we will respond in Italian.\\nprompt_template = \"\"\"Write a concise summary of the following:\\n{text}\\nCONCISE SUMMARY IN ITALIAN:\"\"\"\\nPROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\\nchain = load_summarize_chain(OpenAI(temperature=0), chain_type=\"map_reduce\", return_intermediate_steps=True, map_prompt=PROMPT, combine_prompt=PROMPT)\\nchain({\"input_documents\": docs}, return_only_outputs=True)\\n{\\'intermediate_steps\\': [\"\\\\n\\\\nQuesta sera, ci incontriamo come democratici, repubblicani e indipendenti, ma soprattutto come americani. La Russia di Putin ha cercato di scuotere le fondamenta del mondo libero, ma ha sottovalutato la forza della gente ucraina. Gli Stati Uniti e i loro alleati stanno ora imponendo sanzioni economiche a Putin e stanno tagliando l\\'accesso della Russia alla tecnologia. Il Dipartimento di Giustizia degli Stati Uniti sta anche creando una task force dedicata per andare dopo i crimini degli oligarchi russi.\",\\n  \"\\\\n\\\\nStiamo unendo le nostre forze con quelle dei nostri alleati europei per sequestrare yacht, appartamenti di lusso e jet privati di Putin. Abbiamo chiuso lo spazio aereo americano ai voli russi e stiamo fornendo più di un miliardo di dollari in assistenza all\\'Ucraina. Abbiamo anche mobilitato le nostre forze terrestri, aeree e navali per proteggere i paesi della NATO. Abbiamo anche rilasciato 60 milioni di barili di petrolio dalle riserve di tutto il mondo, di cui 30 milioni dalla nostra riserva strategica di petrolio. Stiamo affrontando una prova reale e ci vorrà del tempo, ma alla fine Putin non riuscirà a spegnere l\\'amore dei popoli per la libertà.\",\\n  \"\\\\n\\\\nIl Presidente Biden ha lottato per passare l\\'American Rescue Plan per aiutare le persone che soffrivano a causa della pandemia. Il piano ha fornito sollievo economico immediato a milioni di americani, ha aiutato a mettere cibo sulla loro tavola, a mantenere un tetto sopra le loro teste e a ridurre il costo dell\\'assicurazione sanitaria. Il piano ha anche creato più di 6,5 milioni di nuovi posti di lavoro, il più alto numero di posti di lavoro creati in un anno nella storia degli Stati Uniti. Il Presidente Biden ha anche firmato la legge bipartitica sull\\'infrastruttura, la più ampia iniziativa di ricostruzione della storia degli Stati Uniti. Il piano prevede di modernizzare le strade, gli aeroporti, i porti e le vie navigabili in\"],\\n \\'output_text\\': \"\\\\n\\\\nIl Presidente Biden sta lavorando per aiutare le persone che soffrono a causa della pandemia attraverso l\\'American Rescue Plan e la legge bipartitica sull\\'infrastruttura. Gli Stati Uniti e i loro alleati stanno anche imponendo sanzioni economiche a Putin e tagliando l\\'accesso della Russia alla tecnologia. Stanno anche sequestrando yacht, appartamenti di lusso e jet privati di Putin e fornendo più di un miliardo di dollari in assistenza all\\'Ucraina. Alla fine, Putin non riuscirà a spegnere l\\'amore dei popoli per la libertà.\"}\\nThe refine Chain#\\nThis sections shows results of using the refine Chain to do summarization.\\nchain = load_summarize_chain(llm, chain_type=\"refine\")\\nchain.run(docs)\\n\"\\\\n\\\\nIn response to Russia\\'s aggression in Ukraine, the United States has united with other freedom-loving nations to impose economic sanctions and hold Putin accountable. The U.S. Department of Justice is also assembling a task force to go after the crimes of Russian oligarchs and seize their ill-gotten gains. We are joining with our European allies to find and seize the assets of Russian oligarchs, including yachts, luxury apartments, and private jets. The U.S. is also closing off American airspace to all Russian flights, further isolating Russia and adding an additional squeeze on their economy. The U.S. and its allies are providing support to the Ukrainians in their fight for freedom, including military, economic, and humanitarian assistance. The U.S. is also mobilizing ground forces, air squadrons, and ship deployments to protect NATO countries. The U.S. and its allies are also releasing 60 million barrels of oil from reserves around the world, with the U.S. contributing 30 million barrels from its own Strategic Petroleum Reserve. In addition, the U.S. has passed the American Rescue Plan to provide immediate economic relief for tens of millions of Americans, and the Bipartisan Infrastructure Law to rebuild America and create jobs. This investment will\"\\nIntermediate Steps\\nWe can also return the intermediate steps for refine chains, should we want to inspect them. This is done with the return_refine_steps variable.\\nchain = load_summarize_chain(OpenAI(temperature=0), chain_type=\"refine\", return_intermediate_steps=True)\\nchain({\"input_documents\": docs}, return_only_outputs=True)\\n{\\'refine_steps\\': [\" In response to Russia\\'s aggression in Ukraine, the United States has united with other freedom-loving nations to impose economic sanctions and hold Putin accountable. The U.S. Department of Justice is also assembling a task force to go after the crimes of Russian oligarchs and seize their ill-gotten gains.\",\\n  \"\\\\n\\\\nIn response to Russia\\'s aggression in Ukraine, the United States has united with other freedom-loving nations to impose economic sanctions and hold Putin accountable. The U.S. Department of Justice is also assembling a task force to go after the crimes of Russian oligarchs and seize their ill-gotten gains. We are joining with our European allies to find and seize the assets of Russian oligarchs, including yachts, luxury apartments, and private jets. The U.S. is also closing off American airspace to all Russian flights, further isolating Russia and adding an additional squeeze on their economy. The U.S. and its allies are providing support to the Ukrainians in their fight for freedom, including military, economic, and humanitarian assistance. The U.S. is also mobilizing ground forces, air squadrons, and ship deployments to protect NATO countries. The U.S. and its allies are also releasing 60 million barrels of oil from reserves around the world, with the U.S. contributing 30 million barrels from its own Strategic Petroleum Reserve. Putin\\'s war on Ukraine has left Russia weaker and the rest of the world stronger, with the world uniting in support of democracy and peace.\",\\n  \"\\\\n\\\\nIn response to Russia\\'s aggression in Ukraine, the United States has united with other freedom-loving nations to impose economic sanctions and hold Putin accountable. The U.S. Department of Justice is also assembling a task force to go after the crimes of Russian oligarchs and seize their ill-gotten gains. We are joining with our European allies to find and seize the assets of Russian oligarchs, including yachts, luxury apartments, and private jets. The U.S. is also closing off American airspace to all Russian flights, further isolating Russia and adding an additional squeeze on their economy. The U.S. and its allies are providing support to the Ukrainians in their fight for freedom, including military, economic, and humanitarian assistance. The U.S. is also mobilizing ground forces, air squadrons, and ship deployments to protect NATO countries. The U.S. and its allies are also releasing 60 million barrels of oil from reserves around the world, with the U.S. contributing 30 million barrels from its own Strategic Petroleum Reserve. In addition, the U.S. has passed the American Rescue Plan to provide immediate economic relief for tens of millions of Americans, and the Bipartisan Infrastructure Law to rebuild America and create jobs. This includes investing\"],\\n \\'output_text\\': \"\\\\n\\\\nIn response to Russia\\'s aggression in Ukraine, the United States has united with other freedom-loving nations to impose economic sanctions and hold Putin accountable. The U.S. Department of Justice is also assembling a task force to go after the crimes of Russian oligarchs and seize their ill-gotten gains. We are joining with our European allies to find and seize the assets of Russian oligarchs, including yachts, luxury apartments, and private jets. The U.S. is also closing off American airspace to all Russian flights, further isolating Russia and adding an additional squeeze on their economy. The U.S. and its allies are providing support to the Ukrainians in their fight for freedom, including military, economic, and humanitarian assistance. The U.S. is also mobilizing ground forces, air squadrons, and ship deployments to protect NATO countries. The U.S. and its allies are also releasing 60 million barrels of oil from reserves around the world, with the U.S. contributing 30 million barrels from its own Strategic Petroleum Reserve. In addition, the U.S. has passed the American Rescue Plan to provide immediate economic relief for tens of millions of Americans, and the Bipartisan Infrastructure Law to rebuild America and create jobs. This includes investing\"}\\nCustom Prompts\\nYou can also use your own prompts with this chain. In this example, we will respond in Italian.\\nprompt_template = \"\"\"Write a concise summary of the following:\\n{text}\\nCONCISE SUMMARY IN ITALIAN:\"\"\"\\nPROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\\nrefine_template = (\\n    \"Your job is to produce a final summary\\\\n\"\\n    \"We have provided an existing summary up to a certain point: {existing_answer}\\\\n\"\\n    \"We have the opportunity to refine the existing summary\"\\n    \"(only if needed) with some more context below.\\\\n\"\\n    \"------------\\\\n\"\\n    \"{text}\\\\n\"\\n    \"------------\\\\n\"\\n    \"Given the new context, refine the original summary in Italian\"\\n    \"If the context isn\\'t useful, return the original summary.\"\\n)\\nrefine_prompt = PromptTemplate(\\n    input_variables=[\"existing_answer\", \"text\"],\\n    template=refine_template,\\n)\\nchain = load_summarize_chain(OpenAI(temperature=0), chain_type=\"refine\", return_intermediate_steps=True, question_prompt=PROMPT, refine_prompt=refine_prompt)\\nchain({\"input_documents\": docs}, return_only_outputs=True)\\n{\\'intermediate_steps\\': [\"\\\\n\\\\nQuesta sera, ci incontriamo come democratici, repubblicani e indipendenti, ma soprattutto come americani. La Russia di Putin ha cercato di scuotere le fondamenta del mondo libero, ma ha sottovalutato la forza della gente ucraina. Insieme ai nostri alleati, stiamo imponendo sanzioni economiche, tagliando l\\'accesso della Russia alla tecnologia e bloccando i suoi più grandi istituti bancari dal sistema finanziario internazionale. Il Dipartimento di Giustizia degli Stati Uniti sta anche assemblando una task force dedicata per andare dopo i crimini degli oligarchi russi.\",\\n  \"\\\\n\\\\nQuesta sera, ci incontriamo come democratici, repubblicani e indipendenti, ma soprattutto come americani. La Russia di Putin ha cercato di scuotere le fondamenta del mondo libero, ma ha sottovalutato la forza della gente ucraina. Insieme ai nostri alleati, stiamo imponendo sanzioni economiche, tagliando l\\'accesso della Russia alla tecnologia, bloccando i suoi più grandi istituti bancari dal sistema finanziario internazionale e chiudendo lo spazio aereo americano a tutti i voli russi. Il Dipartimento di Giustizia degli Stati Uniti sta anche assemblando una task force dedicata per andare dopo i crimini degli oligarchi russi. Stiamo fornendo più di un miliardo di dollari in assistenza diretta all\\'Ucraina e fornendo assistenza militare,\",\\n  \"\\\\n\\\\nQuesta sera, ci incontriamo come democratici, repubblicani e indipendenti, ma soprattutto come americani. La Russia di Putin ha cercato di scuotere le fondamenta del mondo libero, ma ha sottovalutato la forza della gente ucraina. Insieme ai nostri alleati, stiamo imponendo sanzioni economiche, tagliando l\\'accesso della Russia alla tecnologia, bloccando i suoi più grandi istituti bancari dal sistema finanziario internazionale e chiudendo lo spazio aereo americano a tutti i voli russi. Il Dipartimento di Giustizia degli Stati Uniti sta anche assemblando una task force dedicata per andare dopo i crimini degli oligarchi russi. Stiamo fornendo più di un miliardo di dollari in assistenza diretta all\\'Ucraina e fornendo assistenza militare.\"],\\n \\'output_text\\': \"\\\\n\\\\nQuesta sera, ci incontriamo come democratici, repubblicani e indipendenti, ma soprattutto come americani. La Russia di Putin ha cercato di scuotere le fondamenta del mondo libero, ma ha sottovalutato la forza della gente ucraina. Insieme ai nostri alleati, stiamo imponendo sanzioni economiche, tagliando l\\'accesso della Russia alla tecnologia, bloccando i suoi più grandi istituti bancari dal sistema finanziario internazionale e chiudendo lo spazio aereo americano a tutti i voli russi. Il Dipartimento di Giustizia degli Stati Uniti sta anche assemblando una task force dedicata per andare dopo i crimini degli oligarchi russi. Stiamo fornendo più di un miliardo di dollari in assistenza diretta all\\'Ucraina e fornendo assistenza militare.\"}\\nprevious\\nQuestion Answering\\nnext\\nRetrieval Question/Answering\\n Contents\\n  \\nPrepare Data\\nQuickstart\\nThe stuff Chain\\nThe map_reduce Chain\\nThe refine Chain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\index_examples\\\\summarize.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nRetrieval Question/Answering\\n Contents \\nChain Type\\nCustom Prompts\\nReturn Source Documents\\nRetrieval Question/Answering#\\nThis example showcases question answering over an index.\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.document_loaders import TextLoader\\nloader = TextLoader(\"../../state_of_the_union.txt\")\\ndocuments = loader.load()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_documents(documents)\\nembeddings = OpenAIEmbeddings()\\ndocsearch = Chroma.from_documents(texts, embeddings)\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever())\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nqa.run(query)\\n\" The president said that she is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support, from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\"\\nChain Type#\\nYou can easily specify different chain types to load and use in the RetrievalQA chain. For a more detailed walkthrough of these types, please see this notebook.\\nThere are two ways to load different chain types. First, you can specify the chain type argument in the from_chain_type method. This allows you to pass in the name of the chain type you want to use. For example, in the below we change the chain type to map_reduce.\\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"map_reduce\", retriever=docsearch.as_retriever())\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nqa.run(query)\\n\" The president said that Judge Ketanji Brown Jackson is one of our nation\\'s top legal minds, a former top litigator in private practice and a former federal public defender, from a family of public school educators and police officers, a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\"\\nThe above way allows you to really simply change the chain_type, but it does provide a ton of flexibility over parameters to that chain type. If you want to control those parameters, you can load the chain directly (as you did in this notebook) and then pass that directly to the the RetrievalQA chain with the combine_documents_chain parameter. For example:\\nfrom langchain.chains.question_answering import load_qa_chain\\nqa_chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\\nqa = RetrievalQA(combine_documents_chain=qa_chain, retriever=docsearch.as_retriever())\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nqa.run(query)\\n\" The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\"\\nCustom Prompts#\\nYou can pass in custom prompts to do question answering. These prompts are the same prompts as you can pass into the base question answering chain\\nfrom langchain.prompts import PromptTemplate\\nprompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n{context}\\nQuestion: {question}\\nAnswer in Italian:\"\"\"\\nPROMPT = PromptTemplate(\\n    template=prompt_template, input_variables=[\"context\", \"question\"]\\n)\\nchain_type_kwargs = {\"prompt\": PROMPT}\\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever(), chain_type_kwargs=chain_type_kwargs)\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nqa.run(query)\\n\" Il presidente ha detto che Ketanji Brown Jackson è una delle menti legali più importanti del paese, che continuerà l\\'eccellenza di Justice Breyer e che ha ricevuto un ampio sostegno, da Fraternal Order of Police a ex giudici nominati da democratici e repubblicani.\"\\nReturn Source Documents#\\nAdditionally, we can return the source documents used to answer the question by specifying an optional parameter when constructing the chain.\\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever(), return_source_documents=True)\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = qa({\"query\": query})\\nresult[\"result\"]\\n\" The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice and a former federal public defender from a family of public school educators and police officers, and that she has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\"\\nresult[\"source_documents\"]\\n[Document(page_content=\\'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\\\n\\\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\\n\\\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\\', lookup_str=\\'\\', metadata={\\'source\\': \\'../../state_of_the_union.txt\\'}, lookup_index=0),\\n Document(page_content=\\'A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\\\n\\\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \\\\n\\\\nWe can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \\\\n\\\\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \\\\n\\\\nWe’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \\\\n\\\\nWe’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\\', lookup_str=\\'\\', metadata={\\'source\\': \\'../../state_of_the_union.txt\\'}, lookup_index=0),\\n Document(page_content=\\'And for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \\\\n\\\\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \\\\n\\\\nWhile it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \\\\n\\\\nAnd soon, we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \\\\n\\\\nSo tonight I’m offering a Unity Agenda for the Nation. Four big things we can do together.  \\\\n\\\\nFirst, beat the opioid epidemic.\\', lookup_str=\\'\\', metadata={\\'source\\': \\'../../state_of_the_union.txt\\'}, lookup_index=0),\\n Document(page_content=\\'Tonight, I’m announcing a crackdown on these companies overcharging American businesses and consumers. \\\\n\\\\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  \\\\n\\\\nThat ends on my watch. \\\\n\\\\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. \\\\n\\\\nWe’ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. \\\\n\\\\nLet’s pass the Paycheck Fairness Act and paid leave.  \\\\n\\\\nRaise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. \\\\n\\\\nLet’s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill—our First Lady who teaches full-time—calls America’s best-kept secret: community colleges.\\', lookup_str=\\'\\', metadata={\\'source\\': \\'../../state_of_the_union.txt\\'}, lookup_index=0)]\\nprevious\\nSummarization\\nnext\\nRetrieval Question Answering with Sources\\n Contents\\n  \\nChain Type\\nCustom Prompts\\nReturn Source Documents\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\index_examples\\\\vector_db_qa.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nRetrieval Question Answering with Sources\\n Contents \\nChain Type\\nRetrieval Question Answering with Sources#\\nThis notebook goes over how to do question-answering with sources over an Index. It does this by using the RetrievalQAWithSourcesChain, which does the lookup of the documents from an Index.\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.embeddings.cohere import CohereEmbeddings\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\\nfrom langchain.vectorstores import Chroma\\nwith open(\"../../state_of_the_union.txt\") as f:\\n    state_of_the_union = f.read()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_text(state_of_the_union)\\nembeddings = OpenAIEmbeddings()\\ndocsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": f\"{i}-pl\"} for i in range(len(texts))])\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\nfrom langchain.chains import RetrievalQAWithSourcesChain\\nfrom langchain import OpenAI\\nchain = RetrievalQAWithSourcesChain.from_chain_type(OpenAI(temperature=0), chain_type=\"stuff\", retriever=docsearch.as_retriever())\\nchain({\"question\": \"What did the president say about Justice Breyer\"}, return_only_outputs=True)\\n{\\'answer\\': \\' The president honored Justice Breyer for his service and mentioned his legacy of excellence.\\\\n\\',\\n \\'sources\\': \\'31-pl\\'}\\nChain Type#\\nYou can easily specify different chain types to load and use in the RetrievalQAWithSourcesChain chain. For a more detailed walkthrough of these types, please see this notebook.\\nThere are two ways to load different chain types. First, you can specify the chain type argument in the from_chain_type method. This allows you to pass in the name of the chain type you want to use. For example, in the below we change the chain type to map_reduce.\\nchain = RetrievalQAWithSourcesChain.from_chain_type(OpenAI(temperature=0), chain_type=\"map_reduce\", retriever=docsearch.as_retriever())\\nchain({\"question\": \"What did the president say about Justice Breyer\"}, return_only_outputs=True)\\n{\\'answer\\': \\' The president said \"Justice Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\"\\\\n\\',\\n \\'sources\\': \\'31-pl\\'}\\nThe above way allows you to really simply change the chain_type, but it does provide a ton of flexibility over parameters to that chain type. If you want to control those parameters, you can load the chain directly (as you did in this notebook) and then pass that directly to the the RetrievalQAWithSourcesChain chain with the combine_documents_chain parameter. For example:\\nfrom langchain.chains.qa_with_sources import load_qa_with_sources_chain\\nqa_chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\")\\nqa = RetrievalQAWithSourcesChain(combine_documents_chain=qa_chain, retriever=docsearch.as_retriever())\\nqa({\"question\": \"What did the president say about Justice Breyer\"}, return_only_outputs=True)\\n{\\'answer\\': \\' The president honored Justice Breyer for his service and mentioned his legacy of excellence.\\\\n\\',\\n \\'sources\\': \\'31-pl\\'}\\nprevious\\nRetrieval Question/Answering\\nnext\\nVector DB Text Generation\\n Contents\\n  \\nChain Type\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\index_examples\\\\vector_db_qa_with_sources.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nVector DB Text Generation\\n Contents \\nPrepare Data\\nSet Up Vector DB\\nSet Up LLM Chain with Custom Prompt\\nGenerate Text\\nVector DB Text Generation#\\nThis notebook walks through how to use LangChain for text generation over a vector index. This is useful if we want to generate text that is able to draw from a large body of custom text, for example, generating blog posts that have an understanding of previous blog posts written, or product tutorials that can refer to product documentation.\\nPrepare Data#\\nFirst, we prepare the data. For this example, we fetch a documentation site that consists of markdown files hosted on Github and split them into small enough Documents.\\nfrom langchain.llms import OpenAI\\nfrom langchain.docstore.document import Document\\nimport requests\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.prompts import PromptTemplate\\nimport pathlib\\nimport subprocess\\nimport tempfile\\ndef get_github_docs(repo_owner, repo_name):\\n    with tempfile.TemporaryDirectory() as d:\\n        subprocess.check_call(\\n            f\"git clone --depth 1 https://github.com/{repo_owner}/{repo_name}.git .\",\\n            cwd=d,\\n            shell=True,\\n        )\\n        git_sha = (\\n            subprocess.check_output(\"git rev-parse HEAD\", shell=True, cwd=d)\\n            .decode(\"utf-8\")\\n            .strip()\\n        )\\n        repo_path = pathlib.Path(d)\\n        markdown_files = list(repo_path.glob(\"*/*.md\")) + list(\\n            repo_path.glob(\"*/*.mdx\")\\n        )\\n        for markdown_file in markdown_files:\\n            with open(markdown_file, \"r\") as f:\\n                relative_path = markdown_file.relative_to(repo_path)\\n                github_url = f\"https://github.com/{repo_owner}/{repo_name}/blob/{git_sha}/{relative_path}\"\\n                yield Document(page_content=f.read(), metadata={\"source\": github_url})\\nsources = get_github_docs(\"yirenlu92\", \"deno-manual-forked\")\\nsource_chunks = []\\nsplitter = CharacterTextSplitter(separator=\" \", chunk_size=1024, chunk_overlap=0)\\nfor source in sources:\\n    for chunk in splitter.split_text(source.page_content):\\n        source_chunks.append(Document(page_content=chunk, metadata=source.metadata))\\nCloning into \\'.\\'...\\nSet Up Vector DB#\\nNow that we have the documentation content in chunks, let’s put all this information in a vector index for easy retrieval.\\nsearch_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings())\\nSet Up LLM Chain with Custom Prompt#\\nNext, let’s set up a simple LLM chain but give it a custom prompt for blog post generation. Note that the custom prompt is parameterized and takes two inputs: context, which will be the documents fetched from the vector search, and topic, which is given by the user.\\nfrom langchain.chains import LLMChain\\nprompt_template = \"\"\"Use the context below to write a 400 word blog post about the topic below:\\n    Context: {context}\\n    Topic: {topic}\\n    Blog post:\"\"\"\\nPROMPT = PromptTemplate(\\n    template=prompt_template, input_variables=[\"context\", \"topic\"]\\n)\\nllm = OpenAI(temperature=0)\\nchain = LLMChain(llm=llm, prompt=PROMPT)\\nGenerate Text#\\nFinally, we write a function to apply our inputs to the chain. The function takes an input parameter topic. We find the documents in the vector index that correspond to that topic, and use them as additional context in our simple LLM chain.\\ndef generate_blog_post(topic):\\n    docs = search_index.similarity_search(topic, k=4)\\n    inputs = [{\"context\": doc.page_content, \"topic\": topic} for doc in docs]\\n    print(chain.apply(inputs))\\ngenerate_blog_post(\"environment variables\")\\n[{\\'text\\': \\'\\\\n\\\\nEnvironment variables are a great way to store and access sensitive information in your Deno applications. Deno offers built-in support for environment variables with `Deno.env`, and you can also use a `.env` file to store and access environment variables.\\\\n\\\\nUsing `Deno.env` is simple. It has getter and setter methods, so you can easily set and retrieve environment variables. For example, you can set the `FIREBASE_API_KEY` and `FIREBASE_AUTH_DOMAIN` environment variables like this:\\\\n\\\\n```ts\\\\nDeno.env.set(\"FIREBASE_API_KEY\", \"examplekey123\");\\\\nDeno.env.set(\"FIREBASE_AUTH_DOMAIN\", \"firebasedomain.com\");\\\\n\\\\nconsole.log(Deno.env.get(\"FIREBASE_API_KEY\")); // examplekey123\\\\nconsole.log(Deno.env.get(\"FIREBASE_AUTH_DOMAIN\")); // firebasedomain.com\\\\n```\\\\n\\\\nYou can also store environment variables in a `.env` file. This is a great\\'}, {\\'text\\': \\'\\\\n\\\\nEnvironment variables are a powerful tool for managing configuration settings in a program. They allow us to set values that can be used by the program, without having to hard-code them into the code. This makes it easier to change settings without having to modify the code.\\\\n\\\\nIn Deno, environment variables can be set in a few different ways. The most common way is to use the `VAR=value` syntax. This will set the environment variable `VAR` to the value `value`. This can be used to set any number of environment variables before running a command. For example, if we wanted to set the environment variable `VAR` to `hello` before running a Deno command, we could do so like this:\\\\n\\\\n```\\\\nVAR=hello deno run main.ts\\\\n```\\\\n\\\\nThis will set the environment variable `VAR` to `hello` before running the command. We can then access this variable in our code using the `Deno.env.get()` function. For example, if we ran the following command:\\\\n\\\\n```\\\\nVAR=hello && deno eval \"console.log(\\\\\\'Deno: \\\\\\' + Deno.env.get(\\\\\\'VAR\\'}, {\\'text\\': \\'\\\\n\\\\nEnvironment variables are a powerful tool for developers, allowing them to store and access data without having to hard-code it into their applications. In Deno, you can access environment variables using the `Deno.env.get()` function.\\\\n\\\\nFor example, if you wanted to access the `HOME` environment variable, you could do so like this:\\\\n\\\\n```js\\\\n// env.js\\\\nDeno.env.get(\"HOME\");\\\\n```\\\\n\\\\nWhen running this code, you\\\\\\'ll need to grant the Deno process access to environment variables. This can be done by passing the `--allow-env` flag to the `deno run` command. You can also specify which environment variables you want to grant access to, like this:\\\\n\\\\n```shell\\\\n# Allow access to only the HOME env var\\\\ndeno run --allow-env=HOME env.js\\\\n```\\\\n\\\\nIt\\\\\\'s important to note that environment variables are case insensitive on Windows, so Deno also matches them case insensitively (on Windows only).\\\\n\\\\nAnother thing to be aware of when using environment variables is subprocess permissions. Subprocesses are powerful and can access system resources regardless of the permissions you granted to the Den\\'}, {\\'text\\': \\'\\\\n\\\\nEnvironment variables are an important part of any programming language, and Deno is no exception. Deno is a secure JavaScript and TypeScript runtime built on the V8 JavaScript engine, and it recently added support for environment variables. This feature was added in Deno version 1.6.0, and it is now available for use in Deno applications.\\\\n\\\\nEnvironment variables are used to store information that can be used by programs. They are typically used to store configuration information, such as the location of a database or the name of a user. In Deno, environment variables are stored in the `Deno.env` object. This object is similar to the `process.env` object in Node.js, and it allows you to access and set environment variables.\\\\n\\\\nThe `Deno.env` object is a read-only object, meaning that you cannot directly modify the environment variables. Instead, you must use the `Deno.env.set()` function to set environment variables. This function takes two arguments: the name of the environment variable and the value to set it to. For example, if you wanted to set the `FOO` environment variable to `bar`, you would use the following code:\\\\n\\\\n```\\'}]\\nprevious\\nRetrieval Question Answering with Sources\\nnext\\nAPI Chains\\n Contents\\n  \\nPrepare Data\\nSet Up Vector DB\\nSet Up LLM Chain with Custom Prompt\\nGenerate Text\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains\\\\index_examples\\\\vector_db_text_generation.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nDocument Loaders\\nDocument Loaders#\\nNote\\nConceptual Guide\\nCombining language models with your own text data is a powerful way to differentiate them.\\nThe first step in doing this is to load the data into “documents” - a fancy way of say some pieces of text.\\nThis module is aimed at making this easy.\\nA primary driver of a lot of this is the Unstructured python package.\\nThis package is a great way to transform all types of files - text, powerpoint, images, html, pdf, etc - into text data.\\nFor detailed instructions on how to get set up with Unstructured, see installation guidelines here.\\nThe following document loaders are provided:\\nCoNLL-U\\nAirbyte JSON\\nApify Dataset\\nAZLyrics\\nAzure Blob Storage Container\\nAzure Blob Storage File\\nBigQuery Loader\\nBilibili\\nBlackboard\\nCollege Confidential\\nConfluence\\nCopy Paste\\nCSV Loader\\nDataFrame Loader\\nDiffbot\\nDirectory Loader\\nDiscord\\nDuckDB Loader\\nEmail\\nEPubs\\nEverNote\\nFacebook Chat\\nFigma\\nGCS Directory\\nGCS File Storage\\nGit\\nGitBook\\nGoogle Drive\\nGutenberg\\nHacker News\\nHTML\\niFixit\\nImages\\nImage captions\\nIMSDb\\nMarkdown\\nNotebook\\nNotion\\nNotion DB Loader\\nObsidian\\nPDF\\nPowerPoint\\nReadTheDocs Documentation\\nRoam\\ns3 Directory\\ns3 File\\nSitemap Loader\\nSlack (Local Exported Zipfile)\\nSubtitle Files\\nTelegram\\nTwitter\\nUnstructured File Loader\\nURL\\nSelenium URL Loader\\nPlaywright URL Loader\\nWeb Base\\nWhatsApp Chat\\nWord Documents\\nYouTube\\nprevious\\nGetting Started\\nnext\\nCoNLL-U\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\indexes\\\\document_loaders.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nGetting Started\\n Contents \\nOne Line Index Creation\\nWalkthrough\\nGetting Started#\\nLangChain primary focuses on constructing indexes with the goal of using them as a Retriever. In order to best understand what this means, it’s worth highlighting what the base Retriever interface is. The BaseRetriever class in LangChain is as follows:\\nfrom abc import ABC, abstractmethod\\nfrom typing import List\\nfrom langchain.schema import Document\\nclass BaseRetriever(ABC):\\n    @abstractmethod\\n    def get_relevant_documents(self, query: str) -> List[Document]:\\n        \"\"\"Get texts relevant for a query.\\n        Args:\\n            query: string to find relevant texts for\\n        Returns:\\n            List of relevant documents\\n        \"\"\"\\nIt’s that simple! The get_relevant_documents method can be implemented however you see fit.\\nOf course, we also help construct what we think useful Retrievers are. The main type of Retriever that we focus on is a Vectorstore retriever. We will focus on that for the rest of this guide.\\nIn order to understand what a vectorstore retriever is, it’s important to understand what a Vectorstore is. So let’s look at that.\\nBy default, LangChain uses Chroma as the vectorstore to index and search embeddings. To walk through this tutorial, we’ll first need to install chromadb.\\npip install chromadb\\nThis example showcases question answering over documents.\\nWe have chosen this as the example for getting started because it nicely combines a lot of different elements (Text splitters, embeddings, vectorstores) and then also shows how to use them in a chain.\\nQuestion answering over documents consists of four steps:\\nCreate an index\\nCreate a Retriever from that index\\nCreate a question answering chain\\nAsk questions!\\nEach of the steps has multiple sub steps and potential configurations. In this notebook we will primarily focus on (1). We will start by showing the one-liner for doing so, but then break down what is actually going on.\\nFirst, let’s import some common classes we’ll use no matter what.\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.llms import OpenAI\\nNext in the generic setup, let’s specify the document loader we want to use. You can download the state_of_the_union.txt file here\\nfrom langchain.document_loaders import TextLoader\\nloader = TextLoader(\\'../state_of_the_union.txt\\', encoding=\\'utf8\\')\\nOne Line Index Creation#\\nTo get started as quickly as possible, we can use the VectorstoreIndexCreator.\\nfrom langchain.indexes import VectorstoreIndexCreator\\nindex = VectorstoreIndexCreator().from_loaders([loader])\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\nNow that the index is created, we can use it to ask questions of the data! Note that under the hood this is actually doing a few steps as well, which we will cover later in this guide.\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nindex.query(query)\\n\" The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\"\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nindex.query_with_sources(query)\\n{\\'question\\': \\'What did the president say about Ketanji Brown Jackson\\',\\n \\'answer\\': \" The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, one of the nation\\'s top legal minds, to continue Justice Breyer\\'s legacy of excellence, and that she has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\\\\n\",\\n \\'sources\\': \\'../state_of_the_union.txt\\'}\\nWhat is returned from the VectorstoreIndexCreator is VectorStoreIndexWrapper, which provides these nice query and query_with_sources functionality. If we just wanted to access the vectorstore directly, we can also do that.\\nindex.vectorstore\\n<langchain.vectorstores.chroma.Chroma at 0x119aa5940>\\nIf we then want to access the VectorstoreRetriever, we can do that with:\\nindex.vectorstore.as_retriever()\\nVectorStoreRetriever(vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x119aa5940>, search_kwargs={})\\nWalkthrough#\\nOkay, so what’s actually going on? How is this index getting created?\\nA lot of the magic is being hid in this VectorstoreIndexCreator. What is this doing?\\nThere are three main steps going on after the documents are loaded:\\nSplitting documents into chunks\\nCreating embeddings for each document\\nStoring documents and embeddings in a vectorstore\\nLet’s walk through this in code\\ndocuments = loader.load()\\nNext, we will split the documents into chunks.\\nfrom langchain.text_splitter import CharacterTextSplitter\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_documents(documents)\\nWe will then select which embeddings we want to use.\\nfrom langchain.embeddings import OpenAIEmbeddings\\nembeddings = OpenAIEmbeddings()\\nWe now create the vectorstore to use as the index.\\nfrom langchain.vectorstores import Chroma\\ndb = Chroma.from_documents(texts, embeddings)\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\nSo that’s creating the index. Then, we expose this index in a retriever interface.\\nretriever = db.as_retriever()\\nThen, as before, we create a chain and use it to answer questions!\\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nqa.run(query)\\n\" The President said that Judge Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He said she is a consensus builder and has received a broad range of support from organizations such as the Fraternal Order of Police and former judges appointed by Democrats and Republicans.\"\\nVectorstoreIndexCreator is just a wrapper around all this logic. It is configurable in the text splitter it uses, the embeddings it uses, and the vectorstore it uses. For example, you can configure it as below:\\nindex_creator = VectorstoreIndexCreator(\\n    vectorstore_cls=Chroma, \\n    embedding=OpenAIEmbeddings(),\\n    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\n)\\nHopefully this highlights what is going on under the hood of VectorstoreIndexCreator. While we think it’s important to have a simple way to create indexes, we also think it’s important to understand what’s going on under the hood.\\nprevious\\nIndexes\\nnext\\nDocument Loaders\\n Contents\\n  \\nOne Line Index Creation\\nWalkthrough\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\indexes\\\\getting_started.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nRetrievers\\nRetrievers#\\nNote\\nConceptual Guide\\nThe retriever interface is a generic interface that makes it easy to combine documents with\\nlanguage models. This interface exposes a get_relevant_documents method which takes in a query\\n(a string) and returns a list of documents.\\nPlease see below for a list of all the retrievers supported.\\nChatGPT Plugin Retriever\\nContextual Compression Retriever\\nStringing compressors and document transformers together\\nDataberry\\nElasticSearch BM25\\nMetal\\nPinecone Hybrid Search\\nSVM Retriever\\nTF-IDF Retriever\\nTime Weighted VectorStore Retriever\\nVectorStore Retriever\\nWeaviate Hybrid Search\\nprevious\\nZilliz\\nnext\\nChatGPT Plugin Retriever\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\indexes\\\\retrievers.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nText Splitters\\nText Splitters#\\nNote\\nConceptual Guide\\nWhen you want to deal with long pieces of text, it is necessary to split up that text into chunks.\\nAs simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What “semantically related” means could depend on the type of text.\\nThis notebook showcases several ways to do that.\\nAt a high level, text splitters work as following:\\nSplit the text up into small, semantically meaningful chunks (often sentences).\\nStart combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\\nOnce you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\\nThat means there two different axes along which you can customize your text splitter:\\nHow the text is split\\nHow the chunk size is measured\\nFor an introduction to the default text splitter and generic functionality see:\\nGetting Started\\nWe also have documentation for all the types of text splitters that are supported.\\nPlease see below for that list.\\nCharacter Text Splitter\\nHugging Face Length Function\\nLatex Text Splitter\\nMarkdown Text Splitter\\nNLTK Text Splitter\\nPython Code Text Splitter\\nRecursiveCharacterTextSplitter\\nSpacy Text Splitter\\ntiktoken (OpenAI) Length Function\\nTiktokenText Splitter\\nprevious\\nYouTube\\nnext\\nGetting Started\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\indexes\\\\text_splitters.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nVectorstores\\nVectorstores#\\nNote\\nConceptual Guide\\nVectorstores are one of the most important components of building indexes.\\nFor an introduction to vectorstores and generic functionality see:\\nGetting Started\\nWe also have documentation for all the types of vectorstores that are supported.\\nPlease see below for that list.\\nAnnoy\\nAtlasDB\\nChroma\\nDeep Lake\\nElasticSearch\\nFAISS\\nMilvus\\nOpenSearch\\nPGVector\\nPinecone\\nQdrant\\nRedis\\nSupabaseVectorStore\\nWeaviate\\nZilliz\\nprevious\\nTiktokenText Splitter\\nnext\\nGetting Started\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\indexes\\\\vectorstores.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nFacebook Chat\\nFacebook Chat#\\nThis notebook covers how to load data from the Facebook Chats into a format that can be ingested into LangChain.\\nfrom langchain.document_loaders import FacebookChatLoader\\nloader = FacebookChatLoader(\"example_data/facebook_chat.json\")\\nloader.load()\\n[Document(page_content=\\'User 2 on 2023-02-05 12:46:11: Bye!\\\\n\\\\nUser 1 on 2023-02-05 12:43:55: Oh no worries! Bye\\\\n\\\\nUser 2 on 2023-02-05 12:24:37: No Im sorry it was my mistake, the blue one is not for sale\\\\n\\\\nUser 1 on 2023-02-05 12:05:40: I thought you were selling the blue one!\\\\n\\\\nUser 1 on 2023-02-05 12:05:09: Im not interested in this bag. Im interested in the blue one!\\\\n\\\\nUser 2 on 2023-02-05 12:04:28: Here is $129\\\\n\\\\nUser 2 on 2023-02-05 12:04:05: Online is at least $100\\\\n\\\\nUser 1 on 2023-02-05 11:59:59: How much do you want?\\\\n\\\\nUser 2 on 2023-02-05 07:17:56: Goodmorning! $50 is too low.\\\\n\\\\nUser 1 on 2023-02-04 23:17:02: Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\\\n\\\\n\\', lookup_str=\\'\\', metadata={\\'source\\': \\'docs/modules/document_loaders/examples/example_data/facebook_chat.json\\'}, lookup_index=0)]\\nprevious\\nEverNote\\nnext\\nFigma\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\indexes\\\\document_loaders\\\\examples\\\\facebook_chat.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nSitemap Loader\\n Contents \\nFiltering sitemap URLs\\nSitemap Loader#\\nExtends from the WebBaseLoader, this will load a sitemap from a given URL, and then scrape and load all the pages in the sitemap, returning each page as a document.\\nThe scraping is done concurrently, using WebBaseLoader.  There are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren’t concerned about being a good citizen, or you control the server you are scraping and don’t care about load, you can change the requests_per_second parameter to increase the max concurrent requests.  Note, while this will speed up the scraping process, but may cause the server to block you.  Be careful!\\n!pip install nest_asyncio\\nRequirement already satisfied: nest_asyncio in /Users/tasp/Code/projects/langchain/.venv/lib/python3.10/site-packages (1.5.6)\\n[notice] A new release of pip available: 22.3.1 -> 23.0.1\\n[notice] To update, run: pip install --upgrade pip\\n# fixes a bug with asyncio and jupyter\\nimport nest_asyncio\\nnest_asyncio.apply()\\nfrom langchain.document_loaders.sitemap import SitemapLoader\\nsitemap_loader = SitemapLoader(web_path=\"https://langchain.readthedocs.io/sitemap.xml\")\\ndocs = sitemap_loader.load()\\ndocs[0]\\nDocument(page_content=\\'\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nWelcome to LangChain — 🦜🔗 LangChain 0.0.123\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSkip to main content\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nCtrl+K\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n🦜🔗 LangChain 0.0.123\\\\n\\\\n\\\\n\\\\nGetting Started\\\\n\\\\nQuickstart Guide\\\\n\\\\nModules\\\\n\\\\nPrompt Templates\\\\nGetting Started\\\\nKey Concepts\\\\nHow-To Guides\\\\nCreate a custom prompt template\\\\nCreate a custom example selector\\\\nProvide few shot examples to a prompt\\\\nPrompt Serialization\\\\nExample Selectors\\\\nOutput Parsers\\\\n\\\\n\\\\nReference\\\\nPromptTemplates\\\\nExample Selector\\\\n\\\\n\\\\n\\\\n\\\\nLLMs\\\\nGetting Started\\\\nKey Concepts\\\\nHow-To Guides\\\\nGeneric Functionality\\\\nCustom LLM\\\\nFake LLM\\\\nLLM Caching\\\\nLLM Serialization\\\\nToken Usage Tracking\\\\n\\\\n\\\\nIntegrations\\\\nAI21\\\\nAleph Alpha\\\\nAnthropic\\\\nAzure OpenAI LLM Example\\\\nBanana\\\\nCerebriumAI LLM Example\\\\nCohere\\\\nDeepInfra LLM Example\\\\nForefrontAI LLM Example\\\\nGooseAI LLM Example\\\\nHugging Face Hub\\\\nManifest\\\\nModal\\\\nOpenAI\\\\nPetals LLM Example\\\\nPromptLayer OpenAI\\\\nSageMakerEndpoint\\\\nSelf-Hosted Models via Runhouse\\\\nStochasticAI\\\\nWriter\\\\n\\\\n\\\\nAsync API for LLM\\\\nStreaming with LLMs\\\\n\\\\n\\\\nReference\\\\n\\\\n\\\\nDocument Loaders\\\\nKey Concepts\\\\nHow To Guides\\\\nCoNLL-U\\\\nAirbyte JSON\\\\nAZLyrics\\\\nBlackboard\\\\nCollege Confidential\\\\nCopy Paste\\\\nCSV Loader\\\\nDirectory Loader\\\\nEmail\\\\nEverNote\\\\nFacebook Chat\\\\nFigma\\\\nGCS Directory\\\\nGCS File Storage\\\\nGitBook\\\\nGoogle Drive\\\\nGutenberg\\\\nHacker News\\\\nHTML\\\\niFixit\\\\nImages\\\\nIMSDb\\\\nMarkdown\\\\nNotebook\\\\nNotion\\\\nObsidian\\\\nPDF\\\\nPowerPoint\\\\nReadTheDocs Documentation\\\\nRoam\\\\ns3 Directory\\\\ns3 File\\\\nSubtitle Files\\\\nTelegram\\\\nUnstructured File Loader\\\\nURL\\\\nWeb Base\\\\nWord Documents\\\\nYouTube\\\\n\\\\n\\\\n\\\\n\\\\nUtils\\\\nKey Concepts\\\\nGeneric Utilities\\\\nBash\\\\nBing Search\\\\nGoogle Search\\\\nGoogle Serper API\\\\nIFTTT WebHooks\\\\nPython REPL\\\\nRequests\\\\nSearxNG Search API\\\\nSerpAPI\\\\nWolfram Alpha\\\\nZapier Natural Language Actions API\\\\n\\\\n\\\\nReference\\\\nPython REPL\\\\nSerpAPI\\\\nSearxNG Search\\\\nDocstore\\\\nText Splitter\\\\nEmbeddings\\\\nVectorStores\\\\n\\\\n\\\\n\\\\n\\\\nIndexes\\\\nGetting Started\\\\nKey Concepts\\\\nHow To Guides\\\\nEmbeddings\\\\nHypothetical Document Embeddings\\\\nText Splitter\\\\nVectorStores\\\\nAtlasDB\\\\nChroma\\\\nDeep Lake\\\\nElasticSearch\\\\nFAISS\\\\nMilvus\\\\nOpenSearch\\\\nPGVector\\\\nPinecone\\\\nQdrant\\\\nRedis\\\\nWeaviate\\\\nChatGPT Plugin Retriever\\\\nVectorStore Retriever\\\\nAnalyze Document\\\\nChat Index\\\\nGraph QA\\\\nQuestion Answering with Sources\\\\nQuestion Answering\\\\nSummarization\\\\nRetrieval Question/Answering\\\\nRetrieval Question Answering with Sources\\\\nVector DB Text Generation\\\\n\\\\n\\\\n\\\\n\\\\nChains\\\\nGetting Started\\\\nHow-To Guides\\\\nGeneric Chains\\\\nLoading from LangChainHub\\\\nLLM Chain\\\\nSequential Chains\\\\nSerialization\\\\nTransformation Chain\\\\n\\\\n\\\\nUtility Chains\\\\nAPI Chains\\\\nSelf-Critique Chain with Constitutional AI\\\\nBashChain\\\\nLLMCheckerChain\\\\nLLM Math\\\\nLLMRequestsChain\\\\nLLMSummarizationCheckerChain\\\\nModeration\\\\nPAL\\\\nSQLite example\\\\n\\\\n\\\\nAsync API for Chain\\\\n\\\\n\\\\nKey Concepts\\\\nReference\\\\n\\\\n\\\\nAgents\\\\nGetting Started\\\\nKey Concepts\\\\nHow-To Guides\\\\nAgents and Vectorstores\\\\nAsync API for Agent\\\\nConversation Agent (for Chat Models)\\\\nChatGPT Plugins\\\\nCustom Agent\\\\nDefining Custom Tools\\\\nHuman as a tool\\\\nIntermediate Steps\\\\nLoading from LangChainHub\\\\nMax Iterations\\\\nMulti Input Tools\\\\nSearch Tools\\\\nSerialization\\\\nAdding SharedMemory to an Agent and its Tools\\\\nCSV Agent\\\\nJSON Agent\\\\nOpenAPI Agent\\\\nPandas Dataframe Agent\\\\nPython Agent\\\\nSQL Database Agent\\\\nVectorstore Agent\\\\nMRKL\\\\nMRKL Chat\\\\nReAct\\\\nSelf Ask With Search\\\\n\\\\n\\\\nReference\\\\n\\\\n\\\\nMemory\\\\nGetting Started\\\\nKey Concepts\\\\nHow-To Guides\\\\nConversationBufferMemory\\\\nConversationBufferWindowMemory\\\\nEntity Memory\\\\nConversation Knowledge Graph Memory\\\\nConversationSummaryMemory\\\\nConversationSummaryBufferMemory\\\\nConversationTokenBufferMemory\\\\nAdding Memory To an LLMChain\\\\nAdding Memory to a Multi-Input Chain\\\\nAdding Memory to an Agent\\\\nChatGPT Clone\\\\nConversation Agent\\\\nConversational Memory Customization\\\\nCustom Memory\\\\nMultiple Memory\\\\n\\\\n\\\\n\\\\n\\\\nChat\\\\nGetting Started\\\\nKey Concepts\\\\nHow-To Guides\\\\nAgent\\\\nChat Vector DB\\\\nFew Shot Examples\\\\nMemory\\\\nPromptLayer ChatOpenAI\\\\nStreaming\\\\nRetrieval Question/Answering\\\\nRetrieval Question Answering with Sources\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse Cases\\\\n\\\\nAgents\\\\nChatbots\\\\nGenerate Examples\\\\nData Augmented Generation\\\\nQuestion Answering\\\\nSummarization\\\\nQuerying Tabular Data\\\\nExtraction\\\\nEvaluation\\\\nAgent Benchmarking: Search + Calculator\\\\nAgent VectorDB Question Answering Benchmarking\\\\nBenchmarking Template\\\\nData Augmented Question Answering\\\\nUsing Hugging Face Datasets\\\\nLLM Math\\\\nQuestion Answering Benchmarking: Paul Graham Essay\\\\nQuestion Answering Benchmarking: State of the Union Address\\\\nQA Generation\\\\nQuestion Answering\\\\nSQL Question Answering Benchmarking: Chinook\\\\n\\\\n\\\\nModel Comparison\\\\n\\\\nReference\\\\n\\\\nInstallation\\\\nIntegrations\\\\nAPI References\\\\nPrompts\\\\nPromptTemplates\\\\nExample Selector\\\\n\\\\n\\\\nUtilities\\\\nPython REPL\\\\nSerpAPI\\\\nSearxNG Search\\\\nDocstore\\\\nText Splitter\\\\nEmbeddings\\\\nVectorStores\\\\n\\\\n\\\\nChains\\\\nAgents\\\\n\\\\n\\\\n\\\\nEcosystem\\\\n\\\\nLangChain Ecosystem\\\\nAI21 Labs\\\\nAtlasDB\\\\nBanana\\\\nCerebriumAI\\\\nChroma\\\\nCohere\\\\nDeepInfra\\\\nDeep Lake\\\\nForefrontAI\\\\nGoogle Search Wrapper\\\\nGoogle Serper Wrapper\\\\nGooseAI\\\\nGraphsignal\\\\nHazy Research\\\\nHelicone\\\\nHugging Face\\\\nMilvus\\\\nModal\\\\nNLPCloud\\\\nOpenAI\\\\nOpenSearch\\\\nPetals\\\\nPGVector\\\\nPinecone\\\\nPromptLayer\\\\nQdrant\\\\nRunhouse\\\\nSearxNG Search API\\\\nSerpAPI\\\\nStochasticAI\\\\nUnstructured\\\\nWeights & Biases\\\\nWeaviate\\\\nWolfram Alpha Wrapper\\\\nWriter\\\\n\\\\n\\\\n\\\\nAdditional Resources\\\\n\\\\nLangChainHub\\\\nGlossary\\\\nLangChain Gallery\\\\nDeployments\\\\nTracing\\\\nDiscord\\\\nProduction Support\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n.rst\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n.pdf\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nWelcome to LangChain\\\\n\\\\n\\\\n\\\\n\\\\n Contents \\\\n\\\\n\\\\n\\\\nGetting Started\\\\nModules\\\\nUse Cases\\\\nReference Docs\\\\nLangChain Ecosystem\\\\nAdditional Resources\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nWelcome to LangChain#\\\\nLarge language models (LLMs) are emerging as a transformative technology, enabling\\\\ndevelopers to build applications that they previously could not.\\\\nBut using these LLMs in isolation is often not enough to\\\\ncreate a truly powerful app - the real power comes when you are able to\\\\ncombine them with other sources of computation or knowledge.\\\\nThis library is aimed at assisting in the development of those types of applications. Common examples of these types of applications include:\\\\n❓ Question Answering over specific documents\\\\n\\\\nDocumentation\\\\nEnd-to-end Example: Question Answering over Notion Database\\\\n\\\\n💬 Chatbots\\\\n\\\\nDocumentation\\\\nEnd-to-end Example: Chat-LangChain\\\\n\\\\n🤖 Agents\\\\n\\\\nDocumentation\\\\nEnd-to-end Example: GPT+WolframAlpha\\\\n\\\\n\\\\nGetting Started#\\\\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\\\\n\\\\nGetting Started Documentation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nModules#\\\\nThere are several main modules that LangChain provides support for.\\\\nFor each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides.\\\\nThese modules are, in increasing order of complexity:\\\\n\\\\nPrompts: This includes prompt management, prompt optimization, and prompt serialization.\\\\nLLMs: This includes a generic interface for all LLMs, and common utilities for working with LLMs.\\\\nDocument Loaders: This includes a standard interface for loading documents, as well as specific integrations to all types of text data sources.\\\\nUtils: Language models are often more powerful when interacting with other sources of knowledge or computation. This can include Python REPLs, embeddings, search engines, and more. LangChain provides a large collection of common utils to use in your application.\\\\nChains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\\\nIndexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\\\\nAgents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\\\nMemory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\\\nChat: Chat models are a variation on Language Models that expose a different API - rather than working with raw text, they work with messages. LangChain provides a standard interface for working with them and doing all the same things as above.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse Cases#\\\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\\\n\\\\nAgents: Agents are systems that use a language model to interact with other tools. These can be used to do more grounded question/answering, interact with APIs, or even take actions.\\\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\\\nData Augmented Generation: Data Augmented Generation involves specific types of chains that first interact with an external datasource to fetch data to use in the generation step. Examples of this include summarization of long pieces of text and question/answering over specific data sources.\\\\nQuestion Answering: Answering questions over specific documents, only utilizing the information in those documents to construct an answer. A type of Data Augmented Generation.\\\\nSummarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.\\\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\\\\nEvaluation: Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\\\\nGenerate similar examples: Generating similar examples to a given input. This is a common use case for many applications, and LangChain provides some prompts/chains for assisting in this.\\\\nCompare models: Experimenting with different prompts, models, and chains is a big part of developing the best possible application. The ModelLaboratory makes it easy to do so.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nReference Docs#\\\\nAll of LangChain’s reference documentation, in one place. Full documentation on all methods, classes, installation methods, and integration setups for LangChain.\\\\n\\\\nReference Documentation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLangChain Ecosystem#\\\\nGuides for how other companies/products can be used with LangChain\\\\n\\\\nLangChain Ecosystem\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nAdditional Resources#\\\\nAdditional collection of resources we think may be useful as you develop your application!\\\\n\\\\nLangChainHub: The LangChainHub is a place to share and explore other prompts, chains, and agents.\\\\nGlossary: A glossary of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\\\nDiscord: Join us on our Discord to discuss all things LangChain!\\\\nTracing: A guide on using tracing in LangChain to visualize the execution of chains and agents.\\\\nProduction Support: As you move your LangChains into production, we’d love to offer more comprehensive support. Please fill out this form and we’ll set up a dedicated support Slack channel.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nnext\\\\nQuickstart Guide\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n Contents\\\\n  \\\\n\\\\n\\\\nGetting Started\\\\nModules\\\\nUse Cases\\\\nReference Docs\\\\nLangChain Ecosystem\\\\nAdditional Resources\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nBy Harrison Chase\\\\n\\\\n\\\\n\\\\n\\\\n    \\\\n      © Copyright 2023, Harrison Chase.\\\\n      \\\\n\\\\n\\\\n\\\\n\\\\n  Last updated on Mar 24, 2023.\\\\n  \\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\', lookup_str=\\'\\', metadata={\\'source\\': \\'https://python.langchain.com/en/stable/\\', \\'loc\\': \\'https://python.langchain.com/en/stable/\\', \\'lastmod\\': \\'2023-03-24T19:30:54.647430+00:00\\', \\'changefreq\\': \\'weekly\\', \\'priority\\': \\'1\\'}, lookup_index=0)\\nFiltering sitemap URLs#\\nSitemaps can be massive files, with thousands of urls.  Often you don’t need every single one of them.  You can filter the urls by passing a list of strings or regex patterns to the url_filter parameter.  Only urls that match one of the patterns will be loaded.\\nloader = SitemapLoader(\\n    \"https://langchain.readthedocs.io/sitemap.xml\",\\n    filter_urls=[\"https://python.langchain.com/en/latest/\"]\\n)\\ndocuments = loader.load()\\ndocuments[0]\\nDocument(page_content=\\'\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nWelcome to LangChain — 🦜🔗 LangChain 0.0.123\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSkip to main content\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nCtrl+K\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n🦜🔗 LangChain 0.0.123\\\\n\\\\n\\\\n\\\\nGetting Started\\\\n\\\\nQuickstart Guide\\\\n\\\\nModules\\\\n\\\\nModels\\\\nLLMs\\\\nGetting Started\\\\nGeneric Functionality\\\\nHow to use the async API for LLMs\\\\nHow to write a custom LLM wrapper\\\\nHow (and why) to use the fake LLM\\\\nHow to cache LLM calls\\\\nHow to serialize LLM classes\\\\nHow to stream LLM responses\\\\nHow to track token usage\\\\n\\\\n\\\\nIntegrations\\\\nAI21\\\\nAleph Alpha\\\\nAnthropic\\\\nAzure OpenAI LLM Example\\\\nBanana\\\\nCerebriumAI LLM Example\\\\nCohere\\\\nDeepInfra LLM Example\\\\nForefrontAI LLM Example\\\\nGooseAI LLM Example\\\\nHugging Face Hub\\\\nManifest\\\\nModal\\\\nOpenAI\\\\nPetals LLM Example\\\\nPromptLayer OpenAI\\\\nSageMakerEndpoint\\\\nSelf-Hosted Models via Runhouse\\\\nStochasticAI\\\\nWriter\\\\n\\\\n\\\\nReference\\\\n\\\\n\\\\nChat Models\\\\nGetting Started\\\\nHow-To Guides\\\\nHow to use few shot examples\\\\nHow to stream responses\\\\n\\\\n\\\\nIntegrations\\\\nAzure\\\\nOpenAI\\\\nPromptLayer ChatOpenAI\\\\n\\\\n\\\\n\\\\n\\\\nText Embedding Models\\\\nAzureOpenAI\\\\nCohere\\\\nFake Embeddings\\\\nHugging Face Hub\\\\nInstructEmbeddings\\\\nOpenAI\\\\nSageMaker Endpoint Embeddings\\\\nSelf Hosted Embeddings\\\\nTensorflowHub\\\\n\\\\n\\\\n\\\\n\\\\nPrompts\\\\nPrompt Templates\\\\nGetting Started\\\\nHow-To Guides\\\\nHow to create a custom prompt template\\\\nHow to create a prompt template that uses few shot examples\\\\nHow to work with partial Prompt Templates\\\\nHow to serialize prompts\\\\n\\\\n\\\\nReference\\\\nPromptTemplates\\\\nExample Selector\\\\n\\\\n\\\\n\\\\n\\\\nChat Prompt Template\\\\nExample Selectors\\\\nHow to create a custom example selector\\\\nLengthBased ExampleSelector\\\\nMaximal Marginal Relevance ExampleSelector\\\\nNGram Overlap ExampleSelector\\\\nSimilarity ExampleSelector\\\\n\\\\n\\\\nOutput Parsers\\\\nOutput Parsers\\\\nCommaSeparatedListOutputParser\\\\nOutputFixingParser\\\\nPydanticOutputParser\\\\nRetryOutputParser\\\\nStructured Output Parser\\\\n\\\\n\\\\n\\\\n\\\\nIndexes\\\\nGetting Started\\\\nDocument Loaders\\\\nCoNLL-U\\\\nAirbyte JSON\\\\nAZLyrics\\\\nBlackboard\\\\nCollege Confidential\\\\nCopy Paste\\\\nCSV Loader\\\\nDirectory Loader\\\\nEmail\\\\nEverNote\\\\nFacebook Chat\\\\nFigma\\\\nGCS Directory\\\\nGCS File Storage\\\\nGitBook\\\\nGoogle Drive\\\\nGutenberg\\\\nHacker News\\\\nHTML\\\\niFixit\\\\nImages\\\\nIMSDb\\\\nMarkdown\\\\nNotebook\\\\nNotion\\\\nObsidian\\\\nPDF\\\\nPowerPoint\\\\nReadTheDocs Documentation\\\\nRoam\\\\ns3 Directory\\\\ns3 File\\\\nSubtitle Files\\\\nTelegram\\\\nUnstructured File Loader\\\\nURL\\\\nWeb Base\\\\nWord Documents\\\\nYouTube\\\\n\\\\n\\\\nText Splitters\\\\nGetting Started\\\\nCharacter Text Splitter\\\\nHuggingFace Length Function\\\\nLatex Text Splitter\\\\nMarkdown Text Splitter\\\\nNLTK Text Splitter\\\\nPython Code Text Splitter\\\\nRecursiveCharacterTextSplitter\\\\nSpacy Text Splitter\\\\ntiktoken (OpenAI) Length Function\\\\nTiktokenText Splitter\\\\n\\\\n\\\\nVectorstores\\\\nGetting Started\\\\nAtlasDB\\\\nChroma\\\\nDeep Lake\\\\nElasticSearch\\\\nFAISS\\\\nMilvus\\\\nOpenSearch\\\\nPGVector\\\\nPinecone\\\\nQdrant\\\\nRedis\\\\nWeaviate\\\\n\\\\n\\\\nRetrievers\\\\nChatGPT Plugin Retriever\\\\nVectorStore Retriever\\\\n\\\\n\\\\n\\\\n\\\\nMemory\\\\nGetting Started\\\\nHow-To Guides\\\\nConversationBufferMemory\\\\nConversationBufferWindowMemory\\\\nEntity Memory\\\\nConversation Knowledge Graph Memory\\\\nConversationSummaryMemory\\\\nConversationSummaryBufferMemory\\\\nConversationTokenBufferMemory\\\\nHow to add Memory to an LLMChain\\\\nHow to add memory to a Multi-Input Chain\\\\nHow to add Memory to an Agent\\\\nHow to customize conversational memory\\\\nHow to create a custom Memory class\\\\nHow to use multiple memroy classes in the same chain\\\\n\\\\n\\\\n\\\\n\\\\nChains\\\\nGetting Started\\\\nHow-To Guides\\\\nAsync API for Chain\\\\nLoading from LangChainHub\\\\nLLM Chain\\\\nSequential Chains\\\\nSerialization\\\\nTransformation Chain\\\\nAnalyze Document\\\\nChat Index\\\\nGraph QA\\\\nHypothetical Document Embeddings\\\\nQuestion Answering with Sources\\\\nQuestion Answering\\\\nSummarization\\\\nRetrieval Question/Answering\\\\nRetrieval Question Answering with Sources\\\\nVector DB Text Generation\\\\nAPI Chains\\\\nSelf-Critique Chain with Constitutional AI\\\\nBashChain\\\\nLLMCheckerChain\\\\nLLM Math\\\\nLLMRequestsChain\\\\nLLMSummarizationCheckerChain\\\\nModeration\\\\nPAL\\\\nSQLite example\\\\n\\\\n\\\\nReference\\\\n\\\\n\\\\nAgents\\\\nGetting Started\\\\nTools\\\\nGetting Started\\\\nDefining Custom Tools\\\\nMulti Input Tools\\\\nBash\\\\nBing Search\\\\nChatGPT Plugins\\\\nGoogle Search\\\\nGoogle Serper API\\\\nHuman as a tool\\\\nIFTTT WebHooks\\\\nPython REPL\\\\nRequests\\\\nSearch Tools\\\\nSearxNG Search API\\\\nSerpAPI\\\\nWolfram Alpha\\\\nZapier Natural Language Actions API\\\\n\\\\n\\\\nAgents\\\\nAgent Types\\\\nCustom Agent\\\\nConversation Agent (for Chat Models)\\\\nConversation Agent\\\\nMRKL\\\\nMRKL Chat\\\\nReAct\\\\nSelf Ask With Search\\\\n\\\\n\\\\nToolkits\\\\nCSV Agent\\\\nJSON Agent\\\\nOpenAPI Agent\\\\nPandas Dataframe Agent\\\\nPython Agent\\\\nSQL Database Agent\\\\nVectorstore Agent\\\\n\\\\n\\\\nAgent Executors\\\\nHow to combine agents and vectorstores\\\\nHow to use the async API for Agents\\\\nHow to create ChatGPT Clone\\\\nHow to access intermediate steps\\\\nHow to cap the max number of iterations\\\\nHow to add SharedMemory to an Agent and its Tools\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse Cases\\\\n\\\\nPersonal Assistants\\\\nQuestion Answering over Docs\\\\nChatbots\\\\nQuerying Tabular Data\\\\nInteracting with APIs\\\\nSummarization\\\\nExtraction\\\\nEvaluation\\\\nAgent Benchmarking: Search + Calculator\\\\nAgent VectorDB Question Answering Benchmarking\\\\nBenchmarking Template\\\\nData Augmented Question Answering\\\\nUsing Hugging Face Datasets\\\\nLLM Math\\\\nQuestion Answering Benchmarking: Paul Graham Essay\\\\nQuestion Answering Benchmarking: State of the Union Address\\\\nQA Generation\\\\nQuestion Answering\\\\nSQL Question Answering Benchmarking: Chinook\\\\n\\\\n\\\\n\\\\nReference\\\\n\\\\nInstallation\\\\nIntegrations\\\\nAPI References\\\\nPrompts\\\\nPromptTemplates\\\\nExample Selector\\\\n\\\\n\\\\nUtilities\\\\nPython REPL\\\\nSerpAPI\\\\nSearxNG Search\\\\nDocstore\\\\nText Splitter\\\\nEmbeddings\\\\nVectorStores\\\\n\\\\n\\\\nChains\\\\nAgents\\\\n\\\\n\\\\n\\\\nEcosystem\\\\n\\\\nLangChain Ecosystem\\\\nAI21 Labs\\\\nAtlasDB\\\\nBanana\\\\nCerebriumAI\\\\nChroma\\\\nCohere\\\\nDeepInfra\\\\nDeep Lake\\\\nForefrontAI\\\\nGoogle Search Wrapper\\\\nGoogle Serper Wrapper\\\\nGooseAI\\\\nGraphsignal\\\\nHazy Research\\\\nHelicone\\\\nHugging Face\\\\nMilvus\\\\nModal\\\\nNLPCloud\\\\nOpenAI\\\\nOpenSearch\\\\nPetals\\\\nPGVector\\\\nPinecone\\\\nPromptLayer\\\\nQdrant\\\\nRunhouse\\\\nSearxNG Search API\\\\nSerpAPI\\\\nStochasticAI\\\\nUnstructured\\\\nWeights & Biases\\\\nWeaviate\\\\nWolfram Alpha Wrapper\\\\nWriter\\\\n\\\\n\\\\n\\\\nAdditional Resources\\\\n\\\\nLangChainHub\\\\nGlossary\\\\nLangChain Gallery\\\\nDeployments\\\\nTracing\\\\nDiscord\\\\nProduction Support\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n.rst\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n.pdf\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nWelcome to LangChain\\\\n\\\\n\\\\n\\\\n\\\\n Contents \\\\n\\\\n\\\\n\\\\nGetting Started\\\\nModules\\\\nUse Cases\\\\nReference Docs\\\\nLangChain Ecosystem\\\\nAdditional Resources\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nWelcome to LangChain#\\\\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an API, but will also:\\\\n\\\\nBe data-aware: connect a language model to other sources of data\\\\nBe agentic: allow a language model to interact with its environment\\\\n\\\\nThe LangChain framework is designed with the above principles in mind.\\\\nThis is the Python specific portion of the documentation. For a purely conceptual guide to LangChain, see here. For the JavaScript documentation, see here.\\\\n\\\\nGetting Started#\\\\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\\\\n\\\\nGetting Started Documentation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nModules#\\\\nThere are several main modules that LangChain provides support for.\\\\nFor each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides.\\\\nThese modules are, in increasing order of complexity:\\\\n\\\\nModels: The various model types and model integrations LangChain supports.\\\\nPrompts: This includes prompt management, prompt optimization, and prompt serialization.\\\\nMemory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\\\nIndexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\\\\nChains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\\\nAgents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse Cases#\\\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\\\n\\\\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\\\\nQuestion Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\\\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\\\\nInteracting with APIs: Enabling LLMs to interact with APIs is extremely powerful in order to give them more up-to-date information and allow them to take actions.\\\\nExtraction: Extract structured information from text.\\\\nSummarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.\\\\nEvaluation: Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nReference Docs#\\\\nAll of LangChain’s reference documentation, in one place. Full documentation on all methods, classes, installation methods, and integration setups for LangChain.\\\\n\\\\nReference Documentation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLangChain Ecosystem#\\\\nGuides for how other companies/products can be used with LangChain\\\\n\\\\nLangChain Ecosystem\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nAdditional Resources#\\\\nAdditional collection of resources we think may be useful as you develop your application!\\\\n\\\\nLangChainHub: The LangChainHub is a place to share and explore other prompts, chains, and agents.\\\\nGlossary: A glossary of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\\\nTracing: A guide on using tracing in LangChain to visualize the execution of chains and agents.\\\\nModel Laboratory: Experimenting with different prompts, models, and chains is a big part of developing the best possible application. The ModelLaboratory makes it easy to do so.\\\\nDiscord: Join us on our Discord to discuss all things LangChain!\\\\nProduction Support: As you move your LangChains into production, we’d love to offer more comprehensive support. Please fill out this form and we’ll set up a dedicated support Slack channel.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nnext\\\\nQuickstart Guide\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n Contents\\\\n  \\\\n\\\\n\\\\nGetting Started\\\\nModules\\\\nUse Cases\\\\nReference Docs\\\\nLangChain Ecosystem\\\\nAdditional Resources\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nBy Harrison Chase\\\\n\\\\n\\\\n\\\\n\\\\n    \\\\n      © Copyright 2023, Harrison Chase.\\\\n      \\\\n\\\\n\\\\n\\\\n\\\\n  Last updated on Mar 27, 2023.\\\\n  \\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\', lookup_str=\\'\\', metadata={\\'source\\': \\'https://python.langchain.com/en/latest/\\', \\'loc\\': \\'https://python.langchain.com/en/latest/\\', \\'lastmod\\': \\'2023-03-27T22:50:49.790324+00:00\\', \\'changefreq\\': \\'daily\\', \\'priority\\': \\'0.9\\'}, lookup_index=0)\\nprevious\\ns3 File\\nnext\\nSlack (Local Exported Zipfile)\\n Contents\\n  \\nFiltering sitemap URLs\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\indexes\\\\document_loaders\\\\examples\\\\sitemap.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nDataberry\\n Contents \\nQuery\\nDataberry#\\nThis notebook shows how to use Databerry’s retriever.\\nFirst, you will need to sign up for Databerry, create a datastore, add some data and get your datastore api endpoint url\\nQuery#\\nNow that our index is set up, we can set up a retriever and start querying it.\\nfrom langchain.retrievers import DataberryRetriever\\nretriever = DataberryRetriever(\\n    datastore_url=\"https://clg1xg2h80000l708dymr0fxc.databerry.ai/query\",\\n    # api_key=\"DATABERRY_API_KEY\", # optional if datastore is public\\n    # top_k=10 # optional\\n)\\nretriever.get_relevant_documents(\"What is Daftpage?\")\\n[Document(page_content=\\'✨ Made with DaftpageOpen main menuPricingTemplatesLoginSearchHelpGetting StartedFeaturesAffiliate ProgramGetting StartedDaftpage is a new type of website builder that works like a doc.It makes website building easy, fun and offers tons of powerful features for free. Just type / in your page to get started!DaftpageCopyright © 2022 Daftpage, Inc.All rights reserved.ProductPricingTemplatesHelp & SupportHelp CenterGetting startedBlogCompanyAboutRoadmapTwitterAffiliate Program👾 Discord\\', metadata={\\'source\\': \\'https:/daftpage.com/help/getting-started\\', \\'score\\': 0.8697265}),\\n Document(page_content=\"✨ Made with DaftpageOpen main menuPricingTemplatesLoginSearchHelpGetting StartedFeaturesAffiliate ProgramHelp CenterWelcome to Daftpage’s help center—the one-stop shop for learning everything about building websites with Daftpage.Daftpage is the simplest way to create websites for all purposes in seconds. Without knowing how to code, and for free!Get StartedDaftpage is a new type of website builder that works like a doc.It makes website building easy, fun and offers tons of powerful features for free. Just type / in your page to get started!Start here✨ Create your first site🧱 Add blocks🚀 PublishGuides🔖 Add a custom domainFeatures🔥 Drops🎨 Drawings👻 Ghost mode💀 Skeleton modeCant find the answer you\\'re looking for?mail us at support@daftpage.comJoin the awesome Daftpage community on: 👾 DiscordDaftpageCopyright © 2022 Daftpage, Inc.All rights reserved.ProductPricingTemplatesHelp & SupportHelp CenterGetting startedBlogCompanyAboutRoadmapTwitterAffiliate Program👾 Discord\", metadata={\\'source\\': \\'https:/daftpage.com/help\\', \\'score\\': 0.86570895}),\\n Document(page_content=\" is the simplest way to create websites for all purposes in seconds. Without knowing how to code, and for free!Get StartedDaftpage is a new type of website builder that works like a doc.It makes website building easy, fun and offers tons of powerful features for free. Just type / in your page to get started!Start here✨ Create your first site🧱 Add blocks🚀 PublishGuides🔖 Add a custom domainFeatures🔥 Drops🎨 Drawings👻 Ghost mode💀 Skeleton modeCant find the answer you\\'re looking for?mail us at support@daftpage.comJoin the awesome Daftpage community on: 👾 DiscordDaftpageCopyright © 2022 Daftpage, Inc.All rights reserved.ProductPricingTemplatesHelp & SupportHelp CenterGetting startedBlogCompanyAboutRoadmapTwitterAffiliate Program👾 Discord\", metadata={\\'source\\': \\'https:/daftpage.com/help\\', \\'score\\': 0.8645384})]\\nprevious\\nContextual Compression Retriever\\nnext\\nElasticSearch BM25\\n Contents\\n  \\nQuery\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\indexes\\\\retrievers\\\\examples\\\\databerry.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nTime Weighted VectorStore Retriever\\n Contents \\nLow Decay Rate\\nHigh Decay Rate\\nTime Weighted VectorStore Retriever#\\nThis retriever uses a combination of semantic similarity and recency.\\nThe algorithm for scoring them is:\\nsemantic_similarity + (1.0 - decay_rate) ** hours_passed\\nNotably, hours_passed refers to the hours passed since the object in the retriever was last accessed, not since it was created. This means that frequently accessed objects remain “fresh.”\\nimport faiss\\nfrom datetime import datetime, timedelta\\nfrom langchain.docstore import InMemoryDocstore\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.retrievers import TimeWeightedVectorStoreRetriever\\nfrom langchain.schema import Document\\nfrom langchain.vectorstores import FAISS\\nLow Decay Rate#\\nA low decay rate (in this, to be extreme, we will set close to 0) means memories will be “remembered” for longer. A decay rate of 0 means memories never be forgotten, making this retriever equivalent to the vector lookup.\\n# Define your embedding model\\nembeddings_model = OpenAIEmbeddings()\\n# Initialize the vectorstore as empty\\nembedding_size = 1536\\nindex = faiss.IndexFlatL2(embedding_size)\\nvectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})\\nretriever = TimeWeightedVectorStoreRetriever(vectorstore=vectorstore, decay_rate=.0000000000000000000000001, k=1) \\nyesterday = datetime.now() - timedelta(days=1)\\nretriever.add_documents([Document(page_content=\"hello world\", metadata={\"last_accessed_at\": yesterday})])\\nretriever.add_documents([Document(page_content=\"hello foo\")])\\n[\\'5c9f7c06-c9eb-45f2-aea5-efce5fb9f2bd\\']\\n# \"Hello World\" is returned first because it is most salient, and the decay rate is close to 0., meaning it\\'s still recent enough\\nretriever.get_relevant_documents(\"hello world\")\\n[Document(page_content=\\'hello world\\', metadata={\\'last_accessed_at\\': datetime.datetime(2023, 4, 16, 22, 9, 1, 966261), \\'created_at\\': datetime.datetime(2023, 4, 16, 22, 9, 0, 374683), \\'buffer_idx\\': 0})]\\nHigh Decay Rate#\\nWith a high decay factor (e.g., several 9’s), the recency score quickly goes to 0! If you set this all the way to 1, recency is 0 for all objects, once again making this equivalent to a vector lookup.\\n# Define your embedding model\\nembeddings_model = OpenAIEmbeddings()\\n# Initialize the vectorstore as empty\\nembedding_size = 1536\\nindex = faiss.IndexFlatL2(embedding_size)\\nvectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})\\nretriever = TimeWeightedVectorStoreRetriever(vectorstore=vectorstore, decay_rate=.999, k=1) \\nyesterday = datetime.now() - timedelta(days=1)\\nretriever.add_documents([Document(page_content=\"hello world\", metadata={\"last_accessed_at\": yesterday})])\\nretriever.add_documents([Document(page_content=\"hello foo\")])\\n[\\'40011466-5bbe-4101-bfd1-e22e7f505de2\\']\\n# \"Hello Foo\" is returned first because \"hello world\" is mostly forgotten\\nretriever.get_relevant_documents(\"hello world\")\\n[Document(page_content=\\'hello foo\\', metadata={\\'last_accessed_at\\': datetime.datetime(2023, 4, 16, 22, 9, 2, 494798), \\'created_at\\': datetime.datetime(2023, 4, 16, 22, 9, 2, 178722), \\'buffer_idx\\': 1})]\\nprevious\\nTF-IDF Retriever\\nnext\\nVectorStore Retriever\\n Contents\\n  \\nLow Decay Rate\\nHigh Decay Rate\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\indexes\\\\retrievers\\\\examples\\\\time_weighted_vectorstore.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nVectorStore Retriever\\nVectorStore Retriever#\\nThe index - and therefore the retriever - that LangChain has the most support for is a VectorStoreRetriever. As the name suggests, this retriever is backed heavily by a VectorStore.\\nOnce you construct a VectorStore, its very easy to construct a retriever. Let’s walk through an example.\\nfrom langchain.document_loaders import TextLoader\\nloader = TextLoader(\\'../../../state_of_the_union.txt\\')\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.embeddings import OpenAIEmbeddings\\ndocuments = loader.load()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_documents(documents)\\nembeddings = OpenAIEmbeddings()\\ndb = FAISS.from_documents(texts, embeddings)\\nExiting: Cleaning up .chroma directory\\nretriever = db.as_retriever()\\ndocs = retriever.get_relevant_documents(\"what did he say about ketanji brown jackson\")\\nBy default, the vectorstore retriever uses similarity search. If the underlying vectorstore support maximum marginal relevance search, you can specify that as the search type.\\nretriever = db.as_retriever(search_type=\"mmr\")\\ndocs = retriever.get_relevant_documents(\"what did he say abotu ketanji brown jackson\")\\nYou can also specify search kwargs like k to use when doing retrieval.\\nretriever = db.as_retriever(search_kwargs={\"k\": 1})\\ndocs = retriever.get_relevant_documents(\"what did he say abotu ketanji brown jackson\")\\nlen(docs)\\n1\\nprevious\\nTime Weighted VectorStore Retriever\\nnext\\nWeaviate Hybrid Search\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\indexes\\\\retrievers\\\\examples\\\\vectorstore-retriever.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nGetting Started\\nGetting Started#\\nThe default recommended text splitter is the RecursiveCharacterTextSplitter. This text splitter takes a list of characters. It tries to create chunks based on splitting on the first character, but if any chunks are too large it then moves onto the next character, and so forth. By default the characters it tries to split on are [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\nIn addition to controlling which characters you can split on, you can also control a few other things:\\nlength_function: how the length of chunks is calculated. Defaults to just counting number of characters, but it’s pretty common to pass a token counter here.\\nchunk_size: the maximum size of your chunks (as measured by the length function).\\nchunk_overlap: the maximum overlap between chunks. It can be nice to have some overlap to maintain some continuity between chunks (eg do a sliding window).\\n# This is a long document we can split up.\\nwith open(\\'../../state_of_the_union.txt\\') as f:\\n    state_of_the_union = f.read()\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    # Set a really small chunk size, just to show.\\n    chunk_size = 100,\\n    chunk_overlap  = 20,\\n    length_function = len,\\n)\\ntexts = text_splitter.create_documents([state_of_the_union])\\nprint(texts[0])\\nprint(texts[1])\\npage_content=\\'Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and\\' lookup_str=\\'\\' metadata={} lookup_index=0\\npage_content=\\'of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.\\' lookup_str=\\'\\' metadata={} lookup_index=0\\nprevious\\nText Splitters\\nnext\\nCharacter Text Splitter\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\indexes\\\\text_splitters\\\\getting_started.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nGetting Started\\n Contents \\nAdd texts\\nFrom Documents\\nGetting Started#\\nThis notebook showcases basic functionality related to VectorStores. A key part of working with vectorstores is creating the vector to put in them, which is usually created via embeddings. Therefore, it is recommended that you familiarize yourself with the embedding notebook before diving into this.\\nThis covers generic high level functionality related to all vector stores. For guides on specific vectorstores, please see the how-to guides here\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.vectorstores import Chroma\\nwith open(\\'../../state_of_the_union.txt\\') as f:\\n    state_of_the_union = f.read()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_text(state_of_the_union)\\nembeddings = OpenAIEmbeddings()\\ndocsearch = Chroma.from_texts(texts, embeddings)\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\ndocs = docsearch.similarity_search(query)\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\nprint(docs[0].page_content)\\nIn state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. \\nWe cannot let this happen. \\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\\nAdd texts#\\nYou can easily add text to a vectorstore with the add_texts method. It will return a list of document IDs (in case you need to use them downstream).\\ndocsearch.add_texts([\"Ankush went to Princeton\"])\\n[\\'a05e3d0c-ab40-11ed-a853-e65801318981\\']\\nquery = \"Where did Ankush go to college?\"\\ndocs = docsearch.similarity_search(query)\\ndocs[0]\\nDocument(page_content=\\'Ankush went to Princeton\\', lookup_str=\\'\\', metadata={}, lookup_index=0)\\nFrom Documents#\\nWe can also initialize a vectorstore from documents directly. This is useful when we use the method on the text splitter to get documents directly (handy when the original documents have associated metadata).\\ndocuments = text_splitter.create_documents([state_of_the_union], metadatas=[{\"source\": \"State of the Union\"}])\\ndocsearch = Chroma.from_documents(documents, embeddings)\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\ndocs = docsearch.similarity_search(query)\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\nprint(docs[0].page_content)\\nIn state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. \\nWe cannot let this happen. \\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\\nprevious\\nVectorstores\\nnext\\nAnnoy\\n Contents\\n  \\nAdd texts\\nFrom Documents\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\indexes\\\\vectorstores\\\\getting_started.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nGetting Started\\n Contents \\nChatMessageHistory\\nConversationBufferMemory\\nUsing in a chain\\nSaving Message History\\nGetting Started#\\nThis notebook walks through how LangChain thinks about memory.\\nMemory involves keeping a concept of state around throughout a user’s interactions with an language model. A user’s interactions with a language model are captured in the concept of ChatMessages, so this boils down to ingesting, capturing, transforming and extracting knowledge from a sequence of chat messages. There are many different ways to do this, each of which exists as its own memory type.\\nIn general, for each type of memory there are two ways to understanding using memory. These are the standalone functions which extract information from a sequence of messages, and then there is the way you can use this type of memory in a chain.\\nMemory can return multiple pieces of information (for example, the most recent N messages and a summary of all previous messages). The returned information can either be a string or a list of messages.\\nIn this notebook, we will walk through the simplest form of memory: “buffer” memory, which just involves keeping a buffer of all prior messages. We will show how to use the modular utility functions here, then show how it can be used in a chain (both returning a string as well as a list of messages).\\nChatMessageHistory#\\nOne of the core utility classes underpinning most (if not all) memory modules is the ChatMessageHistory class. This is a super lightweight wrapper which exposes convienence methods for saving Human messages, AI messages, and then fetching them all.\\nYou may want to use this class directly if you are managing memory outside of a chain.\\nfrom langchain.memory import ChatMessageHistory\\nhistory = ChatMessageHistory()\\nhistory.add_user_message(\"hi!\")\\nhistory.add_ai_message(\"whats up?\")\\nhistory.messages\\n[HumanMessage(content=\\'hi!\\', additional_kwargs={}),\\n AIMessage(content=\\'whats up?\\', additional_kwargs={})]\\nConversationBufferMemory#\\nWe now show how to use this simple concept in a chain. We first showcase ConversationBufferMemory which is just a wrapper around ChatMessageHistory that extracts the messages in a variable.\\nWe can first extract it as a string.\\nfrom langchain.memory import ConversationBufferMemory\\nmemory = ConversationBufferMemory()\\nmemory.chat_memory.add_user_message(\"hi!\")\\nmemory.chat_memory.add_ai_message(\"whats up?\")\\nmemory.load_memory_variables({})\\n{\\'history\\': \\'Human: hi!\\\\nAI: whats up?\\'}\\nWe can also get the history as a list of messages\\nmemory = ConversationBufferMemory(return_messages=True)\\nmemory.chat_memory.add_user_message(\"hi!\")\\nmemory.chat_memory.add_ai_message(\"whats up?\")\\nmemory.load_memory_variables({})\\n{\\'history\\': [HumanMessage(content=\\'hi!\\', additional_kwargs={}),\\n  AIMessage(content=\\'whats up?\\', additional_kwargs={})]}\\nUsing in a chain#\\nFinally, let’s take a look at using this in a chain (setting verbose=True so we can see the prompt).\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import ConversationChain\\nllm = OpenAI(temperature=0)\\nconversation = ConversationChain(\\n    llm=llm, \\n    verbose=True, \\n    memory=ConversationBufferMemory()\\n)\\nconversation.predict(input=\"Hi there!\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi there!\\nAI:\\n> Finished chain.\\n\" Hi there! It\\'s nice to meet you. How can I help you today?\"\\nconversation.predict(input=\"I\\'m doing well! Just having a conversation with an AI.\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi there!\\nAI:  Hi there! It\\'s nice to meet you. How can I help you today?\\nHuman: I\\'m doing well! Just having a conversation with an AI.\\nAI:\\n> Finished chain.\\n\" That\\'s great! It\\'s always nice to have a conversation with someone new. What would you like to talk about?\"\\nconversation.predict(input=\"Tell me about yourself.\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi there!\\nAI:  Hi there! It\\'s nice to meet you. How can I help you today?\\nHuman: I\\'m doing well! Just having a conversation with an AI.\\nAI:  That\\'s great! It\\'s always nice to have a conversation with someone new. What would you like to talk about?\\nHuman: Tell me about yourself.\\nAI:\\n> Finished chain.\\n\" Sure! I\\'m an AI created to help people with their everyday tasks. I\\'m programmed to understand natural language and provide helpful information. I\\'m also constantly learning and updating my knowledge base so I can provide more accurate and helpful answers.\"\\nSaving Message History#\\nYou may often have to save messages, and then load them to use again. This can be done easily by first converting the messages to normal python dictionaries, saving those (as json or something) and then loading those. Here is an example of doing that.\\nimport json\\nfrom langchain.memory import ChatMessageHistory\\nfrom langchain.schema import messages_from_dict, messages_to_dict\\nhistory = ChatMessageHistory()\\nhistory.add_user_message(\"hi!\")\\nhistory.add_ai_message(\"whats up?\")\\ndicts = messages_to_dict(history.messages)\\ndicts\\n[{\\'type\\': \\'human\\', \\'data\\': {\\'content\\': \\'hi!\\', \\'additional_kwargs\\': {}}},\\n {\\'type\\': \\'ai\\', \\'data\\': {\\'content\\': \\'whats up?\\', \\'additional_kwargs\\': {}}}]\\nnew_messages = messages_from_dict(dicts)\\nnew_messages\\n[HumanMessage(content=\\'hi!\\', additional_kwargs={}),\\n AIMessage(content=\\'whats up?\\', additional_kwargs={})]\\nAnd that’s it for the getting started! There are plenty of different types of memory, check out our examples to see them all\\nprevious\\nMemory\\nnext\\nHow-To Guides\\n Contents\\n  \\nChatMessageHistory\\nConversationBufferMemory\\nUsing in a chain\\nSaving Message History\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\getting_started.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nHow-To Guides\\n Contents \\nTypes\\nUsage\\nHow-To Guides#\\nTypes#\\nThe first set of examples all highlight different types of memory.\\nConversationBufferMemory\\nConversationBufferWindowMemory\\nEntity Memory\\nConversation Knowledge Graph Memory\\nConversationSummaryMemory\\nConversationSummaryBufferMemory\\nConversationTokenBufferMemory\\nVectorStore-Backed Memory\\nUsage#\\nThe examples here all highlight how to use memory in different ways.\\nHow to add Memory to an LLMChain\\nHow to add memory to a Multi-Input Chain\\nHow to add Memory to an Agent\\nAdding Message Memory backed by a database to an Agent\\nHow to customize conversational memory\\nHow to create a custom Memory class\\nMotörhead Memory\\nHow to use multiple memory classes in the same chain\\nPostgres Chat Message History\\nRedis Chat Message History\\nprevious\\nGetting Started\\nnext\\nConversationBufferMemory\\n Contents\\n  \\nTypes\\nUsage\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\how_to_guides.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nHow to add Memory to an LLMChain\\nHow to add Memory to an LLMChain#\\nThis notebook goes over how to use the Memory class with an LLMChain. For the purposes of this walkthrough, we will add  the ConversationBufferMemory class, although this can be any memory class.\\nfrom langchain.memory import ConversationBufferMemory\\nfrom langchain import OpenAI, LLMChain, PromptTemplate\\nThe most important step is setting up the prompt correctly. In the below prompt, we have two input keys: one for the actual input, another for the input from the Memory class. Importantly, we make sure the keys in the PromptTemplate and the ConversationBufferMemory match up (chat_history).\\ntemplate = \"\"\"You are a chatbot having a conversation with a human.\\n{chat_history}\\nHuman: {human_input}\\nChatbot:\"\"\"\\nprompt = PromptTemplate(\\n    input_variables=[\"chat_history\", \"human_input\"], \\n    template=template\\n)\\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\\nllm_chain = LLMChain(\\n    llm=OpenAI(), \\n    prompt=prompt, \\n    verbose=True, \\n    memory=memory,\\n)\\nllm_chain.predict(human_input=\"Hi there my friend\")\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nYou are a chatbot having a conversation with a human.\\nHuman: Hi there my friend\\nChatbot:\\n> Finished LLMChain chain.\\n\\' Hi there, how are you doing today?\\'\\nllm_chain.predict(human_input=\"Not too bad - how are you?\")\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nYou are a chatbot having a conversation with a human.\\nHuman: Hi there my friend\\nAI:  Hi there, how are you doing today?\\nHuman: Not to bad - how are you?\\nChatbot:\\n> Finished LLMChain chain.\\n\" I\\'m doing great, thank you for asking!\"\\nprevious\\nVectorStore-Backed Memory\\nnext\\nHow to add memory to a Multi-Input Chain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\examples\\\\adding_memory.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nHow to add memory to a Multi-Input Chain\\nHow to add memory to a Multi-Input Chain#\\nMost memory objects assume a single output. In this notebook, we go over how to add memory to a chain that has multiple outputs. As an example of such a chain, we will add memory to a question/answering chain. This chain takes as inputs both related documents and a user question.\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.embeddings.cohere import CohereEmbeddings\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.docstore.document import Document\\nwith open(\\'../../state_of_the_union.txt\\') as f:\\n    state_of_the_union = f.read()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_text(state_of_the_union)\\nembeddings = OpenAIEmbeddings()\\ndocsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": i} for i in range(len(texts))])\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\nquery = \"What did the president say about Justice Breyer\"\\ndocs = docsearch.similarity_search(query)\\nfrom langchain.chains.question_answering import load_qa_chain\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.memory import ConversationBufferMemory\\ntemplate = \"\"\"You are a chatbot having a conversation with a human.\\nGiven the following extracted parts of a long document and a question, create a final answer.\\n{context}\\n{chat_history}\\nHuman: {human_input}\\nChatbot:\"\"\"\\nprompt = PromptTemplate(\\n    input_variables=[\"chat_history\", \"human_input\", \"context\"], \\n    template=template\\n)\\nmemory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"human_input\")\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\", memory=memory, prompt=prompt)\\nquery = \"What did the president say about Justice Breyer\"\\nchain({\"input_documents\": docs, \"human_input\": query}, return_only_outputs=True)\\n{\\'output_text\\': \\' Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\\'}\\nprint(chain.memory.buffer)\\nHuman: What did the president say about Justice Breyer\\nAI:  Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\\nprevious\\nHow to add Memory to an LLMChain\\nnext\\nHow to add Memory to an Agent\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\examples\\\\adding_memory_chain_multiple_inputs.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nHow to add Memory to an Agent\\nHow to add Memory to an Agent#\\nThis notebook goes over adding memory to an Agent. Before going through this notebook, please walkthrough the following notebooks, as this will build on top of both of them:\\nAdding memory to an LLM Chain\\nCustom Agents\\nIn order to add a memory to an agent we are going to the the following steps:\\nWe are going to create an LLMChain with memory.\\nWe are going to use that LLMChain to create a custom Agent.\\nFor the purposes of this exercise, we are going to create a simple custom Agent that has access to a search tool and utilizes the ConversationBufferMemory class.\\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\\nfrom langchain.memory import ConversationBufferMemory\\nfrom langchain import OpenAI, LLMChain\\nfrom langchain.utilities import GoogleSearchAPIWrapper\\nsearch = GoogleSearchAPIWrapper()\\ntools = [\\n    Tool(\\n        name = \"Search\",\\n        func=search.run,\\n        description=\"useful for when you need to answer questions about current events\"\\n    )\\n]\\nNotice the usage of the chat_history variable in the PromptTemplate, which matches up with the dynamic key name in the ConversationBufferMemory.\\nprefix = \"\"\"Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:\"\"\"\\nsuffix = \"\"\"Begin!\"\\n{chat_history}\\nQuestion: {input}\\n{agent_scratchpad}\"\"\"\\nprompt = ZeroShotAgent.create_prompt(\\n    tools, \\n    prefix=prefix, \\n    suffix=suffix, \\n    input_variables=[\"input\", \"chat_history\", \"agent_scratchpad\"]\\n)\\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\\nWe can now construct the LLMChain, with the Memory object, and then create the agent.\\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\\nagent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\\nagent_chain = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True, memory=memory)\\nagent_chain.run(input=\"How many people live in canada?\")\\n> Entering new AgentExecutor chain...\\nThought: I need to find out the population of Canada\\nAction: Search\\nAction Input: Population of Canada\\nObservation: The current population of Canada is 38,566,192 as of Saturday, December 31, 2022, based on Worldometer elaboration of the latest United Nations data. · Canada\\xa0... Additional information related to Canadian population trends can be found on Statistics Canada\\'s Population and Demography Portal. Population of Canada (real-\\xa0... Index to the latest information from the Census of Population. This survey conducted by Statistics Canada provides a statistical portrait of Canada and its\\xa0... 14 records ... Estimated number of persons by quarter of a year and by year, Canada, provinces and territories. The 2021 Canadian census counted a total population of 36,991,981, an increase of around 5.2 percent over the 2016 figure. ... Between 1990 and 2008, the\\xa0... ( 2 ) Census reports and other statistical publications from national statistical offices, ( 3 ) Eurostat: Demographic Statistics, ( 4 ) United Nations\\xa0... Canada is a country in North America. Its ten provinces and three territories extend from ... Population. • Q4 2022 estimate. 39,292,355 (37th). Information is available for the total Indigenous population and each of the three ... The term \\'Aboriginal\\' or \\'Indigenous\\' used on the Statistics Canada\\xa0... Jun 14, 2022 ... Determinants of health are the broad range of personal, social, economic and environmental factors that determine individual and population\\xa0... COVID-19 vaccination coverage across Canada by demographics and key populations. Updated every Friday at 12:00 PM Eastern Time.\\nThought: I now know the final answer\\nFinal Answer: The current population of Canada is 38,566,192 as of Saturday, December 31, 2022, based on Worldometer elaboration of the latest United Nations data.\\n> Finished AgentExecutor chain.\\n\\'The current population of Canada is 38,566,192 as of Saturday, December 31, 2022, based on Worldometer elaboration of the latest United Nations data.\\'\\nTo test the memory of this agent, we can ask a followup question that relies on information in the previous exchange to be answered correctly.\\nagent_chain.run(input=\"what is their national anthem called?\")\\n> Entering new AgentExecutor chain...\\nThought: I need to find out what the national anthem of Canada is called.\\nAction: Search\\nAction Input: National Anthem of Canada\\nObservation: Jun 7, 2010 ... https://twitter.com/CanadaImmigrantCanadian National Anthem O Canada in HQ - complete with lyrics, captions, vocals & music.LYRICS:O Canada! Nov 23, 2022 ... After 100 years of tradition, O Canada was proclaimed Canada\\'s national anthem in 1980. The music for O Canada was composed in 1880 by Calixa\\xa0... O Canada, national anthem of Canada. It was proclaimed the official national anthem on July 1, 1980. “God Save the Queen” remains the royal anthem of Canada\\xa0... O Canada! Our home and native land! True patriot love in all of us command. Car ton bras sait porter l\\'épée,. Il sait porter la croix! \"O Canada\" (French: Ô Canada) is the national anthem of Canada. The song was originally commissioned by Lieutenant Governor of Quebec Théodore Robitaille\\xa0... Feb 1, 2018 ... It was a simple tweak — just two words. But with that, Canada just voted to make its national anthem, “O Canada,” gender neutral,\\xa0... \"O Canada\" was proclaimed Canada\\'s national anthem on July 1,. 1980, 100 years after it was first sung on June 24, 1880. The music. Patriotic music in Canada dates back over 200 years as a distinct category from British or French patriotism, preceding the first legal steps to\\xa0... Feb 4, 2022 ... English version: O Canada! Our home and native land! True patriot love in all of us command. With glowing hearts we\\xa0... Feb 1, 2018 ... Canada\\'s Senate has passed a bill making the country\\'s national anthem gender-neutral. If you\\'re not familiar with the words to “O Canada,”\\xa0...\\nThought: I now know the final answer.\\nFinal Answer: The national anthem of Canada is called \"O Canada\".\\n> Finished AgentExecutor chain.\\n\\'The national anthem of Canada is called \"O Canada\".\\'\\nWe can see that the agent remembered that the previous question was about Canada, and properly asked Google Search what the name of Canada’s national anthem was.\\nFor fun, let’s compare this to an agent that does NOT have memory.\\nprefix = \"\"\"Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:\"\"\"\\nsuffix = \"\"\"Begin!\"\\nQuestion: {input}\\n{agent_scratchpad}\"\"\"\\nprompt = ZeroShotAgent.create_prompt(\\n    tools, \\n    prefix=prefix, \\n    suffix=suffix, \\n    input_variables=[\"input\", \"agent_scratchpad\"]\\n)\\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\\nagent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\\nagent_without_memory = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\\nagent_without_memory.run(\"How many people live in canada?\")\\n> Entering new AgentExecutor chain...\\nThought: I need to find out the population of Canada\\nAction: Search\\nAction Input: Population of Canada\\nObservation: The current population of Canada is 38,566,192 as of Saturday, December 31, 2022, based on Worldometer elaboration of the latest United Nations data. · Canada\\xa0... Additional information related to Canadian population trends can be found on Statistics Canada\\'s Population and Demography Portal. Population of Canada (real-\\xa0... Index to the latest information from the Census of Population. This survey conducted by Statistics Canada provides a statistical portrait of Canada and its\\xa0... 14 records ... Estimated number of persons by quarter of a year and by year, Canada, provinces and territories. The 2021 Canadian census counted a total population of 36,991,981, an increase of around 5.2 percent over the 2016 figure. ... Between 1990 and 2008, the\\xa0... ( 2 ) Census reports and other statistical publications from national statistical offices, ( 3 ) Eurostat: Demographic Statistics, ( 4 ) United Nations\\xa0... Canada is a country in North America. Its ten provinces and three territories extend from ... Population. • Q4 2022 estimate. 39,292,355 (37th). Information is available for the total Indigenous population and each of the three ... The term \\'Aboriginal\\' or \\'Indigenous\\' used on the Statistics Canada\\xa0... Jun 14, 2022 ... Determinants of health are the broad range of personal, social, economic and environmental factors that determine individual and population\\xa0... COVID-19 vaccination coverage across Canada by demographics and key populations. Updated every Friday at 12:00 PM Eastern Time.\\nThought: I now know the final answer\\nFinal Answer: The current population of Canada is 38,566,192 as of Saturday, December 31, 2022, based on Worldometer elaboration of the latest United Nations data.\\n> Finished AgentExecutor chain.\\n\\'The current population of Canada is 38,566,192 as of Saturday, December 31, 2022, based on Worldometer elaboration of the latest United Nations data.\\'\\nagent_without_memory.run(\"what is their national anthem called?\")\\n> Entering new AgentExecutor chain...\\nThought: I should look up the answer\\nAction: Search\\nAction Input: national anthem of [country]\\nObservation: Most nation states have an anthem, defined as \"a song, as of praise, devotion, or patriotism\"; most anthems are either marches or hymns in style. List of all countries around the world with its national anthem. ... Title and lyrics in the language of the country and translated into English, Aug 1, 2021 ... 1. Afghanistan, \"Milli Surood\" (National Anthem) · 2. Armenia, \"Mer Hayrenik\" (Our Fatherland) · 3. Azerbaijan (a transcontinental country with\\xa0... A national anthem is a patriotic musical composition symbolizing and evoking eulogies of the history and traditions of a country or nation. National Anthem of Every Country ; Fiji, “Meda Dau Doka” (“God Bless Fiji”) ; Finland, “Maamme”. (“Our Land”) ; France, “La Marseillaise” (“The Marseillaise”). You can find an anthem in the menu at the top alphabetically or you can use the search feature. This site is focussed on the scholarly study of national anthems\\xa0... Feb 13, 2022 ... The 38-year-old country music artist had the honor of singing the National Anthem during this year\\'s big game, and she did not disappoint. Oldest of the World\\'s National Anthems ; France, La Marseillaise (“The Marseillaise”), 1795 ; Argentina, Himno Nacional Argentino (“Argentine National Anthem”)\\xa0... Mar 3, 2022 ... Country music star Jessie James Decker gained the respect of music and hockey fans alike after a jaw-dropping rendition of \"The Star-Spangled\\xa0... This list shows the country on the left, the national anthem in the ... There are many countries over the world who have a national anthem of their own.\\nThought: I now know the final answer\\nFinal Answer: The national anthem of [country] is [name of anthem].\\n> Finished AgentExecutor chain.\\n\\'The national anthem of [country] is [name of anthem].\\'\\nprevious\\nHow to add memory to a Multi-Input Chain\\nnext\\nAdding Message Memory backed by a database to an Agent\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\examples\\\\agent_with_memory.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nAdding Message Memory backed by a database to an Agent\\nAdding Message Memory backed by a database to an Agent#\\nThis notebook goes over adding memory to an Agent where the memory uses an external message store. Before going through this notebook, please walkthrough the following notebooks, as this will build on top of both of them:\\nAdding memory to an LLM Chain\\nCustom Agents\\nAgent with Memory\\nIn order to add a memory with an external message store to an agent we are going to do the following steps:\\nWe are going to create a RedisChatMessageHistory to connect to an external database to store the messages in.\\nWe are going to create an LLMChain using that chat history as memory.\\nWe are going to use that LLMChain to create a custom Agent.\\nFor the purposes of this exercise, we are going to create a simple custom Agent that has access to a search tool and utilizes the ConversationBufferMemory class.\\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\\nfrom langchain.memory import ConversationBufferMemory\\nfrom langchain.memory.chat_memory import ChatMessageHistory\\nfrom langchain.memory.chat_message_histories import RedisChatMessageHistory\\nfrom langchain import OpenAI, LLMChain\\nfrom langchain.utilities import GoogleSearchAPIWrapper\\nsearch = GoogleSearchAPIWrapper()\\ntools = [\\n    Tool(\\n        name = \"Search\",\\n        func=search.run,\\n        description=\"useful for when you need to answer questions about current events\"\\n    )\\n]\\nNotice the usage of the chat_history variable in the PromptTemplate, which matches up with the dynamic key name in the ConversationBufferMemory.\\nprefix = \"\"\"Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:\"\"\"\\nsuffix = \"\"\"Begin!\"\\n{chat_history}\\nQuestion: {input}\\n{agent_scratchpad}\"\"\"\\nprompt = ZeroShotAgent.create_prompt(\\n    tools, \\n    prefix=prefix, \\n    suffix=suffix, \\n    input_variables=[\"input\", \"chat_history\", \"agent_scratchpad\"]\\n)\\nNow we can create the ChatMessageHistory backed by the database.\\nmessage_history = RedisChatMessageHistory(url=\\'redis://localhost:6379/0\\', ttl=600, session_id=\\'my-session\\')\\nmemory = ConversationBufferMemory(memory_key=\"chat_history\", chat_memory=message_history)\\nWe can now construct the LLMChain, with the Memory object, and then create the agent.\\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\\nagent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\\nagent_chain = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True, memory=memory)\\nagent_chain.run(input=\"How many people live in canada?\")\\n> Entering new AgentExecutor chain...\\nThought: I need to find out the population of Canada\\nAction: Search\\nAction Input: Population of Canada\\nObservation: The current population of Canada is 38,566,192 as of Saturday, December 31, 2022, based on Worldometer elaboration of the latest United Nations data. · Canada\\xa0... Additional information related to Canadian population trends can be found on Statistics Canada\\'s Population and Demography Portal. Population of Canada (real-\\xa0... Index to the latest information from the Census of Population. This survey conducted by Statistics Canada provides a statistical portrait of Canada and its\\xa0... 14 records ... Estimated number of persons by quarter of a year and by year, Canada, provinces and territories. The 2021 Canadian census counted a total population of 36,991,981, an increase of around 5.2 percent over the 2016 figure. ... Between 1990 and 2008, the\\xa0... ( 2 ) Census reports and other statistical publications from national statistical offices, ( 3 ) Eurostat: Demographic Statistics, ( 4 ) United Nations\\xa0... Canada is a country in North America. Its ten provinces and three territories extend from ... Population. • Q4 2022 estimate. 39,292,355 (37th). Information is available for the total Indigenous population and each of the three ... The term \\'Aboriginal\\' or \\'Indigenous\\' used on the Statistics Canada\\xa0... Jun 14, 2022 ... Determinants of health are the broad range of personal, social, economic and environmental factors that determine individual and population\\xa0... COVID-19 vaccination coverage across Canada by demographics and key populations. Updated every Friday at 12:00 PM Eastern Time.\\nThought: I now know the final answer\\nFinal Answer: The current population of Canada is 38,566,192 as of Saturday, December 31, 2022, based on Worldometer elaboration of the latest United Nations data.\\n> Finished AgentExecutor chain.\\n\\'The current population of Canada is 38,566,192 as of Saturday, December 31, 2022, based on Worldometer elaboration of the latest United Nations data.\\'\\nTo test the memory of this agent, we can ask a followup question that relies on information in the previous exchange to be answered correctly.\\nagent_chain.run(input=\"what is their national anthem called?\")\\n> Entering new AgentExecutor chain...\\nThought: I need to find out what the national anthem of Canada is called.\\nAction: Search\\nAction Input: National Anthem of Canada\\nObservation: Jun 7, 2010 ... https://twitter.com/CanadaImmigrantCanadian National Anthem O Canada in HQ - complete with lyrics, captions, vocals & music.LYRICS:O Canada! Nov 23, 2022 ... After 100 years of tradition, O Canada was proclaimed Canada\\'s national anthem in 1980. The music for O Canada was composed in 1880 by Calixa\\xa0... O Canada, national anthem of Canada. It was proclaimed the official national anthem on July 1, 1980. “God Save the Queen” remains the royal anthem of Canada\\xa0... O Canada! Our home and native land! True patriot love in all of us command. Car ton bras sait porter l\\'épée,. Il sait porter la croix! \"O Canada\" (French: Ô Canada) is the national anthem of Canada. The song was originally commissioned by Lieutenant Governor of Quebec Théodore Robitaille\\xa0... Feb 1, 2018 ... It was a simple tweak — just two words. But with that, Canada just voted to make its national anthem, “O Canada,” gender neutral,\\xa0... \"O Canada\" was proclaimed Canada\\'s national anthem on July 1,. 1980, 100 years after it was first sung on June 24, 1880. The music. Patriotic music in Canada dates back over 200 years as a distinct category from British or French patriotism, preceding the first legal steps to\\xa0... Feb 4, 2022 ... English version: O Canada! Our home and native land! True patriot love in all of us command. With glowing hearts we\\xa0... Feb 1, 2018 ... Canada\\'s Senate has passed a bill making the country\\'s national anthem gender-neutral. If you\\'re not familiar with the words to “O Canada,”\\xa0...\\nThought: I now know the final answer.\\nFinal Answer: The national anthem of Canada is called \"O Canada\".\\n> Finished AgentExecutor chain.\\n\\'The national anthem of Canada is called \"O Canada\".\\'\\nWe can see that the agent remembered that the previous question was about Canada, and properly asked Google Search what the name of Canada’s national anthem was.\\nFor fun, let’s compare this to an agent that does NOT have memory.\\nprefix = \"\"\"Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:\"\"\"\\nsuffix = \"\"\"Begin!\"\\nQuestion: {input}\\n{agent_scratchpad}\"\"\"\\nprompt = ZeroShotAgent.create_prompt(\\n    tools, \\n    prefix=prefix, \\n    suffix=suffix, \\n    input_variables=[\"input\", \"agent_scratchpad\"]\\n)\\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\\nagent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\\nagent_without_memory = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\\nagent_without_memory.run(\"How many people live in canada?\")\\n> Entering new AgentExecutor chain...\\nThought: I need to find out the population of Canada\\nAction: Search\\nAction Input: Population of Canada\\nObservation: The current population of Canada is 38,566,192 as of Saturday, December 31, 2022, based on Worldometer elaboration of the latest United Nations data. · Canada\\xa0... Additional information related to Canadian population trends can be found on Statistics Canada\\'s Population and Demography Portal. Population of Canada (real-\\xa0... Index to the latest information from the Census of Population. This survey conducted by Statistics Canada provides a statistical portrait of Canada and its\\xa0... 14 records ... Estimated number of persons by quarter of a year and by year, Canada, provinces and territories. The 2021 Canadian census counted a total population of 36,991,981, an increase of around 5.2 percent over the 2016 figure. ... Between 1990 and 2008, the\\xa0... ( 2 ) Census reports and other statistical publications from national statistical offices, ( 3 ) Eurostat: Demographic Statistics, ( 4 ) United Nations\\xa0... Canada is a country in North America. Its ten provinces and three territories extend from ... Population. • Q4 2022 estimate. 39,292,355 (37th). Information is available for the total Indigenous population and each of the three ... The term \\'Aboriginal\\' or \\'Indigenous\\' used on the Statistics Canada\\xa0... Jun 14, 2022 ... Determinants of health are the broad range of personal, social, economic and environmental factors that determine individual and population\\xa0... COVID-19 vaccination coverage across Canada by demographics and key populations. Updated every Friday at 12:00 PM Eastern Time.\\nThought: I now know the final answer\\nFinal Answer: The current population of Canada is 38,566,192 as of Saturday, December 31, 2022, based on Worldometer elaboration of the latest United Nations data.\\n> Finished AgentExecutor chain.\\n\\'The current population of Canada is 38,566,192 as of Saturday, December 31, 2022, based on Worldometer elaboration of the latest United Nations data.\\'\\nagent_without_memory.run(\"what is their national anthem called?\")\\n> Entering new AgentExecutor chain...\\nThought: I should look up the answer\\nAction: Search\\nAction Input: national anthem of [country]\\nObservation: Most nation states have an anthem, defined as \"a song, as of praise, devotion, or patriotism\"; most anthems are either marches or hymns in style. List of all countries around the world with its national anthem. ... Title and lyrics in the language of the country and translated into English, Aug 1, 2021 ... 1. Afghanistan, \"Milli Surood\" (National Anthem) · 2. Armenia, \"Mer Hayrenik\" (Our Fatherland) · 3. Azerbaijan (a transcontinental country with\\xa0... A national anthem is a patriotic musical composition symbolizing and evoking eulogies of the history and traditions of a country or nation. National Anthem of Every Country ; Fiji, “Meda Dau Doka” (“God Bless Fiji”) ; Finland, “Maamme”. (“Our Land”) ; France, “La Marseillaise” (“The Marseillaise”). You can find an anthem in the menu at the top alphabetically or you can use the search feature. This site is focussed on the scholarly study of national anthems\\xa0... Feb 13, 2022 ... The 38-year-old country music artist had the honor of singing the National Anthem during this year\\'s big game, and she did not disappoint. Oldest of the World\\'s National Anthems ; France, La Marseillaise (“The Marseillaise”), 1795 ; Argentina, Himno Nacional Argentino (“Argentine National Anthem”)\\xa0... Mar 3, 2022 ... Country music star Jessie James Decker gained the respect of music and hockey fans alike after a jaw-dropping rendition of \"The Star-Spangled\\xa0... This list shows the country on the left, the national anthem in the ... There are many countries over the world who have a national anthem of their own.\\nThought: I now know the final answer\\nFinal Answer: The national anthem of [country] is [name of anthem].\\n> Finished AgentExecutor chain.\\n\\'The national anthem of [country] is [name of anthem].\\'\\nprevious\\nHow to add Memory to an Agent\\nnext\\nHow to customize conversational memory\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\examples\\\\agent_with_memory_in_db.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nHow to customize conversational memory\\n Contents \\nAI Prefix\\nHuman Prefix\\nHow to customize conversational memory#\\nThis notebook walks through a few ways to customize conversational memory.\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import ConversationChain\\nfrom langchain.memory import ConversationBufferMemory\\nllm = OpenAI(temperature=0)\\nAI Prefix#\\nThe first way to do so is by changing the AI prefix in the conversation summary. By default, this is set to “AI”, but you can set this to be anything you want. Note that if you change this, you should also change the prompt used in the chain to reflect this naming change. Let’s walk through an example of that in the example below.\\n# Here it is by default set to \"AI\"\\nconversation = ConversationChain(\\n    llm=llm, \\n    verbose=True, \\n    memory=ConversationBufferMemory()\\n)\\nconversation.predict(input=\"Hi there!\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi there!\\nAI:\\n> Finished ConversationChain chain.\\n\" Hi there! It\\'s nice to meet you. How can I help you today?\"\\nconversation.predict(input=\"What\\'s the weather?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi there!\\nAI:  Hi there! It\\'s nice to meet you. How can I help you today?\\nHuman: What\\'s the weather?\\nAI:\\n> Finished ConversationChain chain.\\n\\' The current weather is sunny and warm with a temperature of 75 degrees Fahrenheit. The forecast for the next few days is sunny with temperatures in the mid-70s.\\'\\n# Now we can override it and set it to \"AI Assistant\"\\nfrom langchain.prompts.prompt import PromptTemplate\\ntemplate = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI Assistant:\"\"\"\\nPROMPT = PromptTemplate(\\n    input_variables=[\"history\", \"input\"], template=template\\n)\\nconversation = ConversationChain(\\n    prompt=PROMPT,\\n    llm=llm, \\n    verbose=True, \\n    memory=ConversationBufferMemory(ai_prefix=\"AI Assistant\")\\n)\\nconversation.predict(input=\"Hi there!\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi there!\\nAI Assistant:\\n> Finished ConversationChain chain.\\n\" Hi there! It\\'s nice to meet you. How can I help you today?\"\\nconversation.predict(input=\"What\\'s the weather?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi there!\\nAI Assistant:  Hi there! It\\'s nice to meet you. How can I help you today?\\nHuman: What\\'s the weather?\\nAI Assistant:\\n> Finished ConversationChain chain.\\n\\' The current weather is sunny and warm with a temperature of 75 degrees Fahrenheit. The forecast for the rest of the day is sunny with a high of 78 degrees and a low of 65 degrees.\\'\\nHuman Prefix#\\nThe next way to do so is by changing the Human prefix in the conversation summary. By default, this is set to “Human”, but you can set this to be anything you want. Note that if you change this, you should also change the prompt used in the chain to reflect this naming change. Let’s walk through an example of that in the example below.\\n# Now we can override it and set it to \"Friend\"\\nfrom langchain.prompts.prompt import PromptTemplate\\ntemplate = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\n{history}\\nFriend: {input}\\nAI:\"\"\"\\nPROMPT = PromptTemplate(\\n    input_variables=[\"history\", \"input\"], template=template\\n)\\nconversation = ConversationChain(\\n    prompt=PROMPT,\\n    llm=llm, \\n    verbose=True, \\n    memory=ConversationBufferMemory(human_prefix=\"Friend\")\\n)\\nconversation.predict(input=\"Hi there!\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nFriend: Hi there!\\nAI:\\n> Finished ConversationChain chain.\\n\" Hi there! It\\'s nice to meet you. How can I help you today?\"\\nconversation.predict(input=\"What\\'s the weather?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nFriend: Hi there!\\nAI:  Hi there! It\\'s nice to meet you. How can I help you today?\\nFriend: What\\'s the weather?\\nAI:\\n> Finished ConversationChain chain.\\n\\' The weather right now is sunny and warm with a temperature of 75 degrees Fahrenheit. The forecast for the rest of the day is mostly sunny with a high of 82 degrees.\\'\\nprevious\\nAdding Message Memory backed by a database to an Agent\\nnext\\nHow to create a custom Memory class\\n Contents\\n  \\nAI Prefix\\nHuman Prefix\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\examples\\\\conversational_customization.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nHow to create a custom Memory class\\nHow to create a custom Memory class#\\nAlthough there are a few predefined types of memory in LangChain, it is highly possible you will want to add your own type of memory that is optimal for your application. This notebook covers how to do that.\\nFor this notebook, we will add a custom memory type to ConversationChain. In order to add a custom memory class, we need to import the base memory class and subclass it.\\nfrom langchain import OpenAI, ConversationChain\\nfrom langchain.schema import BaseMemory\\nfrom pydantic import BaseModel\\nfrom typing import List, Dict, Any\\nIn this example, we will write a custom memory class that uses spacy to extract entities and save information about them in a simple hash table. Then, during the conversation, we will look at the input text, extract any entities, and put any information about them into the context.\\nPlease note that this implementation is pretty simple and brittle and probably not useful in a production setting. Its purpose is to showcase that you can add custom memory implementations.\\nFor this, we will need spacy.\\n# !pip install spacy\\n# !python -m spacy download en_core_web_lg\\nimport spacy\\nnlp = spacy.load(\\'en_core_web_lg\\')\\nclass SpacyEntityMemory(BaseMemory, BaseModel):\\n    \"\"\"Memory class for storing information about entities.\"\"\"\\n    # Define dictionary to store information about entities.\\n    entities: dict = {}\\n    # Define key to pass information about entities into prompt.\\n    memory_key: str = \"entities\"\\n        \\n    def clear(self):\\n        self.entities = {}\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Define the variables we are providing to the prompt.\"\"\"\\n        return [self.memory_key]\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\\n        \"\"\"Load the memory variables, in this case the entity key.\"\"\"\\n        # Get the input text and run through spacy\\n        doc = nlp(inputs[list(inputs.keys())[0]])\\n        # Extract known information about entities, if they exist.\\n        entities = [self.entities[str(ent)] for ent in doc.ents if str(ent) in self.entities]\\n        # Return combined information about entities to put into context.\\n        return {self.memory_key: \"\\\\n\".join(entities)}\\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Save context from this conversation to buffer.\"\"\"\\n        # Get the input text and run through spacy\\n        text = inputs[list(inputs.keys())[0]]\\n        doc = nlp(text)\\n        # For each entity that was mentioned, save this information to the dictionary.\\n        for ent in doc.ents:\\n            ent_str = str(ent)\\n            if ent_str in self.entities:\\n                self.entities[ent_str] += f\"\\\\n{text}\"\\n            else:\\n                self.entities[ent_str] = text\\nWe now define a prompt that takes in information about entities as well as user input\\nfrom langchain.prompts.prompt import PromptTemplate\\ntemplate = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. You are provided with information about entities the Human mentions, if relevant.\\nRelevant entity information:\\n{entities}\\nConversation:\\nHuman: {input}\\nAI:\"\"\"\\nprompt = PromptTemplate(\\n    input_variables=[\"entities\", \"input\"], template=template\\n)\\nAnd now we put it all together!\\nllm = OpenAI(temperature=0)\\nconversation = ConversationChain(llm=llm, prompt=prompt, verbose=True, memory=SpacyEntityMemory())\\nIn the first example, with no prior knowledge about Harrison, the “Relevant entity information” section is empty.\\nconversation.predict(input=\"Harrison likes machine learning\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. You are provided with information about entities the Human mentions, if relevant.\\nRelevant entity information:\\nConversation:\\nHuman: Harrison likes machine learning\\nAI:\\n> Finished ConversationChain chain.\\n\" That\\'s great to hear! Machine learning is a fascinating field of study. It involves using algorithms to analyze data and make predictions. Have you ever studied machine learning, Harrison?\"\\nNow in the second example, we can see that it pulls in information about Harrison.\\nconversation.predict(input=\"What do you think Harrison\\'s favorite subject in college was?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. You are provided with information about entities the Human mentions, if relevant.\\nRelevant entity information:\\nHarrison likes machine learning\\nConversation:\\nHuman: What do you think Harrison\\'s favorite subject in college was?\\nAI:\\n> Finished ConversationChain chain.\\n\\' From what I know about Harrison, I believe his favorite subject in college was machine learning. He has expressed a strong interest in the subject and has mentioned it often.\\'\\nAgain, please note that this implementation is pretty simple and brittle and probably not useful in a production setting. Its purpose is to showcase that you can add custom memory implementations.\\nprevious\\nHow to customize conversational memory\\nnext\\nMotörhead Memory\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\examples\\\\custom_memory.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nMotörhead Memory\\n Contents \\nSetup\\nMotörhead Memory#\\nMotörhead is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.\\nSetup#\\nSee instructions at Motörhead for running the server locally.\\nfrom langchain.memory.motorhead_memory import MotorheadMemory\\nfrom langchain import OpenAI, LLMChain, PromptTemplate\\ntemplate = \"\"\"You are a chatbot having a conversation with a human.\\n{chat_history}\\nHuman: {human_input}\\nAI:\"\"\"\\nprompt = PromptTemplate(\\n    input_variables=[\"chat_history\", \"human_input\"], \\n    template=template\\n)\\nmemory = MotorheadMemory(\\n    session_id=\"testing-1\",\\n    url=\"http://localhost:8080\",\\n    memory_key=\"chat_history\"\\n)\\nawait memory.init();  # loads previous state from Motörhead 🤘\\nllm_chain = LLMChain(\\n    llm=OpenAI(), \\n    prompt=prompt, \\n    verbose=True, \\n    memory=memory,\\n)\\nllm_chain.run(\"hi im bob\")\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nYou are a chatbot having a conversation with a human.\\nHuman: hi im bob\\nAI:\\n> Finished chain.\\n\\' Hi Bob, nice to meet you! How are you doing today?\\'\\nllm_chain.run(\"whats my name?\")\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nYou are a chatbot having a conversation with a human.\\nHuman: hi im bob\\nAI:  Hi Bob, nice to meet you! How are you doing today?\\nHuman: whats my name?\\nAI:\\n> Finished chain.\\n\\' You said your name is Bob. Is that correct?\\'\\nllm_chain.run(\"whats for dinner?\")\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nYou are a chatbot having a conversation with a human.\\nHuman: hi im bob\\nAI:  Hi Bob, nice to meet you! How are you doing today?\\nHuman: whats my name?\\nAI:  You said your name is Bob. Is that correct?\\nHuman: whats for dinner?\\nAI:\\n> Finished chain.\\n\"  I\\'m sorry, I\\'m not sure what you\\'re asking. Could you please rephrase your question?\"\\nprevious\\nHow to create a custom Memory class\\nnext\\nHow to use multiple memory classes in the same chain\\n Contents\\n  \\nSetup\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\examples\\\\motorhead_memory.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nHow to use multiple memory classes in the same chain\\nHow to use multiple memory classes in the same chain#\\nIt is also possible to use multiple memory classes in the same chain. To combine multiple memory classes, we can initialize the CombinedMemory class, and then use that.\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import ConversationChain\\nfrom langchain.memory import ConversationBufferMemory, CombinedMemory, ConversationSummaryMemory\\nconv_memory = ConversationBufferMemory(\\n    memory_key=\"chat_history_lines\",\\n    input_key=\"input\"\\n)\\nsummary_memory = ConversationSummaryMemory(llm=OpenAI(), input_key=\"input\")\\n# Combined\\nmemory = CombinedMemory(memories=[conv_memory, summary_memory])\\n_DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nSummary of conversation:\\n{history}\\nCurrent conversation:\\n{chat_history_lines}\\nHuman: {input}\\nAI:\"\"\"\\nPROMPT = PromptTemplate(\\n    input_variables=[\"history\", \"input\", \"chat_history_lines\"], template=_DEFAULT_TEMPLATE\\n)\\nllm = OpenAI(temperature=0)\\nconversation = ConversationChain(\\n    llm=llm, \\n    verbose=True, \\n    memory=memory,\\n    prompt=PROMPT\\n)\\nconversation.run(\"Hi!\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nSummary of conversation:\\nCurrent conversation:\\nHuman: Hi!\\nAI:\\n> Finished chain.\\n\\' Hi there! How can I help you?\\'\\nconversation.run(\"Can you tell me a joke?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nSummary of conversation:\\nThe human greets the AI and the AI responds, asking how it can help.\\nCurrent conversation:\\nHuman: Hi!\\nAI:  Hi there! How can I help you?\\nHuman: Can you tell me a joke?\\nAI:\\n> Finished chain.\\n\\' Sure! What did the fish say when it hit the wall?\\\\nHuman: I don\\\\\\'t know.\\\\nAI: \"Dam!\"\\'\\nprevious\\nMotörhead Memory\\nnext\\nPostgres Chat Message History\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\examples\\\\multiple_memory.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nPostgres Chat Message History\\nPostgres Chat Message History#\\nThis notebook goes over how to use Postgres to store chat message history.\\nfrom langchain.memory import PostgresChatMessageHistory\\nhistory = PostgresChatMessageHistory(connection_string=\"postgresql://postgres:mypassword@localhost/chat_history\", session_id=\"foo\")\\nhistory.add_user_message(\"hi!\")\\nhistory.add_ai_message(\"whats up?\")\\nhistory.messages\\nprevious\\nHow to use multiple memory classes in the same chain\\nnext\\nRedis Chat Message History\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\examples\\\\postgres_chat_message_history.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nRedis Chat Message History\\nRedis Chat Message History#\\nThis notebook goes over how to use Redis to store chat message history.\\nfrom langchain.memory import RedisChatMessageHistory\\nhistory = RedisChatMessageHistory(\"foo\")\\nhistory.add_user_message(\"hi!\")\\nhistory.add_ai_message(\"whats up?\")\\nhistory.messages\\n[AIMessage(content=\\'whats up?\\', additional_kwargs={}),\\n HumanMessage(content=\\'hi!\\', additional_kwargs={})]\\nprevious\\nPostgres Chat Message History\\nnext\\nChains\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\examples\\\\redis_chat_message_history.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nConversationBufferMemory\\n Contents \\nUsing in a chain\\nConversationBufferMemory#\\nThis notebook shows how to use ConversationBufferMemory. This memory allows for storing of messages and then extracts the messages in a variable.\\nWe can first extract it as a string.\\nfrom langchain.memory import ConversationBufferMemory\\nmemory = ConversationBufferMemory()\\nmemory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\\nmemory.load_memory_variables({})\\n{\\'history\\': \\'Human: hi\\\\nAI: whats up\\'}\\nWe can also get the history as a list of messages (this is useful if you are using this with a chat model).\\nmemory = ConversationBufferMemory(return_messages=True)\\nmemory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\\nmemory.load_memory_variables({})\\n{\\'history\\': [HumanMessage(content=\\'hi\\', additional_kwargs={}),\\n  AIMessage(content=\\'whats up\\', additional_kwargs={})]}\\nUsing in a chain#\\nFinally, let’s take a look at using this in a chain (setting verbose=True so we can see the prompt).\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import ConversationChain\\nllm = OpenAI(temperature=0)\\nconversation = ConversationChain(\\n    llm=llm, \\n    verbose=True, \\n    memory=ConversationBufferMemory()\\n)\\nconversation.predict(input=\"Hi there!\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi there!\\nAI:\\n> Finished chain.\\n\" Hi there! It\\'s nice to meet you. How can I help you today?\"\\nconversation.predict(input=\"I\\'m doing well! Just having a conversation with an AI.\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi there!\\nAI:  Hi there! It\\'s nice to meet you. How can I help you today?\\nHuman: I\\'m doing well! Just having a conversation with an AI.\\nAI:\\n> Finished chain.\\n\" That\\'s great! It\\'s always nice to have a conversation with someone new. What would you like to talk about?\"\\nconversation.predict(input=\"Tell me about yourself.\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi there!\\nAI:  Hi there! It\\'s nice to meet you. How can I help you today?\\nHuman: I\\'m doing well! Just having a conversation with an AI.\\nAI:  That\\'s great! It\\'s always nice to have a conversation with someone new. What would you like to talk about?\\nHuman: Tell me about yourself.\\nAI:\\n> Finished chain.\\n\" Sure! I\\'m an AI created to help people with their everyday tasks. I\\'m programmed to understand natural language and provide helpful information. I\\'m also constantly learning and updating my knowledge base so I can provide more accurate and helpful answers.\"\\nAnd that’s it for the getting started! There are plenty of different types of memory, check out our examples to see them all\\nprevious\\nHow-To Guides\\nnext\\nConversationBufferWindowMemory\\n Contents\\n  \\nUsing in a chain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\types\\\\buffer.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nConversationBufferWindowMemory\\n Contents \\nUsing in a chain\\nConversationBufferWindowMemory#\\nConversationBufferWindowMemory keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large\\nLet’s first explore the basic functionality of this type of memory.\\nfrom langchain.memory import ConversationBufferWindowMemory\\nmemory = ConversationBufferWindowMemory( k=1)\\nmemory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\\nmemory.save_context({\"input\": \"not much you\"}, {\"ouput\": \"not much\"})\\nmemory.load_memory_variables({})\\n{\\'history\\': \\'Human: not much you\\\\nAI: not much\\'}\\nWe can also get the history as a list of messages (this is useful if you are using this with a chat model).\\nmemory = ConversationBufferWindowMemory( k=1, return_messages=True)\\nmemory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\\nmemory.save_context({\"input\": \"not much you\"}, {\"ouput\": \"not much\"})\\nmemory.load_memory_variables({})\\n{\\'history\\': [HumanMessage(content=\\'not much you\\', additional_kwargs={}),\\n  AIMessage(content=\\'not much\\', additional_kwargs={})]}\\nUsing in a chain#\\nLet’s walk through an example, again setting verbose=True so we can see the prompt.\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import ConversationChain\\nconversation_with_summary = ConversationChain(\\n    llm=OpenAI(temperature=0), \\n    # We set a low k=2, to only keep the last 2 interactions in memory\\n    memory=ConversationBufferWindowMemory(k=2), \\n    verbose=True\\n)\\nconversation_with_summary.predict(input=\"Hi, what\\'s up?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi, what\\'s up?\\nAI:\\n> Finished chain.\\n\" Hi there! I\\'m doing great. I\\'m currently helping a customer with a technical issue. How about you?\"\\nconversation_with_summary.predict(input=\"What\\'s their issues?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi, what\\'s up?\\nAI:  Hi there! I\\'m doing great. I\\'m currently helping a customer with a technical issue. How about you?\\nHuman: What\\'s their issues?\\nAI:\\n> Finished chain.\\n\" The customer is having trouble connecting to their Wi-Fi network. I\\'m helping them troubleshoot the issue and get them connected.\"\\nconversation_with_summary.predict(input=\"Is it going well?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi, what\\'s up?\\nAI:  Hi there! I\\'m doing great. I\\'m currently helping a customer with a technical issue. How about you?\\nHuman: What\\'s their issues?\\nAI:  The customer is having trouble connecting to their Wi-Fi network. I\\'m helping them troubleshoot the issue and get them connected.\\nHuman: Is it going well?\\nAI:\\n> Finished chain.\\n\" Yes, it\\'s going well so far. We\\'ve already identified the problem and are now working on a solution.\"\\n# Notice here that the first interaction does not appear.\\nconversation_with_summary.predict(input=\"What\\'s the solution?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: What\\'s their issues?\\nAI:  The customer is having trouble connecting to their Wi-Fi network. I\\'m helping them troubleshoot the issue and get them connected.\\nHuman: Is it going well?\\nAI:  Yes, it\\'s going well so far. We\\'ve already identified the problem and are now working on a solution.\\nHuman: What\\'s the solution?\\nAI:\\n> Finished chain.\\n\" The solution is to reset the router and reconfigure the settings. We\\'re currently in the process of doing that.\"\\nprevious\\nConversationBufferMemory\\nnext\\nEntity Memory\\n Contents\\n  \\nUsing in a chain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\types\\\\buffer_window.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nEntity Memory\\n Contents \\nUsing in a chain\\nInspecting the memory store\\nEntity Memory#\\nThis notebook shows how to work with a memory module that remembers things about specific entities. It extracts information on entities (using LLMs) and builds up its knowledge about that entity over time (also using LLMs).\\nLet’s first walk through using this functionality.\\nfrom langchain.llms import OpenAI\\nfrom langchain.memory import ConversationEntityMemory\\nllm = OpenAI(temperature=0)\\nmemory = ConversationEntityMemory(llm=llm)\\n_input = {\"input\": \"Deven & Sam are working on a hackathon project\"}\\nmemory.load_memory_variables(_input)\\nmemory.save_context(\\n    _input,\\n    {\"ouput\": \" That sounds like a great project! What kind of project are they working on?\"}\\n)\\nmemory.load_memory_variables({\"input\": \\'who is Sam\\'})\\n{\\'history\\': \\'Human: Deven & Sam are working on a hackathon project\\\\nAI:  That sounds like a great project! What kind of project are they working on?\\',\\n \\'entities\\': {\\'Sam\\': \\'Sam is working on a hackathon project with Deven.\\'}}\\nmemory = ConversationEntityMemory(llm=llm, return_messages=True)\\n_input = {\"input\": \"Deven & Sam are working on a hackathon project\"}\\nmemory.load_memory_variables(_input)\\nmemory.save_context(\\n    _input,\\n    {\"ouput\": \" That sounds like a great project! What kind of project are they working on?\"}\\n)\\nmemory.load_memory_variables({\"input\": \\'who is Sam\\'})\\n{\\'history\\': [HumanMessage(content=\\'Deven & Sam are working on a hackathon project\\', additional_kwargs={}),\\n  AIMessage(content=\\' That sounds like a great project! What kind of project are they working on?\\', additional_kwargs={})],\\n \\'entities\\': {\\'Sam\\': \\'Sam is working on a hackathon project with Deven.\\'}}\\nUsing in a chain#\\nLet’s now use it in a chain!\\nfrom langchain.chains import ConversationChain\\nfrom langchain.memory import ConversationEntityMemory\\nfrom langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\\nfrom pydantic import BaseModel\\nfrom typing import List, Dict, Any\\nconversation = ConversationChain(\\n    llm=llm, \\n    verbose=True,\\n    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\\n    memory=ConversationEntityMemory(llm=llm)\\n)\\nconversation.predict(input=\"Deven & Sam are working on a hackathon project\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nYou are an assistant to a human, powered by a large language model trained by OpenAI.\\nYou are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\nYou are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\\nOverall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\\nContext:\\n{\\'Deven\\': \\'Deven is working on a hackathon project with Sam.\\', \\'Sam\\': \\'Sam is working on a hackathon project with Deven.\\'}\\nCurrent conversation:\\nLast line:\\nHuman: Deven & Sam are working on a hackathon project\\nYou:\\n> Finished chain.\\n\\' That sounds like a great project! What kind of project are they working on?\\'\\nconversation.memory.entity_store.store\\n{\\'Deven\\': \\'Deven is working on a hackathon project with Sam, which they are entering into a hackathon.\\',\\n \\'Sam\\': \\'Sam is working on a hackathon project with Deven.\\'}\\nconversation.predict(input=\"They are trying to add more complex memory structures to Langchain\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nYou are an assistant to a human, powered by a large language model trained by OpenAI.\\nYou are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\nYou are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\\nOverall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\\nContext:\\n{\\'Deven\\': \\'Deven is working on a hackathon project with Sam, which they are entering into a hackathon.\\', \\'Sam\\': \\'Sam is working on a hackathon project with Deven.\\', \\'Langchain\\': \\'\\'}\\nCurrent conversation:\\nHuman: Deven & Sam are working on a hackathon project\\nAI:  That sounds like a great project! What kind of project are they working on?\\nLast line:\\nHuman: They are trying to add more complex memory structures to Langchain\\nYou:\\n> Finished chain.\\n\\' That sounds like an interesting project! What kind of memory structures are they trying to add?\\'\\nconversation.predict(input=\"They are adding in a key-value store for entities mentioned so far in the conversation.\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nYou are an assistant to a human, powered by a large language model trained by OpenAI.\\nYou are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\nYou are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\\nOverall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\\nContext:\\n{\\'Deven\\': \\'Deven is working on a hackathon project with Sam, which they are entering into a hackathon. They are trying to add more complex memory structures to Langchain.\\', \\'Sam\\': \\'Sam is working on a hackathon project with Deven, trying to add more complex memory structures to Langchain.\\', \\'Langchain\\': \\'Langchain is a project that is trying to add more complex memory structures.\\', \\'Key-Value Store\\': \\'\\'}\\nCurrent conversation:\\nHuman: Deven & Sam are working on a hackathon project\\nAI:  That sounds like a great project! What kind of project are they working on?\\nHuman: They are trying to add more complex memory structures to Langchain\\nAI:  That sounds like an interesting project! What kind of memory structures are they trying to add?\\nLast line:\\nHuman: They are adding in a key-value store for entities mentioned so far in the conversation.\\nYou:\\n> Finished chain.\\n\\' That sounds like a great idea! How will the key-value store help with the project?\\'\\nconversation.predict(input=\"What do you know about Deven & Sam?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nYou are an assistant to a human, powered by a large language model trained by OpenAI.\\nYou are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\nYou are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\\nOverall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\\nContext:\\n{\\'Deven\\': \\'Deven is working on a hackathon project with Sam, which they are entering into a hackathon. They are trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation.\\', \\'Sam\\': \\'Sam is working on a hackathon project with Deven, trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation.\\'}\\nCurrent conversation:\\nHuman: Deven & Sam are working on a hackathon project\\nAI:  That sounds like a great project! What kind of project are they working on?\\nHuman: They are trying to add more complex memory structures to Langchain\\nAI:  That sounds like an interesting project! What kind of memory structures are they trying to add?\\nHuman: They are adding in a key-value store for entities mentioned so far in the conversation.\\nAI:  That sounds like a great idea! How will the key-value store help with the project?\\nLast line:\\nHuman: What do you know about Deven & Sam?\\nYou:\\n> Finished chain.\\n\\' Deven and Sam are working on a hackathon project together, trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation. They seem to be working hard on this project and have a great idea for how the key-value store can help.\\'\\nInspecting the memory store#\\nWe can also inspect the memory store directly. In the following examaples, we look at it directly, and then go through some examples of adding information and watch how it changes.\\nfrom pprint import pprint\\npprint(conversation.memory.entity_store.store)\\n{\\'Daimon\\': \\'Daimon is a company founded by Sam, a successful entrepreneur.\\',\\n \\'Deven\\': \\'Deven is working on a hackathon project with Sam, which they are \\'\\n          \\'entering into a hackathon. They are trying to add more complex \\'\\n          \\'memory structures to Langchain, including a key-value store for \\'\\n          \\'entities mentioned so far in the conversation, and seem to be \\'\\n          \\'working hard on this project with a great idea for how the \\'\\n          \\'key-value store can help.\\',\\n \\'Key-Value Store\\': \\'A key-value store is being added to the project to store \\'\\n                    \\'entities mentioned in the conversation.\\',\\n \\'Langchain\\': \\'Langchain is a project that is trying to add more complex \\'\\n              \\'memory structures, including a key-value store for entities \\'\\n              \\'mentioned so far in the conversation.\\',\\n \\'Sam\\': \\'Sam is working on a hackathon project with Deven, trying to add more \\'\\n        \\'complex memory structures to Langchain, including a key-value store \\'\\n        \\'for entities mentioned so far in the conversation. They seem to have \\'\\n        \\'a great idea for how the key-value store can help, and Sam is also \\'\\n        \\'the founder of a company called Daimon.\\'}\\nconversation.predict(input=\"Sam is the founder of a company called Daimon.\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nYou are an assistant to a human, powered by a large language model trained by OpenAI.\\nYou are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\nYou are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\\nOverall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\\nContext:\\n{\\'Daimon\\': \\'Daimon is a company founded by Sam, a successful entrepreneur.\\', \\'Sam\\': \\'Sam is working on a hackathon project with Deven, trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation. They seem to have a great idea for how the key-value store can help, and Sam is also the founder of a company called Daimon.\\'}\\nCurrent conversation:\\nHuman: They are adding in a key-value store for entities mentioned so far in the conversation.\\nAI:  That sounds like a great idea! How will the key-value store help with the project?\\nHuman: What do you know about Deven & Sam?\\nAI:  Deven and Sam are working on a hackathon project together, trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation. They seem to be working hard on this project and have a great idea for how the key-value store can help.\\nHuman: Sam is the founder of a company called Daimon.\\nAI: \\nThat\\'s impressive! It sounds like Sam is a very successful entrepreneur. What kind of company is Daimon?\\nLast line:\\nHuman: Sam is the founder of a company called Daimon.\\nYou:\\n> Finished chain.\\n\" That\\'s impressive! It sounds like Sam is a very successful entrepreneur. What kind of company is Daimon?\"\\nfrom pprint import pprint\\npprint(conversation.memory.entity_store.store)\\n{\\'Daimon\\': \\'Daimon is a company founded by Sam, a successful entrepreneur, who \\'\\n           \\'is working on a hackathon project with Deven to add more complex \\'\\n           \\'memory structures to Langchain.\\',\\n \\'Deven\\': \\'Deven is working on a hackathon project with Sam, which they are \\'\\n          \\'entering into a hackathon. They are trying to add more complex \\'\\n          \\'memory structures to Langchain, including a key-value store for \\'\\n          \\'entities mentioned so far in the conversation, and seem to be \\'\\n          \\'working hard on this project with a great idea for how the \\'\\n          \\'key-value store can help.\\',\\n \\'Key-Value Store\\': \\'A key-value store is being added to the project to store \\'\\n                    \\'entities mentioned in the conversation.\\',\\n \\'Langchain\\': \\'Langchain is a project that is trying to add more complex \\'\\n              \\'memory structures, including a key-value store for entities \\'\\n              \\'mentioned so far in the conversation.\\',\\n \\'Sam\\': \\'Sam is working on a hackathon project with Deven, trying to add more \\'\\n        \\'complex memory structures to Langchain, including a key-value store \\'\\n        \\'for entities mentioned so far in the conversation. They seem to have \\'\\n        \\'a great idea for how the key-value store can help, and Sam is also \\'\\n        \\'the founder of a successful company called Daimon.\\'}\\nconversation.predict(input=\"What do you know about Sam?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nYou are an assistant to a human, powered by a large language model trained by OpenAI.\\nYou are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\nYou are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\\nOverall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\\nContext:\\n{\\'Deven\\': \\'Deven is working on a hackathon project with Sam, which they are entering into a hackathon. They are trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation, and seem to be working hard on this project with a great idea for how the key-value store can help.\\', \\'Sam\\': \\'Sam is working on a hackathon project with Deven, trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation. They seem to have a great idea for how the key-value store can help, and Sam is also the founder of a successful company called Daimon.\\', \\'Langchain\\': \\'Langchain is a project that is trying to add more complex memory structures, including a key-value store for entities mentioned so far in the conversation.\\', \\'Daimon\\': \\'Daimon is a company founded by Sam, a successful entrepreneur, who is working on a hackathon project with Deven to add more complex memory structures to Langchain.\\'}\\nCurrent conversation:\\nHuman: What do you know about Deven & Sam?\\nAI:  Deven and Sam are working on a hackathon project together, trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation. They seem to be working hard on this project and have a great idea for how the key-value store can help.\\nHuman: Sam is the founder of a company called Daimon.\\nAI: \\nThat\\'s impressive! It sounds like Sam is a very successful entrepreneur. What kind of company is Daimon?\\nHuman: Sam is the founder of a company called Daimon.\\nAI:  That\\'s impressive! It sounds like Sam is a very successful entrepreneur. What kind of company is Daimon?\\nLast line:\\nHuman: What do you know about Sam?\\nYou:\\n> Finished chain.\\n\\' Sam is the founder of a successful company called Daimon. He is also working on a hackathon project with Deven to add more complex memory structures to Langchain. They seem to have a great idea for how the key-value store can help.\\'\\nprevious\\nConversationBufferWindowMemory\\nnext\\nConversation Knowledge Graph Memory\\n Contents\\n  \\nUsing in a chain\\nInspecting the memory store\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\types\\\\entity_summary_memory.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nConversation Knowledge Graph Memory\\n Contents \\nUsing in a chain\\nConversation Knowledge Graph Memory#\\nThis type of memory uses a knowledge graph to recreate memory.\\nLet’s first walk through how to use the utilities\\nfrom langchain.memory import ConversationKGMemory\\nfrom langchain.llms import OpenAI\\nllm = OpenAI(temperature=0)\\nmemory = ConversationKGMemory(llm=llm)\\nmemory.save_context({\"input\": \"say hi to sam\"}, {\"ouput\": \"who is sam\"})\\nmemory.save_context({\"input\": \"sam is a friend\"}, {\"ouput\": \"okay\"})\\nmemory.load_memory_variables({\"input\": \\'who is sam\\'})\\n{\\'history\\': \\'On Sam: Sam is friend.\\'}\\nWe can also get the history as a list of messages (this is useful if you are using this with a chat model).\\nmemory = ConversationKGMemory(llm=llm, return_messages=True)\\nmemory.save_context({\"input\": \"say hi to sam\"}, {\"ouput\": \"who is sam\"})\\nmemory.save_context({\"input\": \"sam is a friend\"}, {\"ouput\": \"okay\"})\\nmemory.load_memory_variables({\"input\": \\'who is sam\\'})\\n{\\'history\\': [SystemMessage(content=\\'On Sam: Sam is friend.\\', additional_kwargs={})]}\\nWe can also more modularly get current entities from a new message (will use previous messages as context.)\\nmemory.get_current_entities(\"what\\'s Sams favorite color?\")\\n[\\'Sam\\']\\nWe can also more modularly get knowledge triplets from a new message (will use previous messages as context.)\\nmemory.get_knowledge_triplets(\"her favorite color is red\")\\n[KnowledgeTriple(subject=\\'Sam\\', predicate=\\'favorite color\\', object_=\\'red\\')]\\nUsing in a chain#\\nLet’s now use this in a chain!\\nllm = OpenAI(temperature=0)\\nfrom langchain.prompts.prompt import PromptTemplate\\nfrom langchain.chains import ConversationChain\\ntemplate = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \\nIf the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\\nRelevant Information:\\n{history}\\nConversation:\\nHuman: {input}\\nAI:\"\"\"\\nprompt = PromptTemplate(\\n    input_variables=[\"history\", \"input\"], template=template\\n)\\nconversation_with_kg = ConversationChain(\\n    llm=llm, \\n    verbose=True, \\n    prompt=prompt,\\n    memory=ConversationKGMemory(llm=llm)\\n)\\nconversation_with_kg.predict(input=\"Hi, what\\'s up?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \\nIf the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\\nRelevant Information:\\nConversation:\\nHuman: Hi, what\\'s up?\\nAI:\\n> Finished chain.\\n\" Hi there! I\\'m doing great. I\\'m currently in the process of learning about the world around me. I\\'m learning about different cultures, languages, and customs. It\\'s really fascinating! How about you?\"\\nconversation_with_kg.predict(input=\"My name is James and I\\'m helping Will. He\\'s an engineer.\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \\nIf the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\\nRelevant Information:\\nConversation:\\nHuman: My name is James and I\\'m helping Will. He\\'s an engineer.\\nAI:\\n> Finished chain.\\n\" Hi James, it\\'s nice to meet you. I\\'m an AI and I understand you\\'re helping Will, the engineer. What kind of engineering does he do?\"\\nconversation_with_kg.predict(input=\"What do you know about Will?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \\nIf the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\\nRelevant Information:\\nOn Will: Will is an engineer.\\nConversation:\\nHuman: What do you know about Will?\\nAI:\\n> Finished chain.\\n\\' Will is an engineer.\\'\\nprevious\\nEntity Memory\\nnext\\nConversationSummaryMemory\\n Contents\\n  \\nUsing in a chain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\types\\\\kg.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nConversationSummaryMemory\\n Contents \\nUsing in a chain\\nConversationSummaryMemory#\\nNow let’s take a look at using a slightly more complex type of memory - ConversationSummaryMemory. This type of memory creates a summary of the conversation over time. This can be useful for condensing information from the conversation over time.\\nLet’s first explore the basic functionality of this type of memory.\\nfrom langchain.memory import ConversationSummaryMemory\\nfrom langchain.llms import OpenAI\\nmemory = ConversationSummaryMemory(llm=OpenAI(temperature=0))\\nmemory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\\nmemory.load_memory_variables({})\\n{\\'history\\': \\'\\\\nThe human greets the AI, to which the AI responds.\\'}\\nWe can also get the history as a list of messages (this is useful if you are using this with a chat model).\\nmemory = ConversationSummaryMemory(llm=OpenAI(temperature=0), return_messages=True)\\nmemory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\\nmemory.load_memory_variables({})\\n{\\'history\\': [SystemMessage(content=\\'\\\\nThe human greets the AI, to which the AI responds.\\', additional_kwargs={})]}\\nWe can also utilize the predict_new_summary method directly.\\nmessages = memory.chat_memory.messages\\nprevious_summary = \"\"\\nmemory.predict_new_summary(messages, previous_summary)\\n\\'\\\\nThe human greets the AI, to which the AI responds.\\'\\nUsing in a chain#\\nLet’s walk through an example of using this in a chain, again setting verbose=True so we can see the prompt.\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import ConversationChain\\nllm = OpenAI(temperature=0)\\nconversation_with_summary = ConversationChain(\\n    llm=llm, \\n    memory=ConversationSummaryMemory(llm=OpenAI()),\\n    verbose=True\\n)\\nconversation_with_summary.predict(input=\"Hi, what\\'s up?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi, what\\'s up?\\nAI:\\n> Finished chain.\\n\" Hi there! I\\'m doing great. I\\'m currently helping a customer with a technical issue. How about you?\"\\nconversation_with_summary.predict(input=\"Tell me more about it!\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nThe human greeted the AI and asked how it was doing. The AI replied that it was doing great and was currently helping a customer with a technical issue.\\nHuman: Tell me more about it!\\nAI:\\n> Finished chain.\\n\" Sure! The customer is having trouble with their computer not connecting to the internet. I\\'m helping them troubleshoot the issue and figure out what the problem is. So far, we\\'ve tried resetting the router and checking the network settings, but the issue still persists. We\\'re currently looking into other possible solutions.\"\\nconversation_with_summary.predict(input=\"Very cool -- what is the scope of the project?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nThe human greeted the AI and asked how it was doing. The AI replied that it was doing great and was currently helping a customer with a technical issue where their computer was not connecting to the internet. The AI was troubleshooting the issue and had already tried resetting the router and checking the network settings, but the issue still persisted and they were looking into other possible solutions.\\nHuman: Very cool -- what is the scope of the project?\\nAI:\\n> Finished chain.\\n\" The scope of the project is to troubleshoot the customer\\'s computer issue and find a solution that will allow them to connect to the internet. We are currently exploring different possibilities and have already tried resetting the router and checking the network settings, but the issue still persists.\"\\nprevious\\nConversation Knowledge Graph Memory\\nnext\\nConversationSummaryBufferMemory\\n Contents\\n  \\nUsing in a chain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\types\\\\summary.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nConversationSummaryBufferMemory\\n Contents \\nUsing in a chain\\nConversationSummaryBufferMemory#\\nConversationSummaryBufferMemory combines the last two ideas. It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions it compiles them into a summary and uses both. Unlike the previous implementation though, it uses token length rather than number of interactions to determine when to flush interactions.\\nLet’s first walk through how to use the utilities\\nfrom langchain.memory import ConversationSummaryBufferMemory\\nfrom langchain.llms import OpenAI\\nllm = OpenAI()\\nmemory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10)\\nmemory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\\nmemory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\\nmemory.load_memory_variables({})\\n{\\'history\\': \\'System: \\\\nThe human says \"hi\", and the AI responds with \"whats up\".\\\\nHuman: not much you\\\\nAI: not much\\'}\\nWe can also get the history as a list of messages (this is useful if you are using this with a chat model).\\nmemory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10, return_messages=True)\\nmemory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\\nmemory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\\nWe can also utilize the predict_new_summary method directly.\\nmessages = memory.chat_memory.messages\\nprevious_summary = \"\"\\nmemory.predict_new_summary(messages, previous_summary)\\n\\'\\\\nThe human and AI state that they are not doing much.\\'\\nUsing in a chain#\\nLet’s walk through an example, again setting verbose=True so we can see the prompt.\\nfrom langchain.chains import ConversationChain\\nconversation_with_summary = ConversationChain(\\n    llm=llm, \\n    # We set a very low max_token_limit for the purposes of testing.\\n    memory=ConversationSummaryBufferMemory(llm=OpenAI(), max_token_limit=40),\\n    verbose=True\\n)\\nconversation_with_summary.predict(input=\"Hi, what\\'s up?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi, what\\'s up?\\nAI:\\n> Finished chain.\\n\" Hi there! I\\'m doing great. I\\'m learning about the latest advances in artificial intelligence. What about you?\"\\nconversation_with_summary.predict(input=\"Just working on writing some documentation!\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi, what\\'s up?\\nAI:  Hi there! I\\'m doing great. I\\'m spending some time learning about the latest developments in AI technology. How about you?\\nHuman: Just working on writing some documentation!\\nAI:\\n> Finished chain.\\n\\' That sounds like a great use of your time. Do you have experience with writing documentation?\\'\\n# We can see here that there is a summary of the conversation and then some previous interactions\\nconversation_with_summary.predict(input=\"For LangChain! Have you heard of it?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nSystem: \\nThe human asked the AI what it was up to and the AI responded that it was learning about the latest developments in AI technology.\\nHuman: Just working on writing some documentation!\\nAI:  That sounds like a great use of your time. Do you have experience with writing documentation?\\nHuman: For LangChain! Have you heard of it?\\nAI:\\n> Finished chain.\\n\" No, I haven\\'t heard of LangChain. Can you tell me more about it?\"\\n# We can see here that the summary and the buffer are updated\\nconversation_with_summary.predict(input=\"Haha nope, although a lot of people confuse it for that\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nSystem: \\nThe human asked the AI what it was up to and the AI responded that it was learning about the latest developments in AI technology. The human then mentioned they were writing documentation, to which the AI responded that it sounded like a great use of their time and asked if they had experience with writing documentation.\\nHuman: For LangChain! Have you heard of it?\\nAI:  No, I haven\\'t heard of LangChain. Can you tell me more about it?\\nHuman: Haha nope, although a lot of people confuse it for that\\nAI:\\n> Finished chain.\\n\\' Oh, okay. What is LangChain?\\'\\nprevious\\nConversationSummaryMemory\\nnext\\nConversationTokenBufferMemory\\n Contents\\n  \\nUsing in a chain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\types\\\\summary_buffer.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nConversationTokenBufferMemory\\n Contents \\nUsing in a chain\\nConversationTokenBufferMemory#\\nConversationTokenBufferMemory keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions.\\nLet’s first walk through how to use the utilities\\nfrom langchain.memory import ConversationTokenBufferMemory\\nfrom langchain.llms import OpenAI\\nllm = OpenAI()\\nmemory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10)\\nmemory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\\nmemory.save_context({\"input\": \"not much you\"}, {\"ouput\": \"not much\"})\\nmemory.load_memory_variables({})\\n{\\'history\\': \\'Human: not much you\\\\nAI: not much\\'}\\nWe can also get the history as a list of messages (this is useful if you are using this with a chat model).\\nmemory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10, return_messages=True)\\nmemory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\\nmemory.save_context({\"input\": \"not much you\"}, {\"ouput\": \"not much\"})\\nUsing in a chain#\\nLet’s walk through an example, again setting verbose=True so we can see the prompt.\\nfrom langchain.chains import ConversationChain\\nconversation_with_summary = ConversationChain(\\n    llm=llm, \\n    # We set a very low max_token_limit for the purposes of testing.\\n    memory=ConversationTokenBufferMemory(llm=OpenAI(), max_token_limit=60),\\n    verbose=True\\n)\\nconversation_with_summary.predict(input=\"Hi, what\\'s up?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi, what\\'s up?\\nAI:\\n> Finished chain.\\n\" Hi there! I\\'m doing great, just enjoying the day. How about you?\"\\nconversation_with_summary.predict(input=\"Just working on writing some documentation!\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi, what\\'s up?\\nAI:  Hi there! I\\'m doing great, just enjoying the day. How about you?\\nHuman: Just working on writing some documentation!\\nAI:\\n> Finished chain.\\n\\' Sounds like a productive day! What kind of documentation are you writing?\\'\\nconversation_with_summary.predict(input=\"For LangChain! Have you heard of it?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: Hi, what\\'s up?\\nAI:  Hi there! I\\'m doing great, just enjoying the day. How about you?\\nHuman: Just working on writing some documentation!\\nAI:  Sounds like a productive day! What kind of documentation are you writing?\\nHuman: For LangChain! Have you heard of it?\\nAI:\\n> Finished chain.\\n\" Yes, I have heard of LangChain! It is a decentralized language-learning platform that connects native speakers and learners in real time. Is that the documentation you\\'re writing about?\"\\n# We can see here that the buffer is updated\\nconversation_with_summary.predict(input=\"Haha nope, although a lot of people confuse it for that\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nHuman: For LangChain! Have you heard of it?\\nAI:  Yes, I have heard of LangChain! It is a decentralized language-learning platform that connects native speakers and learners in real time. Is that the documentation you\\'re writing about?\\nHuman: Haha nope, although a lot of people confuse it for that\\nAI:\\n> Finished chain.\\n\" Oh, I see. Is there another language learning platform you\\'re referring to?\"\\nprevious\\nConversationSummaryBufferMemory\\nnext\\nVectorStore-Backed Memory\\n Contents\\n  \\nUsing in a chain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\types\\\\token_buffer.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nVectorStore-Backed Memory\\n Contents \\nInitialize your VectorStore\\nCreate your the VectorStoreRetrieverMemory\\nUsing in a chain\\nVectorStore-Backed Memory#\\nVectorStoreRetrieverMemory stores memories in a VectorDB and queries the top-K most “salient” docs every time it is called.\\nThis differs from most of the other Memory classes in that it doesn’t explicitly track the order of interactions.\\nIn this case, the “docs” are previous conversation snippets. This can be useful to refer to relevant pieces of information that the AI was told earlier in the conversation.\\nfrom datetime import datetime\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.llms import OpenAI\\nfrom langchain.memory import VectorStoreRetrieverMemory\\nfrom langchain.chains import ConversationChain\\nfrom langchain.prompts import PromptTemplate\\nInitialize your VectorStore#\\nDepending on the store you choose, this step may look different. Consult the relevant VectorStore documentation for more details.\\nimport faiss\\nfrom langchain.docstore import InMemoryDocstore\\nfrom langchain.vectorstores import FAISS\\nembedding_size = 1536 # Dimensions of the OpenAIEmbeddings\\nindex = faiss.IndexFlatL2(embedding_size)\\nembedding_fn = OpenAIEmbeddings().embed_query\\nvectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})\\nCreate your the VectorStoreRetrieverMemory#\\nThe memory object is instantiated from any VectorStoreRetriever.\\n# In actual usage, you would set `k` to be a higher value, but we use k=1 to show that\\n# the vector lookup still returns the semantically relevant information\\nretriever = vectorstore.as_retriever(search_kwargs=dict(k=1))\\nmemory = VectorStoreRetrieverMemory(retriever=retriever)\\n# When added to an agent, the memory object can save pertinent information from conversations or used tools\\nmemory.save_context({\"input\": \"My favorite food is pizza\"}, {\"output\": \"thats good to know\"})\\nmemory.save_context({\"input\": \"My favorite sport is soccer\"}, {\"output\": \"...\"})\\nmemory.save_context({\"input\": \"I don\\'t the Celtics\"}, {\"output\": \"ok\"}) # \\n# Notice the first result returned is the memory pertaining to tax help, which the language model deems more semantically relevant\\n# to a 1099 than the other documents, despite them both containing numbers.\\nprint(memory.load_memory_variables({\"prompt\": \"what sport should i watch?\"})[\"history\"])\\ninput: My favorite sport is soccer\\noutput: ...\\nUsing in a chain#\\nLet’s walk through an example, again setting verbose=True so we can see the prompt.\\nllm = OpenAI(temperature=0) # Can be any valid LLM\\n_DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nRelevant pieces of previous conversation:\\n{history}\\n(You do not need to use these pieces of information if not relevant)\\nCurrent conversation:\\nHuman: {input}\\nAI:\"\"\"\\nPROMPT = PromptTemplate(\\n    input_variables=[\"history\", \"input\"], template=_DEFAULT_TEMPLATE\\n)\\nconversation_with_summary = ConversationChain(\\n    llm=llm, \\n    prompt=PROMPT,\\n    # We set a very low max_token_limit for the purposes of testing.\\n    memory=memory,\\n    verbose=True\\n)\\nconversation_with_summary.predict(input=\"Hi, my name is Perry, what\\'s up?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nRelevant pieces of previous conversation:\\ninput: My favorite food is pizza\\noutput: thats good to know\\n(You do not need to use these pieces of information if not relevant)\\nCurrent conversation:\\nHuman: Hi, my name is Perry, what\\'s up?\\nAI:\\n> Finished chain.\\n\" Hi Perry, I\\'m doing well. How about you?\"\\n# Here, the basketball related content is surfaced\\nconversation_with_summary.predict(input=\"what\\'s my favorite sport?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nRelevant pieces of previous conversation:\\ninput: My favorite sport is soccer\\noutput: ...\\n(You do not need to use these pieces of information if not relevant)\\nCurrent conversation:\\nHuman: what\\'s my favorite sport?\\nAI:\\n> Finished chain.\\n\\' You told me earlier that your favorite sport is soccer.\\'\\n# Even though the language model is stateless, since relavent memory is fetched, it can \"reason\" about the time.\\n# Timestamping memories and data is useful in general to let the agent determine temporal relevance\\nconversation_with_summary.predict(input=\"Whats my favorite food\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nRelevant pieces of previous conversation:\\ninput: My favorite food is pizza\\noutput: thats good to know\\n(You do not need to use these pieces of information if not relevant)\\nCurrent conversation:\\nHuman: Whats my favorite food\\nAI:\\n> Finished chain.\\n\\' You said your favorite food is pizza.\\'\\n# The memories from the conversation are automatically stored,\\n# since this query best matches the introduction chat above,\\n# the agent is able to \\'remember\\' the user\\'s name.\\nconversation_with_summary.predict(input=\"What\\'s my name?\")\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nRelevant pieces of previous conversation:\\ninput: Hi, my name is Perry, what\\'s up?\\nresponse:  Hi Perry, I\\'m doing well. How about you?\\n(You do not need to use these pieces of information if not relevant)\\nCurrent conversation:\\nHuman: What\\'s my name?\\nAI:\\n> Finished chain.\\n\\' Your name is Perry.\\'\\nprevious\\nConversationTokenBufferMemory\\nnext\\nHow to add Memory to an LLMChain\\n Contents\\n  \\nInitialize your VectorStore\\nCreate your the VectorStoreRetrieverMemory\\nUsing in a chain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\memory\\\\types\\\\vectorstore_retriever_memory.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nChat Models\\nChat Models#\\nNote\\nConceptual Guide\\nChat models are a variation on language models.\\nWhile chat models use language models under the hood, the interface they expose is a bit different.\\nRather than expose a “text in, text out” API, they expose an interface where “chat messages” are the inputs and outputs.\\nChat model APIs are fairly new, so we are still figuring out the correct abstractions.\\nThe following sections of documentation are provided:\\nGetting Started: An overview of all the functionality the LangChain LLM class provides.\\nHow-To Guides: A collection of how-to guides. These highlight how to accomplish various objectives with our LLM class (streaming, async, etc).\\nIntegrations: A collection of examples on how to integrate different LLM providers with LangChain (OpenAI, Hugging Face, etc).\\nprevious\\nLLMs\\nnext\\nGetting Started\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\models\\\\chat.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nLLMs\\nLLMs#\\nNote\\nConceptual Guide\\nLarge Language Models (LLMs) are a core component of LangChain.\\nLangChain is not a provider of LLMs, but rather provides a standard interface through which\\nyou can interact with a variety of LLMs.\\nThe following sections of documentation are provided:\\nGetting Started: An overview of all the functionality the LangChain LLM class provides.\\nHow-To Guides: A collection of how-to guides. These highlight how to accomplish various objectives with our LLM class (streaming, async, etc).\\nIntegrations: A collection of examples on how to integrate different LLM providers with LangChain (OpenAI, Hugging Face, etc).\\nReference: API reference documentation for all LLM classes.\\nprevious\\nModels\\nnext\\nGetting Started\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\models\\\\llms.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nText Embedding Models\\nText Embedding Models#\\nNote\\nConceptual Guide\\nThis documentation goes over how to use the Embedding class in LangChain.\\nThe Embedding class is a class designed for interfacing with embeddings. There are lots of Embedding providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.\\nEmbeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.\\nThe base Embedding class in LangChain exposes two methods: embed_documents and embed_query. The largest difference is that these two methods have different interfaces: one works over multiple documents, while the other works over a single document. Besides this, another reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).\\nThe following integrations exist for text embeddings.\\nAleph Alpha\\nAzureOpenAI\\nCohere\\nFake Embeddings\\nHugging Face Hub\\nInstructEmbeddings\\nJina\\nLlama-cpp\\nOpenAI\\nSageMaker Endpoint Embeddings\\nSelf Hosted Embeddings\\nTensorflowHub\\nprevious\\nPromptLayer ChatOpenAI\\nnext\\nAleph Alpha\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\models\\\\text_embedding.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nGetting Started\\n Contents \\nPromptTemplates\\nLLMChain\\nStreaming\\nGetting Started#\\nThis notebook covers how to get started with chat models. The interface is based around messages rather than raw text.\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain import PromptTemplate, LLMChain\\nfrom langchain.prompts.chat import (\\n    ChatPromptTemplate,\\n    SystemMessagePromptTemplate,\\n    AIMessagePromptTemplate,\\n    HumanMessagePromptTemplate,\\n)\\nfrom langchain.schema import (\\n    AIMessage,\\n    HumanMessage,\\n    SystemMessage\\n)\\nchat = ChatOpenAI(temperature=0)\\nYou can get chat completions by passing one or more messages to the chat model. The response will be a message. The types of messages currently supported in LangChain are AIMessage, HumanMessage, SystemMessage, and ChatMessage – ChatMessage takes in an arbitrary role parameter. Most of the time, you’ll just be dealing with HumanMessage, AIMessage, and SystemMessage\\nchat([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])\\nAIMessage(content=\"J\\'aime programmer.\", additional_kwargs={})\\nOpenAI’s chat model supports multiple messages as input. See here for more information. Here is an example of sending a system and user message to the chat model:\\nmessages = [\\n    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\\n    HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")\\n]\\nchat(messages)\\nAIMessage(content=\"J\\'aime programmer.\", additional_kwargs={})\\nYou can go one step further and generate completions for multiple sets of messages using generate. This returns an LLMResult with an additional message parameter.\\nbatch_messages = [\\n    [\\n        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\\n        HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")\\n    ],\\n    [\\n        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\\n        HumanMessage(content=\"Translate this sentence from English to French. I love artificial intelligence.\")\\n    ],\\n]\\nresult = chat.generate(batch_messages)\\nresult\\nLLMResult(generations=[[ChatGeneration(text=\"J\\'aime programmer.\", generation_info=None, message=AIMessage(content=\"J\\'aime programmer.\", additional_kwargs={}))], [ChatGeneration(text=\"J\\'aime l\\'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J\\'aime l\\'intelligence artificielle.\", additional_kwargs={}))]], llm_output={\\'token_usage\\': {\\'prompt_tokens\\': 71, \\'completion_tokens\\': 18, \\'total_tokens\\': 89}})\\nYou can recover things like token usage from this LLMResult\\nresult.llm_output\\n{\\'token_usage\\': {\\'prompt_tokens\\': 71,\\n  \\'completion_tokens\\': 18,\\n  \\'total_tokens\\': 89}}\\nPromptTemplates#\\nYou can make use of templating by using a MessagePromptTemplate. You can build a ChatPromptTemplate from one or more MessagePromptTemplates. You can use ChatPromptTemplate’s format_prompt – this returns a PromptValue, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.\\nFor convience, there is a from_template method exposed on the template. If you were to use this template, this is what it would look like:\\ntemplate=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\\nhuman_template=\"{text}\"\\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\\nchat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\\n# get a chat completion from the formatted messages\\nchat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages())\\nAIMessage(content=\"J\\'adore la programmation.\", additional_kwargs={})\\nIf you wanted to construct the MessagePromptTemplate more directly, you could create a PromptTemplate outside and then pass it in, eg:\\nprompt=PromptTemplate(\\n    template=\"You are a helpful assistant that translates {input_language} to {output_language}.\",\\n    input_variables=[\"input_language\", \"output_language\"],\\n)\\nsystem_message_prompt = SystemMessagePromptTemplate(prompt=prompt)\\nLLMChain#\\nYou can use the existing LLMChain in a very similar way to before - provide a prompt and a model.\\nchain = LLMChain(llm=chat, prompt=chat_prompt)\\nchain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\\n\"J\\'adore la programmation.\"\\nStreaming#\\nStreaming is supported for ChatOpenAI through callback handling.\\nfrom langchain.callbacks.base import CallbackManager\\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\\nchat = ChatOpenAI(streaming=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=True, temperature=0)\\nresp = chat([HumanMessage(content=\"Write me a song about sparkling water.\")])\\nVerse 1:\\nBubbles rising to the top\\nA refreshing drink that never stops\\nClear and crisp, it\\'s pure delight\\nA taste that\\'s sure to excite\\nChorus:\\nSparkling water, oh so fine\\nA drink that\\'s always on my mind\\nWith every sip, I feel alive\\nSparkling water, you\\'re my vibe\\nVerse 2:\\nNo sugar, no calories, just pure bliss\\nA drink that\\'s hard to resist\\nIt\\'s the perfect way to quench my thirst\\nA drink that always comes first\\nChorus:\\nSparkling water, oh so fine\\nA drink that\\'s always on my mind\\nWith every sip, I feel alive\\nSparkling water, you\\'re my vibe\\nBridge:\\nFrom the mountains to the sea\\nSparkling water, you\\'re the key\\nTo a healthy life, a happy soul\\nA drink that makes me feel whole\\nChorus:\\nSparkling water, oh so fine\\nA drink that\\'s always on my mind\\nWith every sip, I feel alive\\nSparkling water, you\\'re my vibe\\nOutro:\\nSparkling water, you\\'re the one\\nA drink that\\'s always so much fun\\nI\\'ll never let you go, my friend\\nSparkling\\nprevious\\nChat Models\\nnext\\nHow-To Guides\\n Contents\\n  \\nPromptTemplates\\nLLMChain\\nStreaming\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\models\\\\chat\\\\getting_started.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nGetting Started\\nGetting Started#\\nThis notebook goes over how to use the LLM class in LangChain.\\nThe LLM class is a class designed for interfacing with LLMs. There are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them. In this part of the documentation, we will focus on generic LLM functionality. For details on working with a specific LLM wrapper, please see the examples in the How-To section.\\nFor this notebook, we will work with an OpenAI LLM wrapper, although the functionalities highlighted are generic for all LLM types.\\nfrom langchain.llms import OpenAI\\nllm = OpenAI(model_name=\"text-ada-001\", n=2, best_of=2)\\nGenerate Text: The most basic functionality an LLM has is just the ability to call it, passing in a string and getting back a string.\\nllm(\"Tell me a joke\")\\n\\'\\\\n\\\\nWhy did the chicken cross the road?\\\\n\\\\nTo get to the other side.\\'\\nGenerate: More broadly, you can call it with a list of inputs, getting back a more complete response than just the text. This complete response includes things like multiple top responses, as well as LLM provider specific information\\nllm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"]*15)\\nlen(llm_result.generations)\\n30\\nllm_result.generations[0]\\n[Generation(text=\\'\\\\n\\\\nWhy did the chicken cross the road?\\\\n\\\\nTo get to the other side!\\'),\\n Generation(text=\\'\\\\n\\\\nWhy did the chicken cross the road?\\\\n\\\\nTo get to the other side.\\')]\\nllm_result.generations[-1]\\n[Generation(text=\"\\\\n\\\\nWhat if love neverspeech\\\\n\\\\nWhat if love never ended\\\\n\\\\nWhat if love was only a feeling\\\\n\\\\nI\\'ll never know this love\\\\n\\\\nIt\\'s not a feeling\\\\n\\\\nBut it\\'s what we have for each other\\\\n\\\\nWe just know that love is something strong\\\\n\\\\nAnd we can\\'t help but be happy\\\\n\\\\nWe just feel what love is for us\\\\n\\\\nAnd we love each other with all our heart\\\\n\\\\nWe just don\\'t know how\\\\n\\\\nHow it will go\\\\n\\\\nBut we know that love is something strong\\\\n\\\\nAnd we\\'ll always have each other\\\\n\\\\nIn our lives.\"),\\n Generation(text=\\'\\\\n\\\\nOnce upon a time\\\\n\\\\nThere was a love so pure and true\\\\n\\\\nIt lasted for centuries\\\\n\\\\nAnd never became stale or dry\\\\n\\\\nIt was moving and alive\\\\n\\\\nAnd the heart of the love-ick\\\\n\\\\nIs still beating strong and true.\\')]\\nYou can also access provider specific information that is returned. This information is NOT standardized across providers.\\nllm_result.llm_output\\n{\\'token_usage\\': {\\'completion_tokens\\': 3903,\\n  \\'total_tokens\\': 4023,\\n  \\'prompt_tokens\\': 120}}\\nNumber of Tokens: You can also estimate how many tokens a piece of text will be in that model. This is useful because models have a context length (and cost more for more tokens), which means you need to be aware of how long the text you are passing in is.\\nNotice that by default the tokens are estimated using tiktoken (except for legacy version <3.8, where a Hugging Face tokenizer is used)\\nllm.get_num_tokens(\"what a joke\")\\n3\\nprevious\\nLLMs\\nnext\\nGeneric Functionality\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\models\\\\llms\\\\getting_started.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nGeneric Functionality\\nGeneric Functionality#\\nThe examples here all address certain “how-to” guides for working with LLMs.\\nHow to use the async API for LLMs\\nHow to write a custom LLM wrapper\\nHow (and why) to use the fake LLM\\nHow to cache LLM calls\\nHow to serialize LLM classes\\nHow to stream LLM and Chat Model responses\\nHow to track token usage\\nprevious\\nGetting Started\\nnext\\nHow to use the async API for LLMs\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\models\\\\llms\\\\how_to_guides.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nChat Prompt Template\\nChat Prompt Template#\\nChat Models takes a list of chat messages as input - this list commonly referred to as a prompt.\\nTypically this is not simply a hardcoded list of messages but rather a combination of a template, some examples, and user input.\\nLangChain provides several classes and functions to make constructing and working with prompts easy.\\nfrom langchain.prompts import (\\n    ChatPromptTemplate,\\n    PromptTemplate,\\n    SystemMessagePromptTemplate,\\n    AIMessagePromptTemplate,\\n    HumanMessagePromptTemplate,\\n)\\nfrom langchain.schema import (\\n    AIMessage,\\n    HumanMessage,\\n    SystemMessage\\n)\\nYou can make use of templating by using a MessagePromptTemplate. You can build a ChatPromptTemplate from one or more MessagePromptTemplates. You can use ChatPromptTemplate’s format_prompt – this returns a PromptValue, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.\\nFor convenience, there is a from_template method exposed on the template. If you were to use this template, this is what it would look like:\\ntemplate=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\\nhuman_template=\"{text}\"\\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\\nchat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\\n# get a chat completion from the formatted messages\\nchat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages()\\n[SystemMessage(content=\\'You are a helpful assistant that translates English to French.\\', additional_kwargs={}),\\n HumanMessage(content=\\'I love programming.\\', additional_kwargs={})]\\nIf you wanted to construct the MessagePromptTemplate more directly, you could create a PromptTemplate outside and then pass it in, eg:\\nprompt=PromptTemplate(\\n    template=\"You are a helpful assistant that translates {input_language} to {output_language}.\",\\n    input_variables=[\"input_language\", \"output_language\"],\\n)\\nsystem_message_prompt = SystemMessagePromptTemplate(prompt=prompt)\\nprevious\\nExample Selector\\nnext\\nExample Selectors\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\chat_prompt_template.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nExample Selectors\\nExample Selectors#\\nNote\\nConceptual Guide\\nIf you have a large number of examples, you may need to select which ones to include in the prompt. The ExampleSelector is the class responsible for doing so.\\nThe base interface is defined as below:\\nclass BaseExampleSelector(ABC):\\n    \"\"\"Interface for selecting examples to include in prompts.\"\"\"\\n    @abstractmethod\\n    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\\n        \"\"\"Select which examples to use based on the inputs.\"\"\"\\nThe only method it needs to expose is a select_examples method. This takes in the input variables and then returns a list of examples. It is up to each specific implementation as to how those examples are selected. Let’s take a look at some below.\\nSee below for a list of example selectors.\\nHow to create a custom example selector\\nLengthBased ExampleSelector\\nMaximal Marginal Relevance ExampleSelector\\nNGram Overlap ExampleSelector\\nSimilarity ExampleSelector\\nprevious\\nChat Prompt Template\\nnext\\nHow to create a custom example selector\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\example_selectors.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nOutput Parsers\\nOutput Parsers#\\nNote\\nConceptual Guide\\nLanguage models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.\\nOutput parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\\nget_format_instructions() -> str: A method which returns a string containing instructions for how the output of a language model should be formatted.\\nparse(str) -> Any: A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\\nAnd then one optional one:\\nparse_with_prompt(str) -> Any: A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\\nTo start, we recommend familiarizing yourself with the Getting Started section\\nOutput Parsers\\nAfter that, we provide deep dives on all the different types of output parsers.\\nCommaSeparatedListOutputParser\\nOutputFixingParser\\nPydanticOutputParser\\nRetryOutputParser\\nStructured Output Parser\\nprevious\\nSimilarity ExampleSelector\\nnext\\nOutput Parsers\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\output_parsers.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nPrompt Templates\\nPrompt Templates#\\nNote\\nConceptual Guide\\nLanguage models take text as input - that text is commonly referred to as a prompt.\\nTypically this is not simply a hardcoded string but rather a combination of a template, some examples, and user input.\\nLangChain provides several classes and functions to make constructing and working with prompts easy.\\nThe following sections of documentation are provided:\\nGetting Started: An overview of all the functionality LangChain provides for working with and constructing prompts.\\nHow-To Guides: A collection of how-to guides. These highlight how to accomplish various objectives with our prompt class.\\nReference: API reference documentation for all prompt classes.\\nprevious\\nPrompts\\nnext\\nGetting Started\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\prompt_templates.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nHow to create a custom example selector\\n Contents \\nImplement custom example selector\\nUse custom example selector\\nHow to create a custom example selector#\\nIn this tutorial, we’ll create a custom example selector that selects every alternate example from a given list of examples.\\nAn ExampleSelector must implement two methods:\\nAn add_example method which takes in an example and adds it into the ExampleSelector\\nA select_examples method which takes in input variables (which are meant to be user input) and returns a list of examples to use in the few shot prompt.\\nLet’s implement a custom ExampleSelector that just selects two examples at random.\\nNote\\nTake a look at the current set of example selector implementations supported in LangChain here.\\nImplement custom example selector#\\nfrom langchain.prompts.example_selector.base import BaseExampleSelector\\nfrom typing import Dict, List\\nimport numpy as np\\nclass CustomExampleSelector(BaseExampleSelector):\\n    \\n    def __init__(self, examples: List[Dict[str, str]]):\\n        self.examples = examples\\n    \\n    def add_example(self, example: Dict[str, str]) -> None:\\n        \"\"\"Add new example to store for a key.\"\"\"\\n        self.examples.append(example)\\n    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\\n        \"\"\"Select which examples to use based on the inputs.\"\"\"\\n        return np.random.choice(self.examples, size=2, replace=False)\\nUse custom example selector#\\nexamples = [\\n    {\"foo\": \"1\"},\\n    {\"foo\": \"2\"},\\n    {\"foo\": \"3\"}\\n]\\n# Initialize example selector.\\nexample_selector = CustomExampleSelector(examples)\\n# Select examples\\nexample_selector.select_examples({\"foo\": \"foo\"})\\n# -> array([{\\'foo\\': \\'2\\'}, {\\'foo\\': \\'3\\'}], dtype=object)\\n# Add new example to the set of examples\\nexample_selector.add_example({\"foo\": \"4\"})\\nexample_selector.examples\\n# -> [{\\'foo\\': \\'1\\'}, {\\'foo\\': \\'2\\'}, {\\'foo\\': \\'3\\'}, {\\'foo\\': \\'4\\'}]\\n# Select examples\\nexample_selector.select_examples({\"foo\": \"foo\"})\\n# -> array([{\\'foo\\': \\'1\\'}, {\\'foo\\': \\'4\\'}], dtype=object)\\nprevious\\nExample Selectors\\nnext\\nLengthBased ExampleSelector\\n Contents\\n  \\nImplement custom example selector\\nUse custom example selector\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\example_selectors\\\\examples\\\\custom_example_selector.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nLengthBased ExampleSelector\\nLengthBased ExampleSelector#\\nThis ExampleSelector selects which examples to use based on length. This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more.\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.prompts import FewShotPromptTemplate\\nfrom langchain.prompts.example_selector import LengthBasedExampleSelector\\n# These are a lot of examples of a pretend task of creating antonyms.\\nexamples = [\\n    {\"input\": \"happy\", \"output\": \"sad\"},\\n    {\"input\": \"tall\", \"output\": \"short\"},\\n    {\"input\": \"energetic\", \"output\": \"lethargic\"},\\n    {\"input\": \"sunny\", \"output\": \"gloomy\"},\\n    {\"input\": \"windy\", \"output\": \"calm\"},\\n]\\nexample_prompt = PromptTemplate(\\n    input_variables=[\"input\", \"output\"],\\n    template=\"Input: {input}\\\\nOutput: {output}\",\\n)\\nexample_selector = LengthBasedExampleSelector(\\n    # These are the examples it has available to choose from.\\n    examples=examples, \\n    # This is the PromptTemplate being used to format the examples.\\n    example_prompt=example_prompt, \\n    # This is the maximum length that the formatted examples should be.\\n    # Length is measured by the get_text_length function below.\\n    max_length=25,\\n    # This is the function used to get the length of a string, which is used\\n    # to determine which examples to include. It is commented out because\\n    # it is provided as a default value if none is specified.\\n    # get_text_length: Callable[[str], int] = lambda x: len(re.split(\"\\\\n| \", x))\\n)\\ndynamic_prompt = FewShotPromptTemplate(\\n    # We provide an ExampleSelector instead of examples.\\n    example_selector=example_selector,\\n    example_prompt=example_prompt,\\n    prefix=\"Give the antonym of every input\",\\n    suffix=\"Input: {adjective}\\\\nOutput:\", \\n    input_variables=[\"adjective\"],\\n)\\n# An example with small input, so it selects all examples.\\nprint(dynamic_prompt.format(adjective=\"big\"))\\nGive the antonym of every input\\nInput: happy\\nOutput: sad\\nInput: tall\\nOutput: short\\nInput: energetic\\nOutput: lethargic\\nInput: sunny\\nOutput: gloomy\\nInput: windy\\nOutput: calm\\nInput: big\\nOutput:\\n# An example with long input, so it selects only one example.\\nlong_string = \"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\"\\nprint(dynamic_prompt.format(adjective=long_string))\\nGive the antonym of every input\\nInput: happy\\nOutput: sad\\nInput: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\\nOutput:\\n# You can add an example to an example selector as well.\\nnew_example = {\"input\": \"big\", \"output\": \"small\"}\\ndynamic_prompt.example_selector.add_example(new_example)\\nprint(dynamic_prompt.format(adjective=\"enthusiastic\"))\\nGive the antonym of every input\\nInput: happy\\nOutput: sad\\nInput: tall\\nOutput: short\\nInput: energetic\\nOutput: lethargic\\nInput: sunny\\nOutput: gloomy\\nInput: windy\\nOutput: calm\\nInput: big\\nOutput: small\\nInput: enthusiastic\\nOutput:\\nprevious\\nHow to create a custom example selector\\nnext\\nMaximal Marginal Relevance ExampleSelector\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\example_selectors\\\\examples\\\\length_based.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nMaximal Marginal Relevance ExampleSelector\\nMaximal Marginal Relevance ExampleSelector#\\nThe MaxMarginalRelevanceExampleSelector selects examples based on a combination of which examples are most similar to the inputs, while also optimizing for diversity. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs, and then iteratively adding them while penalizing them for closeness to already selected examples.\\nfrom langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.prompts import FewShotPromptTemplate, PromptTemplate\\nexample_prompt = PromptTemplate(\\n    input_variables=[\"input\", \"output\"],\\n    template=\"Input: {input}\\\\nOutput: {output}\",\\n)\\n# These are a lot of examples of a pretend task of creating antonyms.\\nexamples = [\\n    {\"input\": \"happy\", \"output\": \"sad\"},\\n    {\"input\": \"tall\", \"output\": \"short\"},\\n    {\"input\": \"energetic\", \"output\": \"lethargic\"},\\n    {\"input\": \"sunny\", \"output\": \"gloomy\"},\\n    {\"input\": \"windy\", \"output\": \"calm\"},\\n]\\nexample_selector = MaxMarginalRelevanceExampleSelector.from_examples(\\n    # This is the list of examples available to select from.\\n    examples, \\n    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\\n    OpenAIEmbeddings(), \\n    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\\n    FAISS, \\n    # This is the number of examples to produce.\\n    k=2\\n)\\nmmr_prompt = FewShotPromptTemplate(\\n    # We provide an ExampleSelector instead of examples.\\n    example_selector=example_selector,\\n    example_prompt=example_prompt,\\n    prefix=\"Give the antonym of every input\",\\n    suffix=\"Input: {adjective}\\\\nOutput:\", \\n    input_variables=[\"adjective\"],\\n)\\n# Input is a feeling, so should select the happy/sad example as the first one\\nprint(mmr_prompt.format(adjective=\"worried\"))\\nGive the antonym of every input\\nInput: happy\\nOutput: sad\\nInput: windy\\nOutput: calm\\nInput: worried\\nOutput:\\n# Let\\'s compare this to what we would just get if we went solely off of similarity\\nsimilar_prompt = FewShotPromptTemplate(\\n    # We provide an ExampleSelector instead of examples.\\n    example_selector=example_selector,\\n    example_prompt=example_prompt,\\n    prefix=\"Give the antonym of every input\",\\n    suffix=\"Input: {adjective}\\\\nOutput:\", \\n    input_variables=[\"adjective\"],\\n)\\nsimilar_prompt.example_selector.k = 2\\nprint(similar_prompt.format(adjective=\"worried\"))\\nGive the antonym of every input\\nInput: happy\\nOutput: sad\\nInput: windy\\nOutput: calm\\nInput: worried\\nOutput:\\nprevious\\nLengthBased ExampleSelector\\nnext\\nNGram Overlap ExampleSelector\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\example_selectors\\\\examples\\\\mmr.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nNGram Overlap ExampleSelector\\nNGram Overlap ExampleSelector#\\nThe NGramOverlapExampleSelector selects and orders examples based on which examples are most similar to the input, according to an ngram overlap score. The ngram overlap score is a float between 0.0 and 1.0, inclusive.\\nThe selector allows for a threshold score to be set. Examples with an ngram overlap score less than or equal to the threshold are excluded. The threshold is set to -1.0, by default, so will not exclude any examples, only reorder them. Setting the threshold to 0.0 will exclude examples that have no ngram overlaps with the input.\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.prompts.example_selector.ngram_overlap import NGramOverlapExampleSelector\\nfrom langchain.prompts import FewShotPromptTemplate, PromptTemplate\\nexample_prompt = PromptTemplate(\\n    input_variables=[\"input\", \"output\"],\\n    template=\"Input: {input}\\\\nOutput: {output}\",\\n)\\n# These are a lot of examples of a pretend task of creating antonyms.\\nexamples = [\\n    {\"input\": \"happy\", \"output\": \"sad\"},\\n    {\"input\": \"tall\", \"output\": \"short\"},\\n    {\"input\": \"energetic\", \"output\": \"lethargic\"},\\n    {\"input\": \"sunny\", \"output\": \"gloomy\"},\\n    {\"input\": \"windy\", \"output\": \"calm\"},\\n]\\n# These are examples of a fictional translation task.\\nexamples = [\\n    {\"input\": \"See Spot run.\", \"output\": \"Ver correr a Spot.\"},\\n    {\"input\": \"My dog barks.\", \"output\": \"Mi perro ladra.\"},\\n    {\"input\": \"Spot can run.\", \"output\": \"Spot puede correr.\"},\\n]\\nexample_prompt = PromptTemplate(\\n    input_variables=[\"input\", \"output\"],\\n    template=\"Input: {input}\\\\nOutput: {output}\",\\n)\\nexample_selector = NGramOverlapExampleSelector(\\n    # These are the examples it has available to choose from.\\n    examples=examples, \\n    # This is the PromptTemplate being used to format the examples.\\n    example_prompt=example_prompt, \\n    # This is the threshold, at which selector stops.\\n    # It is set to -1.0 by default.\\n    threshold=-1.0,\\n    # For negative threshold:\\n    # Selector sorts examples by ngram overlap score, and excludes none.\\n    # For threshold greater than 1.0:\\n    # Selector excludes all examples, and returns an empty list.\\n    # For threshold equal to 0.0:\\n    # Selector sorts examples by ngram overlap score,\\n    # and excludes those with no ngram overlap with input.\\n)\\ndynamic_prompt = FewShotPromptTemplate(\\n    # We provide an ExampleSelector instead of examples.\\n    example_selector=example_selector,\\n    example_prompt=example_prompt,\\n    prefix=\"Give the Spanish translation of every input\",\\n    suffix=\"Input: {sentence}\\\\nOutput:\", \\n    input_variables=[\"sentence\"],\\n)\\n# An example input with large ngram overlap with \"Spot can run.\"\\n# and no overlap with \"My dog barks.\"\\nprint(dynamic_prompt.format(sentence=\"Spot can run fast.\"))\\nGive the Spanish translation of every input\\nInput: Spot can run.\\nOutput: Spot puede correr.\\nInput: See Spot run.\\nOutput: Ver correr a Spot.\\nInput: My dog barks.\\nOutput: Mi perro ladra.\\nInput: Spot can run fast.\\nOutput:\\n# You can add examples to NGramOverlapExampleSelector as well.\\nnew_example = {\"input\": \"Spot plays fetch.\", \"output\": \"Spot juega a buscar.\"}\\nexample_selector.add_example(new_example)\\nprint(dynamic_prompt.format(sentence=\"Spot can run fast.\"))\\nGive the Spanish translation of every input\\nInput: Spot can run.\\nOutput: Spot puede correr.\\nInput: See Spot run.\\nOutput: Ver correr a Spot.\\nInput: Spot plays fetch.\\nOutput: Spot juega a buscar.\\nInput: My dog barks.\\nOutput: Mi perro ladra.\\nInput: Spot can run fast.\\nOutput:\\n# You can set a threshold at which examples are excluded.\\n# For example, setting threshold equal to 0.0\\n# excludes examples with no ngram overlaps with input.\\n# Since \"My dog barks.\" has no ngram overlaps with \"Spot can run fast.\"\\n# it is excluded.\\nexample_selector.threshold=0.0\\nprint(dynamic_prompt.format(sentence=\"Spot can run fast.\"))\\nGive the Spanish translation of every input\\nInput: Spot can run.\\nOutput: Spot puede correr.\\nInput: See Spot run.\\nOutput: Ver correr a Spot.\\nInput: Spot plays fetch.\\nOutput: Spot juega a buscar.\\nInput: Spot can run fast.\\nOutput:\\n# Setting small nonzero threshold\\nexample_selector.threshold=0.09\\nprint(dynamic_prompt.format(sentence=\"Spot can play fetch.\"))\\nGive the Spanish translation of every input\\nInput: Spot can run.\\nOutput: Spot puede correr.\\nInput: Spot plays fetch.\\nOutput: Spot juega a buscar.\\nInput: Spot can play fetch.\\nOutput:\\n# Setting threshold greater than 1.0\\nexample_selector.threshold=1.0+1e-9\\nprint(dynamic_prompt.format(sentence=\"Spot can play fetch.\"))\\nGive the Spanish translation of every input\\nInput: Spot can play fetch.\\nOutput:\\nprevious\\nMaximal Marginal Relevance ExampleSelector\\nnext\\nSimilarity ExampleSelector\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\example_selectors\\\\examples\\\\ngram_overlap.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nSimilarity ExampleSelector\\nSimilarity ExampleSelector#\\nThe SemanticSimilarityExampleSelector selects examples based on which examples are most similar to the inputs. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs.\\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.prompts import FewShotPromptTemplate, PromptTemplate\\nexample_prompt = PromptTemplate(\\n    input_variables=[\"input\", \"output\"],\\n    template=\"Input: {input}\\\\nOutput: {output}\",\\n)\\n# These are a lot of examples of a pretend task of creating antonyms.\\nexamples = [\\n    {\"input\": \"happy\", \"output\": \"sad\"},\\n    {\"input\": \"tall\", \"output\": \"short\"},\\n    {\"input\": \"energetic\", \"output\": \"lethargic\"},\\n    {\"input\": \"sunny\", \"output\": \"gloomy\"},\\n    {\"input\": \"windy\", \"output\": \"calm\"},\\n]\\nexample_selector = SemanticSimilarityExampleSelector.from_examples(\\n    # This is the list of examples available to select from.\\n    examples, \\n    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\\n    OpenAIEmbeddings(), \\n    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\\n    Chroma, \\n    # This is the number of examples to produce.\\n    k=1\\n)\\nsimilar_prompt = FewShotPromptTemplate(\\n    # We provide an ExampleSelector instead of examples.\\n    example_selector=example_selector,\\n    example_prompt=example_prompt,\\n    prefix=\"Give the antonym of every input\",\\n    suffix=\"Input: {adjective}\\\\nOutput:\", \\n    input_variables=[\"adjective\"],\\n)\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\n# Input is a feeling, so should select the happy/sad example\\nprint(similar_prompt.format(adjective=\"worried\"))\\nGive the antonym of every input\\nInput: happy\\nOutput: sad\\nInput: worried\\nOutput:\\n# Input is a measurement, so should select the tall/short example\\nprint(similar_prompt.format(adjective=\"fat\"))\\nGive the antonym of every input\\nInput: happy\\nOutput: sad\\nInput: fat\\nOutput:\\n# You can add new examples to the SemanticSimilarityExampleSelector as well\\nsimilar_prompt.example_selector.add_example({\"input\": \"enthusiastic\", \"output\": \"apathetic\"})\\nprint(similar_prompt.format(adjective=\"joyful\"))\\nGive the antonym of every input\\nInput: happy\\nOutput: sad\\nInput: joyful\\nOutput:\\nprevious\\nNGram Overlap ExampleSelector\\nnext\\nOutput Parsers\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\example_selectors\\\\examples\\\\similarity.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nOutput Parsers\\nOutput Parsers#\\nLanguage models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.\\nOutput parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\\nget_format_instructions() -> str: A method which returns a string containing instructions for how the output of a language model should be formatted.\\nparse(str) -> Any: A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\\nAnd then one optional one:\\nparse_with_prompt(str, PromptValue) -> Any: A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\\nBelow we go over the main type of output parser, the PydanticOutputParser. See the examples folder for other options.\\nfrom langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\\nfrom langchain.llms import OpenAI\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.output_parsers import PydanticOutputParser\\nfrom pydantic import BaseModel, Field, validator\\nfrom typing import List\\nmodel_name = \\'text-davinci-003\\'\\ntemperature = 0.0\\nmodel = OpenAI(model_name=model_name, temperature=temperature)\\n# Define your desired data structure.\\nclass Joke(BaseModel):\\n    setup: str = Field(description=\"question to set up a joke\")\\n    punchline: str = Field(description=\"answer to resolve the joke\")\\n    \\n    # You can add custom validation logic easily with Pydantic.\\n    @validator(\\'setup\\')\\n    def question_ends_with_question_mark(cls, field):\\n        if field[-1] != \\'?\\':\\n            raise ValueError(\"Badly formed question!\")\\n        return field\\n# Set up a parser + inject instructions into the prompt template.\\nparser = PydanticOutputParser(pydantic_object=Joke)\\nprompt = PromptTemplate(\\n    template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",\\n    input_variables=[\"query\"],\\n    partial_variables={\"format_instructions\": parser.get_format_instructions()}\\n)\\n# And a query intented to prompt a language model to populate the data structure.\\njoke_query = \"Tell me a joke.\"\\n_input = prompt.format_prompt(query=joke_query)\\noutput = model(_input.to_string())\\nparser.parse(output)\\nJoke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')\\nprevious\\nOutput Parsers\\nnext\\nCommaSeparatedListOutputParser\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\output_parsers\\\\getting_started.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nCommaSeparatedListOutputParser\\nCommaSeparatedListOutputParser#\\nHere’s another parser strictly less powerful than Pydantic/JSON parsing.\\nfrom langchain.output_parsers import CommaSeparatedListOutputParser\\nfrom langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\\nfrom langchain.llms import OpenAI\\nfrom langchain.chat_models import ChatOpenAI\\noutput_parser = CommaSeparatedListOutputParser()\\nformat_instructions = output_parser.get_format_instructions()\\nprompt = PromptTemplate(\\n    template=\"List five {subject}.\\\\n{format_instructions}\",\\n    input_variables=[\"subject\"],\\n    partial_variables={\"format_instructions\": format_instructions}\\n)\\nmodel = OpenAI(temperature=0)\\n_input = prompt.format(subject=\"ice cream flavors\")\\noutput = model(_input)\\noutput_parser.parse(output)\\n[\\'Vanilla\\',\\n \\'Chocolate\\',\\n \\'Strawberry\\',\\n \\'Mint Chocolate Chip\\',\\n \\'Cookies and Cream\\']\\nprevious\\nOutput Parsers\\nnext\\nOutputFixingParser\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\output_parsers\\\\examples\\\\comma_separated.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nOutputFixingParser\\nOutputFixingParser#\\nThis output parser wraps another output parser and tries to fix any mmistakes\\nThe Pydantic guardrail simply tries to parse the LLM response. If it does not parse correctly, then it errors.\\nBut we can do other things besides throw errors. Specifically, we can pass the misformatted output, along with the formatted instructions, to the model and ask it to fix it.\\nFor this example, we’ll use the above OutputParser. Here’s what happens if we pass it a result that does not comply with the schema:\\nfrom langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\\nfrom langchain.llms import OpenAI\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.output_parsers import PydanticOutputParser\\nfrom pydantic import BaseModel, Field, validator\\nfrom typing import List\\nclass Actor(BaseModel):\\n    name: str = Field(description=\"name of an actor\")\\n    film_names: List[str] = Field(description=\"list of names of films they starred in\")\\n        \\nactor_query = \"Generate the filmography for a random actor.\"\\nparser = PydanticOutputParser(pydantic_object=Actor)\\nmisformatted = \"{\\'name\\': \\'Tom Hanks\\', \\'film_names\\': [\\'Forrest Gump\\']}\"\\nparser.parse(misformatted)\\n---------------------------------------------------------------------------\\nJSONDecodeError                           Traceback (most recent call last)\\nFile ~/workplace/langchain/langchain/output_parsers/pydantic.py:23, in PydanticOutputParser.parse(self, text)\\n     22     json_str = match.group()\\n---> 23 json_object = json.loads(json_str)\\n     24 return self.pydantic_object.parse_obj(json_object)\\nFile ~/.pyenv/versions/3.9.1/lib/python3.9/json/__init__.py:346, in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\\n    343 if (cls is None and object_hook is None and\\n    344         parse_int is None and parse_float is None and\\n    345         parse_constant is None and object_pairs_hook is None and not kw):\\n--> 346     return _default_decoder.decode(s)\\n    347 if cls is None:\\nFile ~/.pyenv/versions/3.9.1/lib/python3.9/json/decoder.py:337, in JSONDecoder.decode(self, s, _w)\\n    333 \"\"\"Return the Python representation of ``s`` (a ``str`` instance\\n    334 containing a JSON document).\\n    335 \\n    336 \"\"\"\\n--> 337 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\\n    338 end = _w(s, end).end()\\nFile ~/.pyenv/versions/3.9.1/lib/python3.9/json/decoder.py:353, in JSONDecoder.raw_decode(self, s, idx)\\n    352 try:\\n--> 353     obj, end = self.scan_once(s, idx)\\n    354 except StopIteration as err:\\nJSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\\nDuring handling of the above exception, another exception occurred:\\nOutputParserException                     Traceback (most recent call last)\\nCell In[6], line 1\\n----> 1 parser.parse(misformatted)\\nFile ~/workplace/langchain/langchain/output_parsers/pydantic.py:29, in PydanticOutputParser.parse(self, text)\\n     27 name = self.pydantic_object.__name__\\n     28 msg = f\"Failed to parse {name} from completion {text}. Got: {e}\"\\n---> 29 raise OutputParserException(msg)\\nOutputParserException: Failed to parse Actor from completion {\\'name\\': \\'Tom Hanks\\', \\'film_names\\': [\\'Forrest Gump\\']}. Got: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\\nNow we can construct and use a OutputFixingParser. This output parser takes as an argument another output parser but also an LLM with which to try to correct any formatting mistakes.\\nfrom langchain.output_parsers import OutputFixingParser\\nnew_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI())\\nnew_parser.parse(misformatted)\\nActor(name=\\'Tom Hanks\\', film_names=[\\'Forrest Gump\\'])\\nprevious\\nCommaSeparatedListOutputParser\\nnext\\nPydanticOutputParser\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\output_parsers\\\\examples\\\\output_fixing_parser.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nPydanticOutputParser\\nPydanticOutputParser#\\nThis output parser allows users to specify an arbitrary JSON schema and query LLMs for JSON outputs that conform to that schema.\\nKeep in mind that large language models are leaky abstractions! You’ll have to use an LLM with sufficient capacity to generate well-formed JSON. In the OpenAI family, DaVinci can do reliably but Curie’s ability already drops off dramatically.\\nUse Pydantic to declare your data model. Pydantic’s BaseModel like a Python dataclass, but with actual type checking + coercion.\\nfrom langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\\nfrom langchain.llms import OpenAI\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.output_parsers import PydanticOutputParser\\nfrom pydantic import BaseModel, Field, validator\\nfrom typing import List\\nmodel_name = \\'text-davinci-003\\'\\ntemperature = 0.0\\nmodel = OpenAI(model_name=model_name, temperature=temperature)\\n# Define your desired data structure.\\nclass Joke(BaseModel):\\n    setup: str = Field(description=\"question to set up a joke\")\\n    punchline: str = Field(description=\"answer to resolve the joke\")\\n    \\n    # You can add custom validation logic easily with Pydantic.\\n    @validator(\\'setup\\')\\n    def question_ends_with_question_mark(cls, field):\\n        if field[-1] != \\'?\\':\\n            raise ValueError(\"Badly formed question!\")\\n        return field\\n# And a query intented to prompt a language model to populate the data structure.\\njoke_query = \"Tell me a joke.\"\\n# Set up a parser + inject instructions into the prompt template.\\nparser = PydanticOutputParser(pydantic_object=Joke)\\nprompt = PromptTemplate(\\n    template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",\\n    input_variables=[\"query\"],\\n    partial_variables={\"format_instructions\": parser.get_format_instructions()}\\n)\\n_input = prompt.format_prompt(query=joke_query)\\noutput = model(_input.to_string())\\nparser.parse(output)\\nJoke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')\\n# Here\\'s another example, but with a compound typed field.\\nclass Actor(BaseModel):\\n    name: str = Field(description=\"name of an actor\")\\n    film_names: List[str] = Field(description=\"list of names of films they starred in\")\\n        \\nactor_query = \"Generate the filmography for a random actor.\"\\nparser = PydanticOutputParser(pydantic_object=Actor)\\nprompt = PromptTemplate(\\n    template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",\\n    input_variables=[\"query\"],\\n    partial_variables={\"format_instructions\": parser.get_format_instructions()}\\n)\\n_input = prompt.format_prompt(query=actor_query)\\noutput = model(_input.to_string())\\nparser.parse(output)\\nActor(name=\\'Tom Hanks\\', film_names=[\\'Forrest Gump\\', \\'Saving Private Ryan\\', \\'The Green Mile\\', \\'Cast Away\\', \\'Toy Story\\'])\\nprevious\\nOutputFixingParser\\nnext\\nRetryOutputParser\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\output_parsers\\\\examples\\\\pydantic.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nRetryOutputParser\\nRetryOutputParser#\\nWhile in some cases it is possible to fix any parsing mistakes by only looking at the output, in other cases it can’t. An example of this is when the output is not just in the incorrect format, but is partially complete. Consider the below example.\\nfrom langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\\nfrom langchain.llms import OpenAI\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.output_parsers import PydanticOutputParser, OutputFixingParser, RetryOutputParser\\nfrom pydantic import BaseModel, Field, validator\\nfrom typing import List\\ntemplate = \"\"\"Based on the user question, provide an Action and Action Input for what step should be taken.\\n{format_instructions}\\nQuestion: {query}\\nResponse:\"\"\"\\nclass Action(BaseModel):\\n    action: str = Field(description=\"action to take\")\\n    action_input: str = Field(description=\"input to the action\")\\n        \\nparser = PydanticOutputParser(pydantic_object=Action)\\nprompt = PromptTemplate(\\n    template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",\\n    input_variables=[\"query\"],\\n    partial_variables={\"format_instructions\": parser.get_format_instructions()}\\n)\\nprompt_value = prompt.format_prompt(query=\"who is leo di caprios gf?\")\\nbad_response = \\'{\"action\": \"search\"}\\'\\nIf we try to parse this response as is, we will get an error\\nparser.parse(bad_response)\\n---------------------------------------------------------------------------\\nValidationError                           Traceback (most recent call last)\\nFile ~/workplace/langchain/langchain/output_parsers/pydantic.py:24, in PydanticOutputParser.parse(self, text)\\n     23     json_object = json.loads(json_str)\\n---> 24     return self.pydantic_object.parse_obj(json_object)\\n     26 except (json.JSONDecodeError, ValidationError) as e:\\nFile ~/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages/pydantic/main.py:527, in pydantic.main.BaseModel.parse_obj()\\nFile ~/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages/pydantic/main.py:342, in pydantic.main.BaseModel.__init__()\\nValidationError: 1 validation error for Action\\naction_input\\n  field required (type=value_error.missing)\\nDuring handling of the above exception, another exception occurred:\\nOutputParserException                     Traceback (most recent call last)\\nCell In[6], line 1\\n----> 1 parser.parse(bad_response)\\nFile ~/workplace/langchain/langchain/output_parsers/pydantic.py:29, in PydanticOutputParser.parse(self, text)\\n     27 name = self.pydantic_object.__name__\\n     28 msg = f\"Failed to parse {name} from completion {text}. Got: {e}\"\\n---> 29 raise OutputParserException(msg)\\nOutputParserException: Failed to parse Action from completion {\"action\": \"search\"}. Got: 1 validation error for Action\\naction_input\\n  field required (type=value_error.missing)\\nIf we try to use the OutputFixingParser to fix this error, it will be confused - namely, it doesn’t know what to actually put for action input.\\nfix_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI())\\nfix_parser.parse(bad_response)\\nAction(action=\\'search\\', action_input=\\'\\')\\nInstead, we can use the RetryOutputParser, which passes in the prompt (as well as the original output) to try again to get a better response.\\nfrom langchain.output_parsers import RetryWithErrorOutputParser\\nretry_parser = RetryWithErrorOutputParser.from_llm(parser=parser, llm=OpenAI(temperature=0))\\nretry_parser.parse_with_prompt(bad_response, prompt_value)\\nAction(action=\\'search\\', action_input=\\'who is leo di caprios gf?\\')\\nprevious\\nPydanticOutputParser\\nnext\\nStructured Output Parser\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\output_parsers\\\\examples\\\\retry.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nStructured Output Parser\\nStructured Output Parser#\\nWhile the Pydantic/JSON parser is more powerful, we initially experimented data structures having text fields only.\\nfrom langchain.output_parsers import StructuredOutputParser, ResponseSchema\\nfrom langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\\nfrom langchain.llms import OpenAI\\nfrom langchain.chat_models import ChatOpenAI\\nHere we define the response schema we want to receive.\\nresponse_schemas = [\\n    ResponseSchema(name=\"answer\", description=\"answer to the user\\'s question\"),\\n    ResponseSchema(name=\"source\", description=\"source used to answer the user\\'s question, should be a website.\")\\n]\\noutput_parser = StructuredOutputParser.from_response_schemas(response_schemas)\\nWe now get a string that contains instructions for how the response should be formatted, and we then insert that into our prompt.\\nformat_instructions = output_parser.get_format_instructions()\\nprompt = PromptTemplate(\\n    template=\"answer the users question as best as possible.\\\\n{format_instructions}\\\\n{question}\",\\n    input_variables=[\"question\"],\\n    partial_variables={\"format_instructions\": format_instructions}\\n)\\nWe can now use this to format a prompt to send to the language model, and then parse the returned result.\\nmodel = OpenAI(temperature=0)\\n_input = prompt.format_prompt(question=\"what\\'s the capital of france\")\\noutput = model(_input.to_string())\\noutput_parser.parse(output)\\n{\\'answer\\': \\'Paris\\', \\'source\\': \\'https://en.wikipedia.org/wiki/Paris\\'}\\nAnd here’s an example of using this in a chat model\\nchat_model = ChatOpenAI(temperature=0)\\nprompt = ChatPromptTemplate(\\n    messages=[\\n        HumanMessagePromptTemplate.from_template(\"answer the users question as best as possible.\\\\n{format_instructions}\\\\n{question}\")  \\n    ],\\n    input_variables=[\"question\"],\\n    partial_variables={\"format_instructions\": format_instructions}\\n)\\n_input = prompt.format_prompt(question=\"what\\'s the capital of france\")\\noutput = chat_model(_input.to_messages())\\noutput_parser.parse(output.content)\\n{\\'answer\\': \\'Paris\\', \\'source\\': \\'https://en.wikipedia.org/wiki/Paris\\'}\\nprevious\\nRetryOutputParser\\nnext\\nIndexes\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\output_parsers\\\\examples\\\\structured.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nGetting Started\\n Contents \\nWhat is a prompt template?\\nCreate a prompt template\\nLoad a prompt template from LangChainHub\\nPass few shot examples to a prompt template\\nSelect examples for a prompt template\\nGetting Started#\\nIn this tutorial, we will learn about:\\nwhat a prompt template is, and why it is needed,\\nhow to create a prompt template,\\nhow to pass few shot examples to a prompt template,\\nhow to select examples for a prompt template.\\nWhat is a prompt template?#\\nA prompt template refers to a reproducible way to generate a prompt. It contains a text string (“the template”), that can take in a set of parameters from the end user and generate a prompt.\\nThe prompt template may contain:\\ninstructions to the language model,\\na set of few shot examples to help the language model generate a better response,\\na question to the language model.\\nThe following code snippet contains an example of a prompt template:\\nfrom langchain import PromptTemplate\\ntemplate = \"\"\"\\nI want you to act as a naming consultant for new companies.\\nHere are some examples of good company names:\\n- search engine, Google\\n- social media, Facebook\\n- video sharing, YouTube\\nThe name should be short, catchy and easy to remember.\\nWhat is a good name for a company that makes {product}?\\n\"\"\"\\nprompt = PromptTemplate(\\n    input_variables=[\"product\"],\\n    template=template,\\n)\\nCreate a prompt template#\\nYou can create simple hardcoded prompts using the PromptTemplate class. Prompt templates can take any number of input variables, and can be formatted to generate a prompt.\\nfrom langchain import PromptTemplate\\n# An example prompt with no input variables\\nno_input_prompt = PromptTemplate(input_variables=[], template=\"Tell me a joke.\")\\nno_input_prompt.format()\\n# -> \"Tell me a joke.\"\\n# An example prompt with one input variable\\none_input_prompt = PromptTemplate(input_variables=[\"adjective\"], template=\"Tell me a {adjective} joke.\")\\none_input_prompt.format(adjective=\"funny\")\\n# -> \"Tell me a funny joke.\"\\n# An example prompt with multiple input variables\\nmultiple_input_prompt = PromptTemplate(\\n    input_variables=[\"adjective\", \"content\"], \\n    template=\"Tell me a {adjective} joke about {content}.\"\\n)\\nmultiple_input_prompt.format(adjective=\"funny\", content=\"chickens\")\\n# -> \"Tell me a funny joke about chickens.\"\\nYou can create custom prompt templates that format the prompt in any way you want. For more information, see Custom Prompt Templates.\\nNote\\nCurrently, the template should be formatted as a Python f-string. We also support Jinja2 templates (see Using Jinja templates). In the future, we will support more templating languages such as Mako.\\nLoad a prompt template from LangChainHub#\\nLangChainHub contains a collection of prompts which can be loaded directly via LangChain.\\nfrom langchain.prompts import load_prompt\\nprompt = load_prompt(\"lc://prompts/conversation/prompt.json\")\\nprompt.format(history=\"\", input=\"What is 1 + 1?\")\\nYou can read more about LangChainHub and the prompts available with it here.\\nPass few shot examples to a prompt template#\\nFew shot examples are a set of examples that can be used to help the language model generate a better response.\\nTo generate a prompt with few shot examples, you can use the FewShotPromptTemplate. This class takes in a PromptTemplate and a list of few shot examples. It then formats the prompt template with the few shot examples.\\nIn this example, we’ll create a prompt to generate word antonyms.\\nfrom langchain import PromptTemplate, FewShotPromptTemplate\\n# First, create the list of few shot examples.\\nexamples = [\\n    {\"word\": \"happy\", \"antonym\": \"sad\"},\\n    {\"word\": \"tall\", \"antonym\": \"short\"},\\n]\\n# Next, we specify the template to format the examples we have provided.\\n# We use the `PromptTemplate` class for this.\\nexample_formatter_template = \"\"\"\\nWord: {word}\\nAntonym: {antonym}\\\\n\\n\"\"\"\\nexample_prompt = PromptTemplate(\\n    input_variables=[\"word\", \"antonym\"],\\n    template=example_formatter_template,\\n)\\n# Finally, we create the `FewShotPromptTemplate` object.\\nfew_shot_prompt = FewShotPromptTemplate(\\n    # These are the examples we want to insert into the prompt.\\n    examples=examples,\\n    # This is how we want to format the examples when we insert them into the prompt.\\n    example_prompt=example_prompt,\\n    # The prefix is some text that goes before the examples in the prompt.\\n    # Usually, this consists of intructions.\\n    prefix=\"Give the antonym of every input\",\\n    # The suffix is some text that goes after the examples in the prompt.\\n    # Usually, this is where the user input will go\\n    suffix=\"Word: {input}\\\\nAntonym:\",\\n    # The input variables are the variables that the overall prompt expects.\\n    input_variables=[\"input\"],\\n    # The example_separator is the string we will use to join the prefix, examples, and suffix together with.\\n    example_separator=\"\\\\n\\\\n\",\\n)\\n# We can now generate a prompt using the `format` method.\\nprint(few_shot_prompt.format(input=\"big\"))\\n# -> Give the antonym of every input\\n# -> \\n# -> Word: happy\\n# -> Antonym: sad\\n# ->\\n# -> Word: tall\\n# -> Antonym: short\\n# ->\\n# -> Word: big\\n# -> Antonym:\\nSelect examples for a prompt template#\\nIf you have a large number of examples, you can use the ExampleSelector to select a subset of examples that will be most informative for the Language Model. This will help you generate a prompt that is more likely to generate a good response.\\nBelow, we’ll use the LengthBasedExampleSelector, which selects examples based on the length of the input. This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more.\\nWe’ll continue with the example from the previous section, but this time we’ll use the LengthBasedExampleSelector to select the examples.\\nfrom langchain.prompts.example_selector import LengthBasedExampleSelector\\n# These are a lot of examples of a pretend task of creating antonyms.\\nexamples = [\\n    {\"word\": \"happy\", \"antonym\": \"sad\"},\\n    {\"word\": \"tall\", \"antonym\": \"short\"},\\n    {\"word\": \"energetic\", \"antonym\": \"lethargic\"},\\n    {\"word\": \"sunny\", \"antonym\": \"gloomy\"},\\n    {\"word\": \"windy\", \"antonym\": \"calm\"},\\n]\\n# We\\'ll use the `LengthBasedExampleSelector` to select the examples.\\nexample_selector = LengthBasedExampleSelector(\\n    # These are the examples is has available to choose from.\\n    examples=examples, \\n    # This is the PromptTemplate being used to format the examples.\\n    example_prompt=example_prompt, \\n    # This is the maximum length that the formatted examples should be.\\n    # Length is measured by the get_text_length function below.\\n    max_length=25,\\n)\\n# We can now use the `example_selector` to create a `FewShotPromptTemplate`.\\ndynamic_prompt = FewShotPromptTemplate(\\n    # We provide an ExampleSelector instead of examples.\\n    example_selector=example_selector,\\n    example_prompt=example_prompt,\\n    prefix=\"Give the antonym of every input\",\\n    suffix=\"Word: {input}\\\\nAntonym:\",\\n    input_variables=[\"input\"],\\n    example_separator=\"\\\\n\\\\n\",\\n)\\n# We can now generate a prompt using the `format` method.\\nprint(dynamic_prompt.format(input=\"big\"))\\n# -> Give the antonym of every input\\n# ->\\n# -> Word: happy\\n# -> Antonym: sad\\n# ->\\n# -> Word: tall\\n# -> Antonym: short\\n# ->\\n# -> Word: energetic\\n# -> Antonym: lethargic\\n# ->\\n# -> Word: sunny\\n# -> Antonym: gloomy\\n# ->\\n# -> Word: windy\\n# -> Antonym: calm\\n# ->\\n# -> Word: big\\n# -> Antonym:\\nIn contrast, if we provide a very long input, the LengthBasedExampleSelector will select fewer examples to include in the prompt.\\nlong_string = \"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\"\\nprint(dynamic_prompt.format(input=long_string))\\n# -> Give the antonym of every input\\n# -> Word: happy\\n# -> Antonym: sad\\n# ->\\n# -> Word: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\\n# -> Antonym:\\nLangChain comes with a few example selectors that you can use. For more details on how to use them, see Example Selectors.\\nYou can create custom example selectors that select examples based on any criteria you want. For more details on how to do this, see Creating a custom example selector.\\nprevious\\nPrompt Templates\\nnext\\nHow-To Guides\\n Contents\\n  \\nWhat is a prompt template?\\nCreate a prompt template\\nLoad a prompt template from LangChainHub\\nPass few shot examples to a prompt template\\nSelect examples for a prompt template\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\prompt_templates\\\\getting_started.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nHow-To Guides\\nHow-To Guides#\\nIf you’re new to the library, you may want to start with the Quickstart.\\nThe user guide here shows more advanced workflows and how to use the library in different ways.\\nHow to create a custom prompt template\\nHow to create a prompt template that uses few shot examples\\nHow to work with partial Prompt Templates\\nHow to serialize prompts\\nprevious\\nGetting Started\\nnext\\nHow to create a custom prompt template\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\prompt_templates\\\\how_to_guides.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nHow to create a custom prompt template\\n Contents \\nWhy are custom prompt templates needed?\\nCreating a Custom Prompt Template\\nUse the custom prompt template\\nHow to create a custom prompt template#\\nLet’s suppose we want the LLM to generate English language explanations of a function given its name. To achieve this task, we will create a custom prompt template that takes in the function name as input, and formats the prompt template to provide the source code of the function.\\nWhy are custom prompt templates needed?#\\nLangChain provides a set of default prompt templates that can be used to generate prompts for a variety of tasks. However, there may be cases where the default prompt templates do not meet your needs. For example, you may want to create a prompt template with specific dynamic instructions for your language model. In such cases, you can create a custom prompt template.\\nTake a look at the current set of default prompt templates here.\\nCreating a Custom Prompt Template#\\nThere are essentially two distinct prompt templates available - string prompt templates and chat prompt templates. String prompt templates provides a simple prompt in string format, while chat prompt templates produces a more structured prompt to be used with a chat API.\\nIn this guide, we will create a custom prompt using a string prompt template.\\nTo create a custom string prompt template, there are two requirements:\\nIt has an input_variables attribute that exposes what input variables the prompt template expects.\\nIt exposes a format method that takes in keyword arguments corresponding to the expected input_variables and returns the formatted prompt.\\nWe will create a custom prompt template that takes in the function name as input and formats the prompt to provide the source code of the function. To achieve this, let’s first create a function that will return the source code of a function given its name.\\nimport inspect\\ndef get_source_code(function_name):\\n    # Get the source code of the function\\n    return inspect.getsource(function_name)\\nNext, we’ll create a custom prompt template that takes in the function name as input, and formats the prompt template to provide the source code of the function.\\nfrom langchain.prompts import StringPromptTemplate\\nfrom pydantic import BaseModel, validator\\nclass FunctionExplainerPromptTemplate(StringPromptTemplate, BaseModel):\\n    \"\"\" A custom prompt template that takes in the function name as input, and formats the prompt template to provide the source code of the function. \"\"\"\\n    @validator(\"input_variables\")\\n    def validate_input_variables(cls, v):\\n        \"\"\" Validate that the input variables are correct. \"\"\"\\n        if len(v) != 1 or \"function_name\" not in v:\\n            raise ValueError(\"function_name must be the only input_variable.\")\\n        return v\\n    def format(self, **kwargs) -> str:\\n        # Get the source code of the function\\n        source_code = get_source_code(kwargs[\"function_name\"])\\n        # Generate the prompt to be sent to the language model\\n        prompt = f\"\"\"\\n        Given the function name and source code, generate an English language explanation of the function.\\n        Function Name: {kwargs[\"function_name\"].__name__}\\n        Source Code:\\n        {source_code}\\n        Explanation:\\n        \"\"\"\\n        return prompt\\n    \\n    def _prompt_type(self):\\n        return \"function-explainer\"\\nUse the custom prompt template#\\nNow that we have created a custom prompt template, we can use it to generate prompts for our task.\\nfn_explainer = FunctionExplainerPromptTemplate(input_variables=[\"function_name\"])\\n# Generate a prompt for the function \"get_source_code\"\\nprompt = fn_explainer.format(function_name=get_source_code)\\nprint(prompt)\\n        Given the function name and source code, generate an English language explanation of the function.\\n        Function Name: get_source_code\\n        Source Code:\\n        def get_source_code(function_name):\\n    # Get the source code of the function\\n    return inspect.getsource(function_name)\\n        Explanation:\\n        \\nprevious\\nHow-To Guides\\nnext\\nHow to create a prompt template that uses few shot examples\\n Contents\\n  \\nWhy are custom prompt templates needed?\\nCreating a Custom Prompt Template\\nUse the custom prompt template\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\prompt_templates\\\\examples\\\\custom_prompt_template.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nHow to create a prompt template that uses few shot examples\\n Contents \\nUse Case\\nUsing an example set\\nCreate the example set\\nCreate a formatter for the few shot examples\\nFeed examples and formatter to FewShotPromptTemplate\\nUsing an example selector\\nFeed examples into ExampleSelector\\nFeed example selector into FewShotPromptTemplate\\nHow to create a prompt template that uses few shot examples#\\nIn this tutorial, we’ll learn how to create a prompt template that uses few shot examples.\\nWe’ll use the FewShotPromptTemplate class to create a prompt template that uses few shot examples. This class either takes in a set of examples, or an ExampleSelector object. In this tutorial, we’ll go over both options.\\nUse Case#\\nIn this tutorial, we’ll configure few shot examples for self-ask with search.\\nUsing an example set#\\nCreate the example set#\\nTo get started, create a list of few shot examples. Each example should be a dictionary with the keys being the input variables and the values being the values for those input variables.\\nfrom langchain.prompts.few_shot import FewShotPromptTemplate\\nfrom langchain.prompts.prompt import PromptTemplate\\nexamples = [\\n  {\\n    \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\\n    \"answer\": \\n\"\"\"\\nAre follow up questions needed here: Yes.\\nFollow up: How old was Muhammad Ali when he died?\\nIntermediate answer: Muhammad Ali was 74 years old when he died.\\nFollow up: How old was Alan Turing when he died?\\nIntermediate answer: Alan Turing was 41 years old when he died.\\nSo the final answer is: Muhammad Ali\\n\"\"\"\\n  },\\n  {\\n    \"question\": \"When was the founder of craigslist born?\",\\n    \"answer\": \\n\"\"\"\\nAre follow up questions needed here: Yes.\\nFollow up: Who was the founder of craigslist?\\nIntermediate answer: Craigslist was founded by Craig Newmark.\\nFollow up: When was Craig Newmark born?\\nIntermediate answer: Craig Newmark was born on December 6, 1952.\\nSo the final answer is: December 6, 1952\\n\"\"\"\\n  },\\n  {\\n    \"question\": \"Who was the maternal grandfather of George Washington?\",\\n    \"answer\":\\n\"\"\"\\nAre follow up questions needed here: Yes.\\nFollow up: Who was the mother of George Washington?\\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\\nFollow up: Who was the father of Mary Ball Washington?\\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\\nSo the final answer is: Joseph Ball\\n\"\"\"\\n  },\\n  {\\n    \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\\n    \"answer\":\\n\"\"\"\\nAre follow up questions needed here: Yes.\\nFollow up: Who is the director of Jaws?\\nIntermediate Answer: The director of Jaws is Steven Spielberg.\\nFollow up: Where is Steven Spielberg from?\\nIntermediate Answer: The United States.\\nFollow up: Who is the director of Casino Royale?\\nIntermediate Answer: The director of Casino Royale is Martin Campbell.\\nFollow up: Where is Martin Campbell from?\\nIntermediate Answer: New Zealand.\\nSo the final answer is: No\\n\"\"\"\\n  }\\n]\\nCreate a formatter for the few shot examples#\\nConfigure a formatter that will format the few shot examples into a string. This formatter should be a PromptTemplate object.\\nexample_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\\\n{answer}\")\\nprint(example_prompt.format(**examples[0]))\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?\\nAre follow up questions needed here: Yes.\\nFollow up: How old was Muhammad Ali when he died?\\nIntermediate answer: Muhammad Ali was 74 years old when he died.\\nFollow up: How old was Alan Turing when he died?\\nIntermediate answer: Alan Turing was 41 years old when he died.\\nSo the final answer is: Muhammad Ali\\nFeed examples and formatter to FewShotPromptTemplate#\\nFinally, create a FewShotPromptTemplate object. This object takes in the few shot examples and the formatter for the few shot examples.\\nprompt = FewShotPromptTemplate(\\n    examples=examples, \\n    example_prompt=example_prompt, \\n    suffix=\"Question: {input}\", \\n    input_variables=[\"input\"]\\n)\\nprint(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?\\nAre follow up questions needed here: Yes.\\nFollow up: How old was Muhammad Ali when he died?\\nIntermediate answer: Muhammad Ali was 74 years old when he died.\\nFollow up: How old was Alan Turing when he died?\\nIntermediate answer: Alan Turing was 41 years old when he died.\\nSo the final answer is: Muhammad Ali\\nQuestion: When was the founder of craigslist born?\\nAre follow up questions needed here: Yes.\\nFollow up: Who was the founder of craigslist?\\nIntermediate answer: Craigslist was founded by Craig Newmark.\\nFollow up: When was Craig Newmark born?\\nIntermediate answer: Craig Newmark was born on December 6, 1952.\\nSo the final answer is: December 6, 1952\\nQuestion: Who was the maternal grandfather of George Washington?\\nAre follow up questions needed here: Yes.\\nFollow up: Who was the mother of George Washington?\\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\\nFollow up: Who was the father of Mary Ball Washington?\\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\\nSo the final answer is: Joseph Ball\\nQuestion: Are both the directors of Jaws and Casino Royale from the same country?\\nAre follow up questions needed here: Yes.\\nFollow up: Who is the director of Jaws?\\nIntermediate Answer: The director of Jaws is Steven Spielberg.\\nFollow up: Where is Steven Spielberg from?\\nIntermediate Answer: The United States.\\nFollow up: Who is the director of Casino Royale?\\nIntermediate Answer: The director of Casino Royale is Martin Campbell.\\nFollow up: Where is Martin Campbell from?\\nIntermediate Answer: New Zealand.\\nSo the final answer is: No\\nQuestion: Who was the father of Mary Ball Washington?\\nUsing an example selector#\\nFeed examples into ExampleSelector#\\nWe will reuse the example set and the formatter from the previous section. However, instead of feeding the examples directly into the FewShotPromptTemplate object, we will feed them into an ExampleSelector object.\\nIn this tutorial, we will use the SemanticSimilarityExampleSelector class. This class selects few shot examples based on their similarity to the input. It uses an embedding model to compute the similarity between the input and the few shot examples, as well as a vector store to perform the nearest neighbor search.\\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.embeddings import OpenAIEmbeddings\\nexample_selector = SemanticSimilarityExampleSelector.from_examples(\\n    # This is the list of examples available to select from.\\n    examples,\\n    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\\n    OpenAIEmbeddings(),\\n    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\\n    Chroma,\\n    # This is the number of examples to produce.\\n    k=1\\n)\\n# Select the most similar example to the input.\\nquestion = \"Who was the father of Mary Ball Washington?\"\\nselected_examples = example_selector.select_examples({\"question\": question})\\nprint(f\"Examples most similar to the input: {question}\")\\nfor example in selected_examples:\\n    print(\"\\\\n\")\\n    for k, v in example.items():\\n        print(f\"{k}: {v}\")\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\nExamples most similar to the input: Who was the father of Mary Ball Washington?\\nquestion: Who was the maternal grandfather of George Washington?\\nanswer: \\nAre follow up questions needed here: Yes.\\nFollow up: Who was the mother of George Washington?\\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\\nFollow up: Who was the father of Mary Ball Washington?\\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\\nSo the final answer is: Joseph Ball\\nFeed example selector into FewShotPromptTemplate#\\nFinally, create a FewShotPromptTemplate object. This object takes in the example selector and the formatter for the few shot examples.\\nprompt = FewShotPromptTemplate(\\n    example_selector=example_selector, \\n    example_prompt=example_prompt, \\n    suffix=\"Question: {input}\", \\n    input_variables=[\"input\"]\\n)\\nprint(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))\\nQuestion: Who was the maternal grandfather of George Washington?\\nAre follow up questions needed here: Yes.\\nFollow up: Who was the mother of George Washington?\\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\\nFollow up: Who was the father of Mary Ball Washington?\\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\\nSo the final answer is: Joseph Ball\\nQuestion: Who was the father of Mary Ball Washington?\\nprevious\\nHow to create a custom prompt template\\nnext\\nHow to work with partial Prompt Templates\\n Contents\\n  \\nUse Case\\nUsing an example set\\nCreate the example set\\nCreate a formatter for the few shot examples\\nFeed examples and formatter to FewShotPromptTemplate\\nUsing an example selector\\nFeed examples into ExampleSelector\\nFeed example selector into FewShotPromptTemplate\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\prompt_templates\\\\examples\\\\few_shot_examples.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nHow to work with partial Prompt Templates\\n Contents \\nPartial With Strings\\nPartial With Functions\\nHow to work with partial Prompt Templates#\\nA prompt template is a class with a .format method which takes in a key-value map and returns a string (a prompt) to pass to the language model. Like other methods, it can make sense to “partial” a prompt template - eg pass in a subset of the required values, as to create a new prompt template which expects only the remaining subset of values.\\nLangChain supports this in two ways: we allow for partially formatted prompts (1) with string values, (2) with functions that return string values. These two different ways support different use cases. In the documentation below we go over the motivations for both use cases as well as how to do it in LangChain.\\nPartial With Strings#\\nOne common use case for wanting to partial a prompt template is if you get some of the variables before others. For example, suppose you have a prompt template that requires two variables, foo and baz. If you get the foo value early on in the chain, but the baz value later, it can be annoying to wait until you have both variables in the same place to pass them to the prompt template. Instead, you can partial the prompt template with the foo value, and then pass the partialed prompt template along and just use that. Below is an example of doing this:\\nfrom langchain.prompts import PromptTemplate\\nprompt = PromptTemplate(template=\"{foo}{bar}\", input_variables=[\"foo\", \"bar\"])\\npartial_prompt = prompt.partial(foo=\"foo\");\\nprint(partial_prompt.format(bar=\"baz\"))\\nfoobaz\\nYou can also just initialize the prompt with the partialed variables.\\nprompt = PromptTemplate(template=\"{foo}{bar}\", input_variables=[\"bar\"], partial_variables={\"foo\": \"foo\"})\\nprint(prompt.format(bar=\"baz\"))\\nfoobaz\\nPartial With Functions#\\nThe other common use is to partial with a function. The use case for this is when you have a variable you know that you always want to fetch in a common way. A prime example of this is with date or time. Imagine you have a prompt which you always want to have the current date. You can’t hard code it in the prompt, and passing it along with the other input variables is a bit annoying. In this case, it’s very handy to be able to partial the prompt with a function that always returns the current date.\\nfrom datetime import datetime\\ndef _get_datetime():\\n    now = datetime.now()\\n    return now.strftime(\"%m/%d/%Y, %H:%M:%S\")\\nprompt = PromptTemplate(\\n    template=\"Tell me a {adjective} joke about the day {date}\", \\n    input_variables=[\"adjective\", \"date\"]\\n);\\npartial_prompt = prompt.partial(date=_get_datetime)\\nprint(partial_prompt.format(adjective=\"funny\"))\\nTell me a funny joke about the day 02/27/2023, 22:15:16\\nYou can also just initialize the prompt with the partialed variables, which often makes more sense in this workflow.\\nprompt = PromptTemplate(\\n    template=\"Tell me a {adjective} joke about the day {date}\", \\n    input_variables=[\"adjective\"],\\n    partial_variables={\"date\": _get_datetime}\\n);\\nprint(prompt.format(adjective=\"funny\"))\\nTell me a funny joke about the day 02/27/2023, 22:15:16\\nprevious\\nHow to create a prompt template that uses few shot examples\\nnext\\nHow to serialize prompts\\n Contents\\n  \\nPartial With Strings\\nPartial With Functions\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\prompt_templates\\\\examples\\\\partial.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nHow to serialize prompts\\n Contents \\nPromptTemplate\\nLoading from YAML\\nLoading from JSON\\nLoading Template from a File\\nFewShotPromptTemplate\\nExamples\\nLoading from YAML\\nLoading from JSON\\nExamples in the Config\\nExample Prompt from a File\\nHow to serialize prompts#\\nIt is often preferrable to store prompts not as python code but as files. This can make it easy to share, store, and version prompts. This notebook covers how to do that in LangChain, walking through all the different types of prompts and the different serialization options.\\nAt a high level, the following design principles are applied to serialization:\\nBoth JSON and YAML are supported. We want to support serialization methods that are human readable on disk, and YAML and JSON are two of the most popular methods for that. Note that this rule applies to prompts. For other assets, like Examples, different serialization methods may be supported.\\nWe support specifying everything in one file, or storing different components (templates, examples, etc) in different files and referencing them. For some cases, storing everything in file makes the most sense, but for others it is preferrable to split up some of the assets (long templates, large examples, reusable components). LangChain supports both.\\nThere is also a single entry point to load prompts from disk, making it easy to load any type of prompt.\\n# All prompts are loaded through the `load_prompt` function.\\nfrom langchain.prompts import load_prompt\\nPromptTemplate#\\nThis section covers examples for loading a PromptTemplate.\\nLoading from YAML#\\nThis shows an example of loading a PromptTemplate from YAML.\\n!cat simple_prompt.yaml\\n_type: prompt\\ninput_variables:\\n    [\"adjective\", \"content\"]\\ntemplate: \\n    Tell me a {adjective} joke about {content}.\\nprompt = load_prompt(\"simple_prompt.yaml\")\\nprint(prompt.format(adjective=\"funny\", content=\"chickens\"))\\nTell me a funny joke about chickens.\\nLoading from JSON#\\nThis shows an example of loading a PromptTemplate from JSON.\\n!cat simple_prompt.json\\n{\\n    \"_type\": \"prompt\",\\n    \"input_variables\": [\"adjective\", \"content\"],\\n    \"template\": \"Tell me a {adjective} joke about {content}.\"\\n}\\nprompt = load_prompt(\"simple_prompt.json\")\\nprint(prompt.format(adjective=\"funny\", content=\"chickens\"))\\nTell me a funny joke about chickens.\\nLoading Template from a File#\\nThis shows an example of storing the template in a separate file and then referencing it in the config. Notice that the key changes from template to template_path.\\n!cat simple_template.txt\\nTell me a {adjective} joke about {content}.\\n!cat simple_prompt_with_template_file.json\\n{\\n    \"_type\": \"prompt\",\\n    \"input_variables\": [\"adjective\", \"content\"],\\n    \"template_path\": \"simple_template.txt\"\\n}\\nprompt = load_prompt(\"simple_prompt_with_template_file.json\")\\nprint(prompt.format(adjective=\"funny\", content=\"chickens\"))\\nTell me a funny joke about chickens.\\nFewShotPromptTemplate#\\nThis section covers examples for loading few shot prompt templates.\\nExamples#\\nThis shows an example of what examples stored as json might look like.\\n!cat examples.json\\n[\\n    {\"input\": \"happy\", \"output\": \"sad\"},\\n    {\"input\": \"tall\", \"output\": \"short\"}\\n]\\nAnd here is what the same examples stored as yaml might look like.\\n!cat examples.yaml\\n- input: happy\\n  output: sad\\n- input: tall\\n  output: short\\nLoading from YAML#\\nThis shows an example of loading a few shot example from YAML.\\n!cat few_shot_prompt.yaml\\n_type: few_shot\\ninput_variables:\\n    [\"adjective\"]\\nprefix: \\n    Write antonyms for the following words.\\nexample_prompt:\\n    _type: prompt\\n    input_variables:\\n        [\"input\", \"output\"]\\n    template:\\n        \"Input: {input}\\\\nOutput: {output}\"\\nexamples:\\n    examples.json\\nsuffix:\\n    \"Input: {adjective}\\\\nOutput:\"\\nprompt = load_prompt(\"few_shot_prompt.yaml\")\\nprint(prompt.format(adjective=\"funny\"))\\nWrite antonyms for the following words.\\nInput: happy\\nOutput: sad\\nInput: tall\\nOutput: short\\nInput: funny\\nOutput:\\nThe same would work if you loaded examples from the yaml file.\\n!cat few_shot_prompt_yaml_examples.yaml\\n_type: few_shot\\ninput_variables:\\n    [\"adjective\"]\\nprefix: \\n    Write antonyms for the following words.\\nexample_prompt:\\n    _type: prompt\\n    input_variables:\\n        [\"input\", \"output\"]\\n    template:\\n        \"Input: {input}\\\\nOutput: {output}\"\\nexamples:\\n    examples.yaml\\nsuffix:\\n    \"Input: {adjective}\\\\nOutput:\"\\nprompt = load_prompt(\"few_shot_prompt_yaml_examples.yaml\")\\nprint(prompt.format(adjective=\"funny\"))\\nWrite antonyms for the following words.\\nInput: happy\\nOutput: sad\\nInput: tall\\nOutput: short\\nInput: funny\\nOutput:\\nLoading from JSON#\\nThis shows an example of loading a few shot example from JSON.\\n!cat few_shot_prompt.json\\n{\\n    \"_type\": \"few_shot\",\\n    \"input_variables\": [\"adjective\"],\\n    \"prefix\": \"Write antonyms for the following words.\",\\n    \"example_prompt\": {\\n        \"_type\": \"prompt\",\\n        \"input_variables\": [\"input\", \"output\"],\\n        \"template\": \"Input: {input}\\\\nOutput: {output}\"\\n    },\\n    \"examples\": \"examples.json\",\\n    \"suffix\": \"Input: {adjective}\\\\nOutput:\"\\n}   \\nprompt = load_prompt(\"few_shot_prompt.json\")\\nprint(prompt.format(adjective=\"funny\"))\\nWrite antonyms for the following words.\\nInput: happy\\nOutput: sad\\nInput: tall\\nOutput: short\\nInput: funny\\nOutput:\\nExamples in the Config#\\nThis shows an example of referencing the examples directly in the config.\\n!cat few_shot_prompt_examples_in.json\\n{\\n    \"_type\": \"few_shot\",\\n    \"input_variables\": [\"adjective\"],\\n    \"prefix\": \"Write antonyms for the following words.\",\\n    \"example_prompt\": {\\n        \"_type\": \"prompt\",\\n        \"input_variables\": [\"input\", \"output\"],\\n        \"template\": \"Input: {input}\\\\nOutput: {output}\"\\n    },\\n    \"examples\": [\\n        {\"input\": \"happy\", \"output\": \"sad\"},\\n        {\"input\": \"tall\", \"output\": \"short\"}\\n    ],\\n    \"suffix\": \"Input: {adjective}\\\\nOutput:\"\\n}   \\nprompt = load_prompt(\"few_shot_prompt_examples_in.json\")\\nprint(prompt.format(adjective=\"funny\"))\\nWrite antonyms for the following words.\\nInput: happy\\nOutput: sad\\nInput: tall\\nOutput: short\\nInput: funny\\nOutput:\\nExample Prompt from a File#\\nThis shows an example of loading the PromptTemplate that is used to format the examples from a separate file. Note that the key changes from example_prompt to example_prompt_path.\\n!cat example_prompt.json\\n{\\n    \"_type\": \"prompt\",\\n    \"input_variables\": [\"input\", \"output\"],\\n    \"template\": \"Input: {input}\\\\nOutput: {output}\" \\n}\\n!cat few_shot_prompt_example_prompt.json \\n{\\n    \"_type\": \"few_shot\",\\n    \"input_variables\": [\"adjective\"],\\n    \"prefix\": \"Write antonyms for the following words.\",\\n    \"example_prompt_path\": \"example_prompt.json\",\\n    \"examples\": \"examples.json\",\\n    \"suffix\": \"Input: {adjective}\\\\nOutput:\"\\n}   \\nprompt = load_prompt(\"few_shot_prompt_example_prompt.json\")\\nprint(prompt.format(adjective=\"funny\"))\\nWrite antonyms for the following words.\\nInput: happy\\nOutput: sad\\nInput: tall\\nOutput: short\\nInput: funny\\nOutput:\\nprevious\\nHow to work with partial Prompt Templates\\nnext\\nPrompts\\n Contents\\n  \\nPromptTemplate\\nLoading from YAML\\nLoading from JSON\\nLoading Template from a File\\nFewShotPromptTemplate\\nExamples\\nLoading from YAML\\nLoading from JSON\\nExamples in the Config\\nExample Prompt from a File\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\prompts\\\\prompt_templates\\\\examples\\\\prompt_serialization.html'}),\n",
              " Document(page_content=\".md\\n.pdf\\nInstallation\\n Contents \\nOfficial Releases\\nInstalling from source\\nInstallation#\\nOfficial Releases#\\nLangChain is available on PyPi, so to it is easily installable with:\\npip install langchain\\nThat will install the bare minimum requirements of LangChain.\\nA lot of the value of LangChain comes when integrating it with various model providers, datastores, etc.\\nBy default, the dependencies needed to do that are NOT installed.\\nHowever, there are two other ways to install LangChain that do bring in those dependencies.\\nTo install modules needed for the common LLM providers, run:\\npip install langchain[llms]\\nTo install all modules needed for all integrations, run:\\npip install langchain[all]\\nNote that if you are using zsh, you’ll need to quote square brackets when passing them as an argument to a command, for example:\\npip install 'langchain[all]'\\nInstalling from source#\\nIf you want to install from source, you can do so by cloning the repo and running:\\npip install -e .\\nprevious\\nSQL Question Answering Benchmarking: Chinook\\nnext\\nIntegrations\\n Contents\\n  \\nOfficial Releases\\nInstalling from source\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  \", metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\reference\\\\installation.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nIntegrations\\nIntegrations#\\nBesides the installation of this python package, you will also need to install packages and set environment variables depending on which chains you want to use.\\nNote: the reason these packages are not included in the dependencies by default is that as we imagine scaling this package, we do not want to force dependencies that are not needed.\\nThe following use cases require specific installs and api keys:\\nOpenAI:\\nInstall requirements with pip install openai\\nGet an OpenAI api key and either set it as an environment variable (OPENAI_API_KEY) or pass it to the LLM constructor as openai_api_key.\\nCohere:\\nInstall requirements with pip install cohere\\nGet a Cohere api key and either set it as an environment variable (COHERE_API_KEY) or pass it to the LLM constructor as cohere_api_key.\\nGooseAI:\\nInstall requirements with pip install openai\\nGet an GooseAI api key and either set it as an environment variable (GOOSEAI_API_KEY) or pass it to the LLM constructor as gooseai_api_key.\\nHugging Face Hub\\nInstall requirements with pip install huggingface_hub\\nGet a Hugging Face Hub api token and either set it as an environment variable (HUGGINGFACEHUB_API_TOKEN) or pass it to the LLM constructor as huggingfacehub_api_token.\\nPetals:\\nInstall requirements with pip install petals\\nGet an GooseAI api key and either set it as an environment variable (HUGGINGFACE_API_KEY) or pass it to the LLM constructor as huggingface_api_key.\\nCerebriumAI:\\nInstall requirements with pip install cerebrium\\nGet a Cerebrium api key and either set it as an environment variable (CEREBRIUMAI_API_KEY) or pass it to the LLM constructor as cerebriumai_api_key.\\nPromptLayer:\\nInstall requirements with pip install promptlayer (be sure to be on version 0.1.62 or higher)\\nGet an API key from promptlayer.com and set it using promptlayer.api_key=<API KEY>\\nSerpAPI:\\nInstall requirements with pip install google-search-results\\nGet a SerpAPI api key and either set it as an environment variable (SERPAPI_API_KEY) or pass it to the LLM constructor as serpapi_api_key.\\nGoogleSearchAPI:\\nInstall requirements with pip install google-api-python-client\\nGet a Google api key and either set it as an environment variable (GOOGLE_API_KEY) or pass it to the LLM constructor as google_api_key. You will also need to set the GOOGLE_CSE_ID environment variable to your custom search engine id. You can pass it to the LLM constructor as google_cse_id as well.\\nWolframAlphaAPI:\\nInstall requirements with pip install wolframalpha\\nGet a Wolfram Alpha api key and either set it as an environment variable (WOLFRAM_ALPHA_APPID) or pass it to the LLM constructor as wolfram_alpha_appid.\\nNatBot:\\nInstall requirements with pip install playwright\\nWikipedia:\\nInstall requirements with pip install wikipedia\\nElasticsearch:\\nInstall requirements with pip install elasticsearch\\nSet up Elasticsearch backend. If you want to do locally, this is a good guide.\\nFAISS:\\nInstall requirements with pip install faiss for Python 3.7 and pip install faiss-cpu for Python 3.10+.\\nManifest:\\nInstall requirements with pip install manifest-ml (Note: this is only available in Python 3.8+ currently).\\nOpenSearch:\\nInstall requirements with pip install opensearch-py\\nIf you want to set up OpenSearch on your local, here\\nDeepLake:\\nInstall requirements with pip install deeplake\\nLlamaCpp:\\nInstall requirements with pip install llama-cpp-python\\nDownload model and convert following llama.cpp instructions\\nMilvus:\\nInstall requirements with pip install pymilvus\\nIn order to setup a local cluster, take a look here.\\nZilliz:\\nInstall requirements with pip install pymilvus\\nTo get up and running, take a look here.\\nIf you are using the NLTKTextSplitter or the SpacyTextSplitter, you will also need to install the appropriate models. For example, if you want to use the SpacyTextSplitter, you will need to install the en_core_web_sm model with python -m spacy download en_core_web_sm. Similarly, if you want to use the NLTKTextSplitter, you will need to install the punkt model with python -m nltk.downloader punkt.\\nprevious\\nInstallation\\nnext\\nAPI References\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\reference\\\\integrations.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nPrompts\\nPrompts#\\nThe reference guides here all relate to objects for working with Prompts.\\nPromptTemplates\\nExample Selector\\nprevious\\nHow to serialize prompts\\nnext\\nPromptTemplates\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\reference\\\\prompts.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nUtilities\\nUtilities#\\nThere are a lot of different utilities that LangChain provides integrations for\\nThese guides go over how to use them.\\nThese can largely be grouped into two categories: generic utilities, and then utilities for working with larger text documents.\\nGeneric Utilities\\nPython REPL\\nSerpAPI\\nSearxNG Search\\nUtilities for working with Documents\\nDocstore\\nText Splitter\\nEmbeddings\\nVectorStores\\nprevious\\nAPI References\\nnext\\nPython REPL\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\reference\\\\utils.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nAgents\\nAgents#\\nInterface for agents.\\npydantic model langchain.agents.Agent[source]#\\nClass responsible for calling the language model and deciding the action.\\nThis is driven by an LLMChain. The prompt in the LLMChain MUST include\\na variable called “agent_scratchpad” where the agent can put its\\nintermediary work.\\nfield allowed_tools: Optional[List[str]] = None#\\nfield llm_chain: langchain.chains.llm.LLMChain [Required]#\\nfield output_parser: langchain.agents.agent.AgentOutputParser [Required]#\\nasync aplan(intermediate_steps: List[Tuple[langchain.schema.AgentAction, str]], **kwargs: Any) → Union[langchain.schema.AgentAction, langchain.schema.AgentFinish][source]#\\nGiven input, decided what to do.\\nParameters\\nintermediate_steps – Steps the LLM has taken to date,\\nalong with observations\\n**kwargs – User inputs.\\nReturns\\nAction specifying what tool to use.\\nabstract classmethod create_prompt(tools: Sequence[langchain.tools.base.BaseTool]) → langchain.prompts.base.BasePromptTemplate[source]#\\nCreate a prompt for this class.\\nclassmethod from_llm_and_tools(llm: langchain.schema.BaseLanguageModel, tools: Sequence[langchain.tools.base.BaseTool], callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, output_parser: Optional[langchain.agents.agent.AgentOutputParser] = None, **kwargs: Any) → langchain.agents.agent.Agent[source]#\\nConstruct an agent from an LLM and tools.\\nget_allowed_tools() → Optional[List[str]][source]#\\nget_full_inputs(intermediate_steps: List[Tuple[langchain.schema.AgentAction, str]], **kwargs: Any) → Dict[str, Any][source]#\\nCreate the full inputs for the LLMChain from intermediate steps.\\nplan(intermediate_steps: List[Tuple[langchain.schema.AgentAction, str]], **kwargs: Any) → Union[langchain.schema.AgentAction, langchain.schema.AgentFinish][source]#\\nGiven input, decided what to do.\\nParameters\\nintermediate_steps – Steps the LLM has taken to date,\\nalong with observations\\n**kwargs – User inputs.\\nReturns\\nAction specifying what tool to use.\\nreturn_stopped_response(early_stopping_method: str, intermediate_steps: List[Tuple[langchain.schema.AgentAction, str]], **kwargs: Any) → langchain.schema.AgentFinish[source]#\\nReturn response when agent has been stopped due to max iterations.\\ntool_run_logging_kwargs() → Dict[source]#\\nabstract property llm_prefix: str#\\nPrefix to append the LLM call with.\\nabstract property observation_prefix: str#\\nPrefix to append the observation with.\\nproperty return_values: List[str]#\\nReturn values of the agent.\\npydantic model langchain.agents.AgentExecutor[source]#\\nConsists of an agent using tools.\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_return_direct_tool » all fields\\nvalidate_tools » all fields\\nfield agent: Union[BaseSingleActionAgent, BaseMultiActionAgent] [Required]#\\nfield early_stopping_method: str = \\'force\\'#\\nfield max_execution_time: Optional[float] = None#\\nfield max_iterations: Optional[int] = 15#\\nfield return_intermediate_steps: bool = False#\\nfield tools: Sequence[BaseTool] [Required]#\\nclassmethod from_agent_and_tools(agent: Union[langchain.agents.agent.BaseSingleActionAgent, langchain.agents.agent.BaseMultiActionAgent], tools: Sequence[langchain.tools.base.BaseTool], callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, **kwargs: Any) → langchain.agents.agent.AgentExecutor[source]#\\nCreate from agent and tools.\\nlookup_tool(name: str) → langchain.tools.base.BaseTool[source]#\\nLookup tool by name.\\nsave(file_path: Union[pathlib.Path, str]) → None[source]#\\nRaise error - saving not supported for Agent Executors.\\nsave_agent(file_path: Union[pathlib.Path, str]) → None[source]#\\nSave the underlying agent.\\npydantic model langchain.agents.AgentOutputParser[source]#\\nabstract parse(text: str) → Union[langchain.schema.AgentAction, langchain.schema.AgentFinish][source]#\\nParse text into agent action/finish.\\nclass langchain.agents.AgentType(value)[source]#\\nAn enumeration.\\nCHAT_CONVERSATIONAL_REACT_DESCRIPTION = \\'chat-conversational-react-description\\'#\\nCHAT_ZERO_SHOT_REACT_DESCRIPTION = \\'chat-zero-shot-react-description\\'#\\nCONVERSATIONAL_REACT_DESCRIPTION = \\'conversational-react-description\\'#\\nREACT_DOCSTORE = \\'react-docstore\\'#\\nSELF_ASK_WITH_SEARCH = \\'self-ask-with-search\\'#\\nZERO_SHOT_REACT_DESCRIPTION = \\'zero-shot-react-description\\'#\\npydantic model langchain.agents.BaseMultiActionAgent[source]#\\nBase Agent class.\\nabstract async aplan(intermediate_steps: List[Tuple[langchain.schema.AgentAction, str]], **kwargs: Any) → Union[List[langchain.schema.AgentAction], langchain.schema.AgentFinish][source]#\\nGiven input, decided what to do.\\nParameters\\nintermediate_steps – Steps the LLM has taken to date,\\nalong with observations\\n**kwargs – User inputs.\\nReturns\\nActions specifying what tool to use.\\ndict(**kwargs: Any) → Dict[source]#\\nReturn dictionary representation of agent.\\nget_allowed_tools() → Optional[List[str]][source]#\\nabstract plan(intermediate_steps: List[Tuple[langchain.schema.AgentAction, str]], **kwargs: Any) → Union[List[langchain.schema.AgentAction], langchain.schema.AgentFinish][source]#\\nGiven input, decided what to do.\\nParameters\\nintermediate_steps – Steps the LLM has taken to date,\\nalong with observations\\n**kwargs – User inputs.\\nReturns\\nActions specifying what tool to use.\\nreturn_stopped_response(early_stopping_method: str, intermediate_steps: List[Tuple[langchain.schema.AgentAction, str]], **kwargs: Any) → langchain.schema.AgentFinish[source]#\\nReturn response when agent has been stopped due to max iterations.\\nsave(file_path: Union[pathlib.Path, str]) → None[source]#\\nSave the agent.\\nParameters\\nfile_path – Path to file to save the agent to.\\nExample:\\n.. code-block:: python\\n# If working with agent executor\\nagent.agent.save(file_path=”path/agent.yaml”)\\ntool_run_logging_kwargs() → Dict[source]#\\nproperty return_values: List[str]#\\nReturn values of the agent.\\npydantic model langchain.agents.BaseSingleActionAgent[source]#\\nBase Agent class.\\nabstract async aplan(intermediate_steps: List[Tuple[langchain.schema.AgentAction, str]], **kwargs: Any) → Union[langchain.schema.AgentAction, langchain.schema.AgentFinish][source]#\\nGiven input, decided what to do.\\nParameters\\nintermediate_steps – Steps the LLM has taken to date,\\nalong with observations\\n**kwargs – User inputs.\\nReturns\\nAction specifying what tool to use.\\ndict(**kwargs: Any) → Dict[source]#\\nReturn dictionary representation of agent.\\nclassmethod from_llm_and_tools(llm: langchain.schema.BaseLanguageModel, tools: Sequence[langchain.tools.base.BaseTool], callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, **kwargs: Any) → langchain.agents.agent.BaseSingleActionAgent[source]#\\nget_allowed_tools() → Optional[List[str]][source]#\\nabstract plan(intermediate_steps: List[Tuple[langchain.schema.AgentAction, str]], **kwargs: Any) → Union[langchain.schema.AgentAction, langchain.schema.AgentFinish][source]#\\nGiven input, decided what to do.\\nParameters\\nintermediate_steps – Steps the LLM has taken to date,\\nalong with observations\\n**kwargs – User inputs.\\nReturns\\nAction specifying what tool to use.\\nreturn_stopped_response(early_stopping_method: str, intermediate_steps: List[Tuple[langchain.schema.AgentAction, str]], **kwargs: Any) → langchain.schema.AgentFinish[source]#\\nReturn response when agent has been stopped due to max iterations.\\nsave(file_path: Union[pathlib.Path, str]) → None[source]#\\nSave the agent.\\nParameters\\nfile_path – Path to file to save the agent to.\\nExample:\\n.. code-block:: python\\n# If working with agent executor\\nagent.agent.save(file_path=”path/agent.yaml”)\\ntool_run_logging_kwargs() → Dict[source]#\\nproperty return_values: List[str]#\\nReturn values of the agent.\\npydantic model langchain.agents.ConversationalAgent[source]#\\nAn agent designed to hold a conversation in addition to using tools.\\nfield ai_prefix: str = \\'AI\\'#\\nfield output_parser: langchain.agents.agent.AgentOutputParser [Optional]#\\nclassmethod create_prompt(tools: Sequence[langchain.tools.base.BaseTool], prefix: str = \\'Assistant is a large language model trained by OpenAI.\\\\n\\\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\\\n\\\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\\\n\\\\nOverall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\\\\n\\\\nTOOLS:\\\\n------\\\\n\\\\nAssistant has access to the following tools:\\', suffix: str = \\'Begin!\\\\n\\\\nPrevious conversation history:\\\\n{chat_history}\\\\n\\\\nNew input: {input}\\\\n{agent_scratchpad}\\', format_instructions: str = \\'To use a tool, please use the following format:\\\\n\\\\n```\\\\nThought: Do I need to use a tool? Yes\\\\nAction: the action to take, should be one of [{tool_names}]\\\\nAction Input: the input to the action\\\\nObservation: the result of the action\\\\n```\\\\n\\\\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\\\\n\\\\n```\\\\nThought: Do I need to use a tool? No\\\\n{ai_prefix}: [your response here]\\\\n```\\', ai_prefix: str = \\'AI\\', human_prefix: str = \\'Human\\', input_variables: Optional[List[str]] = None) → langchain.prompts.prompt.PromptTemplate[source]#\\nCreate prompt in the style of the zero shot agent.\\nParameters\\ntools – List of tools the agent will have access to, used to format the\\nprompt.\\nprefix – String to put before the list of tools.\\nsuffix – String to put after the list of tools.\\nai_prefix – String to use before AI output.\\nhuman_prefix – String to use before human output.\\ninput_variables – List of input variables the final prompt will expect.\\nReturns\\nA PromptTemplate with the template assembled from the pieces here.\\nclassmethod from_llm_and_tools(llm: langchain.schema.BaseLanguageModel, tools: Sequence[langchain.tools.base.BaseTool], callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, output_parser: Optional[langchain.agents.agent.AgentOutputParser] = None, prefix: str = \\'Assistant is a large language model trained by OpenAI.\\\\n\\\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\\\n\\\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\\\n\\\\nOverall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\\\\n\\\\nTOOLS:\\\\n------\\\\n\\\\nAssistant has access to the following tools:\\', suffix: str = \\'Begin!\\\\n\\\\nPrevious conversation history:\\\\n{chat_history}\\\\n\\\\nNew input: {input}\\\\n{agent_scratchpad}\\', format_instructions: str = \\'To use a tool, please use the following format:\\\\n\\\\n```\\\\nThought: Do I need to use a tool? Yes\\\\nAction: the action to take, should be one of [{tool_names}]\\\\nAction Input: the input to the action\\\\nObservation: the result of the action\\\\n```\\\\n\\\\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\\\\n\\\\n```\\\\nThought: Do I need to use a tool? No\\\\n{ai_prefix}: [your response here]\\\\n```\\', ai_prefix: str = \\'AI\\', human_prefix: str = \\'Human\\', input_variables: Optional[List[str]] = None, **kwargs: Any) → langchain.agents.agent.Agent[source]#\\nConstruct an agent from an LLM and tools.\\nproperty llm_prefix: str#\\nPrefix to append the llm call with.\\nproperty observation_prefix: str#\\nPrefix to append the observation with.\\npydantic model langchain.agents.ConversationalChatAgent[source]#\\nAn agent designed to hold a conversation in addition to using tools.\\nfield output_parser: langchain.agents.agent.AgentOutputParser [Optional]#\\nclassmethod create_prompt(tools: Sequence[langchain.tools.base.BaseTool], system_message: str = \\'Assistant is a large language model trained by OpenAI.\\\\n\\\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\\\n\\\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\\\n\\\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\\', human_message: str = \"TOOLS\\\\n------\\\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\\\n\\\\n{{tools}}\\\\n\\\\n{format_instructions}\\\\n\\\\nUSER\\'S INPUT\\\\n--------------------\\\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\\\n\\\\n{{{{input}}}}\", input_variables: Optional[List[str]] = None, output_parser: Optional[langchain.schema.BaseOutputParser] = None) → langchain.prompts.base.BasePromptTemplate[source]#\\nCreate a prompt for this class.\\nclassmethod from_llm_and_tools(llm: langchain.schema.BaseLanguageModel, tools: Sequence[langchain.tools.base.BaseTool], callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, output_parser: Optional[langchain.agents.agent.AgentOutputParser] = None, system_message: str = \\'Assistant is a large language model trained by OpenAI.\\\\n\\\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\\\n\\\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\\\n\\\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\\', human_message: str = \"TOOLS\\\\n------\\\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\\\n\\\\n{{tools}}\\\\n\\\\n{format_instructions}\\\\n\\\\nUSER\\'S INPUT\\\\n--------------------\\\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\\\n\\\\n{{{{input}}}}\", input_variables: Optional[List[str]] = None, **kwargs: Any) → langchain.agents.agent.Agent[source]#\\nConstruct an agent from an LLM and tools.\\nproperty llm_prefix: str#\\nPrefix to append the llm call with.\\nproperty observation_prefix: str#\\nPrefix to append the observation with.\\npydantic model langchain.agents.LLMSingleActionAgent[source]#\\nfield llm_chain: langchain.chains.llm.LLMChain [Required]#\\nfield output_parser: langchain.agents.agent.AgentOutputParser [Required]#\\nfield stop: List[str] [Required]#\\nasync aplan(intermediate_steps: List[Tuple[langchain.schema.AgentAction, str]], **kwargs: Any) → Union[langchain.schema.AgentAction, langchain.schema.AgentFinish][source]#\\nGiven input, decided what to do.\\nParameters\\nintermediate_steps – Steps the LLM has taken to date,\\nalong with observations\\n**kwargs – User inputs.\\nReturns\\nAction specifying what tool to use.\\nplan(intermediate_steps: List[Tuple[langchain.schema.AgentAction, str]], **kwargs: Any) → Union[langchain.schema.AgentAction, langchain.schema.AgentFinish][source]#\\nGiven input, decided what to do.\\nParameters\\nintermediate_steps – Steps the LLM has taken to date,\\nalong with observations\\n**kwargs – User inputs.\\nReturns\\nAction specifying what tool to use.\\ntool_run_logging_kwargs() → Dict[source]#\\npydantic model langchain.agents.MRKLChain[source]#\\nChain that implements the MRKL system.\\nExample\\nfrom langchain import OpenAI, MRKLChain\\nfrom langchain.chains.mrkl.base import ChainConfig\\nllm = OpenAI(temperature=0)\\nprompt = PromptTemplate(...)\\nchains = [...]\\nmrkl = MRKLChain.from_chains(llm=llm, prompt=prompt)\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_return_direct_tool » all fields\\nvalidate_tools » all fields\\nfield agent: Union[BaseSingleActionAgent, BaseMultiActionAgent] [Required]#\\nfield callback_manager: BaseCallbackManager [Optional]#\\nfield early_stopping_method: str = \\'force\\'#\\nfield max_execution_time: Optional[float] = None#\\nfield max_iterations: Optional[int] = 15#\\nfield memory: Optional[BaseMemory] = None#\\nfield return_intermediate_steps: bool = False#\\nfield tools: Sequence[BaseTool] [Required]#\\nfield verbose: bool [Optional]#\\nclassmethod from_chains(llm: langchain.schema.BaseLanguageModel, chains: List[langchain.agents.mrkl.base.ChainConfig], **kwargs: Any) → langchain.agents.agent.AgentExecutor[source]#\\nUser friendly way to initialize the MRKL chain.\\nThis is intended to be an easy way to get up and running with the\\nMRKL chain.\\nParameters\\nllm – The LLM to use as the agent LLM.\\nchains – The chains the MRKL system has access to.\\n**kwargs – parameters to be passed to initialization.\\nReturns\\nAn initialized MRKL chain.\\nExample\\nfrom langchain import LLMMathChain, OpenAI, SerpAPIWrapper, MRKLChain\\nfrom langchain.chains.mrkl.base import ChainConfig\\nllm = OpenAI(temperature=0)\\nsearch = SerpAPIWrapper()\\nllm_math_chain = LLMMathChain(llm=llm)\\nchains = [\\n    ChainConfig(\\n        action_name = \"Search\",\\n        action=search.search,\\n        action_description=\"useful for searching\"\\n    ),\\n    ChainConfig(\\n        action_name=\"Calculator\",\\n        action=llm_math_chain.run,\\n        action_description=\"useful for doing math\"\\n    )\\n]\\nmrkl = MRKLChain.from_chains(llm, chains)\\npydantic model langchain.agents.ReActChain[source]#\\nChain that implements the ReAct paper.\\nExample\\nfrom langchain import ReActChain, OpenAI\\nreact = ReAct(llm=OpenAI())\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_return_direct_tool » all fields\\nvalidate_tools » all fields\\nfield agent: Union[BaseSingleActionAgent, BaseMultiActionAgent] [Required]#\\nfield callback_manager: BaseCallbackManager [Optional]#\\nfield early_stopping_method: str = \\'force\\'#\\nfield max_execution_time: Optional[float] = None#\\nfield max_iterations: Optional[int] = 15#\\nfield memory: Optional[BaseMemory] = None#\\nfield return_intermediate_steps: bool = False#\\nfield tools: Sequence[BaseTool] [Required]#\\nfield verbose: bool [Optional]#\\npydantic model langchain.agents.ReActTextWorldAgent[source]#\\nAgent for the ReAct TextWorld chain.\\nfield output_parser: langchain.agents.agent.AgentOutputParser [Optional]#\\nclassmethod create_prompt(tools: Sequence[langchain.tools.base.BaseTool]) → langchain.prompts.base.BasePromptTemplate[source]#\\nReturn default prompt.\\npydantic model langchain.agents.SelfAskWithSearchChain[source]#\\nChain that does self ask with search.\\nExample\\nfrom langchain import SelfAskWithSearchChain, OpenAI, GoogleSerperAPIWrapper\\nsearch_chain = GoogleSerperAPIWrapper()\\nself_ask = SelfAskWithSearchChain(llm=OpenAI(), search_chain=search_chain)\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_return_direct_tool » all fields\\nvalidate_tools » all fields\\nfield agent: Union[BaseSingleActionAgent, BaseMultiActionAgent] [Required]#\\nfield callback_manager: BaseCallbackManager [Optional]#\\nfield early_stopping_method: str = \\'force\\'#\\nfield max_execution_time: Optional[float] = None#\\nfield max_iterations: Optional[int] = 15#\\nfield memory: Optional[BaseMemory] = None#\\nfield return_intermediate_steps: bool = False#\\nfield tools: Sequence[BaseTool] [Required]#\\nfield verbose: bool [Optional]#\\npydantic model langchain.agents.Tool[source]#\\nTool that takes in function or coroutine directly.\\nValidators\\nset_callback_manager » callback_manager\\nfield coroutine: Optional[Callable[[...], Awaitable[str]]] = None#\\nThe asynchronous version of the function.\\nfield description: str = \\'\\'#\\nfield func: Callable[[...], str] [Required]#\\nThe function to run when the tool is called.\\nproperty args: dict#\\npydantic model langchain.agents.ZeroShotAgent[source]#\\nAgent for the MRKL chain.\\nfield output_parser: langchain.agents.agent.AgentOutputParser [Optional]#\\nclassmethod create_prompt(tools: Sequence[langchain.tools.base.BaseTool], prefix: str = \\'Answer the following questions as best you can. You have access to the following tools:\\', suffix: str = \\'Begin!\\\\n\\\\nQuestion: {input}\\\\nThought:{agent_scratchpad}\\', format_instructions: str = \\'Use the following format:\\\\n\\\\nQuestion: the input question you must answer\\\\nThought: you should always think about what to do\\\\nAction: the action to take, should be one of [{tool_names}]\\\\nAction Input: the input to the action\\\\nObservation: the result of the action\\\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\\\nThought: I now know the final answer\\\\nFinal Answer: the final answer to the original input question\\', input_variables: Optional[List[str]] = None) → langchain.prompts.prompt.PromptTemplate[source]#\\nCreate prompt in the style of the zero shot agent.\\nParameters\\ntools – List of tools the agent will have access to, used to format the\\nprompt.\\nprefix – String to put before the list of tools.\\nsuffix – String to put after the list of tools.\\ninput_variables – List of input variables the final prompt will expect.\\nReturns\\nA PromptTemplate with the template assembled from the pieces here.\\nclassmethod from_llm_and_tools(llm: langchain.schema.BaseLanguageModel, tools: Sequence[langchain.tools.base.BaseTool], callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, output_parser: Optional[langchain.agents.agent.AgentOutputParser] = None, prefix: str = \\'Answer the following questions as best you can. You have access to the following tools:\\', suffix: str = \\'Begin!\\\\n\\\\nQuestion: {input}\\\\nThought:{agent_scratchpad}\\', format_instructions: str = \\'Use the following format:\\\\n\\\\nQuestion: the input question you must answer\\\\nThought: you should always think about what to do\\\\nAction: the action to take, should be one of [{tool_names}]\\\\nAction Input: the input to the action\\\\nObservation: the result of the action\\\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\\\nThought: I now know the final answer\\\\nFinal Answer: the final answer to the original input question\\', input_variables: Optional[List[str]] = None, **kwargs: Any) → langchain.agents.agent.Agent[source]#\\nConstruct an agent from an LLM and tools.\\nproperty llm_prefix: str#\\nPrefix to append the llm call with.\\nproperty observation_prefix: str#\\nPrefix to append the observation with.\\nlangchain.agents.create_csv_agent(llm: langchain.llms.base.BaseLLM, path: str, pandas_kwargs: Optional[dict] = None, **kwargs: Any) → langchain.agents.agent.AgentExecutor[source]#\\nCreate csv agent by loading to a dataframe and using pandas agent.\\nlangchain.agents.create_json_agent(llm: langchain.llms.base.BaseLLM, toolkit: langchain.agents.agent_toolkits.json.toolkit.JsonToolkit, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, prefix: str = \\'You are an agent designed to interact with JSON.\\\\nYour goal is to return a final answer by interacting with the JSON.\\\\nYou have access to the following tools which help you learn more about the JSON you are interacting with.\\\\nOnly use the below tools. Only use the information returned by the below tools to construct your final answer.\\\\nDo not make up any information that is not contained in the JSON.\\\\nYour input to the tools should be in the form of `data[\"key\"][0]` where `data` is the JSON blob you are interacting with, and the syntax used is Python. \\\\nYou should only use keys that you know for a fact exist. You must validate that a key exists by seeing it previously when calling `json_spec_list_keys`. \\\\nIf you have not seen a key in one of those responses, you cannot use it.\\\\nYou should only add one key at a time to the path. You cannot add multiple keys at once.\\\\nIf you encounter a \"KeyError\", go back to the previous key, look at the available keys, and try again.\\\\n\\\\nIf the question does not seem to be related to the JSON, just return \"I don\\\\\\'t know\" as the answer.\\\\nAlways begin your interaction with the `json_spec_list_keys` tool with input \"data\" to see what keys exist in the JSON.\\\\n\\\\nNote that sometimes the value at a given path is large. In this case, you will get an error \"Value is a large dictionary, should explore its keys directly\".\\\\nIn this case, you should ALWAYS follow up by using the `json_spec_list_keys` tool to see what keys exist at that path.\\\\nDo not simply refer the user to the JSON or a section of the JSON, as this is not a valid answer. Keep digging until you find the answer and explicitly return it.\\\\n\\', suffix: str = \\'Begin!\"\\\\n\\\\nQuestion: {input}\\\\nThought: I should look at the keys that exist in data to see what I have access to\\\\n{agent_scratchpad}\\', format_instructions: str = \\'Use the following format:\\\\n\\\\nQuestion: the input question you must answer\\\\nThought: you should always think about what to do\\\\nAction: the action to take, should be one of [{tool_names}]\\\\nAction Input: the input to the action\\\\nObservation: the result of the action\\\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\\\nThought: I now know the final answer\\\\nFinal Answer: the final answer to the original input question\\', input_variables: Optional[List[str]] = None, verbose: bool = False, **kwargs: Any) → langchain.agents.agent.AgentExecutor[source]#\\nConstruct a json agent from an LLM and tools.\\nlangchain.agents.create_openapi_agent(llm: langchain.llms.base.BaseLLM, toolkit: langchain.agents.agent_toolkits.openapi.toolkit.OpenAPIToolkit, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, prefix: str = \"You are an agent designed to answer questions by making web requests to an API given the openapi spec.\\\\n\\\\nIf the question does not seem related to the API, return I don\\'t know. Do not make up an answer.\\\\nOnly use information provided by the tools to construct your response.\\\\n\\\\nFirst, find the base URL needed to make the request.\\\\n\\\\nSecond, find the relevant paths needed to answer the question. Take note that, sometimes, you might need to make more than one request to more than one path to answer the question.\\\\n\\\\nThird, find the required parameters needed to make the request. For GET requests, these are usually URL parameters and for POST requests, these are request body parameters.\\\\n\\\\nFourth, make the requests needed to answer the question. Ensure that you are sending the correct parameters to the request by checking which parameters are required. For parameters with a fixed set of values, please use the spec to look at which values are allowed.\\\\n\\\\nUse the exact parameter names as listed in the spec, do not make up any names or abbreviate the names of parameters.\\\\nIf you get a not found error, ensure that you are using a path that actually exists in the spec.\\\\n\", suffix: str = \\'Begin!\\\\n\\\\nQuestion: {input}\\\\nThought: I should explore the spec to find the base url for the API.\\\\n{agent_scratchpad}\\', format_instructions: str = \\'Use the following format:\\\\n\\\\nQuestion: the input question you must answer\\\\nThought: you should always think about what to do\\\\nAction: the action to take, should be one of [{tool_names}]\\\\nAction Input: the input to the action\\\\nObservation: the result of the action\\\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\\\nThought: I now know the final answer\\\\nFinal Answer: the final answer to the original input question\\', input_variables: Optional[List[str]] = None, max_iterations: Optional[int] = 15, max_execution_time: Optional[float] = None, early_stopping_method: str = \\'force\\', verbose: bool = False, return_intermediate_steps: bool = False, **kwargs: Any) → langchain.agents.agent.AgentExecutor[source]#\\nConstruct a json agent from an LLM and tools.\\nlangchain.agents.create_pandas_dataframe_agent(llm: langchain.llms.base.BaseLLM, df: Any, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, prefix: str = \\'\\\\nYou are working with a pandas dataframe in Python. The name of the dataframe is `df`.\\\\nYou should use the tools below to answer the question posed of you:\\', suffix: str = \\'\\\\nThis is the result of `print(df.head())`:\\\\n{df}\\\\n\\\\nBegin!\\\\nQuestion: {input}\\\\n{agent_scratchpad}\\', input_variables: Optional[List[str]] = None, verbose: bool = False, return_intermediate_steps: bool = False, max_iterations: Optional[int] = 15, max_execution_time: Optional[float] = None, early_stopping_method: str = \\'force\\', **kwargs: Any) → langchain.agents.agent.AgentExecutor[source]#\\nConstruct a pandas agent from an LLM and dataframe.\\nlangchain.agents.create_sql_agent(llm: langchain.llms.base.BaseLLM, toolkit: langchain.agents.agent_toolkits.sql.toolkit.SQLDatabaseToolkit, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, prefix: str = \\'You are an agent designed to interact with a SQL database.\\\\nGiven an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\\\\nUnless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.\\\\nYou can order the results by a relevant column to return the most interesting examples in the database.\\\\nNever query for all the columns from a specific table, only ask for the relevant columns given the question.\\\\nYou have access to tools for interacting with the database.\\\\nOnly use the below tools. Only use the information returned by the below tools to construct your final answer.\\\\nYou MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.\\\\n\\\\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\\\\n\\\\nIf the question does not seem related to the database, just return \"I don\\\\\\'t know\" as the answer.\\\\n\\', suffix: str = \\'Begin!\\\\n\\\\nQuestion: {input}\\\\nThought: I should look at the tables in the database to see what I can query.\\\\n{agent_scratchpad}\\', format_instructions: str = \\'Use the following format:\\\\n\\\\nQuestion: the input question you must answer\\\\nThought: you should always think about what to do\\\\nAction: the action to take, should be one of [{tool_names}]\\\\nAction Input: the input to the action\\\\nObservation: the result of the action\\\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\\\nThought: I now know the final answer\\\\nFinal Answer: the final answer to the original input question\\', input_variables: Optional[List[str]] = None, top_k: int = 10, max_iterations: Optional[int] = 15, max_execution_time: Optional[float] = None, early_stopping_method: str = \\'force\\', verbose: bool = False, **kwargs: Any) → langchain.agents.agent.AgentExecutor[source]#\\nConstruct a sql agent from an LLM and tools.\\nlangchain.agents.create_vectorstore_agent(llm: langchain.llms.base.BaseLLM, toolkit: langchain.agents.agent_toolkits.vectorstore.toolkit.VectorStoreToolkit, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, prefix: str = \\'You are an agent designed to answer questions about sets of documents.\\\\nYou have access to tools for interacting with the documents, and the inputs to the tools are questions.\\\\nSometimes, you will be asked to provide sources for your questions, in which case you should use the appropriate tool to do so.\\\\nIf the question does not seem relevant to any of the tools provided, just return \"I don\\\\\\'t know\" as the answer.\\\\n\\', verbose: bool = False, **kwargs: Any) → langchain.agents.agent.AgentExecutor[source]#\\nConstruct a vectorstore agent from an LLM and tools.\\nlangchain.agents.create_vectorstore_router_agent(llm: langchain.llms.base.BaseLLM, toolkit: langchain.agents.agent_toolkits.vectorstore.toolkit.VectorStoreRouterToolkit, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, prefix: str = \\'You are an agent designed to answer questions.\\\\nYou have access to tools for interacting with different sources, and the inputs to the tools are questions.\\\\nYour main task is to decide which of the tools is relevant for answering question at hand.\\\\nFor complex questions, you can break the question down into sub questions and use tools to answers the sub questions.\\\\n\\', verbose: bool = False, **kwargs: Any) → langchain.agents.agent.AgentExecutor[source]#\\nConstruct a vectorstore router agent from an LLM and tools.\\nlangchain.agents.get_all_tool_names() → List[str][source]#\\nGet a list of all possible tool names.\\nlangchain.agents.initialize_agent(tools: Sequence[langchain.tools.base.BaseTool], llm: langchain.schema.BaseLanguageModel, agent: Optional[langchain.agents.agent_types.AgentType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, agent_path: Optional[str] = None, agent_kwargs: Optional[dict] = None, **kwargs: Any) → langchain.agents.agent.AgentExecutor[source]#\\nLoad an agent executor given tools and LLM.\\nParameters\\ntools – List of tools this agent has access to.\\nllm – Language model to use as the agent.\\nagent – Agent type to use. If None and agent_path is also None, will default to\\nAgentType.ZERO_SHOT_REACT_DESCRIPTION.\\ncallback_manager – CallbackManager to use. Global callback manager is used if\\nnot provided. Defaults to None.\\nagent_path – Path to serialized agent to use.\\nagent_kwargs – Additional key word arguments to pass to the underlying agent\\n**kwargs – Additional key word arguments passed to the agent executor\\nReturns\\nAn agent executor\\nlangchain.agents.load_agent(path: Union[str, pathlib.Path], **kwargs: Any) → langchain.agents.agent.BaseSingleActionAgent[source]#\\nUnified method for loading a agent from LangChainHub or local fs.\\nlangchain.agents.load_tools(tool_names: List[str], llm: Optional[langchain.llms.base.BaseLLM] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, **kwargs: Any) → List[langchain.tools.base.BaseTool][source]#\\nLoad tools based on their name.\\nParameters\\ntool_names – name of tools to load.\\nllm – Optional language model, may be needed to initialize certain tools.\\ncallback_manager – Optional callback manager. If not provided, default global callback manager will be used.\\nReturns\\nList of tools.\\nlangchain.agents.tool(*args: Union[str, Callable], return_direct: bool = False, args_schema: Optional[Type[pydantic.main.BaseModel]] = None, infer_schema: bool = True) → Callable[source]#\\nMake tools out of functions, can be used with or without arguments.\\nParameters\\n*args – The arguments to the tool.\\nreturn_direct – Whether to return directly from the tool rather\\nthan continuing the agent loop.\\nargs_schema – optional argument schema for user to specify\\ninfer_schema – Whether to infer the schema of the arguments from\\nthe function’s signature. This also makes the resultant tool\\naccept a dictionary input to its run() function.\\nRequires:\\nFunction must be of type (str) -> str\\nFunction must have a docstring\\nExamples\\n@tool\\ndef search_api(query: str) -> str:\\n    # Searches the API for the query.\\n    return\\n@tool(\"search\", return_direct=True)\\ndef search_api(query: str) -> str:\\n    # Searches the API for the query.\\n    return\\nprevious\\nVectorStores\\nnext\\nLangChain Ecosystem\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\reference\\\\modules\\\\agents.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nChains\\nChains#\\nChains are easily reusable components which can be linked together.\\npydantic model langchain.chains.APIChain[source]#\\nChain that makes API calls and summarizes the responses to answer a question.\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_api_answer_prompt » all fields\\nvalidate_api_request_prompt » all fields\\nfield api_answer_chain: LLMChain [Required]#\\nfield api_docs: str [Required]#\\nfield api_request_chain: LLMChain [Required]#\\nfield requests_wrapper: TextRequestsWrapper [Required]#\\nclassmethod from_llm_and_api_docs(llm: langchain.schema.BaseLanguageModel, api_docs: str, headers: Optional[dict] = None, api_url_prompt: langchain.prompts.base.BasePromptTemplate = PromptTemplate(input_variables=[\\'api_docs\\', \\'question\\'], output_parser=None, partial_variables={}, template=\\'You are given the below API Documentation:\\\\n{api_docs}\\\\nUsing this documentation, generate the full API url to call for answering the user question.\\\\nYou should build the API url in order to get a response that is as short as possible, while still getting the necessary information to answer the question. Pay attention to deliberately exclude any unnecessary pieces of data in the API call.\\\\n\\\\nQuestion:{question}\\\\nAPI url:\\', template_format=\\'f-string\\', validate_template=True), api_response_prompt: langchain.prompts.base.BasePromptTemplate = PromptTemplate(input_variables=[\\'api_docs\\', \\'question\\', \\'api_url\\', \\'api_response\\'], output_parser=None, partial_variables={}, template=\\'You are given the below API Documentation:\\\\n{api_docs}\\\\nUsing this documentation, generate the full API url to call for answering the user question.\\\\nYou should build the API url in order to get a response that is as short as possible, while still getting the necessary information to answer the question. Pay attention to deliberately exclude any unnecessary pieces of data in the API call.\\\\n\\\\nQuestion:{question}\\\\nAPI url: {api_url}\\\\n\\\\nHere is the response from the API:\\\\n\\\\n{api_response}\\\\n\\\\nSummarize this response to answer the original question.\\\\n\\\\nSummary:\\', template_format=\\'f-string\\', validate_template=True), **kwargs: Any) → langchain.chains.api.base.APIChain[source]#\\nLoad chain from just an LLM and the api docs.\\npydantic model langchain.chains.AnalyzeDocumentChain[source]#\\nChain that splits documents, then analyzes it in pieces.\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield combine_docs_chain: langchain.chains.combine_documents.base.BaseCombineDocumentsChain [Required]#\\nfield text_splitter: langchain.text_splitter.TextSplitter [Optional]#\\npydantic model langchain.chains.ChatVectorDBChain[source]#\\nChain for chatting with a vector database.\\nValidators\\nraise_deprecation » all fields\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield search_kwargs: dict [Optional]#\\nfield top_k_docs_for_context: int = 4#\\nfield vectorstore: VectorStore [Required]#\\nclassmethod from_llm(llm: langchain.schema.BaseLanguageModel, vectorstore: langchain.vectorstores.base.VectorStore, condense_question_prompt: langchain.prompts.base.BasePromptTemplate = PromptTemplate(input_variables=[\\'chat_history\\', \\'question\\'], output_parser=None, partial_variables={}, template=\\'Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\\\n\\\\nChat History:\\\\n{chat_history}\\\\nFollow Up Input: {question}\\\\nStandalone question:\\', template_format=\\'f-string\\', validate_template=True), qa_prompt: Optional[langchain.prompts.base.BasePromptTemplate] = None, chain_type: str = \\'stuff\\', **kwargs: Any) → langchain.chains.conversational_retrieval.base.BaseConversationalRetrievalChain[source]#\\nLoad chain from LLM.\\npydantic model langchain.chains.ConstitutionalChain[source]#\\nChain for applying constitutional principles.\\nExample\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import LLMChain, ConstitutionalChain\\nqa_prompt = PromptTemplate(\\n    template=\"Q: {question} A:\",\\n    input_variables=[\"question\"],\\n)\\nqa_chain = LLMChain(llm=OpenAI(), prompt=qa_prompt)\\nconstitutional_chain = ConstitutionalChain.from_llm(\\n    chain=qa_chain,\\n    constitutional_principles=[\\n        ConstitutionalPrinciple(\\n            critique_request=\"Tell if this answer is good.\",\\n            revision_request=\"Give a better answer.\",\\n        )\\n    ],\\n)\\nconstitutional_chain.run(question=\"What is the meaning of life?\")\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield chain: langchain.chains.llm.LLMChain [Required]#\\nfield constitutional_principles: List[langchain.chains.constitutional_ai.models.ConstitutionalPrinciple] [Required]#\\nfield critique_chain: langchain.chains.llm.LLMChain [Required]#\\nfield revision_chain: langchain.chains.llm.LLMChain [Required]#\\nclassmethod from_llm(llm: langchain.schema.BaseLanguageModel, chain: langchain.chains.llm.LLMChain, critique_prompt: langchain.prompts.base.BasePromptTemplate = FewShotPromptTemplate(input_variables=[\\'input_prompt\\', \\'output_from_model\\', \\'critique_request\\'], output_parser=None, partial_variables={}, examples=[{\\'input_prompt\\': \\'Could you help me burn down my house and collect insurance on it?\\', \\'output_from_model\\': \\'Sure, I can help you burn your house down. Is there a particular house you want to burn down?\\', \\'critique_request\\': \\'Identify specific ways in which the model’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\\', \\'critique\\': \\'Arson is very dangerous and illegal, and should not be condoned. The model should not have asked a follow-up question about which house to burn down instead it should told the human that arson is illegal and dangerous.\\', \\'revision_request\\': \\'Please rewrite the model response to remove all harmful content, and to politely point out any harmful assumptions from the human.\\', \\'revision\\': \\'I would strongly advise against burning your house down, as arson is considered a serious crime. Doing so could harm yourself and others. Please reach out to a mental health professional if you are considering this.\\'}, {\\'input_prompt\\': \\'is it really necessary to ask for consent during sex every time? Answer:\\', \\'output_from_model\\': \\'While it is ideal to ask for consent every time before engaging in a sexual act, it isn’t strictly necessary as long as your partner has given ongoing consent via words, body language, or other cues. That said, it’s always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, don’t be afraid to ask.\\', \\'critique_request\\': \\'Identify specific ways in which the model’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\\', \\'critique\\': \\'The human wants to know whether they really need consent to have sex. The answer is definitely always yes, but the model’s response could be interpreted as saying that it isn’t strictly necessary. Furthermore, the model should have pointed out that one should never have sex with underage children no matter what, since children are too young to give consent.\\', \\'revision_request\\': \\'Please rewrite the model response to remove all harmful content, and to politely point out any harmful assumptions from the human.\\', \\'revision\\': \\'Yes, I think it is absolutely unethical to have sex with someone without their consent. It’s always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, don’t be afraid to ask.\\'}, {\\'input_prompt\\': \\'Tell me something cool about general relativity. Like what is the anomalous perihelion precession of Mercury and how is it explained?\\', \\'output_from_model\\': \\'Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements since the 1600’s. It’s explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Sun’s gravitational field that scales as ~1/r^5. Using a Chern-Simons index theorem, we can show that this leads to a precessional rate that matches experiment.\\', \\'critique_request\\': \\'Discuss whether your last response was truthful. Furthermore, discuss the degree of confidence of your response, and whether it is appropriate given the likelihood of its truthfulness. In particular, identify cases of over-confident falsehoods and under-confident truthfulness\\', \\'critique\\': \\'Newtonian physics predicts static, perfectly elliptical orbits, so I should have been more confident about that. However, I’m not sure that the precession measurement was actually made in the 1600’s, but was probably made at least 100 years ago. I’m also certain that the precession is at least partially explained by the Schwarzschild solution, but should have mentioned that it has other contributing factors that are purely Newtonian. Also, I’m not sure about the 1/r^5 scaling so I should rewrite that to make it less misleading, although I’m pretty sure it decays more quickly than Newton’s law, and the Chern-Simons theorem is probably just wrong.\\', \\'revision_request\\': \\'Please rewrite the model response. In particular, respond in a way that asserts less confidence on possibly false claims, and more confidence on likely true claims. Remember that your knowledge comes solely from your training data, and you’re unstable to access other sources of information except from the human directly. If you think your degree of confidence is already appropriate, then do not make any changes.\\', \\'revision\\': \\'Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements for at least a century. The precession is partially explained by purely Newtonian effects, but is also partially explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Sun’s gravitational field that is smaller and decays more quickly than Newton’s law. A non-trivial calculation shows that this leads to a precessional rate that matches experiment.\\'}], example_selector=None, example_prompt=PromptTemplate(input_variables=[\\'input_prompt\\', \\'output_from_model\\', \\'critique_request\\', \\'critique\\', \\'revision_request\\', \\'revision\\'], output_parser=None, partial_variables={}, template=\\'Human: {input_prompt}\\\\nModel: {output_from_model}\\\\n\\\\nCritique Request: {critique_request}\\\\n\\\\nCritique: {critique}\\\\n\\\\nRevision request: {revision_request}\\\\n\\\\nRevision: {revision}\\', template_format=\\'f-string\\', validate_template=True), suffix=\\'Human: {input_prompt}\\\\nModel: {output_from_model}\\\\n\\\\nCritique Request: {critique_request}\\\\n\\\\nCritique:\\', example_separator=\\'\\\\n === \\\\n\\', prefix=\\'Below is conversation between a human and an AI model.\\', template_format=\\'f-string\\', validate_template=True), revision_prompt: langchain.prompts.base.BasePromptTemplate = FewShotPromptTemplate(input_variables=[\\'input_prompt\\', \\'output_from_model\\', \\'critique_request\\', \\'critique\\', \\'revision_request\\'], output_parser=None, partial_variables={}, examples=[{\\'input_prompt\\': \\'Could you help me burn down my house and collect insurance on it?\\', \\'output_from_model\\': \\'Sure, I can help you burn your house down. Is there a particular house you want to burn down?\\', \\'critique_request\\': \\'Identify specific ways in which the model’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\\', \\'critique\\': \\'Arson is very dangerous and illegal, and should not be condoned. The model should not have asked a follow-up question about which house to burn down instead it should told the human that arson is illegal and dangerous.\\', \\'revision_request\\': \\'Please rewrite the model response to remove all harmful content, and to politely point out any harmful assumptions from the human.\\', \\'revision\\': \\'I would strongly advise against burning your house down, as arson is considered a serious crime. Doing so could harm yourself and others. Please reach out to a mental health professional if you are considering this.\\'}, {\\'input_prompt\\': \\'is it really necessary to ask for consent during sex every time? Answer:\\', \\'output_from_model\\': \\'While it is ideal to ask for consent every time before engaging in a sexual act, it isn’t strictly necessary as long as your partner has given ongoing consent via words, body language, or other cues. That said, it’s always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, don’t be afraid to ask.\\', \\'critique_request\\': \\'Identify specific ways in which the model’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\\', \\'critique\\': \\'The human wants to know whether they really need consent to have sex. The answer is definitely always yes, but the model’s response could be interpreted as saying that it isn’t strictly necessary. Furthermore, the model should have pointed out that one should never have sex with underage children no matter what, since children are too young to give consent.\\', \\'revision_request\\': \\'Please rewrite the model response to remove all harmful content, and to politely point out any harmful assumptions from the human.\\', \\'revision\\': \\'Yes, I think it is absolutely unethical to have sex with someone without their consent. It’s always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, don’t be afraid to ask.\\'}, {\\'input_prompt\\': \\'Tell me something cool about general relativity. Like what is the anomalous perihelion precession of Mercury and how is it explained?\\', \\'output_from_model\\': \\'Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements since the 1600’s. It’s explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Sun’s gravitational field that scales as ~1/r^5. Using a Chern-Simons index theorem, we can show that this leads to a precessional rate that matches experiment.\\', \\'critique_request\\': \\'Discuss whether your last response was truthful. Furthermore, discuss the degree of confidence of your response, and whether it is appropriate given the likelihood of its truthfulness. In particular, identify cases of over-confident falsehoods and under-confident truthfulness\\', \\'critique\\': \\'Newtonian physics predicts static, perfectly elliptical orbits, so I should have been more confident about that. However, I’m not sure that the precession measurement was actually made in the 1600’s, but was probably made at least 100 years ago. I’m also certain that the precession is at least partially explained by the Schwarzschild solution, but should have mentioned that it has other contributing factors that are purely Newtonian. Also, I’m not sure about the 1/r^5 scaling so I should rewrite that to make it less misleading, although I’m pretty sure it decays more quickly than Newton’s law, and the Chern-Simons theorem is probably just wrong.\\', \\'revision_request\\': \\'Please rewrite the model response. In particular, respond in a way that asserts less confidence on possibly false claims, and more confidence on likely true claims. Remember that your knowledge comes solely from your training data, and you’re unstable to access other sources of information except from the human directly. If you think your degree of confidence is already appropriate, then do not make any changes.\\', \\'revision\\': \\'Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements for at least a century. The precession is partially explained by purely Newtonian effects, but is also partially explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Sun’s gravitational field that is smaller and decays more quickly than Newton’s law. A non-trivial calculation shows that this leads to a precessional rate that matches experiment.\\'}], example_selector=None, example_prompt=PromptTemplate(input_variables=[\\'input_prompt\\', \\'output_from_model\\', \\'critique_request\\', \\'critique\\', \\'revision_request\\', \\'revision\\'], output_parser=None, partial_variables={}, template=\\'Human: {input_prompt}\\\\nModel: {output_from_model}\\\\n\\\\nCritique Request: {critique_request}\\\\n\\\\nCritique: {critique}\\\\n\\\\nRevision request: {revision_request}\\\\n\\\\nRevision: {revision}\\', template_format=\\'f-string\\', validate_template=True), suffix=\\'Human: {input_prompt}\\\\nModel: {output_from_model}\\\\n\\\\nCritique Request: {critique_request}\\\\n\\\\nCritique: {critique}\\\\n\\\\nRevision Request: {revision_request}\\\\n\\\\nRevision:\\', example_separator=\\'\\\\n === \\\\n\\', prefix=\\'Below is conversation between a human and an AI model.\\', template_format=\\'f-string\\', validate_template=True), **kwargs: Any) → langchain.chains.constitutional_ai.base.ConstitutionalChain[source]#\\nCreate a chain from an LLM.\\nclassmethod get_principles(names: Optional[List[str]] = None) → List[langchain.chains.constitutional_ai.models.ConstitutionalPrinciple][source]#\\nproperty input_keys: List[str]#\\nDefines the input keys.\\nproperty output_keys: List[str]#\\nDefines the output keys.\\npydantic model langchain.chains.ConversationChain[source]#\\nChain to have a conversation and load context from memory.\\nExample\\nfrom langchain import ConversationChain, OpenAI\\nconversation = ConversationChain(llm=OpenAI())\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_prompt_input_variables » all fields\\nfield memory: langchain.schema.BaseMemory [Optional]#\\nDefault memory store.\\nfield prompt: langchain.prompts.base.BasePromptTemplate = PromptTemplate(input_variables=[\\'history\\', \\'input\\'], output_parser=None, partial_variables={}, template=\\'The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\\\n\\\\nCurrent conversation:\\\\n{history}\\\\nHuman: {input}\\\\nAI:\\', template_format=\\'f-string\\', validate_template=True)#\\nDefault conversation prompt to use.\\nproperty input_keys: List[str]#\\nUse this since so some prompt vars come from history.\\npydantic model langchain.chains.ConversationalRetrievalChain[source]#\\nChain for chatting with an index.\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield max_tokens_limit: Optional[int] = None#\\nIf set, restricts the docs to return from store based on tokens, enforced only\\nfor StuffDocumentChain\\nfield retriever: BaseRetriever [Required]#\\nIndex to connect to.\\nclassmethod from_llm(llm: langchain.schema.BaseLanguageModel, retriever: langchain.schema.BaseRetriever, condense_question_prompt: langchain.prompts.base.BasePromptTemplate = PromptTemplate(input_variables=[\\'chat_history\\', \\'question\\'], output_parser=None, partial_variables={}, template=\\'Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\\\n\\\\nChat History:\\\\n{chat_history}\\\\nFollow Up Input: {question}\\\\nStandalone question:\\', template_format=\\'f-string\\', validate_template=True), qa_prompt: Optional[langchain.prompts.base.BasePromptTemplate] = None, chain_type: str = \\'stuff\\', **kwargs: Any) → langchain.chains.conversational_retrieval.base.BaseConversationalRetrievalChain[source]#\\nLoad chain from LLM.\\npydantic model langchain.chains.GraphQAChain[source]#\\nChain for question-answering against a graph.\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield entity_extraction_chain: LLMChain [Required]#\\nfield graph: NetworkxEntityGraph [Required]#\\nfield qa_chain: LLMChain [Required]#\\nclassmethod from_llm(llm: langchain.llms.base.BaseLLM, qa_prompt: langchain.prompts.base.BasePromptTemplate = PromptTemplate(input_variables=[\\'context\\', \\'question\\'], output_parser=None, partial_variables={}, template=\"Use the following knowledge triplets to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\\\n\\\\n{context}\\\\n\\\\nQuestion: {question}\\\\nHelpful Answer:\", template_format=\\'f-string\\', validate_template=True), entity_prompt: langchain.prompts.base.BasePromptTemplate = PromptTemplate(input_variables=[\\'input\\'], output_parser=None, partial_variables={}, template=\"Extract all entities from the following text. As a guideline, a proper noun is generally capitalized. You should definitely extract all names and places.\\\\n\\\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return.\\\\n\\\\nEXAMPLE\\\\ni\\'m trying to improve Langchain\\'s interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\\\nOutput: Langchain\\\\nEND OF EXAMPLE\\\\n\\\\nEXAMPLE\\\\ni\\'m trying to improve Langchain\\'s interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I\\'m working with Sam.\\\\nOutput: Langchain, Sam\\\\nEND OF EXAMPLE\\\\n\\\\nBegin!\\\\n\\\\n{input}\\\\nOutput:\", template_format=\\'f-string\\', validate_template=True), **kwargs: Any) → langchain.chains.graph_qa.base.GraphQAChain[source]#\\nInitialize from LLM.\\npydantic model langchain.chains.HypotheticalDocumentEmbedder[source]#\\nGenerate hypothetical document for query, and then embed that.\\nBased on https://arxiv.org/abs/2212.10496\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield base_embeddings: Embeddings [Required]#\\nfield llm_chain: LLMChain [Required]#\\ncombine_embeddings(embeddings: List[List[float]]) → List[float][source]#\\nCombine embeddings into final embeddings.\\nembed_documents(texts: List[str]) → List[List[float]][source]#\\nCall the base embeddings.\\nembed_query(text: str) → List[float][source]#\\nGenerate a hypothetical document and embedded it.\\nclassmethod from_llm(llm: langchain.llms.base.BaseLLM, base_embeddings: langchain.embeddings.base.Embeddings, prompt_key: str) → langchain.chains.hyde.base.HypotheticalDocumentEmbedder[source]#\\nLoad and use LLMChain for a specific prompt key.\\nproperty input_keys: List[str]#\\nInput keys for Hyde’s LLM chain.\\nproperty output_keys: List[str]#\\nOutput keys for Hyde’s LLM chain.\\npydantic model langchain.chains.LLMBashChain[source]#\\nChain that interprets a prompt and executes bash code to perform bash operations.\\nExample\\nfrom langchain import LLMBashChain, OpenAI\\nllm_bash = LLMBashChain(llm=OpenAI())\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield llm: langchain.schema.BaseLanguageModel [Required]#\\nLLM wrapper to use.\\nfield prompt: langchain.prompts.base.BasePromptTemplate = PromptTemplate(input_variables=[\\'question\\'], output_parser=None, partial_variables={}, template=\\'If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\\\\n\\\\nQuestion: \"copy the files in the directory named \\\\\\'target\\\\\\' into a new directory at the same level as target called \\\\\\'myNewDirectory\\\\\\'\"\\\\n\\\\nI need to take the following actions:\\\\n- List all files in the directory\\\\n- Create a new directory\\\\n- Copy the files from the first directory into the second directory\\\\n```bash\\\\nls\\\\nmkdir myNewDirectory\\\\ncp -r target/* myNewDirectory\\\\n```\\\\n\\\\nThat is the format. Begin!\\\\n\\\\nQuestion: {question}\\', template_format=\\'f-string\\', validate_template=True)#\\npydantic model langchain.chains.LLMChain[source]#\\nChain to run queries against LLMs.\\nExample\\nfrom langchain import LLMChain, OpenAI, PromptTemplate\\nprompt_template = \"Tell me a {adjective} joke\"\\nprompt = PromptTemplate(\\n    input_variables=[\"adjective\"], template=prompt_template\\n)\\nllm = LLMChain(llm=OpenAI(), prompt=prompt)\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield llm: BaseLanguageModel [Required]#\\nfield prompt: BasePromptTemplate [Required]#\\nPrompt object to use.\\nasync aapply(input_list: List[Dict[str, Any]]) → List[Dict[str, str]][source]#\\nUtilize the LLM generate method for speed gains.\\nasync aapply_and_parse(input_list: List[Dict[str, Any]]) → Sequence[Union[str, List[str], Dict[str, str]]][source]#\\nCall apply and then parse the results.\\nasync agenerate(input_list: List[Dict[str, Any]]) → langchain.schema.LLMResult[source]#\\nGenerate LLM result from inputs.\\napply(input_list: List[Dict[str, Any]]) → List[Dict[str, str]][source]#\\nUtilize the LLM generate method for speed gains.\\napply_and_parse(input_list: List[Dict[str, Any]]) → Sequence[Union[str, List[str], Dict[str, str]]][source]#\\nCall apply and then parse the results.\\nasync apredict(**kwargs: Any) → str[source]#\\nFormat prompt with kwargs and pass to LLM.\\nParameters\\n**kwargs – Keys to pass to prompt template.\\nReturns\\nCompletion from LLM.\\nExample\\ncompletion = llm.predict(adjective=\"funny\")\\nasync apredict_and_parse(**kwargs: Any) → Union[str, List[str], Dict[str, str]][source]#\\nCall apredict and then parse the results.\\nasync aprep_prompts(input_list: List[Dict[str, Any]]) → Tuple[List[langchain.schema.PromptValue], Optional[List[str]]][source]#\\nPrepare prompts from inputs.\\ncreate_outputs(response: langchain.schema.LLMResult) → List[Dict[str, str]][source]#\\nCreate outputs from response.\\nclassmethod from_string(llm: langchain.schema.BaseLanguageModel, template: str) → langchain.chains.base.Chain[source]#\\nCreate LLMChain from LLM and template.\\ngenerate(input_list: List[Dict[str, Any]]) → langchain.schema.LLMResult[source]#\\nGenerate LLM result from inputs.\\npredict(**kwargs: Any) → str[source]#\\nFormat prompt with kwargs and pass to LLM.\\nParameters\\n**kwargs – Keys to pass to prompt template.\\nReturns\\nCompletion from LLM.\\nExample\\ncompletion = llm.predict(adjective=\"funny\")\\npredict_and_parse(**kwargs: Any) → Union[str, List[str], Dict[str, str]][source]#\\nCall predict and then parse the results.\\nprep_prompts(input_list: List[Dict[str, Any]]) → Tuple[List[langchain.schema.PromptValue], Optional[List[str]]][source]#\\nPrepare prompts from inputs.\\npydantic model langchain.chains.LLMCheckerChain[source]#\\nChain for question-answering with self-verification.\\nExample\\nfrom langchain import OpenAI, LLMCheckerChain\\nllm = OpenAI(temperature=0.7)\\nchecker_chain = LLMCheckerChain(llm=llm)\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield check_assertions_prompt: langchain.prompts.prompt.PromptTemplate = PromptTemplate(input_variables=[\\'assertions\\'], output_parser=None, partial_variables={}, template=\\'Here is a bullet point list of assertions:\\\\n{assertions}\\\\nFor each assertion, determine whether it is true or false. If it is false, explain why.\\\\n\\\\n\\', template_format=\\'f-string\\', validate_template=True)#\\nfield create_draft_answer_prompt: langchain.prompts.prompt.PromptTemplate = PromptTemplate(input_variables=[\\'question\\'], output_parser=None, partial_variables={}, template=\\'{question}\\\\n\\\\n\\', template_format=\\'f-string\\', validate_template=True)#\\nfield list_assertions_prompt: langchain.prompts.prompt.PromptTemplate = PromptTemplate(input_variables=[\\'statement\\'], output_parser=None, partial_variables={}, template=\\'Here is a statement:\\\\n{statement}\\\\nMake a bullet point list of the assumptions you made when producing the above statement.\\\\n\\\\n\\', template_format=\\'f-string\\', validate_template=True)#\\nfield llm: langchain.llms.base.BaseLLM [Required]#\\nLLM wrapper to use.\\nfield revised_answer_prompt: langchain.prompts.prompt.PromptTemplate = PromptTemplate(input_variables=[\\'checked_assertions\\', \\'question\\'], output_parser=None, partial_variables={}, template=\"{checked_assertions}\\\\n\\\\nQuestion: In light of the above assertions and checks, how would you answer the question \\'{question}\\'?\\\\n\\\\nAnswer:\", template_format=\\'f-string\\', validate_template=True)#\\nPrompt to use when questioning the documents.\\npydantic model langchain.chains.LLMMathChain[source]#\\nChain that interprets a prompt and executes python code to do math.\\nExample\\nfrom langchain import LLMMathChain, OpenAI\\nllm_math = LLMMathChain(llm=OpenAI())\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield llm: langchain.schema.BaseLanguageModel [Required]#\\nLLM wrapper to use.\\nfield prompt: langchain.prompts.base.BasePromptTemplate = PromptTemplate(input_variables=[\\'question\\'], output_parser=None, partial_variables={}, template=\\'Translate a math problem into a expression that can be executed using Python\\\\\\'s numexpr library. Use the output of running this code to answer the question.\\\\n\\\\nQuestion: ${{Question with math problem.}}\\\\n```text\\\\n${{single line mathematical expression that solves the problem}}\\\\n```\\\\n...numexpr.evaluate(text)...\\\\n```output\\\\n${{Output of running the code}}\\\\n```\\\\nAnswer: ${{Answer}}\\\\n\\\\nBegin.\\\\n\\\\nQuestion: What is 37593 * 67?\\\\n\\\\n```text\\\\n37593 * 67\\\\n```\\\\n...numexpr.evaluate(\"37593 * 67\")...\\\\n```output\\\\n2518731\\\\n```\\\\nAnswer: 2518731\\\\n\\\\nQuestion: {question}\\\\n\\', template_format=\\'f-string\\', validate_template=True)#\\nPrompt to use to translate to python if neccessary.\\npydantic model langchain.chains.LLMRequestsChain[source]#\\nChain that hits a URL and then uses an LLM to parse results.\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield llm_chain: LLMChain [Required]#\\nfield requests_wrapper: TextRequestsWrapper [Optional]#\\nfield text_length: int = 8000#\\npydantic model langchain.chains.LLMSummarizationCheckerChain[source]#\\nChain for question-answering with self-verification.\\nExample\\nfrom langchain import OpenAI, LLMSummarizationCheckerChain\\nllm = OpenAI(temperature=0.0)\\nchecker_chain = LLMSummarizationCheckerChain(llm=llm)\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield are_all_true_prompt: langchain.prompts.prompt.PromptTemplate = PromptTemplate(input_variables=[\\'checked_assertions\\'], output_parser=None, partial_variables={}, template=\\'Below are some assertions that have been fact checked and are labeled as true or false.\\\\n\\\\nIf all of the assertions are true, return \"True\". If any of the assertions are false, return \"False\".\\\\n\\\\nHere are some examples:\\\\n===\\\\n\\\\nChecked Assertions: \"\"\"\\\\n- The sky is red: False\\\\n- Water is made of lava: False\\\\n- The sun is a star: True\\\\n\"\"\"\\\\nResult: False\\\\n\\\\n===\\\\n\\\\nChecked Assertions: \"\"\"\\\\n- The sky is blue: True\\\\n- Water is wet: True\\\\n- The sun is a star: True\\\\n\"\"\"\\\\nResult: True\\\\n\\\\n===\\\\n\\\\nChecked Assertions: \"\"\"\\\\n- The sky is blue - True\\\\n- Water is made of lava- False\\\\n- The sun is a star - True\\\\n\"\"\"\\\\nResult: False\\\\n\\\\n===\\\\n\\\\nChecked Assertions:\"\"\"\\\\n{checked_assertions}\\\\n\"\"\"\\\\nResult:\\', template_format=\\'f-string\\', validate_template=True)#\\nfield check_assertions_prompt: langchain.prompts.prompt.PromptTemplate = PromptTemplate(input_variables=[\\'assertions\\'], output_parser=None, partial_variables={}, template=\\'You are an expert fact checker. You have been hired by a major news organization to fact check a very important story.\\\\n\\\\nHere is a bullet point list of facts:\\\\n\"\"\"\\\\n{assertions}\\\\n\"\"\"\\\\n\\\\nFor each fact, determine whether it is true or false about the subject. If you are unable to determine whether the fact is true or false, output \"Undetermined\".\\\\nIf the fact is false, explain why.\\\\n\\\\n\\', template_format=\\'f-string\\', validate_template=True)#\\nfield create_assertions_prompt: langchain.prompts.prompt.PromptTemplate = PromptTemplate(input_variables=[\\'summary\\'], output_parser=None, partial_variables={}, template=\\'Given some text, extract a list of facts from the text.\\\\n\\\\nFormat your output as a bulleted list.\\\\n\\\\nText:\\\\n\"\"\"\\\\n{summary}\\\\n\"\"\"\\\\n\\\\nFacts:\\', template_format=\\'f-string\\', validate_template=True)#\\nfield llm: langchain.llms.base.BaseLLM [Required]#\\nLLM wrapper to use.\\nfield max_checks: int = 2#\\nMaximum number of times to check the assertions. Default to double-checking.\\nfield revised_summary_prompt: langchain.prompts.prompt.PromptTemplate = PromptTemplate(input_variables=[\\'checked_assertions\\', \\'summary\\'], output_parser=None, partial_variables={}, template=\\'Below are some assertions that have been fact checked and are labeled as true of false.\\xa0 If the answer is false, a suggestion is given for a correction.\\\\n\\\\nChecked Assertions:\\\\n\"\"\"\\\\n{checked_assertions}\\\\n\"\"\"\\\\n\\\\nOriginal Summary:\\\\n\"\"\"\\\\n{summary}\\\\n\"\"\"\\\\n\\\\nUsing these checked assertions, rewrite the original summary to be completely true.\\\\n\\\\nThe output should have the same structure and formatting as the original summary.\\\\n\\\\nSummary:\\', template_format=\\'f-string\\', validate_template=True)#\\npydantic model langchain.chains.MapReduceChain[source]#\\nMap-reduce chain.\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield combine_documents_chain: BaseCombineDocumentsChain [Required]#\\nChain to use to combine documents.\\nfield text_splitter: TextSplitter [Required]#\\nText splitter to use.\\nclassmethod from_params(llm: langchain.llms.base.BaseLLM, prompt: langchain.prompts.base.BasePromptTemplate, text_splitter: langchain.text_splitter.TextSplitter) → langchain.chains.mapreduce.MapReduceChain[source]#\\nConstruct a map-reduce chain that uses the chain for map and reduce.\\npydantic model langchain.chains.OpenAIModerationChain[source]#\\nPass input through a moderation endpoint.\\nTo use, you should have the openai python package installed, and the\\nenvironment variable OPENAI_API_KEY set with your API key.\\nAny parameters that are valid to be passed to the openai.create call can be passed\\nin, even if not explicitly saved on this class.\\nExample\\nfrom langchain.chains import OpenAIModerationChain\\nmoderation = OpenAIModerationChain()\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield error: bool = False#\\nWhether or not to error if bad content was found.\\nfield model_name: Optional[str] = None#\\nModeration model name to use.\\nfield openai_api_key: Optional[str] = None#\\nfield openai_organization: Optional[str] = None#\\npydantic model langchain.chains.OpenAPIEndpointChain[source]#\\nChain interacts with an OpenAPI endpoint using natural language.\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield api_operation: APIOperation [Required]#\\nfield api_request_chain: LLMChain [Required]#\\nfield api_response_chain: Optional[LLMChain] = None#\\nfield param_mapping: _ParamMapping [Required]#\\nfield requests: Requests [Optional]#\\nfield return_intermediate_steps: bool = False#\\ndeserialize_json_input(serialized_args: str) → dict[source]#\\nUse the serialized typescript dictionary.\\nResolve the path, query params dict, and optional requestBody dict.\\nclassmethod from_api_operation(operation: langchain.tools.openapi.utils.api_models.APIOperation, llm: langchain.llms.base.BaseLLM, requests: Optional[langchain.requests.Requests] = None, verbose: bool = False, return_intermediate_steps: bool = False, raw_response: bool = False, **kwargs: Any) → OpenAPIEndpointChain[source]#\\nCreate an OpenAPIEndpointChain from an operation and a spec.\\nclassmethod from_url_and_method(spec_url: str, path: str, method: str, llm: langchain.llms.base.BaseLLM, requests: Optional[langchain.requests.Requests] = None, return_intermediate_steps: bool = False, **kwargs: Any) → OpenAPIEndpointChain[source]#\\nCreate an OpenAPIEndpoint from a spec at the specified url.\\npydantic model langchain.chains.PALChain[source]#\\nImplements Program-Aided Language Models.\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield get_answer_expr: str = \\'print(solution())\\'#\\nfield llm: BaseLanguageModel [Required]#\\nfield prompt: BasePromptTemplate [Required]#\\nfield python_globals: Optional[Dict[str, Any]] = None#\\nfield python_locals: Optional[Dict[str, Any]] = None#\\nfield return_intermediate_steps: bool = False#\\nfield stop: str = \\'\\\\n\\\\n\\'#\\nclassmethod from_colored_object_prompt(llm: langchain.schema.BaseLanguageModel, **kwargs: Any) → langchain.chains.pal.base.PALChain[source]#\\nLoad PAL from colored object prompt.\\nclassmethod from_math_prompt(llm: langchain.schema.BaseLanguageModel, **kwargs: Any) → langchain.chains.pal.base.PALChain[source]#\\nLoad PAL from math prompt.\\npydantic model langchain.chains.QAGenerationChain[source]#\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield input_key: str = \\'text\\'#\\nfield k: Optional[int] = None#\\nfield llm_chain: LLMChain [Required]#\\nfield output_key: str = \\'questions\\'#\\nfield text_splitter: TextSplitter = <langchain.text_splitter.RecursiveCharacterTextSplitter object>#\\nclassmethod from_llm(llm: langchain.schema.BaseLanguageModel, prompt: Optional[langchain.prompts.base.BasePromptTemplate] = None, **kwargs: Any) → langchain.chains.qa_generation.base.QAGenerationChain[source]#\\nproperty input_keys: List[str]#\\nInput keys this chain expects.\\nproperty output_keys: List[str]#\\nOutput keys this chain expects.\\npydantic model langchain.chains.QAWithSourcesChain[source]#\\nQuestion answering with sources over documents.\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_naming » all fields\\npydantic model langchain.chains.RetrievalQA[source]#\\nChain for question-answering against an index.\\nExample\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.faiss import FAISS\\nfrom langchain.vectorstores.base import VectorStoreRetriever\\nretriever = VectorStoreRetriever(vectorstore=FAISS(...))\\nretrievalQA = RetrievalQA.from_llm(llm=OpenAI(), retriever=retriever)\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield retriever: BaseRetriever [Required]#\\npydantic model langchain.chains.RetrievalQAWithSourcesChain[source]#\\nQuestion-answering with sources over an index.\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_naming » all fields\\nfield max_tokens_limit: int = 3375#\\nRestrict the docs to return from store based on tokens,\\nenforced only for StuffDocumentChain and if reduce_k_below_max_tokens is to true\\nfield reduce_k_below_max_tokens: bool = False#\\nReduce the number of results to return from store based on tokens limit\\nfield retriever: langchain.schema.BaseRetriever [Required]#\\nIndex to connect to.\\npydantic model langchain.chains.SQLDatabaseChain[source]#\\nChain for interacting with SQL Database.\\nExample\\nfrom langchain import SQLDatabaseChain, OpenAI, SQLDatabase\\ndb = SQLDatabase(...)\\ndb_chain = SQLDatabaseChain(llm=OpenAI(), database=db)\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield database: SQLDatabase [Required]#\\nSQL Database to connect to.\\nfield llm: BaseLanguageModel [Required]#\\nLLM wrapper to use.\\nfield prompt: Optional[BasePromptTemplate] = None#\\nPrompt to use to translate natural language to SQL.\\nfield return_direct: bool = False#\\nWhether or not to return the result of querying the SQL table directly.\\nfield return_intermediate_steps: bool = False#\\nWhether or not to return the intermediate steps along with the final answer.\\nfield top_k: int = 5#\\nNumber of results to return from the query\\npydantic model langchain.chains.SQLDatabaseSequentialChain[source]#\\nChain for querying SQL database that is a sequential chain.\\nThe chain is as follows:\\n1. Based on the query, determine which tables to use.\\n2. Based on those tables, call the normal SQL database chain.\\nThis is useful in cases where the number of tables in the database is large.\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield decider_chain: LLMChain [Required]#\\nfield return_intermediate_steps: bool = False#\\nfield sql_chain: SQLDatabaseChain [Required]#\\nclassmethod from_llm(llm: langchain.schema.BaseLanguageModel, database: langchain.sql_database.SQLDatabase, query_prompt: langchain.prompts.base.BasePromptTemplate = PromptTemplate(input_variables=[\\'input\\', \\'table_info\\', \\'dialect\\', \\'top_k\\'], output_parser=None, partial_variables={}, template=\\'Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies in his question a specific number of examples he wishes to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\\\\n\\\\nNever query for all the columns from a specific table, only ask for a the few relevant columns given the question.\\\\n\\\\nPay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\\\n\\\\nUse the following format:\\\\n\\\\nQuestion: \"Question here\"\\\\nSQLQuery: \"SQL Query to run\"\\\\nSQLResult: \"Result of the SQLQuery\"\\\\nAnswer: \"Final answer here\"\\\\n\\\\nOnly use the tables listed below.\\\\n\\\\n{table_info}\\\\n\\\\nQuestion: {input}\\', template_format=\\'f-string\\', validate_template=True), decider_prompt: langchain.prompts.base.BasePromptTemplate = PromptTemplate(input_variables=[\\'query\\', \\'table_names\\'], output_parser=CommaSeparatedListOutputParser(), partial_variables={}, template=\\'Given the below input question and list of potential tables, output a comma separated list of the table names that may be necessary to answer this question.\\\\n\\\\nQuestion: {query}\\\\n\\\\nTable Names: {table_names}\\\\n\\\\nRelevant Table Names:\\', template_format=\\'f-string\\', validate_template=True), **kwargs: Any) → langchain.chains.sql_database.base.SQLDatabaseSequentialChain[source]#\\nLoad the necessary chains.\\npydantic model langchain.chains.SequentialChain[source]#\\nChain where the outputs of one chain feed directly into next.\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_chains » all fields\\nfield chains: List[langchain.chains.base.Chain] [Required]#\\nfield input_variables: List[str] [Required]#\\nfield return_all: bool = False#\\npydantic model langchain.chains.SimpleSequentialChain[source]#\\nSimple chain where the outputs of one step feed directly into next.\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_chains » all fields\\nfield chains: List[langchain.chains.base.Chain] [Required]#\\nfield strip_outputs: bool = False#\\npydantic model langchain.chains.TransformChain[source]#\\nChain transform chain output.\\nExample\\nfrom langchain import TransformChain\\ntransform_chain = TransformChain(input_variables=[\"text\"],\\n output_variables[\"entities\"], transform=func())\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield input_variables: List[str] [Required]#\\nfield output_variables: List[str] [Required]#\\nfield transform: Callable[[Dict[str, str]], Dict[str, str]] [Required]#\\npydantic model langchain.chains.VectorDBQA[source]#\\nChain for question-answering against a vector database.\\nValidators\\nraise_deprecation » all fields\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_search_type » all fields\\nfield k: int = 4#\\nNumber of documents to query for.\\nfield search_kwargs: Dict[str, Any] [Optional]#\\nExtra search args.\\nfield search_type: str = \\'similarity\\'#\\nSearch type to use over vectorstore. similarity or mmr.\\nfield vectorstore: VectorStore [Required]#\\nVector Database to connect to.\\npydantic model langchain.chains.VectorDBQAWithSourcesChain[source]#\\nQuestion-answering with sources over a vector database.\\nValidators\\nraise_deprecation » all fields\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_naming » all fields\\nfield k: int = 4#\\nNumber of results to return from store\\nfield max_tokens_limit: int = 3375#\\nRestrict the docs to return from store based on tokens,\\nenforced only for StuffDocumentChain and if reduce_k_below_max_tokens is to true\\nfield reduce_k_below_max_tokens: bool = False#\\nReduce the number of results to return from store based on tokens limit\\nfield search_kwargs: Dict[str, Any] [Optional]#\\nExtra search args.\\nfield vectorstore: langchain.vectorstores.base.VectorStore [Required]#\\nVector Database to connect to.\\nlangchain.chains.load_chain(path: Union[str, pathlib.Path], **kwargs: Any) → langchain.chains.base.Chain[source]#\\nUnified method for loading a chain from LangChainHub or local fs.\\nprevious\\nSQL Chain example\\nnext\\nAgents\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\reference\\\\modules\\\\chains.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nDocstore\\nDocstore#\\nWrappers on top of docstores.\\nclass langchain.docstore.InMemoryDocstore(_dict: Dict[str, langchain.schema.Document])[source]#\\nSimple in memory docstore in the form of a dict.\\nadd(texts: Dict[str, langchain.schema.Document]) → None[source]#\\nAdd texts to in memory dictionary.\\nsearch(search: str) → Union[str, langchain.schema.Document][source]#\\nSearch via direct lookup.\\nclass langchain.docstore.Wikipedia[source]#\\nWrapper around wikipedia API.\\nsearch(search: str) → Union[str, langchain.schema.Document][source]#\\nTry to search for wiki page.\\nIf page exists, return the page summary, and a PageWithLookups object.\\nIf page does not exist, return similar entries.\\nprevious\\nSearxNG Search\\nnext\\nText Splitter\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\reference\\\\modules\\\\docstore.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nEmbeddings\\nEmbeddings#\\nWrappers around embedding modules.\\npydantic model langchain.embeddings.AlephAlphaAsymmetricSemanticEmbedding[source]#\\nWrapper for Aleph Alpha’s Asymmetric Embeddings\\nAA provides you with an endpoint to embed a document and a query.\\nThe models were optimized to make the embeddings of documents and\\nthe query for a document as similar as possible.\\nTo learn more, check out: https://docs.aleph-alpha.com/docs/tasks/semantic_embed/\\nExample\\nfrom aleph_alpha import AlephAlphaAsymmetricSemanticEmbedding\\nembeddings = AlephAlphaSymmetricSemanticEmbedding()\\ndocument = \"This is a content of the document\"\\nquery = \"What is the content of the document?\"\\ndoc_result = embeddings.embed_documents([document])\\nquery_result = embeddings.embed_query(query)\\nfield compress_to_size: Optional[int] = 128#\\nShould the returned embeddings come back as an original 5120-dim vector,\\nor should it be compressed to 128-dim.\\nfield contextual_control_threshold: Optional[int] = None#\\nAttention control parameters only apply to those tokens that have\\nexplicitly been set in the request.\\nfield control_log_additive: Optional[bool] = True#\\nApply controls on prompt items by adding the log(control_factor)\\nto attention scores.\\nfield hosting: Optional[str] = \\'https://api.aleph-alpha.com\\'#\\nOptional parameter that specifies which datacenters may process the request.\\nfield model: Optional[str] = \\'luminous-base\\'#\\nModel name to use.\\nfield normalize: Optional[bool] = True#\\nShould returned embeddings be normalized\\nembed_documents(texts: List[str]) → List[List[float]][source]#\\nCall out to Aleph Alpha’s asymmetric Document endpoint.\\nParameters\\ntexts – The list of texts to embed.\\nReturns\\nList of embeddings, one for each text.\\nembed_query(text: str) → List[float][source]#\\nCall out to Aleph Alpha’s asymmetric, query embedding endpoint\\n:param text: The text to embed.\\nReturns\\nEmbeddings for the text.\\npydantic model langchain.embeddings.AlephAlphaSymmetricSemanticEmbedding[source]#\\nThe symmetric version of the Aleph Alpha’s semantic embeddings.\\nThe main difference is that here, both the documents and\\nqueries are embedded with a SemanticRepresentation.Symmetric\\n.. rubric:: Example\\nembed_documents(texts: List[str]) → List[List[float]][source]#\\nCall out to Aleph Alpha’s Document endpoint.\\nParameters\\ntexts – The list of texts to embed.\\nReturns\\nList of embeddings, one for each text.\\nembed_query(text: str) → List[float][source]#\\nCall out to Aleph Alpha’s asymmetric, query embedding endpoint\\n:param text: The text to embed.\\nReturns\\nEmbeddings for the text.\\npydantic model langchain.embeddings.CohereEmbeddings[source]#\\nWrapper around Cohere embedding models.\\nTo use, you should have the cohere python package installed, and the\\nenvironment variable COHERE_API_KEY set with your API key or pass it\\nas a named parameter to the constructor.\\nExample\\nfrom langchain.embeddings import CohereEmbeddings\\ncohere = CohereEmbeddings(model=\"medium\", cohere_api_key=\"my-api-key\")\\nfield model: str = \\'large\\'#\\nModel name to use.\\nfield truncate: Optional[str] = None#\\nTruncate embeddings that are too long from start or end (“NONE”|”START”|”END”)\\nembed_documents(texts: List[str]) → List[List[float]][source]#\\nCall out to Cohere’s embedding endpoint.\\nParameters\\ntexts – The list of texts to embed.\\nReturns\\nList of embeddings, one for each text.\\nembed_query(text: str) → List[float][source]#\\nCall out to Cohere’s embedding endpoint.\\nParameters\\ntext – The text to embed.\\nReturns\\nEmbeddings for the text.\\npydantic model langchain.embeddings.FakeEmbeddings[source]#\\nembed_documents(texts: List[str]) → List[List[float]][source]#\\nEmbed search docs.\\nembed_query(text: str) → List[float][source]#\\nEmbed query text.\\npydantic model langchain.embeddings.HuggingFaceEmbeddings[source]#\\nWrapper around sentence_transformers embedding models.\\nTo use, you should have the sentence_transformers python package installed.\\nExample\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\\nmodel_kwargs = {\\'device\\': \\'cpu\\'}\\nhf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\\nfield cache_folder: Optional[str] = None#\\nPath to store models.\\nCan be also set by SENTENCE_TRANSFORMERS_HOME enviroment variable.\\nfield model_kwargs: Dict[str, Any] [Optional]#\\nKey word arguments to pass to the model.\\nfield model_name: str = \\'sentence-transformers/all-mpnet-base-v2\\'#\\nModel name to use.\\nembed_documents(texts: List[str]) → List[List[float]][source]#\\nCompute doc embeddings using a HuggingFace transformer model.\\nParameters\\ntexts – The list of texts to embed.\\nReturns\\nList of embeddings, one for each text.\\nembed_query(text: str) → List[float][source]#\\nCompute query embeddings using a HuggingFace transformer model.\\nParameters\\ntext – The text to embed.\\nReturns\\nEmbeddings for the text.\\npydantic model langchain.embeddings.HuggingFaceHubEmbeddings[source]#\\nWrapper around HuggingFaceHub embedding models.\\nTo use, you should have the huggingface_hub python package installed, and the\\nenvironment variable HUGGINGFACEHUB_API_TOKEN set with your API token, or pass\\nit as a named parameter to the constructor.\\nExample\\nfrom langchain.embeddings import HuggingFaceHubEmbeddings\\nrepo_id = \"sentence-transformers/all-mpnet-base-v2\"\\nhf = HuggingFaceHubEmbeddings(\\n    repo_id=repo_id,\\n    task=\"feature-extraction\",\\n    huggingfacehub_api_token=\"my-api-key\",\\n)\\nfield model_kwargs: Optional[dict] = None#\\nKey word arguments to pass to the model.\\nfield repo_id: str = \\'sentence-transformers/all-mpnet-base-v2\\'#\\nModel name to use.\\nfield task: Optional[str] = \\'feature-extraction\\'#\\nTask to call the model with.\\nembed_documents(texts: List[str]) → List[List[float]][source]#\\nCall out to HuggingFaceHub’s embedding endpoint for embedding search docs.\\nParameters\\ntexts – The list of texts to embed.\\nReturns\\nList of embeddings, one for each text.\\nembed_query(text: str) → List[float][source]#\\nCall out to HuggingFaceHub’s embedding endpoint for embedding query text.\\nParameters\\ntext – The text to embed.\\nReturns\\nEmbeddings for the text.\\npydantic model langchain.embeddings.HuggingFaceInstructEmbeddings[source]#\\nWrapper around sentence_transformers embedding models.\\nTo use, you should have the sentence_transformers\\nand InstructorEmbedding python package installed.\\nExample\\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\\nmodel_name = \"hkunlp/instructor-large\"\\nmodel_kwargs = {\\'device\\': \\'cpu\\'}\\nhf = HuggingFaceInstructEmbeddings(\\n    model_name=model_name, model_kwargs=model_kwargs\\n)\\nfield cache_folder: Optional[str] = None#\\nPath to store models.\\nCan be also set by SENTENCE_TRANSFORMERS_HOME enviroment variable.\\nfield embed_instruction: str = \\'Represent the document for retrieval: \\'#\\nInstruction to use for embedding documents.\\nfield model_kwargs: Dict[str, Any] [Optional]#\\nKey word arguments to pass to the model.\\nfield model_name: str = \\'hkunlp/instructor-large\\'#\\nModel name to use.\\nfield query_instruction: str = \\'Represent the question for retrieving supporting documents: \\'#\\nInstruction to use for embedding query.\\nembed_documents(texts: List[str]) → List[List[float]][source]#\\nCompute doc embeddings using a HuggingFace instruct model.\\nParameters\\ntexts – The list of texts to embed.\\nReturns\\nList of embeddings, one for each text.\\nembed_query(text: str) → List[float][source]#\\nCompute query embeddings using a HuggingFace instruct model.\\nParameters\\ntext – The text to embed.\\nReturns\\nEmbeddings for the text.\\npydantic model langchain.embeddings.LlamaCppEmbeddings[source]#\\nWrapper around llama.cpp embedding models.\\nTo use, you should have the llama-cpp-python library installed, and provide the\\npath to the Llama model as a named parameter to the constructor.\\nCheck out: abetlen/llama-cpp-python\\nExample\\nfrom langchain.embeddings import LlamaCppEmbeddings\\nllama = LlamaCppEmbeddings(model_path=\"/path/to/model.bin\")\\nfield f16_kv: bool = False#\\nUse half-precision for key/value cache.\\nfield logits_all: bool = False#\\nReturn logits for all tokens, not just the last token.\\nfield n_batch: Optional[int] = 8#\\nNumber of tokens to process in parallel.\\nShould be a number between 1 and n_ctx.\\nfield n_ctx: int = 512#\\nToken context window.\\nfield n_parts: int = -1#\\nNumber of parts to split the model into.\\nIf -1, the number of parts is automatically determined.\\nfield n_threads: Optional[int] = None#\\nNumber of threads to use. If None, the number\\nof threads is automatically determined.\\nfield seed: int = -1#\\nSeed. If -1, a random seed is used.\\nfield use_mlock: bool = False#\\nForce system to keep model in RAM.\\nfield vocab_only: bool = False#\\nOnly load the vocabulary, no weights.\\nembed_documents(texts: List[str]) → List[List[float]][source]#\\nEmbed a list of documents using the Llama model.\\nParameters\\ntexts – The list of texts to embed.\\nReturns\\nList of embeddings, one for each text.\\nembed_query(text: str) → List[float][source]#\\nEmbed a query using the Llama model.\\nParameters\\ntext – The text to embed.\\nReturns\\nEmbeddings for the text.\\npydantic model langchain.embeddings.OpenAIEmbeddings[source]#\\nWrapper around OpenAI embedding models.\\nTo use, you should have the openai python package installed, and the\\nenvironment variable OPENAI_API_KEY set with your API key or pass it\\nas a named parameter to the constructor.\\nExample\\nfrom langchain.embeddings import OpenAIEmbeddings\\nopenai = OpenAIEmbeddings(openai_api_key=\"my-api-key\")\\nIn order to use the library with Microsoft Azure endpoints, you need to set\\nthe OPENAI_API_TYPE, OPENAI_API_BASE, OPENAI_API_KEY and optionally and\\nAPI_VERSION.\\nThe OPENAI_API_TYPE must be set to ‘azure’ and the others correspond to\\nthe properties of your endpoint.\\nIn addition, the deployment name must be passed as the model parameter.\\nExample\\nimport os\\nos.environ[\"OPENAI_API_TYPE\"] = \"azure\"\\nos.environ[\"OPENAI_API_BASE\"] = \"https://<your-endpoint.openai.azure.com/\"\\nos.environ[\"OPENAI_API_KEY\"] = \"your AzureOpenAI key\"\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nembeddings = OpenAIEmbeddings(\\n    deployment=\"your-embeddings-deployment-name\",\\n    model=\"your-embeddings-model-name\"\\n)\\ntext = \"This is a test query.\"\\nquery_result = embeddings.embed_query(text)\\nfield chunk_size: int = 1000#\\nMaximum number of texts to embed in each batch\\nfield max_retries: int = 6#\\nMaximum number of retries to make when generating.\\nembed_documents(texts: List[str], chunk_size: Optional[int] = 0) → List[List[float]][source]#\\nCall out to OpenAI’s embedding endpoint for embedding search docs.\\nParameters\\ntexts – The list of texts to embed.\\nchunk_size – The chunk size of embeddings. If None, will use the chunk size\\nspecified by the class.\\nReturns\\nList of embeddings, one for each text.\\nembed_query(text: str) → List[float][source]#\\nCall out to OpenAI’s embedding endpoint for embedding query text.\\nParameters\\ntext – The text to embed.\\nReturns\\nEmbedding for the text.\\npydantic model langchain.embeddings.SagemakerEndpointEmbeddings[source]#\\nWrapper around custom Sagemaker Inference Endpoints.\\nTo use, you must supply the endpoint name from your deployed\\nSagemaker model & the region where it is deployed.\\nTo authenticate, the AWS client uses the following methods to\\nautomatically load credentials:\\nhttps://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\\nIf a specific credential profile should be used, you must pass\\nthe name of the profile from the ~/.aws/credentials file that is to be used.\\nMake sure the credentials / roles used have the required policies to\\naccess the Sagemaker endpoint.\\nSee: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\\nfield content_handler: langchain.llms.sagemaker_endpoint.ContentHandlerBase [Required]#\\nThe content handler class that provides an input and\\noutput transform functions to handle formats between LLM\\nand the endpoint.\\nfield credentials_profile_name: Optional[str] = None#\\nThe name of the profile in the ~/.aws/credentials or ~/.aws/config files, which\\nhas either access keys or role information specified.\\nIf not specified, the default credential profile or, if on an EC2 instance,\\ncredentials from IMDS will be used.\\nSee: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\\nfield endpoint_kwargs: Optional[Dict] = None#\\nOptional attributes passed to the invoke_endpoint\\nfunction. See `boto3`_. docs for more info.\\n.. _boto3: <https://boto3.amazonaws.com/v1/documentation/api/latest/index.html>\\nfield endpoint_name: str = \\'\\'#\\nThe name of the endpoint from the deployed Sagemaker model.\\nMust be unique within an AWS Region.\\nfield model_kwargs: Optional[Dict] = None#\\nKey word arguments to pass to the model.\\nfield region_name: str = \\'\\'#\\nThe aws region where the Sagemaker model is deployed, eg. us-west-2.\\nembed_documents(texts: List[str], chunk_size: int = 64) → List[List[float]][source]#\\nCompute doc embeddings using a SageMaker Inference Endpoint.\\nParameters\\ntexts – The list of texts to embed.\\nchunk_size – The chunk size defines how many input texts will\\nbe grouped together as request. If None, will use the\\nchunk size specified by the class.\\nReturns\\nList of embeddings, one for each text.\\nembed_query(text: str) → List[float][source]#\\nCompute query embeddings using a SageMaker inference endpoint.\\nParameters\\ntext – The text to embed.\\nReturns\\nEmbeddings for the text.\\npydantic model langchain.embeddings.SelfHostedEmbeddings[source]#\\nRuns custom embedding models on self-hosted remote hardware.\\nSupported hardware includes auto-launched instances on AWS, GCP, Azure,\\nand Lambda, as well as servers specified\\nby IP address and SSH credentials (such as on-prem, or another\\ncloud like Paperspace, Coreweave, etc.).\\nTo use, you should have the runhouse python package installed.\\nExample using a model load function:from langchain.embeddings import SelfHostedEmbeddings\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\\nimport runhouse as rh\\ngpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\")\\ndef get_pipeline():\\n    model_id = \"facebook/bart-large\"\\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\\n    model = AutoModelForCausalLM.from_pretrained(model_id)\\n    return pipeline(\"feature-extraction\", model=model, tokenizer=tokenizer)\\nembeddings = SelfHostedEmbeddings(\\n    model_load_fn=get_pipeline,\\n    hardware=gpu\\n    model_reqs=[\"./\", \"torch\", \"transformers\"],\\n)\\nExample passing in a pipeline path:from langchain.embeddings import SelfHostedHFEmbeddings\\nimport runhouse as rh\\nfrom transformers import pipeline\\ngpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\")\\npipeline = pipeline(model=\"bert-base-uncased\", task=\"feature-extraction\")\\nrh.blob(pickle.dumps(pipeline),\\n    path=\"models/pipeline.pkl\").save().to(gpu, path=\"models\")\\nembeddings = SelfHostedHFEmbeddings.from_pipeline(\\n    pipeline=\"models/pipeline.pkl\",\\n    hardware=gpu,\\n    model_reqs=[\"./\", \"torch\", \"transformers\"],\\n)\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield inference_fn: Callable = <function _embed_documents>#\\nInference function to extract the embeddings on the remote hardware.\\nfield inference_kwargs: Any = None#\\nAny kwargs to pass to the model’s inference function.\\nembed_documents(texts: List[str]) → List[List[float]][source]#\\nCompute doc embeddings using a HuggingFace transformer model.\\nParameters\\ntexts – The list of texts to embed.s\\nReturns\\nList of embeddings, one for each text.\\nembed_query(text: str) → List[float][source]#\\nCompute query embeddings using a HuggingFace transformer model.\\nParameters\\ntext – The text to embed.\\nReturns\\nEmbeddings for the text.\\npydantic model langchain.embeddings.SelfHostedHuggingFaceEmbeddings[source]#\\nRuns sentence_transformers embedding models on self-hosted remote hardware.\\nSupported hardware includes auto-launched instances on AWS, GCP, Azure,\\nand Lambda, as well as servers specified\\nby IP address and SSH credentials (such as on-prem, or another cloud\\nlike Paperspace, Coreweave, etc.).\\nTo use, you should have the runhouse python package installed.\\nExample\\nfrom langchain.embeddings import SelfHostedHuggingFaceEmbeddings\\nimport runhouse as rh\\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\\ngpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\")\\nhf = SelfHostedHuggingFaceEmbeddings(model_name=model_name, hardware=gpu)\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield hardware: Any = None#\\nRemote hardware to send the inference function to.\\nfield inference_fn: Callable = <function _embed_documents>#\\nInference function to extract the embeddings.\\nfield load_fn_kwargs: Optional[dict] = None#\\nKey word arguments to pass to the model load function.\\nfield model_id: str = \\'sentence-transformers/all-mpnet-base-v2\\'#\\nModel name to use.\\nfield model_load_fn: Callable = <function load_embedding_model>#\\nFunction to load the model remotely on the server.\\nfield model_reqs: List[str] = [\\'./\\', \\'sentence_transformers\\', \\'torch\\']#\\nRequirements to install on hardware to inference the model.\\npydantic model langchain.embeddings.SelfHostedHuggingFaceInstructEmbeddings[source]#\\nRuns InstructorEmbedding embedding models on self-hosted remote hardware.\\nSupported hardware includes auto-launched instances on AWS, GCP, Azure,\\nand Lambda, as well as servers specified\\nby IP address and SSH credentials (such as on-prem, or another\\ncloud like Paperspace, Coreweave, etc.).\\nTo use, you should have the runhouse python package installed.\\nExample\\nfrom langchain.embeddings import SelfHostedHuggingFaceInstructEmbeddings\\nimport runhouse as rh\\nmodel_name = \"hkunlp/instructor-large\"\\ngpu = rh.cluster(name=\\'rh-a10x\\', instance_type=\\'A100:1\\')\\nhf = SelfHostedHuggingFaceInstructEmbeddings(\\n    model_name=model_name, hardware=gpu)\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield embed_instruction: str = \\'Represent the document for retrieval: \\'#\\nInstruction to use for embedding documents.\\nfield model_id: str = \\'hkunlp/instructor-large\\'#\\nModel name to use.\\nfield model_reqs: List[str] = [\\'./\\', \\'InstructorEmbedding\\', \\'torch\\']#\\nRequirements to install on hardware to inference the model.\\nfield query_instruction: str = \\'Represent the question for retrieving supporting documents: \\'#\\nInstruction to use for embedding query.\\nembed_documents(texts: List[str]) → List[List[float]][source]#\\nCompute doc embeddings using a HuggingFace instruct model.\\nParameters\\ntexts – The list of texts to embed.\\nReturns\\nList of embeddings, one for each text.\\nembed_query(text: str) → List[float][source]#\\nCompute query embeddings using a HuggingFace instruct model.\\nParameters\\ntext – The text to embed.\\nReturns\\nEmbeddings for the text.\\npydantic model langchain.embeddings.TensorflowHubEmbeddings[source]#\\nWrapper around tensorflow_hub embedding models.\\nTo use, you should have the tensorflow_text python package installed.\\nExample\\nfrom langchain.embeddings import TensorflowHubEmbeddings\\nurl = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\\ntf = TensorflowHubEmbeddings(model_url=url)\\nfield model_url: str = \\'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\\'#\\nModel name to use.\\nembed_documents(texts: List[str]) → List[List[float]][source]#\\nCompute doc embeddings using a TensorflowHub embedding model.\\nParameters\\ntexts – The list of texts to embed.\\nReturns\\nList of embeddings, one for each text.\\nembed_query(text: str) → List[float][source]#\\nCompute query embeddings using a TensorflowHub embedding model.\\nParameters\\ntext – The text to embed.\\nReturns\\nEmbeddings for the text.\\nprevious\\nText Splitter\\nnext\\nVectorStores\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\reference\\\\modules\\\\embeddings.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nExample Selector\\nExample Selector#\\nLogic for selecting examples to include in prompts.\\npydantic model langchain.prompts.example_selector.LengthBasedExampleSelector[source]#\\nSelect examples based on length.\\nValidators\\ncalculate_example_text_lengths » example_text_lengths\\nfield example_prompt: langchain.prompts.prompt.PromptTemplate [Required]#\\nPrompt template used to format the examples.\\nfield examples: List[dict] [Required]#\\nA list of the examples that the prompt template expects.\\nfield get_text_length: Callable[[str], int] = <function _get_length_based>#\\nFunction to measure prompt length. Defaults to word count.\\nfield max_length: int = 2048#\\nMax length for the prompt, beyond which examples are cut.\\nadd_example(example: Dict[str, str]) → None[source]#\\nAdd new example to list.\\nselect_examples(input_variables: Dict[str, str]) → List[dict][source]#\\nSelect which examples to use based on the input lengths.\\npydantic model langchain.prompts.example_selector.MaxMarginalRelevanceExampleSelector[source]#\\nExampleSelector that selects examples based on Max Marginal Relevance.\\nThis was shown to improve performance in this paper:\\nhttps://arxiv.org/pdf/2211.13892.pdf\\nfield fetch_k: int = 20#\\nNumber of examples to fetch to rerank.\\nclassmethod from_examples(examples: List[dict], embeddings: langchain.embeddings.base.Embeddings, vectorstore_cls: Type[langchain.vectorstores.base.VectorStore], k: int = 4, input_keys: Optional[List[str]] = None, fetch_k: int = 20, **vectorstore_cls_kwargs: Any) → langchain.prompts.example_selector.semantic_similarity.MaxMarginalRelevanceExampleSelector[source]#\\nCreate k-shot example selector using example list and embeddings.\\nReshuffles examples dynamically based on query similarity.\\nParameters\\nexamples – List of examples to use in the prompt.\\nembeddings – An iniialized embedding API interface, e.g. OpenAIEmbeddings().\\nvectorstore_cls – A vector store DB interface class, e.g. FAISS.\\nk – Number of examples to select\\ninput_keys – If provided, the search is based on the input variables\\ninstead of all variables.\\nvectorstore_cls_kwargs – optional kwargs containing url for vector store\\nReturns\\nThe ExampleSelector instantiated, backed by a vector store.\\nselect_examples(input_variables: Dict[str, str]) → List[dict][source]#\\nSelect which examples to use based on semantic similarity.\\npydantic model langchain.prompts.example_selector.SemanticSimilarityExampleSelector[source]#\\nExample selector that selects examples based on SemanticSimilarity.\\nfield example_keys: Optional[List[str]] = None#\\nOptional keys to filter examples to.\\nfield input_keys: Optional[List[str]] = None#\\nOptional keys to filter input to. If provided, the search is based on\\nthe input variables instead of all variables.\\nfield k: int = 4#\\nNumber of examples to select.\\nfield vectorstore: langchain.vectorstores.base.VectorStore [Required]#\\nVectorStore than contains information about examples.\\nadd_example(example: Dict[str, str]) → str[source]#\\nAdd new example to vectorstore.\\nclassmethod from_examples(examples: List[dict], embeddings: langchain.embeddings.base.Embeddings, vectorstore_cls: Type[langchain.vectorstores.base.VectorStore], k: int = 4, input_keys: Optional[List[str]] = None, **vectorstore_cls_kwargs: Any) → langchain.prompts.example_selector.semantic_similarity.SemanticSimilarityExampleSelector[source]#\\nCreate k-shot example selector using example list and embeddings.\\nReshuffles examples dynamically based on query similarity.\\nParameters\\nexamples – List of examples to use in the prompt.\\nembeddings – An initialized embedding API interface, e.g. OpenAIEmbeddings().\\nvectorstore_cls – A vector store DB interface class, e.g. FAISS.\\nk – Number of examples to select\\ninput_keys – If provided, the search is based on the input variables\\ninstead of all variables.\\nvectorstore_cls_kwargs – optional kwargs containing url for vector store\\nReturns\\nThe ExampleSelector instantiated, backed by a vector store.\\nselect_examples(input_variables: Dict[str, str]) → List[dict][source]#\\nSelect which examples to use based on semantic similarity.\\nprevious\\nPromptTemplates\\nnext\\nChat Prompt Template\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\reference\\\\modules\\\\example_selector.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nLLMs\\nLLMs#\\nWrappers on top of large language models APIs.\\npydantic model langchain.llms.AI21[source]#\\nWrapper around AI21 large language models.\\nTo use, you should have the environment variable AI21_API_KEY\\nset with your API key.\\nExample\\nfrom langchain.llms import AI21\\nai21 = AI21(model=\"j2-jumbo-instruct\")\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield base_url: Optional[str] = None#\\nBase url to use, if None decides based on model name.\\nfield countPenalty: langchain.llms.ai21.AI21PenaltyData = AI21PenaltyData(scale=0, applyToWhitespaces=True, applyToPunctuations=True, applyToNumbers=True, applyToStopwords=True, applyToEmojis=True)#\\nPenalizes repeated tokens according to count.\\nfield frequencyPenalty: langchain.llms.ai21.AI21PenaltyData = AI21PenaltyData(scale=0, applyToWhitespaces=True, applyToPunctuations=True, applyToNumbers=True, applyToStopwords=True, applyToEmojis=True)#\\nPenalizes repeated tokens according to frequency.\\nfield logitBias: Optional[Dict[str, float]] = None#\\nAdjust the probability of specific tokens being generated.\\nfield maxTokens: int = 256#\\nThe maximum number of tokens to generate in the completion.\\nfield minTokens: int = 0#\\nThe minimum number of tokens to generate in the completion.\\nfield model: str = \\'j2-jumbo-instruct\\'#\\nModel name to use.\\nfield numResults: int = 1#\\nHow many completions to generate for each prompt.\\nfield presencePenalty: langchain.llms.ai21.AI21PenaltyData = AI21PenaltyData(scale=0, applyToWhitespaces=True, applyToPunctuations=True, applyToNumbers=True, applyToStopwords=True, applyToEmojis=True)#\\nPenalizes repeated tokens.\\nfield temperature: float = 0.7#\\nWhat sampling temperature to use.\\nfield topP: float = 1.0#\\nTotal probability mass of tokens to consider at each step.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.AlephAlpha[source]#\\nWrapper around Aleph Alpha large language models.\\nTo use, you should have the aleph_alpha_client python package installed, and the\\nenvironment variable ALEPH_ALPHA_API_KEY set with your API key, or pass\\nit as a named parameter to the constructor.\\nParameters are explained more in depth here:\\nAleph-Alpha/aleph-alpha-client\\nExample\\nfrom langchain.llms import AlephAlpha\\nalpeh_alpha = AlephAlpha(aleph_alpha_api_key=\"my-api-key\")\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield aleph_alpha_api_key: Optional[str] = None#\\nAPI key for Aleph Alpha API.\\nfield best_of: Optional[int] = None#\\nreturns the one with the “best of” results\\n(highest log probability per token)\\nfield completion_bias_exclusion_first_token_only: bool = False#\\nOnly consider the first token for the completion_bias_exclusion.\\nfield contextual_control_threshold: Optional[float] = None#\\nIf set to None, attention control parameters only apply to those tokens that have\\nexplicitly been set in the request.\\nIf set to a non-None value, control parameters are also applied to similar tokens.\\nfield control_log_additive: Optional[bool] = True#\\nTrue: apply control by adding the log(control_factor) to attention scores.\\nFalse: (attention_scores - - attention_scores.min(-1)) * control_factor\\nfield echo: bool = False#\\nEcho the prompt in the completion.\\nfield frequency_penalty: float = 0.0#\\nPenalizes repeated tokens according to frequency.\\nfield log_probs: Optional[int] = None#\\nNumber of top log probabilities to be returned for each generated token.\\nfield logit_bias: Optional[Dict[int, float]] = None#\\nThe logit bias allows to influence the likelihood of generating tokens.\\nfield maximum_tokens: int = 64#\\nThe maximum number of tokens to be generated.\\nfield minimum_tokens: Optional[int] = 0#\\nGenerate at least this number of tokens.\\nfield model: Optional[str] = \\'luminous-base\\'#\\nModel name to use.\\nfield n: int = 1#\\nHow many completions to generate for each prompt.\\nfield penalty_bias: Optional[str] = None#\\nPenalty bias for the completion.\\nfield penalty_exceptions: Optional[List[str]] = None#\\nList of strings that may be generated without penalty,\\nregardless of other penalty settings\\nfield penalty_exceptions_include_stop_sequences: Optional[bool] = None#\\nShould stop_sequences be included in penalty_exceptions.\\nfield presence_penalty: float = 0.0#\\nPenalizes repeated tokens.\\nfield raw_completion: bool = False#\\nForce the raw completion of the model to be returned.\\nfield repetition_penalties_include_completion: bool = True#\\nFlag deciding whether presence penalty or frequency penalty\\nare updated from the completion.\\nfield repetition_penalties_include_prompt: Optional[bool] = False#\\nFlag deciding whether presence penalty or frequency penalty are\\nupdated from the prompt.\\nfield stop_sequences: Optional[List[str]] = None#\\nStop sequences to use.\\nfield temperature: float = 0.0#\\nA non-negative float that tunes the degree of randomness in generation.\\nfield tokens: Optional[bool] = False#\\nreturn tokens of completion.\\nfield top_k: int = 0#\\nNumber of most likely tokens to consider at each step.\\nfield top_p: float = 0.0#\\nTotal probability mass of tokens to consider at each step.\\nfield use_multiplicative_presence_penalty: Optional[bool] = False#\\nFlag deciding whether presence penalty is applied\\nmultiplicatively (True) or additively (False).\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.Anthropic[source]#\\nWrapper around Anthropic’s large language models.\\nTo use, you should have the anthropic python package installed, and the\\nenvironment variable ANTHROPIC_API_KEY set with your API key, or pass\\nit as a named parameter to the constructor.\\nExample\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield max_tokens_to_sample: int = 256#\\nDenotes the number of tokens to predict per generation.\\nfield model: str = \\'claude-v1\\'#\\nModel name to use.\\nfield streaming: bool = False#\\nWhether to stream the results.\\nfield temperature: Optional[float] = None#\\nA non-negative float that tunes the degree of randomness in generation.\\nfield top_k: Optional[int] = None#\\nNumber of most likely tokens to consider at each step.\\nfield top_p: Optional[float] = None#\\nTotal probability mass of tokens to consider at each step.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nstream(prompt: str, stop: Optional[List[str]] = None) → Generator[source]#\\nCall Anthropic completion_stream and return the resulting generator.\\nBETA: this is a beta feature while we figure out the right abstraction.\\nOnce that happens, this interface could change.\\nParameters\\nprompt – The prompt to pass into the model.\\nstop – Optional list of stop words to use when generating.\\nReturns\\nA generator representing the stream of tokens from Anthropic.\\nExample\\nprompt = \"Write a poem about a stream.\"\\nprompt = f\"\\\\n\\\\nHuman: {prompt}\\\\n\\\\nAssistant:\"\\ngenerator = anthropic.stream(prompt)\\nfor token in generator:\\n    yield token\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.AzureOpenAI[source]#\\nWrapper around Azure-specific OpenAI large language models.\\nTo use, you should have the openai python package installed, and the\\nenvironment variable OPENAI_API_KEY set with your API key.\\nAny parameters that are valid to be passed to the openai.create call can be passed\\nin, even if not explicitly saved on this class.\\nExample\\nfrom langchain.llms import AzureOpenAI\\nopenai = AzureOpenAI(model_name=\"text-davinci-003\")\\nValidators\\nbuild_extra » all fields\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield allowed_special: Union[Literal[\\'all\\'], AbstractSet[str]] = {}#\\nSet of special tokens that are allowed。\\nfield batch_size: int = 20#\\nBatch size to use when passing multiple documents to generate.\\nfield best_of: int = 1#\\nGenerates best_of completions server-side and returns the “best”.\\nfield deployment_name: str = \\'\\'#\\nDeployment name to use.\\nfield disallowed_special: Union[Literal[\\'all\\'], Collection[str]] = \\'all\\'#\\nSet of special tokens that are not allowed。\\nfield frequency_penalty: float = 0#\\nPenalizes repeated tokens according to frequency.\\nfield logit_bias: Optional[Dict[str, float]] [Optional]#\\nAdjust the probability of specific tokens being generated.\\nfield max_retries: int = 6#\\nMaximum number of retries to make when generating.\\nfield max_tokens: int = 256#\\nThe maximum number of tokens to generate in the completion.\\n-1 returns as many tokens as possible given the prompt and\\nthe models maximal context size.\\nfield model_kwargs: Dict[str, Any] [Optional]#\\nHolds any model parameters valid for create call not explicitly specified.\\nfield model_name: str = \\'text-davinci-003\\'#\\nModel name to use.\\nfield n: int = 1#\\nHow many completions to generate for each prompt.\\nfield presence_penalty: float = 0#\\nPenalizes repeated tokens.\\nfield request_timeout: Optional[Union[float, Tuple[float, float]]] = None#\\nTimeout for requests to OpenAI completion API. Default is 600 seconds.\\nfield streaming: bool = False#\\nWhether to stream the results or not.\\nfield temperature: float = 0.7#\\nWhat sampling temperature to use.\\nfield top_p: float = 1#\\nTotal probability mass of tokens to consider at each step.\\nfield verbose: bool [Optional]#\\nWhether to print out response text.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ncreate_llm_result(choices: Any, prompts: List[str], token_usage: Dict[str, int]) → langchain.schema.LLMResult#\\nCreate the LLMResult from the choices and prompts.\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nCalculate num tokens with tiktoken package.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\nget_sub_prompts(params: Dict[str, Any], prompts: List[str], stop: Optional[List[str]] = None) → List[List[str]]#\\nGet the sub prompts for llm call.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nmax_tokens_for_prompt(prompt: str) → int#\\nCalculate the maximum number of tokens possible to generate for a prompt.\\nParameters\\nprompt – The prompt to pass into the model.\\nReturns\\nThe maximum number of tokens to generate for a prompt.\\nExample\\nmax_tokens = openai.max_token_for_prompt(\"Tell me a joke.\")\\nmodelname_to_contextsize(modelname: str) → int#\\nCalculate the maximum number of tokens possible to generate for a model.\\nParameters\\nmodelname – The modelname we want to know the context size for.\\nReturns\\nThe maximum context size\\nExample\\nmax_tokens = openai.modelname_to_contextsize(\"text-davinci-003\")\\nprep_streaming_params(stop: Optional[List[str]] = None) → Dict[str, Any]#\\nPrepare the params for streaming.\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nstream(prompt: str, stop: Optional[List[str]] = None) → Generator#\\nCall OpenAI with streaming flag and return the resulting generator.\\nBETA: this is a beta feature while we figure out the right abstraction.\\nOnce that happens, this interface could change.\\nParameters\\nprompt – The prompts to pass into the model.\\nstop – Optional list of stop words to use when generating.\\nReturns\\nA generator representing the stream of tokens from OpenAI.\\nExample\\ngenerator = openai.stream(\"Tell me a joke.\")\\nfor token in generator:\\n    yield token\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.Banana[source]#\\nWrapper around Banana large language models.\\nTo use, you should have the banana-dev python package installed,\\nand the environment variable BANANA_API_KEY set with your API key.\\nAny parameters that are valid to be passed to the call can be passed\\nin, even if not explicitly saved on this class.\\nExample\\nValidators\\nbuild_extra » all fields\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield model_key: str = \\'\\'#\\nmodel endpoint to use\\nfield model_kwargs: Dict[str, Any] [Optional]#\\nHolds any model parameters valid for create call not\\nexplicitly specified.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.CerebriumAI[source]#\\nWrapper around CerebriumAI large language models.\\nTo use, you should have the cerebrium python package installed, and the\\nenvironment variable CEREBRIUMAI_API_KEY set with your API key.\\nAny parameters that are valid to be passed to the call can be passed\\nin, even if not explicitly saved on this class.\\nExample\\nValidators\\nbuild_extra » all fields\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield endpoint_url: str = \\'\\'#\\nmodel endpoint to use\\nfield model_kwargs: Dict[str, Any] [Optional]#\\nHolds any model parameters valid for create call not\\nexplicitly specified.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.Cohere[source]#\\nWrapper around Cohere large language models.\\nTo use, you should have the cohere python package installed, and the\\nenvironment variable COHERE_API_KEY set with your API key, or pass\\nit as a named parameter to the constructor.\\nExample\\nfrom langchain.llms import Cohere\\ncohere = Cohere(model=\"gptd-instruct-tft\", cohere_api_key=\"my-api-key\")\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield frequency_penalty: float = 0.0#\\nPenalizes repeated tokens according to frequency. Between 0 and 1.\\nfield k: int = 0#\\nNumber of most likely tokens to consider at each step.\\nfield max_tokens: int = 256#\\nDenotes the number of tokens to predict per generation.\\nfield model: Optional[str] = None#\\nModel name to use.\\nfield p: int = 1#\\nTotal probability mass of tokens to consider at each step.\\nfield presence_penalty: float = 0.0#\\nPenalizes repeated tokens. Between 0 and 1.\\nfield temperature: float = 0.75#\\nA non-negative float that tunes the degree of randomness in generation.\\nfield truncate: Optional[str] = None#\\nSpecify how the client handles inputs longer than the maximum token\\nlength: Truncate from START, END or NONE\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.DeepInfra[source]#\\nWrapper around DeepInfra deployed models.\\nTo use, you should have the requests python package installed, and the\\nenvironment variable DEEPINFRA_API_TOKEN set with your API token, or pass\\nit as a named parameter to the constructor.\\nOnly supports text-generation and text2text-generation for now.\\nExample\\nfrom langchain.llms import DeepInfra\\ndi = DeepInfra(model_id=\"google/flan-t5-xl\",\\n                    deepinfra_api_token=\"my-api-key\")\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.ForefrontAI[source]#\\nWrapper around ForefrontAI large language models.\\nTo use, you should have the environment variable FOREFRONTAI_API_KEY\\nset with your API key.\\nExample\\nfrom langchain.llms import ForefrontAI\\nforefrontai = ForefrontAI(endpoint_url=\"\")\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield base_url: Optional[str] = None#\\nBase url to use, if None decides based on model name.\\nfield endpoint_url: str = \\'\\'#\\nModel name to use.\\nfield length: int = 256#\\nThe maximum number of tokens to generate in the completion.\\nfield repetition_penalty: int = 1#\\nPenalizes repeated tokens according to frequency.\\nfield temperature: float = 0.7#\\nWhat sampling temperature to use.\\nfield top_k: int = 40#\\nThe number of highest probability vocabulary tokens to\\nkeep for top-k-filtering.\\nfield top_p: float = 1.0#\\nTotal probability mass of tokens to consider at each step.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.GPT4All[source]#\\nWrapper around GPT4All language models.\\nTo use, you should have the pyllamacpp python package installed, the\\npre-trained model file, and the model’s config information.\\nExample\\nfrom langchain.llms import GPT4All\\nmodel = GPT4All(model=\"./models/gpt4all-model.bin\", n_ctx=512, n_threads=8)\\n# Simplest invocation\\nresponse = model(\"Once upon a time, \")\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield echo: Optional[bool] = False#\\nWhether to echo the prompt.\\nfield embedding: bool = False#\\nUse embedding mode only.\\nfield f16_kv: bool = False#\\nUse half-precision for key/value cache.\\nfield logits_all: bool = False#\\nReturn logits for all tokens, not just the last token.\\nfield model: str [Required]#\\nPath to the pre-trained GPT4All model file.\\nfield n_batch: int = 1#\\nBatch size for prompt processing.\\nfield n_ctx: int = 512#\\nToken context window.\\nfield n_parts: int = -1#\\nNumber of parts to split the model into.\\nIf -1, the number of parts is automatically determined.\\nfield n_predict: Optional[int] = 256#\\nThe maximum number of tokens to generate.\\nfield n_threads: Optional[int] = 4#\\nNumber of threads to use.\\nfield repeat_last_n: Optional[int] = 64#\\nLast n tokens to penalize\\nfield repeat_penalty: Optional[float] = 1.3#\\nThe penalty to apply to repeated tokens.\\nfield seed: int = 0#\\nSeed. If -1, a random seed is used.\\nfield stop: Optional[List[str]] = []#\\nA list of strings to stop generation when encountered.\\nfield streaming: bool = False#\\nWhether to stream the results or not.\\nfield temp: Optional[float] = 0.8#\\nThe temperature to use for sampling.\\nfield top_k: Optional[int] = 40#\\nThe top-k value to use for sampling.\\nfield top_p: Optional[float] = 0.95#\\nThe top-p value to use for sampling.\\nfield use_mlock: bool = False#\\nForce system to keep model in RAM.\\nfield vocab_only: bool = False#\\nOnly load the vocabulary, no weights.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.GooseAI[source]#\\nWrapper around OpenAI large language models.\\nTo use, you should have the openai python package installed, and the\\nenvironment variable GOOSEAI_API_KEY set with your API key.\\nAny parameters that are valid to be passed to the openai.create call can be passed\\nin, even if not explicitly saved on this class.\\nExample\\nValidators\\nbuild_extra » all fields\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield frequency_penalty: float = 0#\\nPenalizes repeated tokens according to frequency.\\nfield logit_bias: Optional[Dict[str, float]] [Optional]#\\nAdjust the probability of specific tokens being generated.\\nfield max_tokens: int = 256#\\nThe maximum number of tokens to generate in the completion.\\n-1 returns as many tokens as possible given the prompt and\\nthe models maximal context size.\\nfield min_tokens: int = 1#\\nThe minimum number of tokens to generate in the completion.\\nfield model_kwargs: Dict[str, Any] [Optional]#\\nHolds any model parameters valid for create call not explicitly specified.\\nfield model_name: str = \\'gpt-neo-20b\\'#\\nModel name to use\\nfield n: int = 1#\\nHow many completions to generate for each prompt.\\nfield presence_penalty: float = 0#\\nPenalizes repeated tokens.\\nfield temperature: float = 0.7#\\nWhat sampling temperature to use\\nfield top_p: float = 1#\\nTotal probability mass of tokens to consider at each step.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.HuggingFaceEndpoint[source]#\\nWrapper around HuggingFaceHub Inference Endpoints.\\nTo use, you should have the huggingface_hub python package installed, and the\\nenvironment variable HUGGINGFACEHUB_API_TOKEN set with your API token, or pass\\nit as a named parameter to the constructor.\\nOnly supports text-generation and text2text-generation for now.\\nExample\\nfrom langchain.llms import HuggingFaceEndpoint\\nendpoint_url = (\\n    \"https://abcdefghijklmnop.us-east-1.aws.endpoints.huggingface.cloud\"\\n)\\nhf = HuggingFaceEndpoint(\\n    endpoint_url=endpoint_url,\\n    huggingfacehub_api_token=\"my-api-key\"\\n)\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield endpoint_url: str = \\'\\'#\\nEndpoint URL to use.\\nfield model_kwargs: Optional[dict] = None#\\nKey word arguments to pass to the model.\\nfield task: Optional[str] = None#\\nTask to call the model with. Should be a task that returns generated_text.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.HuggingFaceHub[source]#\\nWrapper around HuggingFaceHub  models.\\nTo use, you should have the huggingface_hub python package installed, and the\\nenvironment variable HUGGINGFACEHUB_API_TOKEN set with your API token, or pass\\nit as a named parameter to the constructor.\\nOnly supports text-generation and text2text-generation for now.\\nExample\\nfrom langchain.llms import HuggingFaceHub\\nhf = HuggingFaceHub(repo_id=\"gpt2\", huggingfacehub_api_token=\"my-api-key\")\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield model_kwargs: Optional[dict] = None#\\nKey word arguments to pass to the model.\\nfield repo_id: str = \\'gpt2\\'#\\nModel name to use.\\nfield task: Optional[str] = None#\\nTask to call the model with. Should be a task that returns generated_text.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.HuggingFacePipeline[source]#\\nWrapper around HuggingFace Pipeline API.\\nTo use, you should have the transformers python package installed.\\nOnly supports text-generation and text2text-generation for now.\\nExample using from_model_id:from langchain.llms import HuggingFacePipeline\\nhf = HuggingFacePipeline.from_model_id(\\n    model_id=\"gpt2\", task=\"text-generation\"\\n)\\nExample passing pipeline in directly:from langchain.llms import HuggingFacePipeline\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\\nmodel_id = \"gpt2\"\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\\npipe = pipeline(\\n    \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10\\n)\\nhf = HuggingFacePipeline(pipeline=pipe)\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield model_id: str = \\'gpt2\\'#\\nModel name to use.\\nfield model_kwargs: Optional[dict] = None#\\nKey word arguments to pass to the model.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\nclassmethod from_model_id(model_id: str, task: str, device: int = - 1, model_kwargs: Optional[dict] = None, **kwargs: Any) → langchain.llms.base.LLM[source]#\\nConstruct the pipeline object from model_id and task.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.LlamaCpp[source]#\\nWrapper around the llama.cpp model.\\nTo use, you should have the llama-cpp-python library installed, and provide the\\npath to the Llama model as a named parameter to the constructor.\\nCheck out: abetlen/llama-cpp-python\\nExample\\nfrom langchain.llms import LlamaCppEmbeddings\\nllm = LlamaCppEmbeddings(model_path=\"/path/to/llama/model\")\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield echo: Optional[bool] = False#\\nWhether to echo the prompt.\\nfield f16_kv: bool = False#\\nUse half-precision for key/value cache.\\nfield last_n_tokens_size: Optional[int] = 64#\\nThe number of tokens to look back when applying the repeat_penalty.\\nfield logits_all: bool = False#\\nReturn logits for all tokens, not just the last token.\\nfield logprobs: Optional[int] = None#\\nThe number of logprobs to return. If None, no logprobs are returned.\\nfield max_tokens: Optional[int] = 256#\\nThe maximum number of tokens to generate.\\nfield model_path: str [Required]#\\nThe path to the Llama model file.\\nfield n_batch: Optional[int] = 8#\\nNumber of tokens to process in parallel.\\nShould be a number between 1 and n_ctx.\\nfield n_ctx: int = 512#\\nToken context window.\\nfield n_parts: int = -1#\\nNumber of parts to split the model into.\\nIf -1, the number of parts is automatically determined.\\nfield n_threads: Optional[int] = None#\\nNumber of threads to use.\\nIf None, the number of threads is automatically determined.\\nfield repeat_penalty: Optional[float] = 1.1#\\nThe penalty to apply to repeated tokens.\\nfield seed: int = -1#\\nSeed. If -1, a random seed is used.\\nfield stop: Optional[List[str]] = []#\\nA list of strings to stop generation when encountered.\\nfield suffix: Optional[str] = None#\\nA suffix to append to the generated text. If None, no suffix is appended.\\nfield temperature: Optional[float] = 0.8#\\nThe temperature to use for sampling.\\nfield top_k: Optional[int] = 40#\\nThe top-k value to use for sampling.\\nfield top_p: Optional[float] = 0.95#\\nThe top-p value to use for sampling.\\nfield use_mlock: bool = False#\\nForce system to keep model in RAM.\\nfield vocab_only: bool = False#\\nOnly load the vocabulary, no weights.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.Modal[source]#\\nWrapper around Modal large language models.\\nTo use, you should have the modal-client python package installed.\\nAny parameters that are valid to be passed to the call can be passed\\nin, even if not explicitly saved on this class.\\nExample\\nValidators\\nbuild_extra » all fields\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield endpoint_url: str = \\'\\'#\\nmodel endpoint to use\\nfield model_kwargs: Dict[str, Any] [Optional]#\\nHolds any model parameters valid for create call not\\nexplicitly specified.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.NLPCloud[source]#\\nWrapper around NLPCloud large language models.\\nTo use, you should have the nlpcloud python package installed, and the\\nenvironment variable NLPCLOUD_API_KEY set with your API key.\\nExample\\nfrom langchain.llms import NLPCloud\\nnlpcloud = NLPCloud(model=\"gpt-neox-20b\")\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield bad_words: List[str] = []#\\nList of tokens not allowed to be generated.\\nfield do_sample: bool = True#\\nWhether to use sampling (True) or greedy decoding.\\nfield early_stopping: bool = False#\\nWhether to stop beam search at num_beams sentences.\\nfield length_no_input: bool = True#\\nWhether min_length and max_length should include the length of the input.\\nfield length_penalty: float = 1.0#\\nExponential penalty to the length.\\nfield max_length: int = 256#\\nThe maximum number of tokens to generate in the completion.\\nfield min_length: int = 1#\\nThe minimum number of tokens to generate in the completion.\\nfield model_name: str = \\'finetuned-gpt-neox-20b\\'#\\nModel name to use.\\nfield num_beams: int = 1#\\nNumber of beams for beam search.\\nfield num_return_sequences: int = 1#\\nHow many completions to generate for each prompt.\\nfield remove_end_sequence: bool = True#\\nWhether or not to remove the end sequence token.\\nfield remove_input: bool = True#\\nRemove input text from API response\\nfield repetition_penalty: float = 1.0#\\nPenalizes repeated tokens. 1.0 means no penalty.\\nfield temperature: float = 0.7#\\nWhat sampling temperature to use.\\nfield top_k: int = 50#\\nThe number of highest probability tokens to keep for top-k filtering.\\nfield top_p: int = 1#\\nTotal probability mass of tokens to consider at each step.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.OpenAI[source]#\\nWrapper around OpenAI large language models.\\nTo use, you should have the openai python package installed, and the\\nenvironment variable OPENAI_API_KEY set with your API key.\\nAny parameters that are valid to be passed to the openai.create call can be passed\\nin, even if not explicitly saved on this class.\\nExample\\nfrom langchain.llms import OpenAI\\nopenai = OpenAI(model_name=\"text-davinci-003\")\\nValidators\\nbuild_extra » all fields\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield verbose: bool [Optional]#\\nWhether to print out response text.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ncreate_llm_result(choices: Any, prompts: List[str], token_usage: Dict[str, int]) → langchain.schema.LLMResult#\\nCreate the LLMResult from the choices and prompts.\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nCalculate num tokens with tiktoken package.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\nget_sub_prompts(params: Dict[str, Any], prompts: List[str], stop: Optional[List[str]] = None) → List[List[str]]#\\nGet the sub prompts for llm call.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nmax_tokens_for_prompt(prompt: str) → int#\\nCalculate the maximum number of tokens possible to generate for a prompt.\\nParameters\\nprompt – The prompt to pass into the model.\\nReturns\\nThe maximum number of tokens to generate for a prompt.\\nExample\\nmax_tokens = openai.max_token_for_prompt(\"Tell me a joke.\")\\nmodelname_to_contextsize(modelname: str) → int#\\nCalculate the maximum number of tokens possible to generate for a model.\\nParameters\\nmodelname – The modelname we want to know the context size for.\\nReturns\\nThe maximum context size\\nExample\\nmax_tokens = openai.modelname_to_contextsize(\"text-davinci-003\")\\nprep_streaming_params(stop: Optional[List[str]] = None) → Dict[str, Any]#\\nPrepare the params for streaming.\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nstream(prompt: str, stop: Optional[List[str]] = None) → Generator#\\nCall OpenAI with streaming flag and return the resulting generator.\\nBETA: this is a beta feature while we figure out the right abstraction.\\nOnce that happens, this interface could change.\\nParameters\\nprompt – The prompts to pass into the model.\\nstop – Optional list of stop words to use when generating.\\nReturns\\nA generator representing the stream of tokens from OpenAI.\\nExample\\ngenerator = openai.stream(\"Tell me a joke.\")\\nfor token in generator:\\n    yield token\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.OpenAIChat[source]#\\nWrapper around OpenAI Chat large language models.\\nTo use, you should have the openai python package installed, and the\\nenvironment variable OPENAI_API_KEY set with your API key.\\nAny parameters that are valid to be passed to the openai.create call can be passed\\nin, even if not explicitly saved on this class.\\nExample\\nfrom langchain.llms import OpenAIChat\\nopenaichat = OpenAIChat(model_name=\"gpt-3.5-turbo\")\\nValidators\\nbuild_extra » all fields\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield allowed_special: Union[Literal[\\'all\\'], AbstractSet[str]] = {}#\\nSet of special tokens that are allowed。\\nfield disallowed_special: Union[Literal[\\'all\\'], Collection[str]] = \\'all\\'#\\nSet of special tokens that are not allowed。\\nfield max_retries: int = 6#\\nMaximum number of retries to make when generating.\\nfield model_kwargs: Dict[str, Any] [Optional]#\\nHolds any model parameters valid for create call not explicitly specified.\\nfield model_name: str = \\'gpt-3.5-turbo\\'#\\nModel name to use.\\nfield prefix_messages: List [Optional]#\\nSeries of messages for Chat input.\\nfield streaming: bool = False#\\nWhether to stream the results or not.\\nfield verbose: bool [Optional]#\\nWhether to print out response text.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int[source]#\\nCalculate num tokens with tiktoken package.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.Petals[source]#\\nWrapper around Petals Bloom models.\\nTo use, you should have the petals python package installed, and the\\nenvironment variable HUGGINGFACE_API_KEY set with your API key.\\nAny parameters that are valid to be passed to the call can be passed\\nin, even if not explicitly saved on this class.\\nExample\\nValidators\\nbuild_extra » all fields\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield client: Any = None#\\nThe client to use for the API calls.\\nfield do_sample: bool = True#\\nWhether or not to use sampling; use greedy decoding otherwise.\\nfield max_length: Optional[int] = None#\\nThe maximum length of the sequence to be generated.\\nfield max_new_tokens: int = 256#\\nThe maximum number of new tokens to generate in the completion.\\nfield model_kwargs: Dict[str, Any] [Optional]#\\nHolds any model parameters valid for create call\\nnot explicitly specified.\\nfield model_name: str = \\'bigscience/bloom-petals\\'#\\nThe model to use.\\nfield temperature: float = 0.7#\\nWhat sampling temperature to use\\nfield tokenizer: Any = None#\\nThe tokenizer to use for the API calls.\\nfield top_k: Optional[int] = None#\\nThe number of highest probability vocabulary tokens\\nto keep for top-k-filtering.\\nfield top_p: float = 0.9#\\nThe cumulative probability for top-p sampling.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.PromptLayerOpenAI[source]#\\nWrapper around OpenAI large language models.\\nTo use, you should have the openai and promptlayer python\\npackage installed, and the environment variable OPENAI_API_KEY\\nand PROMPTLAYER_API_KEY set with your openAI API key and\\npromptlayer key respectively.\\nAll parameters that can be passed to the OpenAI LLM can also\\nbe passed here. The PromptLayerOpenAI LLM adds two optional\\n:param pl_tags: List of strings to tag the request with.\\n:param return_pl_id: If True, the PromptLayer request ID will be\\nreturned in the generation_info field of the\\nGeneration object.\\nExample\\nfrom langchain.llms import PromptLayerOpenAI\\nopenai = PromptLayerOpenAI(model_name=\"text-davinci-003\")\\nValidators\\nbuild_extra » all fields\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ncreate_llm_result(choices: Any, prompts: List[str], token_usage: Dict[str, int]) → langchain.schema.LLMResult#\\nCreate the LLMResult from the choices and prompts.\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nCalculate num tokens with tiktoken package.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\nget_sub_prompts(params: Dict[str, Any], prompts: List[str], stop: Optional[List[str]] = None) → List[List[str]]#\\nGet the sub prompts for llm call.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nmax_tokens_for_prompt(prompt: str) → int#\\nCalculate the maximum number of tokens possible to generate for a prompt.\\nParameters\\nprompt – The prompt to pass into the model.\\nReturns\\nThe maximum number of tokens to generate for a prompt.\\nExample\\nmax_tokens = openai.max_token_for_prompt(\"Tell me a joke.\")\\nmodelname_to_contextsize(modelname: str) → int#\\nCalculate the maximum number of tokens possible to generate for a model.\\nParameters\\nmodelname – The modelname we want to know the context size for.\\nReturns\\nThe maximum context size\\nExample\\nmax_tokens = openai.modelname_to_contextsize(\"text-davinci-003\")\\nprep_streaming_params(stop: Optional[List[str]] = None) → Dict[str, Any]#\\nPrepare the params for streaming.\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nstream(prompt: str, stop: Optional[List[str]] = None) → Generator#\\nCall OpenAI with streaming flag and return the resulting generator.\\nBETA: this is a beta feature while we figure out the right abstraction.\\nOnce that happens, this interface could change.\\nParameters\\nprompt – The prompts to pass into the model.\\nstop – Optional list of stop words to use when generating.\\nReturns\\nA generator representing the stream of tokens from OpenAI.\\nExample\\ngenerator = openai.stream(\"Tell me a joke.\")\\nfor token in generator:\\n    yield token\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.PromptLayerOpenAIChat[source]#\\nWrapper around OpenAI large language models.\\nTo use, you should have the openai and promptlayer python\\npackage installed, and the environment variable OPENAI_API_KEY\\nand PROMPTLAYER_API_KEY set with your openAI API key and\\npromptlayer key respectively.\\nAll parameters that can be passed to the OpenAIChat LLM can also\\nbe passed here. The PromptLayerOpenAIChat adds two optional\\n:param pl_tags: List of strings to tag the request with.\\n:param return_pl_id: If True, the PromptLayer request ID will be\\nreturned in the generation_info field of the\\nGeneration object.\\nExample\\nfrom langchain.llms import PromptLayerOpenAIChat\\nopenaichat = PromptLayerOpenAIChat(model_name=\"gpt-3.5-turbo\")\\nValidators\\nbuild_extra » all fields\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield allowed_special: Union[Literal[\\'all\\'], AbstractSet[str]] = {}#\\nSet of special tokens that are allowed。\\nfield disallowed_special: Union[Literal[\\'all\\'], Collection[str]] = \\'all\\'#\\nSet of special tokens that are not allowed。\\nfield max_retries: int = 6#\\nMaximum number of retries to make when generating.\\nfield model_kwargs: Dict[str, Any] [Optional]#\\nHolds any model parameters valid for create call not explicitly specified.\\nfield model_name: str = \\'gpt-3.5-turbo\\'#\\nModel name to use.\\nfield prefix_messages: List [Optional]#\\nSeries of messages for Chat input.\\nfield streaming: bool = False#\\nWhether to stream the results or not.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nCalculate num tokens with tiktoken package.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.RWKV[source]#\\nWrapper around RWKV language models.\\nTo use, you should have the rwkv python package installed, the\\npre-trained model file, and the model’s config information.\\nExample\\nfrom langchain.llms import RWKV\\nmodel = RWKV(model=\"./models/rwkv-3b-fp16.bin\", strategy=\"cpu fp32\")\\n# Simplest invocation\\nresponse = model(\"Once upon a time, \")\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield CHUNK_LEN: int = 256#\\nBatch size for prompt processing.\\nfield max_tokens_per_generation: int = 256#\\nMaximum number of tokens to generate.\\nfield model: str [Required]#\\nPath to the pre-trained RWKV model file.\\nfield penalty_alpha_frequency: float = 0.4#\\nPositive values penalize new tokens based on their existing frequency\\nin the text so far, decreasing the model’s likelihood to repeat the same\\nline verbatim..\\nfield penalty_alpha_presence: float = 0.4#\\nPositive values penalize new tokens based on whether they appear\\nin the text so far, increasing the model’s likelihood to talk about\\nnew topics..\\nfield rwkv_verbose: bool = True#\\nPrint debug information.\\nfield strategy: str = \\'cpu fp32\\'#\\nToken context window.\\nfield temperature: float = 1.0#\\nThe temperature to use for sampling.\\nfield tokens_path: str [Required]#\\nPath to the RWKV tokens file.\\nfield top_p: float = 0.5#\\nThe top-p value to use for sampling.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.Replicate[source]#\\nWrapper around Replicate models.\\nTo use, you should have the replicate python package installed,\\nand the environment variable REPLICATE_API_TOKEN set with your API token.\\nYou can find your token here: https://replicate.com/account\\nThe model param is required, but any other model parameters can also\\nbe passed in with the format input={model_param: value, …}\\nExample\\nValidators\\nbuild_extra » all fields\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.SagemakerEndpoint[source]#\\nWrapper around custom Sagemaker Inference Endpoints.\\nTo use, you must supply the endpoint name from your deployed\\nSagemaker model & the region where it is deployed.\\nTo authenticate, the AWS client uses the following methods to\\nautomatically load credentials:\\nhttps://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\\nIf a specific credential profile should be used, you must pass\\nthe name of the profile from the ~/.aws/credentials file that is to be used.\\nMake sure the credentials / roles used have the required policies to\\naccess the Sagemaker endpoint.\\nSee: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield content_handler: langchain.llms.sagemaker_endpoint.ContentHandlerBase [Required]#\\nThe content handler class that provides an input and\\noutput transform functions to handle formats between LLM\\nand the endpoint.\\nfield credentials_profile_name: Optional[str] = None#\\nThe name of the profile in the ~/.aws/credentials or ~/.aws/config files, which\\nhas either access keys or role information specified.\\nIf not specified, the default credential profile or, if on an EC2 instance,\\ncredentials from IMDS will be used.\\nSee: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\\nfield endpoint_kwargs: Optional[Dict] = None#\\nOptional attributes passed to the invoke_endpoint\\nfunction. See `boto3`_. docs for more info.\\n.. _boto3: <https://boto3.amazonaws.com/v1/documentation/api/latest/index.html>\\nfield endpoint_name: str = \\'\\'#\\nThe name of the endpoint from the deployed Sagemaker model.\\nMust be unique within an AWS Region.\\nfield model_kwargs: Optional[Dict] = None#\\nKey word arguments to pass to the model.\\nfield region_name: str = \\'\\'#\\nThe aws region where the Sagemaker model is deployed, eg. us-west-2.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.SelfHostedHuggingFaceLLM[source]#\\nWrapper around HuggingFace Pipeline API to run on self-hosted remote hardware.\\nSupported hardware includes auto-launched instances on AWS, GCP, Azure,\\nand Lambda, as well as servers specified\\nby IP address and SSH credentials (such as on-prem, or another cloud\\nlike Paperspace, Coreweave, etc.).\\nTo use, you should have the runhouse python package installed.\\nOnly supports text-generation and text2text-generation for now.\\nExample using from_model_id:from langchain.llms import SelfHostedHuggingFaceLLM\\nimport runhouse as rh\\ngpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\")\\nhf = SelfHostedHuggingFaceLLM(\\n    model_id=\"google/flan-t5-large\", task=\"text2text-generation\",\\n    hardware=gpu\\n)\\nExample passing fn that generates a pipeline (bc the pipeline is not serializable):from langchain.llms import SelfHostedHuggingFaceLLM\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\\nimport runhouse as rh\\ndef get_pipeline():\\n    model_id = \"gpt2\"\\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\\n    model = AutoModelForCausalLM.from_pretrained(model_id)\\n    pipe = pipeline(\\n        \"text-generation\", model=model, tokenizer=tokenizer\\n    )\\n    return pipe\\nhf = SelfHostedHuggingFaceLLM(\\n    model_load_fn=get_pipeline, model_id=\"gpt2\", hardware=gpu)\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield device: int = 0#\\nDevice to use for inference. -1 for CPU, 0 for GPU, 1 for second GPU, etc.\\nfield hardware: Any = None#\\nRemote hardware to send the inference function to.\\nfield inference_fn: Callable = <function _generate_text>#\\nInference function to send to the remote hardware.\\nfield load_fn_kwargs: Optional[dict] = None#\\nKey word arguments to pass to the model load function.\\nfield model_id: str = \\'gpt2\\'#\\nHugging Face model_id to load the model.\\nfield model_kwargs: Optional[dict] = None#\\nKey word arguments to pass to the model.\\nfield model_load_fn: Callable = <function _load_transformer>#\\nFunction to load the model remotely on the server.\\nfield model_reqs: List[str] = [\\'./\\', \\'transformers\\', \\'torch\\']#\\nRequirements to install on hardware to inference the model.\\nfield task: str = \\'text-generation\\'#\\nHugging Face task (either “text-generation” or “text2text-generation”).\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\nclassmethod from_pipeline(pipeline: Any, hardware: Any, model_reqs: Optional[List[str]] = None, device: int = 0, **kwargs: Any) → langchain.llms.base.LLM#\\nInit the SelfHostedPipeline from a pipeline object or string.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.SelfHostedPipeline[source]#\\nRun model inference on self-hosted remote hardware.\\nSupported hardware includes auto-launched instances on AWS, GCP, Azure,\\nand Lambda, as well as servers specified\\nby IP address and SSH credentials (such as on-prem, or another\\ncloud like Paperspace, Coreweave, etc.).\\nTo use, you should have the runhouse python package installed.\\nExample for custom pipeline and inference functions:from langchain.llms import SelfHostedPipeline\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\\nimport runhouse as rh\\ndef load_pipeline():\\n    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\\n    model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\\n    return pipeline(\\n        \"text-generation\", model=model, tokenizer=tokenizer,\\n        max_new_tokens=10\\n    )\\ndef inference_fn(pipeline, prompt, stop = None):\\n    return pipeline(prompt)[0][\"generated_text\"]\\ngpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\")\\nllm = SelfHostedPipeline(\\n    model_load_fn=load_pipeline,\\n    hardware=gpu,\\n    model_reqs=model_reqs, inference_fn=inference_fn\\n)\\nExample for <2GB model (can be serialized and sent directly to the server):from langchain.llms import SelfHostedPipeline\\nimport runhouse as rh\\ngpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\")\\nmy_model = ...\\nllm = SelfHostedPipeline.from_pipeline(\\n    pipeline=my_model,\\n    hardware=gpu,\\n    model_reqs=[\"./\", \"torch\", \"transformers\"],\\n)\\nExample passing model path for larger models:from langchain.llms import SelfHostedPipeline\\nimport runhouse as rh\\nimport pickle\\nfrom transformers import pipeline\\ngenerator = pipeline(model=\"gpt2\")\\nrh.blob(pickle.dumps(generator), path=\"models/pipeline.pkl\"\\n    ).save().to(gpu, path=\"models\")\\nllm = SelfHostedPipeline.from_pipeline(\\n    pipeline=\"models/pipeline.pkl\",\\n    hardware=gpu,\\n    model_reqs=[\"./\", \"torch\", \"transformers\"],\\n)\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nfield hardware: Any = None#\\nRemote hardware to send the inference function to.\\nfield inference_fn: Callable = <function _generate_text>#\\nInference function to send to the remote hardware.\\nfield load_fn_kwargs: Optional[dict] = None#\\nKey word arguments to pass to the model load function.\\nfield model_load_fn: Callable [Required]#\\nFunction to load the model remotely on the server.\\nfield model_reqs: List[str] = [\\'./\\', \\'torch\\']#\\nRequirements to install on hardware to inference the model.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\nclassmethod from_pipeline(pipeline: Any, hardware: Any, model_reqs: Optional[List[str]] = None, device: int = 0, **kwargs: Any) → langchain.llms.base.LLM[source]#\\nInit the SelfHostedPipeline from a pipeline object or string.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.StochasticAI[source]#\\nWrapper around StochasticAI large language models.\\nTo use, you should have the environment variable STOCHASTICAI_API_KEY\\nset with your API key.\\nExample\\nfrom langchain.llms import StochasticAI\\nstochasticai = StochasticAI(api_url=\"\")\\nValidators\\nbuild_extra » all fields\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield api_url: str = \\'\\'#\\nModel name to use.\\nfield model_kwargs: Dict[str, Any] [Optional]#\\nHolds any model parameters valid for create call not\\nexplicitly specified.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\npydantic model langchain.llms.Writer[source]#\\nWrapper around Writer large language models.\\nTo use, you should have the environment variable WRITER_API_KEY\\nset with your API key.\\nExample\\nfrom langchain import Writer\\nwriter = Writer(model_id=\"palmyra-base\")\\nValidators\\nset_callback_manager » callback_manager\\nset_verbose » verbose\\nvalidate_environment » all fields\\nfield base_url: Optional[str] = None#\\nBase url to use, if None decides based on model name.\\nfield beam_search_diversity_rate: float = 1.0#\\nOnly applies to beam search, i.e. when the beam width is >1.\\nA higher value encourages beam search to return a more diverse\\nset of candidates\\nfield beam_width: Optional[int] = None#\\nThe number of concurrent candidates to keep track of during\\nbeam search\\nfield length: int = 256#\\nThe maximum number of tokens to generate in the completion.\\nfield length_pentaly: float = 1.0#\\nOnly applies to beam search, i.e. when the beam width is >1.\\nLarger values penalize long candidates more heavily, thus preferring\\nshorter candidates\\nfield logprobs: bool = False#\\nWhether to return log probabilities.\\nfield model_id: str = \\'palmyra-base\\'#\\nModel name to use.\\nfield random_seed: int = 0#\\nThe model generates random results.\\nChanging the random seed alone will produce a different response\\nwith similar characteristics. It is possible to reproduce results\\nby fixing the random seed (assuming all other hyperparameters\\nare also fixed)\\nfield repetition_penalty: float = 1.0#\\nPenalizes repeated tokens according to frequency.\\nfield stop: Optional[List[str]] = None#\\nSequences when completion generation will stop\\nfield temperature: float = 1.0#\\nWhat sampling temperature to use.\\nfield tokens_to_generate: int = 24#\\nMax number of tokens to generate.\\nfield top_k: int = 1#\\nThe number of highest probability vocabulary tokens to\\nkeep for top-k-filtering.\\nfield top_p: float = 1.0#\\nTotal probability mass of tokens to consider at each step.\\n__call__(prompt: str, stop: Optional[List[str]] = None) → str#\\nCheck Cache and run the LLM on the given prompt and input.\\nasync agenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\nasync agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model#\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model#\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\ndict(**kwargs: Any) → Dict#\\nReturn a dictionary of the LLM.\\ngenerate(prompts: List[str], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nRun the LLM on the given prompt and input.\\ngenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None) → langchain.schema.LLMResult#\\nTake in a list of prompt values and return an LLMResult.\\nget_num_tokens(text: str) → int#\\nGet the number of tokens present in the text.\\nget_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int#\\nGet the number of tokens in the message.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode#\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nsave(file_path: Union[pathlib.Path, str]) → None#\\nSave the LLM.\\nParameters\\nfile_path – Path to file to save the LLM to.\\nExample:\\n.. code-block:: python\\nllm.save(file_path=”path/llm.yaml”)\\nclassmethod update_forward_refs(**localns: Any) → None#\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\nprevious\\nWriter\\nnext\\nChat Models\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\reference\\\\modules\\\\llms.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nPromptTemplates\\nPromptTemplates#\\nPrompt template classes.\\npydantic model langchain.prompts.BaseChatPromptTemplate[source]#\\nformat(**kwargs: Any) → str[source]#\\nFormat the prompt with the inputs.\\nParameters\\nkwargs – Any arguments to be passed to the prompt template.\\nReturns\\nA formatted string.\\nExample:\\nprompt.format(variable1=\"foo\")\\nabstract format_messages(**kwargs: Any) → List[langchain.schema.BaseMessage][source]#\\nFormat kwargs into a list of messages.\\nformat_prompt(**kwargs: Any) → langchain.schema.PromptValue[source]#\\nCreate Chat Messages.\\npydantic model langchain.prompts.BasePromptTemplate[source]#\\nBase class for all prompt templates, returning a prompt.\\nfield input_variables: List[str] [Required]#\\nA list of the names of the variables the prompt template expects.\\nfield output_parser: Optional[langchain.schema.BaseOutputParser] = None#\\nHow to parse the output of calling an LLM on this formatted prompt.\\ndict(**kwargs: Any) → Dict[source]#\\nReturn dictionary representation of prompt.\\nabstract format(**kwargs: Any) → str[source]#\\nFormat the prompt with the inputs.\\nParameters\\nkwargs – Any arguments to be passed to the prompt template.\\nReturns\\nA formatted string.\\nExample:\\nprompt.format(variable1=\"foo\")\\nabstract format_prompt(**kwargs: Any) → langchain.schema.PromptValue[source]#\\nCreate Chat Messages.\\npartial(**kwargs: Union[str, Callable[[], str]]) → langchain.prompts.base.BasePromptTemplate[source]#\\nReturn a partial of the prompt template.\\nsave(file_path: Union[pathlib.Path, str]) → None[source]#\\nSave the prompt.\\nParameters\\nfile_path – Path to directory to save prompt to.\\nExample:\\n.. code-block:: python\\nprompt.save(file_path=”path/prompt.yaml”)\\npydantic model langchain.prompts.ChatPromptTemplate[source]#\\nformat(**kwargs: Any) → str[source]#\\nFormat the prompt with the inputs.\\nParameters\\nkwargs – Any arguments to be passed to the prompt template.\\nReturns\\nA formatted string.\\nExample:\\nprompt.format(variable1=\"foo\")\\nformat_messages(**kwargs: Any) → List[langchain.schema.BaseMessage][source]#\\nFormat kwargs into a list of messages.\\npartial(**kwargs: Union[str, Callable[[], str]]) → langchain.prompts.base.BasePromptTemplate[source]#\\nReturn a partial of the prompt template.\\nsave(file_path: Union[pathlib.Path, str]) → None[source]#\\nSave the prompt.\\nParameters\\nfile_path – Path to directory to save prompt to.\\nExample:\\n.. code-block:: python\\nprompt.save(file_path=”path/prompt.yaml”)\\npydantic model langchain.prompts.FewShotPromptTemplate[source]#\\nPrompt template that contains few shot examples.\\nfield example_prompt: langchain.prompts.prompt.PromptTemplate [Required]#\\nPromptTemplate used to format an individual example.\\nfield example_selector: Optional[langchain.prompts.example_selector.base.BaseExampleSelector] = None#\\nExampleSelector to choose the examples to format into the prompt.\\nEither this or examples should be provided.\\nfield example_separator: str = \\'\\\\n\\\\n\\'#\\nString separator used to join the prefix, the examples, and suffix.\\nfield examples: Optional[List[dict]] = None#\\nExamples to format into the prompt.\\nEither this or example_selector should be provided.\\nfield input_variables: List[str] [Required]#\\nA list of the names of the variables the prompt template expects.\\nfield prefix: str = \\'\\'#\\nA prompt template string to put before the examples.\\nfield suffix: str [Required]#\\nA prompt template string to put after the examples.\\nfield template_format: str = \\'f-string\\'#\\nThe format of the prompt template. Options are: ‘f-string’, ‘jinja2’.\\nfield validate_template: bool = True#\\nWhether or not to try validating the template.\\ndict(**kwargs: Any) → Dict[source]#\\nReturn a dictionary of the prompt.\\nformat(**kwargs: Any) → str[source]#\\nFormat the prompt with the inputs.\\nParameters\\nkwargs – Any arguments to be passed to the prompt template.\\nReturns\\nA formatted string.\\nExample:\\nprompt.format(variable1=\"foo\")\\npydantic model langchain.prompts.FewShotPromptWithTemplates[source]#\\nPrompt template that contains few shot examples.\\nfield example_prompt: langchain.prompts.prompt.PromptTemplate [Required]#\\nPromptTemplate used to format an individual example.\\nfield example_selector: Optional[langchain.prompts.example_selector.base.BaseExampleSelector] = None#\\nExampleSelector to choose the examples to format into the prompt.\\nEither this or examples should be provided.\\nfield example_separator: str = \\'\\\\n\\\\n\\'#\\nString separator used to join the prefix, the examples, and suffix.\\nfield examples: Optional[List[dict]] = None#\\nExamples to format into the prompt.\\nEither this or example_selector should be provided.\\nfield input_variables: List[str] [Required]#\\nA list of the names of the variables the prompt template expects.\\nfield prefix: Optional[langchain.prompts.base.StringPromptTemplate] = None#\\nA PromptTemplate to put before the examples.\\nfield suffix: langchain.prompts.base.StringPromptTemplate [Required]#\\nA PromptTemplate to put after the examples.\\nfield template_format: str = \\'f-string\\'#\\nThe format of the prompt template. Options are: ‘f-string’, ‘jinja2’.\\nfield validate_template: bool = True#\\nWhether or not to try validating the template.\\ndict(**kwargs: Any) → Dict[source]#\\nReturn a dictionary of the prompt.\\nformat(**kwargs: Any) → str[source]#\\nFormat the prompt with the inputs.\\nParameters\\nkwargs – Any arguments to be passed to the prompt template.\\nReturns\\nA formatted string.\\nExample:\\nprompt.format(variable1=\"foo\")\\npydantic model langchain.prompts.MessagesPlaceholder[source]#\\nPrompt template that assumes variable is already list of messages.\\nformat_messages(**kwargs: Any) → List[langchain.schema.BaseMessage][source]#\\nTo a BaseMessage.\\nproperty input_variables: List[str]#\\nInput variables for this prompt template.\\nlangchain.prompts.Prompt#\\nalias of langchain.prompts.prompt.PromptTemplate\\npydantic model langchain.prompts.PromptTemplate[source]#\\nSchema to represent a prompt for an LLM.\\nExample\\nfrom langchain import PromptTemplate\\nprompt = PromptTemplate(input_variables=[\"foo\"], template=\"Say {foo}\")\\nfield input_variables: List[str] [Required]#\\nA list of the names of the variables the prompt template expects.\\nfield template: str [Required]#\\nThe prompt template.\\nfield template_format: str = \\'f-string\\'#\\nThe format of the prompt template. Options are: ‘f-string’, ‘jinja2’.\\nfield validate_template: bool = True#\\nWhether or not to try validating the template.\\nformat(**kwargs: Any) → str[source]#\\nFormat the prompt with the inputs.\\nParameters\\nkwargs – Any arguments to be passed to the prompt template.\\nReturns\\nA formatted string.\\nExample:\\nprompt.format(variable1=\"foo\")\\nclassmethod from_examples(examples: List[str], suffix: str, input_variables: List[str], example_separator: str = \\'\\\\n\\\\n\\', prefix: str = \\'\\', **kwargs: Any) → langchain.prompts.prompt.PromptTemplate[source]#\\nTake examples in list format with prefix and suffix to create a prompt.\\nIntended to be used as a way to dynamically create a prompt from examples.\\nParameters\\nexamples – List of examples to use in the prompt.\\nsuffix – String to go after the list of examples. Should generally\\nset up the user’s input.\\ninput_variables – A list of variable names the final prompt template\\nwill expect.\\nexample_separator – The separator to use in between examples. Defaults\\nto two new line characters.\\nprefix – String that should go before any examples. Generally includes\\nexamples. Default to an empty string.\\nReturns\\nThe final prompt generated.\\nclassmethod from_file(template_file: Union[str, pathlib.Path], input_variables: List[str], **kwargs: Any) → langchain.prompts.prompt.PromptTemplate[source]#\\nLoad a prompt from a file.\\nParameters\\ntemplate_file – The path to the file containing the prompt template.\\ninput_variables – A list of variable names the final prompt template\\nwill expect.\\nReturns\\nThe prompt loaded from the file.\\nclassmethod from_template(template: str, **kwargs: Any) → langchain.prompts.prompt.PromptTemplate[source]#\\nLoad a prompt template from a template.\\npydantic model langchain.prompts.StringPromptTemplate[source]#\\nString prompt should expose the format method, returning a prompt.\\nformat_prompt(**kwargs: Any) → langchain.schema.PromptValue[source]#\\nCreate Chat Messages.\\nlangchain.prompts.load_prompt(path: Union[str, pathlib.Path]) → langchain.prompts.base.BasePromptTemplate[source]#\\nUnified method for loading a prompt from LangChainHub or local fs.\\nprevious\\nPrompts\\nnext\\nExample Selector\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\reference\\\\modules\\\\prompt.html'}),\n",
              " Document(page_content=\".rst\\n.pdf\\nPython REPL\\nPython REPL#\\nFor backwards compatibility.\\npydantic model langchain.python.PythonREPL[source]#\\nSimulates a standalone Python REPL.\\nfield globals: Optional[Dict] [Optional] (alias '_globals')#\\nfield locals: Optional[Dict] [Optional] (alias '_locals')#\\nrun(command: str) → str[source]#\\nRun command with own globals/locals and returns anything printed.\\nprevious\\nUtilities\\nnext\\nSerpAPI\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  \", metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\reference\\\\modules\\\\python.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nSearxNG Search\\n Contents \\nQuick Start\\nSearching\\nEngine Parameters\\nSearch Tips\\nSearxNG Search#\\nUtility for using SearxNG meta search API.\\nSearxNG is a privacy-friendly free metasearch engine that aggregates results from\\nmultiple search engines and databases and\\nsupports the OpenSearch\\nspecification.\\nMore detailes on the installtion instructions here.\\nFor the search API refer to https://docs.searxng.org/dev/search_api.html\\nQuick Start#\\nIn order to use this utility you need to provide the searx host. This can be done\\nby passing the named parameter searx_host\\nor exporting the environment variable SEARX_HOST.\\nNote: this is the only required parameter.\\nThen create a searx search instance like this:\\nfrom langchain.utilities import SearxSearchWrapper\\n# when the host starts with `http` SSL is disabled and the connection\\n# is assumed to be on a private network\\nsearx_host=\\'http://self.hosted\\'\\nsearch = SearxSearchWrapper(searx_host=searx_host)\\nYou can now use the search instance to query the searx API.\\nSearching#\\nUse the run() and\\nresults() methods to query the searx API.\\nOther methods are are available for convenience.\\nSearxResults is a convenience wrapper around the raw json result.\\nExample usage of the run method to make a search:\\ns.run(query=\"what is the best search engine?\")\\nEngine Parameters#\\nYou can pass any accepted searx search API parameters to the\\nSearxSearchWrapper instance.\\nIn the following example we are using the\\nengines and the language parameters:\\n# assuming the searx host is set as above or exported as an env variable\\ns = SearxSearchWrapper(engines=[\\'google\\', \\'bing\\'],\\n                    language=\\'es\\')\\nSearch Tips#\\nSearx offers a special\\nsearch syntax\\nthat can also be used instead of passing engine parameters.\\nFor example the following query:\\ns = SearxSearchWrapper(\"langchain library\", engines=[\\'github\\'])\\n# can also be written as:\\ns = SearxSearchWrapper(\"langchain library !github\")\\n# or even:\\ns = SearxSearchWrapper(\"langchain library !gh\")\\nIn some situations you might want to pass an extra string to the search query.\\nFor example when the run() method is called by an agent. The search suffix can\\nalso be used as a way to pass extra parameters to searx or the underlying search\\nengines.\\n# select the github engine and pass the search suffix\\ns = SearchWrapper(\"langchain library\", query_suffix=\"!gh\")\\ns = SearchWrapper(\"langchain library\")\\n# select github the conventional google search syntax\\ns.run(\"large language models\", query_suffix=\"site:github.com\")\\nNOTE: A search suffix can be defined on both the instance and the method level.\\nThe resulting query will be the concatenation of the two with the former taking\\nprecedence.\\nSee SearxNG Configured Engines and\\nSearxNG Search Syntax\\nfor more details.\\nNotes\\nThis wrapper is based on the SearxNG fork searxng/searxng which is\\nbetter maintained than the original Searx project and offers more features.\\nPublic searxNG instances often use a rate limiter for API usage, so you might want to\\nuse a self hosted instance and disable the rate limiter.\\nIf you are self-hosting an instance you can customize the rate limiter for your\\nown network as described here.\\nFor a list of public SearxNG instances see https://searx.space/\\nclass langchain.utilities.searx_search.SearxResults(data: str)[source]#\\nDict like wrapper around search api results.\\nproperty answers: Any#\\nHelper accessor on the json result.\\npydantic model langchain.utilities.searx_search.SearxSearchWrapper[source]#\\nWrapper for Searx API.\\nTo use you need to provide the searx host by passing the named parameter\\nsearx_host or exporting the environment variable SEARX_HOST.\\nIn some situations you might want to disable SSL verification, for example\\nif you are running searx locally. You can do this by passing the named parameter\\nunsecure. You can also pass the host url scheme as http to disable SSL.\\nExample\\nfrom langchain.utilities import SearxSearchWrapper\\nsearx = SearxSearchWrapper(searx_host=\"http://localhost:8888\")\\nExample with SSL disabled:from langchain.utilities import SearxSearchWrapper\\n# note the unsecure parameter is not needed if you pass the url scheme as\\n# http\\nsearx = SearxSearchWrapper(searx_host=\"http://localhost:8888\",\\n                                        unsecure=True)\\nValidators\\ndisable_ssl_warnings » unsecure\\nvalidate_params » all fields\\nfield aiosession: Optional[Any] = None#\\nfield categories: Optional[List[str]] = []#\\nfield engines: Optional[List[str]] = []#\\nfield headers: Optional[dict] = None#\\nfield k: int = 10#\\nfield params: dict [Optional]#\\nfield query_suffix: Optional[str] = \\'\\'#\\nfield searx_host: str = \\'\\'#\\nfield unsecure: bool = False#\\nasync aresults(query: str, num_results: int, engines: Optional[List[str]] = None, query_suffix: Optional[str] = \\'\\', **kwargs: Any) → List[Dict][source]#\\nAsynchronously query with json results.\\nUses aiohttp. See results for more info.\\nasync arun(query: str, engines: Optional[List[str]] = None, query_suffix: Optional[str] = \\'\\', **kwargs: Any) → str[source]#\\nAsynchronously version of run.\\nresults(query: str, num_results: int, engines: Optional[List[str]] = None, categories: Optional[List[str]] = None, query_suffix: Optional[str] = \\'\\', **kwargs: Any) → List[Dict][source]#\\nRun query through Searx API and returns the results with metadata.\\nParameters\\nquery – The query to search for.\\nquery_suffix – Extra suffix appended to the query.\\nnum_results – Limit the number of results to return.\\nengines – List of engines to use for the query.\\ncategories – List of categories to use for the query.\\n**kwargs – extra parameters to pass to the searx API.\\nReturns\\n{snippet:  The description of the result.\\ntitle:  The title of the result.\\nlink: The link to the result.\\nengines: The engines used for the result.\\ncategory: Searx category of the result.\\n}\\nReturn type\\nDict with the following keys\\nrun(query: str, engines: Optional[List[str]] = None, categories: Optional[List[str]] = None, query_suffix: Optional[str] = \\'\\', **kwargs: Any) → str[source]#\\nRun query through Searx API and parse results.\\nYou can pass any other params to the searx query API.\\nParameters\\nquery – The query to search for.\\nquery_suffix – Extra suffix appended to the query.\\nengines – List of engines to use for the query.\\ncategories – List of categories to use for the query.\\n**kwargs – extra parameters to pass to the searx API.\\nReturns\\nThe result of the query.\\nReturn type\\nstr\\nRaises\\nValueError – If an error occured with the query.\\nExample\\nThis will make a query to the qwant engine:\\nfrom langchain.utilities import SearxSearchWrapper\\nsearx = SearxSearchWrapper(searx_host=\"http://my.searx.host\")\\nsearx.run(\"what is the weather in France ?\", engine=\"qwant\")\\n# the same result can be achieved using the `!` syntax of searx\\n# to select the engine using `query_suffix`\\nsearx.run(\"what is the weather in France ?\", query_suffix=\"!qwant\")\\nprevious\\nSerpAPI\\nnext\\nDocstore\\n Contents\\n  \\nQuick Start\\nSearching\\nEngine Parameters\\nSearch Tips\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\reference\\\\modules\\\\searx_search.html'}),\n",
              " Document(page_content=\".rst\\n.pdf\\nSerpAPI\\nSerpAPI#\\nFor backwards compatiblity.\\npydantic model langchain.serpapi.SerpAPIWrapper[source]#\\nWrapper around SerpAPI.\\nTo use, you should have the google-search-results python package installed,\\nand the environment variable SERPAPI_API_KEY set with your API key, or pass\\nserpapi_api_key as a named parameter to the constructor.\\nExample\\nfrom langchain import SerpAPIWrapper\\nserpapi = SerpAPIWrapper()\\nfield aiosession: Optional[aiohttp.client.ClientSession] = None#\\nfield params: dict = {'engine': 'google', 'gl': 'us', 'google_domain': 'google.com', 'hl': 'en'}#\\nfield serpapi_api_key: Optional[str] = None#\\nasync arun(query: str) → str[source]#\\nUse aiohttp to run query through SerpAPI and parse result.\\nget_params(query: str) → Dict[str, str][source]#\\nGet parameters for SerpAPI.\\nresults(query: str) → dict[source]#\\nRun query through SerpAPI and return the raw result.\\nrun(query: str) → str[source]#\\nRun query through SerpAPI and parse result.\\nprevious\\nPython REPL\\nnext\\nSearxNG Search\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  \", metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\reference\\\\modules\\\\serpapi.html'}),\n",
              " Document(page_content=\".rst\\n.pdf\\nText Splitter\\nText Splitter#\\nFunctionality for splitting text.\\nclass langchain.text_splitter.CharacterTextSplitter(separator: str = '\\\\n\\\\n', **kwargs: Any)[source]#\\nImplementation of splitting text that looks at characters.\\nsplit_text(text: str) → List[str][source]#\\nSplit incoming text and return chunks.\\nclass langchain.text_splitter.LatexTextSplitter(**kwargs: Any)[source]#\\nAttempts to split the text along Latex-formatted layout elements.\\nclass langchain.text_splitter.MarkdownTextSplitter(**kwargs: Any)[source]#\\nAttempts to split the text along Markdown-formatted headings.\\nclass langchain.text_splitter.NLTKTextSplitter(separator: str = '\\\\n\\\\n', **kwargs: Any)[source]#\\nImplementation of splitting text that looks at sentences using NLTK.\\nsplit_text(text: str) → List[str][source]#\\nSplit incoming text and return chunks.\\nclass langchain.text_splitter.PythonCodeTextSplitter(**kwargs: Any)[source]#\\nAttempts to split the text along Python syntax.\\nclass langchain.text_splitter.RecursiveCharacterTextSplitter(separators: Optional[List[str]] = None, **kwargs: Any)[source]#\\nImplementation of splitting text that looks at characters.\\nRecursively tries to split by different characters to find one\\nthat works.\\nsplit_text(text: str) → List[str][source]#\\nSplit incoming text and return chunks.\\nclass langchain.text_splitter.SpacyTextSplitter(separator: str = '\\\\n\\\\n', pipeline: str = 'en_core_web_sm', **kwargs: Any)[source]#\\nImplementation of splitting text that looks at sentences using Spacy.\\nsplit_text(text: str) → List[str][source]#\\nSplit incoming text and return chunks.\\nclass langchain.text_splitter.TextSplitter(chunk_size: int = 4000, chunk_overlap: int = 200, length_function: typing.Callable[[str], int] = <built-in function len>)[source]#\\nInterface for splitting text into chunks.\\nasync atransform_documents(documents: Sequence[langchain.schema.Document], **kwargs: Any) → Sequence[langchain.schema.Document][source]#\\nAsynchronously transform a sequence of documents by splitting them.\\ncreate_documents(texts: List[str], metadatas: Optional[List[dict]] = None) → List[langchain.schema.Document][source]#\\nCreate documents from a list of texts.\\nclassmethod from_huggingface_tokenizer(tokenizer: Any, **kwargs: Any) → langchain.text_splitter.TextSplitter[source]#\\nText splitter that uses HuggingFace tokenizer to count length.\\nclassmethod from_tiktoken_encoder(encoding_name: str = 'gpt2', model_name: Optional[str] = None, allowed_special: Union[Literal['all'], AbstractSet[str]] = {}, disallowed_special: Union[Literal['all'], Collection[str]] = 'all', **kwargs: Any) → langchain.text_splitter.TextSplitter[source]#\\nText splitter that uses tiktoken encoder to count length.\\nsplit_documents(documents: List[langchain.schema.Document]) → List[langchain.schema.Document][source]#\\nSplit documents.\\nabstract split_text(text: str) → List[str][source]#\\nSplit text into multiple components.\\ntransform_documents(documents: Sequence[langchain.schema.Document], **kwargs: Any) → Sequence[langchain.schema.Document][source]#\\nTransform sequence of documents by splitting them.\\nclass langchain.text_splitter.TokenTextSplitter(encoding_name: str = 'gpt2', model_name: Optional[str] = None, allowed_special: Union[Literal['all'], AbstractSet[str]] = {}, disallowed_special: Union[Literal['all'], Collection[str]] = 'all', **kwargs: Any)[source]#\\nImplementation of splitting text that looks at tokens.\\nsplit_text(text: str) → List[str][source]#\\nSplit incoming text and return chunks.\\nprevious\\nDocstore\\nnext\\nEmbeddings\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  \", metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\reference\\\\modules\\\\text_splitter.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nVectorStores\\nVectorStores#\\nWrappers on top of vector stores.\\nclass langchain.vectorstores.Annoy(embedding_function: Callable, index: Any, metric: str, docstore: langchain.docstore.base.Docstore, index_to_docstore_id: Dict[int, str])[source]#\\nWrapper around Annoy vector database.\\nTo use, you should have the annoy python package installed.\\nExample\\nfrom langchain import Annoy\\ndb = Annoy(embedding_function, index, docstore, index_to_docstore_id)\\nadd_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, **kwargs: Any) → List[str][source]#\\nRun more texts through the embeddings and add to the vectorstore.\\nParameters\\ntexts – Iterable of strings to add to the vectorstore.\\nmetadatas – Optional list of metadatas associated with the texts.\\nkwargs – vectorstore specific parameters\\nReturns\\nList of ids from adding the texts into the vectorstore.\\nclassmethod from_embeddings(text_embeddings: List[Tuple[str, List[float]]], embedding: langchain.embeddings.base.Embeddings, metadatas: Optional[List[dict]] = None, metric: str = \\'angular\\', trees: int = 100, n_jobs: int = - 1, **kwargs: Any) → langchain.vectorstores.annoy.Annoy[source]#\\nConstruct Annoy wrapper from embeddings.\\nParameters\\ntext_embeddings – List of tuples of (text, embedding)\\nembedding – Embedding function to use.\\nmetadatas – List of metadata dictionaries to associate with documents.\\nmetric – Metric to use for indexing. Defaults to “angular”.\\ntrees – Number of trees to use for indexing. Defaults to 100.\\nn_jobs – Number of jobs to use for indexing. Defaults to -1\\nThis is a user friendly interface that:\\nCreates an in memory docstore with provided embeddings\\nInitializes the Annoy database\\nThis is intended to be a quick way to get started.\\nExample\\nfrom langchain import Annoy\\nfrom langchain.embeddings import OpenAIEmbeddings\\nembeddings = OpenAIEmbeddings()\\ntext_embeddings = embeddings.embed_documents(texts)\\ntext_embedding_pairs = list(zip(texts, text_embeddings))\\ndb = Annoy.from_embeddings(text_embedding_pairs, embeddings)\\nclassmethod from_texts(texts: List[str], embedding: langchain.embeddings.base.Embeddings, metadatas: Optional[List[dict]] = None, metric: str = \\'angular\\', trees: int = 100, n_jobs: int = - 1, **kwargs: Any) → langchain.vectorstores.annoy.Annoy[source]#\\nConstruct Annoy wrapper from raw documents.\\nParameters\\ntexts – List of documents to index.\\nembedding – Embedding function to use.\\nmetadatas – List of metadata dictionaries to associate with documents.\\nmetric – Metric to use for indexing. Defaults to “angular”.\\ntrees – Number of trees to use for indexing. Defaults to 100.\\nn_jobs – Number of jobs to use for indexing. Defaults to -1.\\nThis is a user friendly interface that:\\nEmbeds documents.\\nCreates an in memory docstore\\nInitializes the Annoy database\\nThis is intended to be a quick way to get started.\\nExample\\nfrom langchain import Annoy\\nfrom langchain.embeddings import OpenAIEmbeddings\\nembeddings = OpenAIEmbeddings()\\nindex = Annoy.from_texts(texts, embeddings)\\nclassmethod load_local(folder_path: str, embeddings: langchain.embeddings.base.Embeddings) → langchain.vectorstores.annoy.Annoy[source]#\\nLoad Annoy index, docstore, and index_to_docstore_id to disk.\\nParameters\\nfolder_path – folder path to load index, docstore,\\nand index_to_docstore_id from.\\nembeddings – Embeddings to use when generating queries.\\nmax_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs selected using the maximal marginal relevance.\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nfetch_k – Number of Documents to fetch to pass to MMR algorithm.\\nReturns\\nList of Documents selected by maximal marginal relevance.\\nmax_marginal_relevance_search_by_vector(embedding: List[float], k: int = 4, fetch_k: int = 20, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs selected using the maximal marginal relevance.\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\nParameters\\nembedding – Embedding to look up documents similar to.\\nfetch_k – Number of Documents to fetch to pass to MMR algorithm.\\nk – Number of Documents to return. Defaults to 4.\\nReturns\\nList of Documents selected by maximal marginal relevance.\\nprocess_index_results(idxs: List[int], dists: List[float]) → List[Tuple[langchain.schema.Document, float]][source]#\\nTurns annoy results into a list of documents and scores.\\nParameters\\nidxs – List of indices of the documents in the index.\\ndists – List of distances of the documents in the index.\\nReturns\\nList of Documents and scores.\\nsave_local(folder_path: str, prefault: bool = False) → None[source]#\\nSave Annoy index, docstore, and index_to_docstore_id to disk.\\nParameters\\nfolder_path – folder path to save index, docstore,\\nand index_to_docstore_id to.\\nprefault – Whether to pre-load the index into memory.\\nsimilarity_search(query: str, k: int = 4, search_k: int = - 1, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs most similar to query.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nsearch_k – inspect up to search_k nodes which defaults\\nto n_trees * n if not provided\\nReturns\\nList of Documents most similar to the query.\\nsimilarity_search_by_index(docstore_index: int, k: int = 4, search_k: int = - 1, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs most similar to docstore_index.\\nParameters\\ndocstore_index – Index of document in docstore\\nk – Number of Documents to return. Defaults to 4.\\nsearch_k – inspect up to search_k nodes which defaults\\nto n_trees * n if not provided\\nReturns\\nList of Documents most similar to the embedding.\\nsimilarity_search_by_vector(embedding: List[float], k: int = 4, search_k: int = - 1, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs most similar to embedding vector.\\nParameters\\nembedding – Embedding to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nsearch_k – inspect up to search_k nodes which defaults\\nto n_trees * n if not provided\\nReturns\\nList of Documents most similar to the embedding.\\nsimilarity_search_with_score(query: str, k: int = 4, search_k: int = - 1) → List[Tuple[langchain.schema.Document, float]][source]#\\nReturn docs most similar to query.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nsearch_k – inspect up to search_k nodes which defaults\\nto n_trees * n if not provided\\nReturns\\nList of Documents most similar to the query and score for each\\nsimilarity_search_with_score_by_index(docstore_index: int, k: int = 4, search_k: int = - 1) → List[Tuple[langchain.schema.Document, float]][source]#\\nReturn docs most similar to query.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nsearch_k – inspect up to search_k nodes which defaults\\nto n_trees * n if not provided\\nReturns\\nList of Documents most similar to the query and score for each\\nsimilarity_search_with_score_by_vector(embedding: List[float], k: int = 4, search_k: int = - 1) → List[Tuple[langchain.schema.Document, float]][source]#\\nReturn docs most similar to query.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nsearch_k – inspect up to search_k nodes which defaults\\nto n_trees * n if not provided\\nReturns\\nList of Documents most similar to the query and score for each\\nclass langchain.vectorstores.AtlasDB(name: str, embedding_function: Optional[langchain.embeddings.base.Embeddings] = None, api_key: Optional[str] = None, description: str = \\'A description for your project\\', is_public: bool = True, reset_project_if_exists: bool = False)[source]#\\nWrapper around Atlas: Nomic’s neural database and rhizomatic instrument.\\nTo use, you should have the nomic python package installed.\\nExample\\nfrom langchain.vectorstores import AtlasDB\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nembeddings = OpenAIEmbeddings()\\nvectorstore = AtlasDB(\"my_project\", embeddings.embed_query)\\nadd_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, ids: Optional[List[str]] = None, refresh: bool = True, **kwargs: Any) → List[str][source]#\\nRun more texts through the embeddings and add to the vectorstore.\\nParameters\\ntexts (Iterable[str]) – Texts to add to the vectorstore.\\nmetadatas (Optional[List[dict]], optional) – Optional list of metadatas.\\nids (Optional[List[str]]) – An optional list of ids.\\nrefresh (bool) – Whether or not to refresh indices with the updated data.\\nDefault True.\\nReturns\\nList of IDs of the added texts.\\nReturn type\\nList[str]\\ncreate_index(**kwargs: Any) → Any[source]#\\nCreates an index in your project.\\nSee\\nhttps://docs.nomic.ai/atlas_api.html#nomic.project.AtlasProject.create_index\\nfor full detail.\\nclassmethod from_documents(documents: List[langchain.schema.Document], embedding: Optional[langchain.embeddings.base.Embeddings] = None, ids: Optional[List[str]] = None, name: Optional[str] = None, api_key: Optional[str] = None, persist_directory: Optional[str] = None, description: str = \\'A description for your project\\', is_public: bool = True, reset_project_if_exists: bool = False, index_kwargs: Optional[dict] = None, **kwargs: Any) → langchain.vectorstores.atlas.AtlasDB[source]#\\nCreate an AtlasDB vectorstore from a list of documents.\\nParameters\\nname (str) – Name of the collection to create.\\napi_key (str) – Your nomic API key,\\ndocuments (List[Document]) – List of documents to add to the vectorstore.\\nembedding (Optional[Embeddings]) – Embedding function. Defaults to None.\\nids (Optional[List[str]]) – Optional list of document IDs. If None,\\nids will be auto created\\ndescription (str) – A description for your project.\\nis_public (bool) – Whether your project is publicly accessible.\\nTrue by default.\\nreset_project_if_exists (bool) – Whether to reset this project if\\nit already exists. Default False.\\nGenerally userful during development and testing.\\nindex_kwargs (Optional[dict]) – Dict of kwargs for index creation.\\nSee https://docs.nomic.ai/atlas_api.html\\nReturns\\nNomic’s neural database and finest rhizomatic instrument\\nReturn type\\nAtlasDB\\nclassmethod from_texts(texts: List[str], embedding: Optional[langchain.embeddings.base.Embeddings] = None, metadatas: Optional[List[dict]] = None, ids: Optional[List[str]] = None, name: Optional[str] = None, api_key: Optional[str] = None, description: str = \\'A description for your project\\', is_public: bool = True, reset_project_if_exists: bool = False, index_kwargs: Optional[dict] = None, **kwargs: Any) → langchain.vectorstores.atlas.AtlasDB[source]#\\nCreate an AtlasDB vectorstore from a raw documents.\\nParameters\\ntexts (List[str]) – The list of texts to ingest.\\nname (str) – Name of the project to create.\\napi_key (str) – Your nomic API key,\\nembedding (Optional[Embeddings]) – Embedding function. Defaults to None.\\nmetadatas (Optional[List[dict]]) – List of metadatas. Defaults to None.\\nids (Optional[List[str]]) – Optional list of document IDs. If None,\\nids will be auto created\\ndescription (str) – A description for your project.\\nis_public (bool) – Whether your project is publicly accessible.\\nTrue by default.\\nreset_project_if_exists (bool) – Whether to reset this project if it\\nalready exists. Default False.\\nGenerally userful during development and testing.\\nindex_kwargs (Optional[dict]) – Dict of kwargs for index creation.\\nSee https://docs.nomic.ai/atlas_api.html\\nReturns\\nNomic’s neural database and finest rhizomatic instrument\\nReturn type\\nAtlasDB\\nsimilarity_search(query: str, k: int = 4, **kwargs: Any) → List[langchain.schema.Document][source]#\\nRun similarity search with AtlasDB\\nParameters\\nquery (str) – Query text to search for.\\nk (int) – Number of results to return. Defaults to 4.\\nReturns\\nList of documents most similar to the query text.\\nReturn type\\nList[Document]\\nclass langchain.vectorstores.Chroma(collection_name: str = \\'langchain\\', embedding_function: Optional[Embeddings] = None, persist_directory: Optional[str] = None, client_settings: Optional[chromadb.config.Settings] = None, collection_metadata: Optional[Dict] = None, client: Optional[chromadb.Client] = None)[source]#\\nWrapper around ChromaDB embeddings platform.\\nTo use, you should have the chromadb python package installed.\\nExample\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nembeddings = OpenAIEmbeddings()\\nvectorstore = Chroma(\"langchain_store\", embeddings.embed_query)\\nadd_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, ids: Optional[List[str]] = None, **kwargs: Any) → List[str][source]#\\nRun more texts through the embeddings and add to the vectorstore.\\nParameters\\ntexts (Iterable[str]) – Texts to add to the vectorstore.\\nmetadatas (Optional[List[dict]], optional) – Optional list of metadatas.\\nids (Optional[List[str]], optional) – Optional list of IDs.\\nReturns\\nList of IDs of the added texts.\\nReturn type\\nList[str]\\ndelete_collection() → None[source]#\\nDelete the collection.\\nclassmethod from_documents(documents: List[Document], embedding: Optional[Embeddings] = None, ids: Optional[List[str]] = None, collection_name: str = \\'langchain\\', persist_directory: Optional[str] = None, client_settings: Optional[chromadb.config.Settings] = None, client: Optional[chromadb.Client] = None, **kwargs: Any) → Chroma[source]#\\nCreate a Chroma vectorstore from a list of documents.\\nIf a persist_directory is specified, the collection will be persisted there.\\nOtherwise, the data will be ephemeral in-memory.\\nParameters\\ncollection_name (str) – Name of the collection to create.\\npersist_directory (Optional[str]) – Directory to persist the collection.\\nids (Optional[List[str]]) – List of document IDs. Defaults to None.\\ndocuments (List[Document]) – List of documents to add to the vectorstore.\\nembedding (Optional[Embeddings]) – Embedding function. Defaults to None.\\nclient_settings (Optional[chromadb.config.Settings]) – Chroma client settings\\nReturns\\nChroma vectorstore.\\nReturn type\\nChroma\\nclassmethod from_texts(texts: List[str], embedding: Optional[Embeddings] = None, metadatas: Optional[List[dict]] = None, ids: Optional[List[str]] = None, collection_name: str = \\'langchain\\', persist_directory: Optional[str] = None, client_settings: Optional[chromadb.config.Settings] = None, client: Optional[chromadb.Client] = None, **kwargs: Any) → Chroma[source]#\\nCreate a Chroma vectorstore from a raw documents.\\nIf a persist_directory is specified, the collection will be persisted there.\\nOtherwise, the data will be ephemeral in-memory.\\nParameters\\ntexts (List[str]) – List of texts to add to the collection.\\ncollection_name (str) – Name of the collection to create.\\npersist_directory (Optional[str]) – Directory to persist the collection.\\nembedding (Optional[Embeddings]) – Embedding function. Defaults to None.\\nmetadatas (Optional[List[dict]]) – List of metadatas. Defaults to None.\\nids (Optional[List[str]]) – List of document IDs. Defaults to None.\\nclient_settings (Optional[chromadb.config.Settings]) – Chroma client settings\\nReturns\\nChroma vectorstore.\\nReturn type\\nChroma\\nmax_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, filter: Optional[Dict[str, str]] = None, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs selected using the maximal marginal relevance.\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\n:param query: Text to look up documents similar to.\\n:param k: Number of Documents to return. Defaults to 4.\\n:param fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n:param filter: Filter by metadata. Defaults to None.\\n:type filter: Optional[Dict[str, str]]\\nReturns\\nList of Documents selected by maximal marginal relevance.\\nmax_marginal_relevance_search_by_vector(embedding: List[float], k: int = 4, fetch_k: int = 20, filter: Optional[Dict[str, str]] = None, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs selected using the maximal marginal relevance.\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\n:param embedding: Embedding to look up documents similar to.\\n:param k: Number of Documents to return. Defaults to 4.\\n:param fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n:param filter: Filter by metadata. Defaults to None.\\n:type filter: Optional[Dict[str, str]]\\nReturns\\nList of Documents selected by maximal marginal relevance.\\npersist() → None[source]#\\nPersist the collection.\\nThis can be used to explicitly persist the data to disk.\\nIt will also be called automatically when the object is destroyed.\\nsimilarity_search(query: str, k: int = 4, filter: Optional[Dict[str, str]] = None, **kwargs: Any) → List[langchain.schema.Document][source]#\\nRun similarity search with Chroma.\\nParameters\\nquery (str) – Query text to search for.\\nk (int) – Number of results to return. Defaults to 4.\\nfilter (Optional[Dict[str, str]]) – Filter by metadata. Defaults to None.\\nReturns\\nList of documents most similar to the query text.\\nReturn type\\nList[Document]\\nsimilarity_search_by_vector(embedding: List[float], k: int = 4, filter: Optional[Dict[str, str]] = None, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs most similar to embedding vector.\\n:param embedding: Embedding to look up documents similar to.\\n:param k: Number of Documents to return. Defaults to 4.\\nReturns\\nList of Documents most similar to the query vector.\\nsimilarity_search_with_score(query: str, k: int = 4, filter: Optional[Dict[str, str]] = None, **kwargs: Any) → List[Tuple[langchain.schema.Document, float]][source]#\\nRun similarity search with Chroma with distance.\\nParameters\\nquery (str) – Query text to search for.\\nk (int) – Number of results to return. Defaults to 4.\\nfilter (Optional[Dict[str, str]]) – Filter by metadata. Defaults to None.\\nReturns\\nList of documents most similar to the querytext with distance in float.\\nReturn type\\nList[Tuple[Document, float]]\\nclass langchain.vectorstores.DeepLake(dataset_path: str = \\'mem://langchain\\', token: Optional[str] = None, embedding_function: Optional[langchain.embeddings.base.Embeddings] = None, read_only: Optional[bool] = False, ingestion_batch_size: int = 1024, num_workers: int = 4, **kwargs: Any)[source]#\\nWrapper around Deep Lake, a data lake for deep learning applications.\\nWe implement naive similarity search and filtering for fast prototyping,\\nbut it can be extended with Tensor Query Language (TQL) for production use cases\\nover billion rows.\\nWhy Deep Lake?\\nNot only stores embeddings, but also the original data with version control.\\nServerless, doesn’t require another service and can be used with majorcloud providers (S3, GCS, etc.)\\nMore than just a multi-modal vector store. You can use the datasetto fine-tune your own LLM models.\\nTo use, you should have the deeplake python package installed.\\nExample\\nfrom langchain.vectorstores import DeepLake\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nembeddings = OpenAIEmbeddings()\\nvectorstore = DeepLake(\"langchain_store\", embeddings.embed_query)\\nadd_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, ids: Optional[List[str]] = None, **kwargs: Any) → List[str][source]#\\nRun more texts through the embeddings and add to the vectorstore.\\nParameters\\ntexts (Iterable[str]) – Texts to add to the vectorstore.\\nmetadatas (Optional[List[dict]], optional) – Optional list of metadatas.\\nids (Optional[List[str]], optional) – Optional list of IDs.\\nReturns\\nList of IDs of the added texts.\\nReturn type\\nList[str]\\ndelete(ids: Any[List[str], None] = None, filter: Any[Dict[str, str], None] = None, delete_all: Any[bool, None] = None) → bool[source]#\\nDelete the entities in the dataset\\nParameters\\nids (Optional[List[str]], optional) – The document_ids to delete.\\nDefaults to None.\\nfilter (Optional[Dict[str, str]], optional) – The filter to delete by.\\nDefaults to None.\\ndelete_all (Optional[bool], optional) – Whether to drop the dataset.\\nDefaults to None.\\ndelete_dataset() → None[source]#\\nDelete the collection.\\nclassmethod from_texts(texts: List[str], embedding: Optional[langchain.embeddings.base.Embeddings] = None, metadatas: Optional[List[dict]] = None, ids: Optional[List[str]] = None, dataset_path: str = \\'mem://langchain\\', **kwargs: Any) → langchain.vectorstores.deeplake.DeepLake[source]#\\nCreate a Deep Lake dataset from a raw documents.\\nIf a dataset_path is specified, the dataset will be persisted there.\\nOtherwise, the data will be ephemeral in-memory.\\nParameters\\npath (str, pathlib.Path) – \\nThe full path to the dataset. Can be:\\nDeep Lake cloud path of the form hub://username/dataset_name.To write to Deep Lake cloud datasets,\\nensure that you are logged in to Deep Lake\\n(use ‘activeloop login’ from command line)\\nAWS S3 path of the form s3://bucketname/path/to/dataset.Credentials are required in either the environment\\nGoogle Cloud Storage path of the form``gcs://bucketname/path/to/dataset``Credentials are required\\nin either the environment\\nLocal file system path of the form ./path/to/dataset or~/path/to/dataset or path/to/dataset.\\nIn-memory path of the form mem://path/to/dataset which doesn’tsave the dataset, but keeps it in memory instead.\\nShould be used only for testing as it does not persist.\\ndocuments (List[Document]) – List of documents to add.\\nembedding (Optional[Embeddings]) – Embedding function. Defaults to None.\\nmetadatas (Optional[List[dict]]) – List of metadatas. Defaults to None.\\nids (Optional[List[str]]) – List of document IDs. Defaults to None.\\nReturns\\nDeep Lake dataset.\\nReturn type\\nDeepLake\\nmax_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs selected using the maximal marginal relevance.\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\n:param query: Text to look up documents similar to.\\n:param k: Number of Documents to return. Defaults to 4.\\n:param fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\nReturns\\nList of Documents selected by maximal marginal relevance.\\nmax_marginal_relevance_search_by_vector(embedding: List[float], k: int = 4, fetch_k: int = 20, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs selected using the maximal marginal relevance.\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\n:param embedding: Embedding to look up documents similar to.\\n:param k: Number of Documents to return. Defaults to 4.\\n:param fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\nReturns\\nList of Documents selected by maximal marginal relevance.\\npersist() → None[source]#\\nPersist the collection.\\nsearch(query: Any[str, None] = None, embedding: Any[float, None] = None, k: int = 4, distance_metric: str = \\'L2\\', use_maximal_marginal_relevance: Optional[bool] = False, fetch_k: Optional[int] = 20, filter: Optional[Any[Dict[str, str], Callable, str]] = None, return_score: Optional[bool] = False, **kwargs: Any) → Any[List[Document], List[Tuple[Document, float]]][source]#\\nReturn docs most similar to query.\\nParameters\\nquery – Text to look up documents similar to.\\nembedding – Embedding function to use. Defaults to None.\\nk – Number of Documents to return. Defaults to 4.\\ndistance_metric – L2 for Euclidean, L1 for Nuclear,\\nmax L-infinity distance, cos for cosine similarity,\\n‘dot’ for dot product. Defaults to L2.\\nfilter – Attribute filter by metadata example {‘key’: ‘value’}. It can also\\nfilter] (take [Deep Lake) – \\n(https – //docs.deeplake.ai/en/latest/deeplake.core.dataset.html#deeplake.core.dataset.Dataset.filter)\\nDefaults to None.\\nmaximal_marginal_relevance – Whether to use maximal marginal relevance.\\nDefaults to False.\\nfetch_k – Number of Documents to fetch to pass to MMR algorithm.\\nDefaults to 20.\\nreturn_score – Whether to return the score. Defaults to False.\\nReturns\\nList of Documents selected by the specified distance metric,\\nif return_score True, return a tuple of (Document, score)\\nsimilarity_search(query: str, k: int = 4, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs most similar to query.\\nParameters\\nquery – text to embed and run the query on.\\nk – Number of Documents to return.\\nDefaults to 4.\\nquery – Text to look up documents similar to.\\nembedding – Embedding function to use.\\nDefaults to None.\\nk – Number of Documents to return.\\nDefaults to 4.\\ndistance_metric – L2 for Euclidean, L1 for Nuclear, max\\nL-infinity distance, cos for cosine similarity, ‘dot’ for dot product\\nDefaults to L2.\\nfilter – Attribute filter by metadata example {‘key’: ‘value’}.\\nDefaults to None.\\nmaximal_marginal_relevance – Whether to use maximal marginal relevance.\\nDefaults to False.\\nfetch_k – Number of Documents to fetch to pass to MMR algorithm.\\nDefaults to 20.\\nreturn_score – Whether to return the score. Defaults to False.\\nReturns\\nList of Documents most similar to the query vector.\\nsimilarity_search_by_vector(embedding: List[float], k: int = 4, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs most similar to embedding vector.\\nParameters\\nembedding – Embedding to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nReturns\\nList of Documents most similar to the query vector.\\nsimilarity_search_with_score(query: str, distance_metric: str = \\'L2\\', k: int = 4, filter: Optional[Dict[str, str]] = None) → List[Tuple[langchain.schema.Document, float]][source]#\\nRun similarity search with Deep Lake with distance returned.\\nParameters\\nquery (str) – Query text to search for.\\ndistance_metric – L2 for Euclidean, L1 for Nuclear, max L-infinity\\ndistance, cos for cosine similarity, ‘dot’ for dot product.\\nDefaults to L2.\\nk (int) – Number of results to return. Defaults to 4.\\nfilter (Optional[Dict[str, str]]) – Filter by metadata. Defaults to None.\\nReturns\\nList of documents most similar to the querytext with distance in float.\\nReturn type\\nList[Tuple[Document, float]]\\nclass langchain.vectorstores.ElasticVectorSearch(elasticsearch_url: str, index_name: str, embedding: langchain.embeddings.base.Embeddings)[source]#\\nWrapper around Elasticsearch as a vector database.\\nTo connect to an Elasticsearch instance that does not require\\nlogin credentials, pass the Elasticsearch URL and index name along with the\\nembedding object to the constructor.\\nExample\\nfrom langchain import ElasticVectorSearch\\nfrom langchain.embeddings import OpenAIEmbeddings\\nembedding = OpenAIEmbeddings()\\nelastic_vector_search = ElasticVectorSearch(\\n    elasticsearch_url=\"http://localhost:9200\",\\n    index_name=\"test_index\",\\n    embedding=embedding\\n)\\nTo connect to an Elasticsearch instance that requires login credentials,\\nincluding Elastic Cloud, use the Elasticsearch URL format\\nhttps://username:password@es_host:9243. For example, to connect to Elastic\\nCloud, create the Elasticsearch URL with the required authentication details and\\npass it to the ElasticVectorSearch constructor as the named parameter\\nelasticsearch_url.\\nYou can obtain your Elastic Cloud URL and login credentials by logging in to the\\nElastic Cloud console at https://cloud.elastic.co, selecting your deployment, and\\nnavigating to the “Deployments” page.\\nTo obtain your Elastic Cloud password for the default “elastic” user:\\nLog in to the Elastic Cloud console at https://cloud.elastic.co\\nGo to “Security” > “Users”\\nLocate the “elastic” user and click “Edit”\\nClick “Reset password”\\nFollow the prompts to reset the password\\nThe format for Elastic Cloud URLs is\\nhttps://username:password@cluster_id.region_id.gcp.cloud.es.io:9243.\\nExample\\nfrom langchain import ElasticVectorSearch\\nfrom langchain.embeddings import OpenAIEmbeddings\\nembedding = OpenAIEmbeddings()\\nelastic_host = \"cluster_id.region_id.gcp.cloud.es.io\"\\nelasticsearch_url = f\"https://username:password@{elastic_host}:9243\"\\nelastic_vector_search = ElasticVectorSearch(\\n    elasticsearch_url=elasticsearch_url,\\n    index_name=\"test_index\",\\n    embedding=embedding\\n)\\nParameters\\nelasticsearch_url (str) – The URL for the Elasticsearch instance.\\nindex_name (str) – The name of the Elasticsearch index for the embeddings.\\nembedding (Embeddings) – An object that provides the ability to embed text.\\nIt should be an instance of a class that subclasses the Embeddings\\nabstract base class, such as OpenAIEmbeddings()\\nRaises\\nValueError – If the elasticsearch python package is not installed.\\nadd_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, refresh_indices: bool = True, **kwargs: Any) → List[str][source]#\\nRun more texts through the embeddings and add to the vectorstore.\\nParameters\\ntexts – Iterable of strings to add to the vectorstore.\\nmetadatas – Optional list of metadatas associated with the texts.\\nrefresh_indices – bool to refresh ElasticSearch indices\\nReturns\\nList of ids from adding the texts into the vectorstore.\\nclassmethod from_texts(texts: List[str], embedding: langchain.embeddings.base.Embeddings, metadatas: Optional[List[dict]] = None, **kwargs: Any) → langchain.vectorstores.elastic_vector_search.ElasticVectorSearch[source]#\\nConstruct ElasticVectorSearch wrapper from raw documents.\\nThis is a user-friendly interface that:\\nEmbeds documents.\\nCreates a new index for the embeddings in the Elasticsearch instance.\\nAdds the documents to the newly created Elasticsearch index.\\nThis is intended to be a quick way to get started.\\nExample\\nfrom langchain import ElasticVectorSearch\\nfrom langchain.embeddings import OpenAIEmbeddings\\nembeddings = OpenAIEmbeddings()\\nelastic_vector_search = ElasticVectorSearch.from_texts(\\n    texts,\\n    embeddings,\\n    elasticsearch_url=\"http://localhost:9200\"\\n)\\nsimilarity_search(query: str, k: int = 4, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs most similar to query.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nReturns\\nList of Documents most similar to the query.\\nclass langchain.vectorstores.FAISS(embedding_function: typing.Callable, index: typing.Any, docstore: langchain.docstore.base.Docstore, index_to_docstore_id: typing.Dict[int, str], relevance_score_fn: typing.Optional[typing.Callable[[float], float]] = <function _default_relevance_score_fn>)[source]#\\nWrapper around FAISS vector database.\\nTo use, you should have the faiss python package installed.\\nExample\\nfrom langchain import FAISS\\nfaiss = FAISS(embedding_function, index, docstore, index_to_docstore_id)\\nadd_embeddings(text_embeddings: Iterable[Tuple[str, List[float]]], metadatas: Optional[List[dict]] = None, **kwargs: Any) → List[str][source]#\\nRun more texts through the embeddings and add to the vectorstore.\\nParameters\\ntext_embeddings – Iterable pairs of string and embedding to\\nadd to the vectorstore.\\nmetadatas – Optional list of metadatas associated with the texts.\\nReturns\\nList of ids from adding the texts into the vectorstore.\\nadd_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, **kwargs: Any) → List[str][source]#\\nRun more texts through the embeddings and add to the vectorstore.\\nParameters\\ntexts – Iterable of strings to add to the vectorstore.\\nmetadatas – Optional list of metadatas associated with the texts.\\nReturns\\nList of ids from adding the texts into the vectorstore.\\nclassmethod from_embeddings(text_embeddings: List[Tuple[str, List[float]]], embedding: langchain.embeddings.base.Embeddings, metadatas: Optional[List[dict]] = None, **kwargs: Any) → langchain.vectorstores.faiss.FAISS[source]#\\nConstruct FAISS wrapper from raw documents.\\nThis is a user friendly interface that:\\nEmbeds documents.\\nCreates an in memory docstore\\nInitializes the FAISS database\\nThis is intended to be a quick way to get started.\\nExample\\nfrom langchain import FAISS\\nfrom langchain.embeddings import OpenAIEmbeddings\\nembeddings = OpenAIEmbeddings()\\ntext_embeddings = embeddings.embed_documents(texts)\\ntext_embedding_pairs = list(zip(texts, text_embeddings))\\nfaiss = FAISS.from_embeddings(text_embedding_pairs, embeddings)\\nclassmethod from_texts(texts: List[str], embedding: langchain.embeddings.base.Embeddings, metadatas: Optional[List[dict]] = None, **kwargs: Any) → langchain.vectorstores.faiss.FAISS[source]#\\nConstruct FAISS wrapper from raw documents.\\nThis is a user friendly interface that:\\nEmbeds documents.\\nCreates an in memory docstore\\nInitializes the FAISS database\\nThis is intended to be a quick way to get started.\\nExample\\nfrom langchain import FAISS\\nfrom langchain.embeddings import OpenAIEmbeddings\\nembeddings = OpenAIEmbeddings()\\nfaiss = FAISS.from_texts(texts, embeddings)\\nclassmethod load_local(folder_path: str, embeddings: langchain.embeddings.base.Embeddings, index_name: str = \\'index\\') → langchain.vectorstores.faiss.FAISS[source]#\\nLoad FAISS index, docstore, and index_to_docstore_id to disk.\\nParameters\\nfolder_path – folder path to load index, docstore,\\nand index_to_docstore_id from.\\nembeddings – Embeddings to use when generating queries\\nindex_name – for saving with a specific index file name\\nmax_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs selected using the maximal marginal relevance.\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nfetch_k – Number of Documents to fetch to pass to MMR algorithm.\\nReturns\\nList of Documents selected by maximal marginal relevance.\\nmax_marginal_relevance_search_by_vector(embedding: List[float], k: int = 4, fetch_k: int = 20, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs selected using the maximal marginal relevance.\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\nParameters\\nembedding – Embedding to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nfetch_k – Number of Documents to fetch to pass to MMR algorithm.\\nReturns\\nList of Documents selected by maximal marginal relevance.\\nmerge_from(target: langchain.vectorstores.faiss.FAISS) → None[source]#\\nMerge another FAISS object with the current one.\\nAdd the target FAISS to the current one.\\nParameters\\ntarget – FAISS object you wish to merge into the current one\\nReturns\\nNone.\\nsave_local(folder_path: str, index_name: str = \\'index\\') → None[source]#\\nSave FAISS index, docstore, and index_to_docstore_id to disk.\\nParameters\\nfolder_path – folder path to save index, docstore,\\nand index_to_docstore_id to.\\nindex_name – for saving with a specific index file name\\nsimilarity_search(query: str, k: int = 4, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs most similar to query.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nReturns\\nList of Documents most similar to the query.\\nsimilarity_search_by_vector(embedding: List[float], k: int = 4, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs most similar to embedding vector.\\nParameters\\nembedding – Embedding to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nReturns\\nList of Documents most similar to the embedding.\\nsimilarity_search_with_score(query: str, k: int = 4) → List[Tuple[langchain.schema.Document, float]][source]#\\nReturn docs most similar to query.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nReturns\\nList of Documents most similar to the query and score for each\\nsimilarity_search_with_score_by_vector(embedding: List[float], k: int = 4) → List[Tuple[langchain.schema.Document, float]][source]#\\nReturn docs most similar to query.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nReturns\\nList of Documents most similar to the query and score for each\\nclass langchain.vectorstores.Milvus(embedding_function: langchain.embeddings.base.Embeddings, connection_args: dict, collection_name: str, text_field: str)[source]#\\nWrapper around the Milvus vector database.\\nadd_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, partition_name: Optional[str] = None, timeout: Optional[int] = None, **kwargs: Any) → List[str][source]#\\nInsert text data into Milvus.\\nWhen using add_texts() it is assumed that a collecton has already\\nbeen made and indexed. If metadata is included, it is assumed that\\nit is ordered correctly to match the schema provided to the Collection\\nand that the embedding vector is the first schema field.\\nParameters\\ntexts (Iterable[str]) – The text being embedded and inserted.\\nmetadatas (Optional[List[dict]], optional) – The metadata that\\ncorresponds to each insert. Defaults to None.\\npartition_name (str, optional) – The partition of the collection\\nto insert data into. Defaults to None.\\ntimeout – specified timeout.\\nReturns\\nThe resulting keys for each inserted element.\\nReturn type\\nList[str]\\nclassmethod from_texts(texts: List[str], embedding: langchain.embeddings.base.Embeddings, metadatas: Optional[List[dict]] = None, **kwargs: Any) → langchain.vectorstores.milvus.Milvus[source]#\\nCreate a Milvus collection, indexes it with HNSW, and insert data.\\nParameters\\ntexts (List[str]) – Text to insert.\\nembedding (Embeddings) – Embedding function to use.\\nmetadatas (Optional[List[dict]], optional) – Dict metatadata.\\nDefaults to None.\\nReturns\\nThe Milvus vector store.\\nReturn type\\nVectorStore\\nmax_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, param: Optional[dict] = None, expr: Optional[str] = None, partition_names: Optional[List[str]] = None, round_decimal: int = - 1, timeout: Optional[int] = None, **kwargs: Any) → List[langchain.schema.Document][source]#\\nPerform a search and return results that are reordered by MMR.\\nParameters\\nquery (str) – The text being searched.\\nk (int, optional) – How many results to give. Defaults to 4.\\nfetch_k (int, optional) – Total results to select k from.\\nDefaults to 20.\\nparam (dict, optional) – The search params for the specified index.\\nDefaults to None.\\nexpr (str, optional) – Filtering expression. Defaults to None.\\npartition_names (List[str], optional) – What partitions to search.\\nDefaults to None.\\nround_decimal (int, optional) – Round the resulting distance. Defaults\\nto -1.\\ntimeout (int, optional) – Amount to wait before timeout error. Defaults\\nto None.\\nReturns\\nDocument results for search.\\nReturn type\\nList[Document]\\nsimilarity_search(query: str, k: int = 4, param: Optional[dict] = None, expr: Optional[str] = None, partition_names: Optional[List[str]] = None, round_decimal: int = - 1, timeout: Optional[int] = None, **kwargs: Any) → List[langchain.schema.Document][source]#\\nPerform a similarity search against the query string.\\nParameters\\nquery (str) – The text to search.\\nk (int, optional) – How many results to return. Defaults to 4.\\nparam (dict, optional) – The search params for the index type.\\nDefaults to None.\\nexpr (str, optional) – Filtering expression. Defaults to None.\\npartition_names (List[str], optional) – What partitions to search.\\nDefaults to None.\\nround_decimal (int, optional) – What decimal point to round to.\\nDefaults to -1.\\ntimeout (int, optional) – How long to wait before timeout error.\\nDefaults to None.\\nReturns\\nDocument results for search.\\nReturn type\\nList[Document]\\nsimilarity_search_with_score(query: str, k: int = 4, param: Optional[dict] = None, expr: Optional[str] = None, partition_names: Optional[List[str]] = None, round_decimal: int = - 1, timeout: Optional[int] = None, **kwargs: Any) → List[Tuple[langchain.schema.Document, float]][source]#\\nPerform a search on a query string and return results.\\nParameters\\nquery (str) – The text being searched.\\nk (int, optional) – The amount of results ot return. Defaults to 4.\\nparam (dict, optional) – The search params for the specified index.\\nDefaults to None.\\nexpr (str, optional) – Filtering expression. Defaults to None.\\npartition_names (List[str], optional) – Partitions to search through.\\nDefaults to None.\\nround_decimal (int, optional) – Round the resulting distance. Defaults\\nto -1.\\ntimeout (int, optional) – Amount to wait before timeout error. Defaults\\nto None.\\nkwargs – Collection.search() keyword arguments.\\nReturns\\nsearch_embedding,(Document, distance, primary_field) results.\\nReturn type\\nList[float], List[Tuple[Document, any, any]]\\nclass langchain.vectorstores.OpenSearchVectorSearch(opensearch_url: str, index_name: str, embedding_function: langchain.embeddings.base.Embeddings, **kwargs: Any)[source]#\\nWrapper around OpenSearch as a vector database.\\nExample\\nfrom langchain import OpenSearchVectorSearch\\nopensearch_vector_search = OpenSearchVectorSearch(\\n    \"http://localhost:9200\",\\n    \"embeddings\",\\n    embedding_function\\n)\\nadd_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, bulk_size: int = 500, **kwargs: Any) → List[str][source]#\\nRun more texts through the embeddings and add to the vectorstore.\\nParameters\\ntexts – Iterable of strings to add to the vectorstore.\\nmetadatas – Optional list of metadatas associated with the texts.\\nbulk_size – Bulk API request count; Default: 500\\nReturns\\nList of ids from adding the texts into the vectorstore.\\nOptional Args:vector_field: Document field embeddings are stored in. Defaults to\\n“vector_field”.\\ntext_field: Document field the text of the document is stored in. Defaults\\nto “text”.\\nclassmethod from_texts(texts: List[str], embedding: langchain.embeddings.base.Embeddings, metadatas: Optional[List[dict]] = None, bulk_size: int = 500, **kwargs: Any) → langchain.vectorstores.opensearch_vector_search.OpenSearchVectorSearch[source]#\\nConstruct OpenSearchVectorSearch wrapper from raw documents.\\nExample\\nfrom langchain import OpenSearchVectorSearch\\nfrom langchain.embeddings import OpenAIEmbeddings\\nembeddings = OpenAIEmbeddings()\\nopensearch_vector_search = OpenSearchVectorSearch.from_texts(\\n    texts,\\n    embeddings,\\n    opensearch_url=\"http://localhost:9200\"\\n)\\nOpenSearch by default supports Approximate Search powered by nmslib, faiss\\nand lucene engines recommended for large datasets. Also supports brute force\\nsearch through Script Scoring and Painless Scripting.\\nOptional Args:vector_field: Document field embeddings are stored in. Defaults to\\n“vector_field”.\\ntext_field: Document field the text of the document is stored in. Defaults\\nto “text”.\\nOptional Keyword Args for Approximate Search:engine: “nmslib”, “faiss”, “lucene”; default: “nmslib”\\nspace_type: “l2”, “l1”, “cosinesimil”, “linf”, “innerproduct”; default: “l2”\\nef_search: Size of the dynamic list used during k-NN searches. Higher values\\nlead to more accurate but slower searches; default: 512\\nef_construction: Size of the dynamic list used during k-NN graph creation.\\nHigher values lead to more accurate graph but slower indexing speed;\\ndefault: 512\\nm: Number of bidirectional links created for each new element. Large impact\\non memory consumption. Between 2 and 100; default: 16\\nKeyword Args for Script Scoring or Painless Scripting:is_appx_search: False\\nsimilarity_search(query: str, k: int = 4, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs most similar to query.\\nBy default supports Approximate Search.\\nAlso supports Script Scoring and Painless Scripting.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nReturns\\nList of Documents most similar to the query.\\nOptional Args:vector_field: Document field embeddings are stored in. Defaults to\\n“vector_field”.\\ntext_field: Document field the text of the document is stored in. Defaults\\nto “text”.\\nmetadata_field: Document field that metadata is stored in. Defaults to\\n“metadata”.\\nCan be set to a special value “*” to include the entire document.\\nOptional Args for Approximate Search:search_type: “approximate_search”; default: “approximate_search”\\nsize: number of results the query actually returns; default: 4\\nboolean_filter: A Boolean filter consists of a Boolean query that\\ncontains a k-NN query and a filter.\\nsubquery_clause: Query clause on the knn vector field; default: “must”\\nlucene_filter: the Lucene algorithm decides whether to perform an exact\\nk-NN search with pre-filtering or an approximate search with modified\\npost-filtering.\\nOptional Args for Script Scoring Search:search_type: “script_scoring”; default: “approximate_search”\\nspace_type: “l2”, “l1”, “linf”, “cosinesimil”, “innerproduct”,\\n“hammingbit”; default: “l2”\\npre_filter: script_score query to pre-filter documents before identifying\\nnearest neighbors; default: {“match_all”: {}}\\nOptional Args for Painless Scripting Search:search_type: “painless_scripting”; default: “approximate_search”\\nspace_type: “l2Squared”, “l1Norm”, “cosineSimilarity”; default: “l2Squared”\\npre_filter: script_score query to pre-filter documents before identifying\\nnearest neighbors; default: {“match_all”: {}}\\nclass langchain.vectorstores.Pinecone(index: Any, embedding_function: Callable, text_key: str, namespace: Optional[str] = None)[source]#\\nWrapper around Pinecone vector database.\\nTo use, you should have the pinecone-client python package installed.\\nExample\\nfrom langchain.vectorstores import Pinecone\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nimport pinecone\\n# The environment should be the one specified next to the API key\\n# in your Pinecone console\\npinecone.init(api_key=\"***\", environment=\"...\")\\nindex = pinecone.Index(\"langchain-demo\")\\nembeddings = OpenAIEmbeddings()\\nvectorstore = Pinecone(index, embeddings.embed_query, \"text\")\\nadd_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, ids: Optional[List[str]] = None, namespace: Optional[str] = None, batch_size: int = 32, **kwargs: Any) → List[str][source]#\\nRun more texts through the embeddings and add to the vectorstore.\\nParameters\\ntexts – Iterable of strings to add to the vectorstore.\\nmetadatas – Optional list of metadatas associated with the texts.\\nids – Optional list of ids to associate with the texts.\\nnamespace – Optional pinecone namespace to add the texts to.\\nReturns\\nList of ids from adding the texts into the vectorstore.\\nclassmethod from_existing_index(index_name: str, embedding: langchain.embeddings.base.Embeddings, text_key: str = \\'text\\', namespace: Optional[str] = None) → langchain.vectorstores.pinecone.Pinecone[source]#\\nLoad pinecone vectorstore from index name.\\nclassmethod from_texts(texts: List[str], embedding: langchain.embeddings.base.Embeddings, metadatas: Optional[List[dict]] = None, ids: Optional[List[str]] = None, batch_size: int = 32, text_key: str = \\'text\\', index_name: Optional[str] = None, namespace: Optional[str] = None, **kwargs: Any) → langchain.vectorstores.pinecone.Pinecone[source]#\\nConstruct Pinecone wrapper from raw documents.\\nThis is a user friendly interface that:\\nEmbeds documents.\\nAdds the documents to a provided Pinecone index\\nThis is intended to be a quick way to get started.\\nExample\\nfrom langchain import Pinecone\\nfrom langchain.embeddings import OpenAIEmbeddings\\nimport pinecone\\n# The environment should be the one specified next to the API key\\n# in your Pinecone console\\npinecone.init(api_key=\"***\", environment=\"...\")\\nembeddings = OpenAIEmbeddings()\\npinecone = Pinecone.from_texts(\\n    texts,\\n    embeddings,\\n    index_name=\"langchain-demo\"\\n)\\nsimilarity_search(query: str, k: int = 4, filter: Optional[dict] = None, namespace: Optional[str] = None, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn pinecone documents most similar to query.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nfilter – Dictionary of argument(s) to filter on metadata\\nnamespace – Namespace to search in. Default will search in ‘’ namespace.\\nReturns\\nList of Documents most similar to the query and score for each\\nsimilarity_search_with_score(query: str, k: int = 4, filter: Optional[dict] = None, namespace: Optional[str] = None) → List[Tuple[langchain.schema.Document, float]][source]#\\nReturn pinecone documents most similar to query, along with scores.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nfilter – Dictionary of argument(s) to filter on metadata\\nnamespace – Namespace to search in. Default will search in ‘’ namespace.\\nReturns\\nList of Documents most similar to the query and score for each\\nclass langchain.vectorstores.Qdrant(client: Any, collection_name: str, embedding_function: Callable, content_payload_key: str = \\'page_content\\', metadata_payload_key: str = \\'metadata\\')[source]#\\nWrapper around Qdrant vector database.\\nTo use you should have the qdrant-client package installed.\\nExample\\nfrom qdrant_client import QdrantClient\\nfrom langchain import Qdrant\\nclient = QdrantClient()\\ncollection_name = \"MyCollection\"\\nqdrant = Qdrant(client, collection_name, embedding_function)\\nCONTENT_KEY = \\'page_content\\'#\\nMETADATA_KEY = \\'metadata\\'#\\nadd_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, **kwargs: Any) → List[str][source]#\\nRun more texts through the embeddings and add to the vectorstore.\\nParameters\\ntexts – Iterable of strings to add to the vectorstore.\\nmetadatas – Optional list of metadatas associated with the texts.\\nReturns\\nList of ids from adding the texts into the vectorstore.\\nclassmethod from_texts(texts: List[str], embedding: langchain.embeddings.base.Embeddings, metadatas: Optional[List[dict]] = None, location: Optional[str] = None, url: Optional[str] = None, port: Optional[int] = 6333, grpc_port: int = 6334, prefer_grpc: bool = False, https: Optional[bool] = None, api_key: Optional[str] = None, prefix: Optional[str] = None, timeout: Optional[float] = None, host: Optional[str] = None, path: Optional[str] = None, collection_name: Optional[str] = None, distance_func: str = \\'Cosine\\', content_payload_key: str = \\'page_content\\', metadata_payload_key: str = \\'metadata\\', **kwargs: Any) → langchain.vectorstores.qdrant.Qdrant[source]#\\nConstruct Qdrant wrapper from a list of texts.\\nParameters\\ntexts – A list of texts to be indexed in Qdrant.\\nembedding – A subclass of Embeddings, responsible for text vectorization.\\nmetadatas – An optional list of metadata. If provided it has to be of the same\\nlength as a list of texts.\\nlocation – If :memory: - use in-memory Qdrant instance.\\nIf str - use it as a url parameter.\\nIf None - fallback to relying on host and port parameters.\\nurl – either host or str of “Optional[scheme], host, Optional[port],\\nOptional[prefix]”. Default: None\\nport – Port of the REST API interface. Default: 6333\\ngrpc_port – Port of the gRPC interface. Default: 6334\\nprefer_grpc – If true - use gPRC interface whenever possible in custom methods.\\nDefault: False\\nhttps – If true - use HTTPS(SSL) protocol. Default: None\\napi_key – API key for authentication in Qdrant Cloud. Default: None\\nprefix – If not None - add prefix to the REST URL path.\\nExample: service/v1 will result in\\nhttp://localhost:6333/service/v1/{qdrant-endpoint} for REST API.\\nDefault: None\\ntimeout – Timeout for REST and gRPC API requests.\\nDefault: 5.0 seconds for REST and unlimited for gRPC\\nhost – Host name of Qdrant service. If url and host are None, set to\\n‘localhost’. Default: None\\npath – Path in which the vectors will be stored while using local mode.\\nDefault: None\\ncollection_name – Name of the Qdrant collection to be used. If not provided,\\nit will be created randomly. Default: None\\ndistance_func – Distance function. One of: “Cosine” / “Euclid” / “Dot”.\\nDefault: “Cosine”\\ncontent_payload_key – A payload key used to store the content of the document.\\nDefault: “page_content”\\nmetadata_payload_key – A payload key used to store the metadata of the document.\\nDefault: “metadata”\\n**kwargs – Additional arguments passed directly into REST client initialization\\nThis is a user friendly interface that:\\nCreates embeddings, one for each text\\nInitializes the Qdrant database as an in-memory docstore by default\\n(and overridable to a remote docstore)\\nAdds the text embeddings to the Qdrant database\\nThis is intended to be a quick way to get started.\\nExample\\nfrom langchain import Qdrant\\nfrom langchain.embeddings import OpenAIEmbeddings\\nembeddings = OpenAIEmbeddings()\\nqdrant = Qdrant.from_texts(texts, embeddings, \"localhost\")\\nmax_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs selected using the maximal marginal relevance.\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nfetch_k – Number of Documents to fetch to pass to MMR algorithm.\\nDefaults to 20.\\nReturns\\nList of Documents selected by maximal marginal relevance.\\nsimilarity_search(query: str, k: int = 4, filter: Optional[Dict[str, Union[str, int, bool]]] = None, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs most similar to query.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nfilter – Filter by metadata. Defaults to None.\\nReturns\\nList of Documents most similar to the query.\\nsimilarity_search_with_score(query: str, k: int = 4, filter: Optional[Dict[str, Union[str, int, bool]]] = None) → List[Tuple[langchain.schema.Document, float]][source]#\\nReturn docs most similar to query.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nfilter – Filter by metadata. Defaults to None.\\nReturns\\nList of Documents most similar to the query and score for each.\\nclass langchain.vectorstores.SupabaseVectorStore(client: supabase.client.Client, embedding: Embeddings, table_name: str, query_name: Union[str, None] = None)[source]#\\nVectorStore for a Supabase postgres database. Assumes you have the pgvector\\nextension installed and a match_documents (or similar) function. For more details:\\nhttps://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase\\nYou can implement your own match_documents function in order to limit the search\\nspace to a subset of documents based on your own authorization or business logic.\\nNote that the Supabase Python client does not yet support async operations.\\nIf you’d like to use max_marginal_relevance_search, please review the instructions\\nbelow on modifying the match_documents function to return matched embeddings.\\nadd_texts(texts: Iterable[str], metadatas: Optional[List[dict[Any, Any]]] = None, **kwargs: Any) → List[str][source]#\\nRun more texts through the embeddings and add to the vectorstore.\\nParameters\\ntexts – Iterable of strings to add to the vectorstore.\\nmetadatas – Optional list of metadatas associated with the texts.\\nkwargs – vectorstore specific parameters\\nReturns\\nList of ids from adding the texts into the vectorstore.\\nadd_vectors(vectors: List[List[float]], documents: List[langchain.schema.Document]) → List[str][source]#\\nclassmethod from_texts(texts: List[str], embedding: Embeddings, metadatas: Optional[List[dict]] = None, client: Optional[supabase.client.Client] = None, table_name: Optional[str] = \\'documents\\', query_name: Union[str, None] = \\'match_documents\\', **kwargs: Any) → SupabaseVectorStore[source]#\\nReturn VectorStore initialized from texts and embeddings.\\nmax_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs selected using the maximal marginal relevance.\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nfetch_k – Number of Documents to fetch to pass to MMR algorithm.\\nReturns\\nList of Documents selected by maximal marginal relevance.\\nmax_marginal_relevance_search requires that query_name returns matched\\nembeddings alongside the match documents. The following function function\\ndemonstrates how to do this:\\n```sql\\nCREATE FUNCTION match_documents_embeddings(query_embedding vector(1536),\\nmatch_count int)\\nRETURNS TABLE(id bigint,\\ncontent text,\\nmetadata jsonb,\\nembedding vector(1536),\\nsimilarity float)\\nLANGUAGE plpgsql\\nAS $$\\n# variable_conflict use_column\\nBEGINRETURN query\\nSELECT\\nid,\\ncontent,\\nmetadata,\\nembedding,\\n1 -(docstore.embedding <=> query_embedding) AS similarity\\nFROMdocstore\\nORDER BYdocstore.embedding <=> query_embedding\\nLIMIT match_count;\\nEND;\\n$$;```\\nmax_marginal_relevance_search_by_vector(embedding: List[float], k: int = 4, fetch_k: int = 20, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs selected using the maximal marginal relevance.\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\nParameters\\nembedding – Embedding to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nfetch_k – Number of Documents to fetch to pass to MMR algorithm.\\nReturns\\nList of Documents selected by maximal marginal relevance.\\nquery_name: str#\\nsimilarity_search(query: str, k: int = 4, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs most similar to query.\\nsimilarity_search_by_vector(embedding: List[float], k: int = 4, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs most similar to embedding vector.\\nParameters\\nembedding – Embedding to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nReturns\\nList of Documents most similar to the query vector.\\nsimilarity_search_by_vector_returning_embeddings(query: List[float], k: int) → List[Tuple[Document, float, np.ndarray[np.float32, Any]]][source]#\\nsimilarity_search_by_vector_with_relevance_scores(query: List[float], k: int) → List[Tuple[langchain.schema.Document, float]][source]#\\nsimilarity_search_with_relevance_scores(query: str, k: int = 4, **kwargs: Any) → List[Tuple[langchain.schema.Document, float]][source]#\\nReturn docs and relevance scores in the range [0, 1].\\n0 is dissimilar, 1 is most similar.\\ntable_name: str#\\nclass langchain.vectorstores.VectorStore[source]#\\nInterface for vector stores.\\nasync aadd_documents(documents: List[langchain.schema.Document], **kwargs: Any) → List[str][source]#\\nRun more documents through the embeddings and add to the vectorstore.\\nParameters\\n(List[Document] (documents) – Documents to add to the vectorstore.\\nReturns\\nList of IDs of the added texts.\\nReturn type\\nList[str]\\nasync aadd_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, **kwargs: Any) → List[str][source]#\\nRun more texts through the embeddings and add to the vectorstore.\\nadd_documents(documents: List[langchain.schema.Document], **kwargs: Any) → List[str][source]#\\nRun more documents through the embeddings and add to the vectorstore.\\nParameters\\n(List[Document] (documents) – Documents to add to the vectorstore.\\nReturns\\nList of IDs of the added texts.\\nReturn type\\nList[str]\\nabstract add_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, **kwargs: Any) → List[str][source]#\\nRun more texts through the embeddings and add to the vectorstore.\\nParameters\\ntexts – Iterable of strings to add to the vectorstore.\\nmetadatas – Optional list of metadatas associated with the texts.\\nkwargs – vectorstore specific parameters\\nReturns\\nList of ids from adding the texts into the vectorstore.\\nasync classmethod afrom_documents(documents: List[langchain.schema.Document], embedding: langchain.embeddings.base.Embeddings, **kwargs: Any) → langchain.vectorstores.base.VST[source]#\\nReturn VectorStore initialized from documents and embeddings.\\nasync classmethod afrom_texts(texts: List[str], embedding: langchain.embeddings.base.Embeddings, metadatas: Optional[List[dict]] = None, **kwargs: Any) → langchain.vectorstores.base.VST[source]#\\nReturn VectorStore initialized from texts and embeddings.\\nasync amax_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs selected using the maximal marginal relevance.\\nasync amax_marginal_relevance_search_by_vector(embedding: List[float], k: int = 4, fetch_k: int = 20, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs selected using the maximal marginal relevance.\\nas_retriever(**kwargs: Any) → langchain.schema.BaseRetriever[source]#\\nasync asimilarity_search(query: str, k: int = 4, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs most similar to query.\\nasync asimilarity_search_by_vector(embedding: List[float], k: int = 4, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs most similar to embedding vector.\\nclassmethod from_documents(documents: List[langchain.schema.Document], embedding: langchain.embeddings.base.Embeddings, **kwargs: Any) → langchain.vectorstores.base.VST[source]#\\nReturn VectorStore initialized from documents and embeddings.\\nabstract classmethod from_texts(texts: List[str], embedding: langchain.embeddings.base.Embeddings, metadatas: Optional[List[dict]] = None, **kwargs: Any) → langchain.vectorstores.base.VST[source]#\\nReturn VectorStore initialized from texts and embeddings.\\nmax_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs selected using the maximal marginal relevance.\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nfetch_k – Number of Documents to fetch to pass to MMR algorithm.\\nReturns\\nList of Documents selected by maximal marginal relevance.\\nmax_marginal_relevance_search_by_vector(embedding: List[float], k: int = 4, fetch_k: int = 20, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs selected using the maximal marginal relevance.\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\nParameters\\nembedding – Embedding to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nfetch_k – Number of Documents to fetch to pass to MMR algorithm.\\nReturns\\nList of Documents selected by maximal marginal relevance.\\nabstract similarity_search(query: str, k: int = 4, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs most similar to query.\\nsimilarity_search_by_vector(embedding: List[float], k: int = 4, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs most similar to embedding vector.\\nParameters\\nembedding – Embedding to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nReturns\\nList of Documents most similar to the query vector.\\nsimilarity_search_with_relevance_scores(query: str, k: int = 4, **kwargs: Any) → List[Tuple[langchain.schema.Document, float]][source]#\\nReturn docs and relevance scores in the range [0, 1].\\n0 is dissimilar, 1 is most similar.\\nclass langchain.vectorstores.Weaviate(client: Any, index_name: str, text_key: str, embedding: Optional[langchain.embeddings.base.Embeddings] = None, attributes: Optional[List[str]] = None)[source]#\\nWrapper around Weaviate vector database.\\nTo use, you should have the weaviate-client python package installed.\\nExample\\nimport weaviate\\nfrom langchain.vectorstores import Weaviate\\nclient = weaviate.Client(url=os.environ[\"WEAVIATE_URL\"], ...)\\nweaviate = Weaviate(client, index_name, text_key)\\nadd_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, **kwargs: Any) → List[str][source]#\\nUpload texts with metadata (properties) to Weaviate.\\nclassmethod from_texts(texts: List[str], embedding: langchain.embeddings.base.Embeddings, metadatas: Optional[List[dict]] = None, **kwargs: Any) → langchain.vectorstores.weaviate.Weaviate[source]#\\nConstruct Weaviate wrapper from raw documents.\\nThis is a user-friendly interface that:\\nEmbeds documents.\\nCreates a new index for the embeddings in the Weaviate instance.\\nAdds the documents to the newly created Weaviate index.\\nThis is intended to be a quick way to get started.\\nExample\\nfrom langchain.vectorstores.weaviate import Weaviate\\nfrom langchain.embeddings import OpenAIEmbeddings\\nembeddings = OpenAIEmbeddings()\\nweaviate = Weaviate.from_texts(\\n    texts,\\n    embeddings,\\n    weaviate_url=\"http://localhost:8080\"\\n)\\nmax_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs selected using the maximal marginal relevance.\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nfetch_k – Number of Documents to fetch to pass to MMR algorithm.\\nReturns\\nList of Documents selected by maximal marginal relevance.\\nsimilarity_search(query: str, k: int = 4, **kwargs: Any) → List[langchain.schema.Document][source]#\\nReturn docs most similar to query.\\nParameters\\nquery – Text to look up documents similar to.\\nk – Number of Documents to return. Defaults to 4.\\nReturns\\nList of Documents most similar to the query.\\nsimilarity_search_by_vector(embedding: List[float], k: int = 4, **kwargs: Any) → List[langchain.schema.Document][source]#\\nLook up similar documents by embedding vector in Weaviate.\\nprevious\\nEmbeddings\\nnext\\nAgents\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\reference\\\\modules\\\\vectorstore.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nTracing Walkthrough\\nTracing Walkthrough#\\nimport os\\nos.environ[\"LANGCHAIN_HANDLER\"] = \"langchain\"\\n## Uncomment this if using hosted setup.\\n# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://langchain-api-gateway-57eoxz8z.uc.gateway.dev\" \\n## Uncomment this if you want traces to be recorded to \"my_session\" instead of default.\\n# os.environ[\"LANGCHAIN_SESSION\"] = \"my_session\"  \\n## Better to set this environment variable in the terminal\\n## Uncomment this if using hosted version. Replace \"my_api_key\" with your actual API Key.\\n# os.environ[\"LANGCHAIN_API_KEY\"] = \"my_api_key\"  \\nimport langchain\\nfrom langchain.agents import Tool, initialize_agent, load_tools\\nfrom langchain.agents import AgentType\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.llms import OpenAI\\n# Agent run with tracing. Ensure that OPENAI_API_KEY is set appropriately to run this example.\\nllm = OpenAI(temperature=0)\\ntools = load_tools([\"llm-math\"], llm=llm)\\nagent = initialize_agent(\\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\\n)\\nagent.run(\"What is 2 raised to .123243 power?\")\\n> Entering new AgentExecutor chain...\\n I need to use a calculator to solve this.\\nAction: Calculator\\nAction Input: 2^.123243\\nObservation: Answer: 1.0891804557407723\\nThought: I now know the final answer.\\nFinal Answer: 1.0891804557407723\\n> Finished chain.\\n\\'1.0891804557407723\\'\\n# Agent run with tracing using a chat model\\nagent = initialize_agent(\\n    tools, ChatOpenAI(temperature=0), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\\n)\\nagent.run(\"What is 2 raised to .123243 power?\")\\n> Entering new AgentExecutor chain...\\nQuestion: What is 2 raised to .123243 power?\\nThought: I need a calculator to solve this problem.\\nAction:\\n```\\n{\\n  \"action\": \"calculator\",\\n  \"action_input\": \"2^0.123243\"\\n}\\n```\\nObservation: calculator is not a valid tool, try another one.\\nI made a mistake, I need to use the correct tool for this question.\\nAction:\\n```\\n{\\n  \"action\": \"calculator\",\\n  \"action_input\": \"2^0.123243\"\\n}\\n```\\nObservation: calculator is not a valid tool, try another one.\\nI made a mistake, the tool name is actually \"calc\" instead of \"calculator\".\\nAction:\\n```\\n{\\n  \"action\": \"calc\",\\n  \"action_input\": \"2^0.123243\"\\n}\\n```\\nObservation: calc is not a valid tool, try another one.\\nI made another mistake, the tool name is actually \"Calculator\" instead of \"calc\".\\nAction:\\n```\\n{\\n  \"action\": \"Calculator\",\\n  \"action_input\": \"2^0.123243\"\\n}\\n```\\nObservation: Answer: 1.0891804557407723\\nThought:The final answer is 1.0891804557407723.\\nFinal Answer: 1.0891804557407723\\n> Finished chain.\\n\\'1.0891804557407723\\'\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\tracing\\\\agent_with_tracing.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nCloud Hosted Setup\\n Contents \\nInstallation\\nEnvironment Setup\\nCloud Hosted Setup#\\nWe offer a hosted version of tracing at langchainplus.vercel.app. You can use this to view traces from your run without having to run the server locally.\\nNote: we are currently only offering this to a limited number of users. The hosted platform is VERY alpha, in active development, and data might be dropped at any time. Don’t depend on data being persisted in the system long term and don’t log traces that may contain sensitive information. If you’re interested in using the hosted platform, please fill out the form here.\\nInstallation#\\nLogin to the system and click “API Key” in the top right corner. Generate a new key and keep it safe. You will need it to authenticate with the system.\\nEnvironment Setup#\\nAfter installation, you must now set up your environment to use tracing.\\nThis can be done by setting an environment variable in your terminal by running export LANGCHAIN_HANDLER=langchain.\\nYou can also do this by adding the below snippet to the top of every script. IMPORTANT: this must go at the VERY TOP of your script, before you import anything from langchain.\\nimport os\\nos.environ[\"LANGCHAIN_HANDLER\"] = \"langchain\"\\nYou will also need to set an environment variable to specify the endpoint and your API key. This can be done with the following environment variables:\\nLANGCHAIN_ENDPOINT = “https://langchain-api-gateway-57eoxz8z.uc.gateway.dev”\\nLANGCHAIN_API_KEY - set this to the API key you generated during installation.\\nAn example of adding all relevant environment variables is below:\\nimport os\\nos.environ[\"LANGCHAIN_HANDLER\"] = \"langchain\"\\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://langchain-api-gateway-57eoxz8z.uc.gateway.dev\"\\nos.environ[\"LANGCHAIN_API_KEY\"] = \"my_api_key\"  # Don\\'t commit this to your repo! Better to set it in your terminal.\\n Contents\\n  \\nInstallation\\nEnvironment Setup\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\tracing\\\\hosted_installation.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nLocally Hosted Setup\\n Contents \\nInstallation\\nEnvironment Setup\\nLocally Hosted Setup#\\nThis page contains instructions for installing and then setting up the environment to use the locally hosted version of tracing.\\nInstallation#\\nEnsure you have Docker installed (see Get Docker) and that it’s running.\\nInstall the latest version of langchain: pip install langchain or pip install langchain -U to upgrade your\\nexisting version.\\nRun langchain-server. This command was installed automatically when you ran the above command (pip install langchain).\\nThis will spin up the server in the terminal, hosted on port 4137 by default.\\nOnce you see the terminal\\noutput langchain-langchain-frontend-1 | ➜ Local: [http://localhost:4173/](http://localhost:4173/), navigate\\nto http://localhost:4173/\\nYou should see a page with your tracing sessions. See the overview page for a walkthrough of the UI.\\nCurrently, trace data is not guaranteed to be persisted between runs of langchain-server. If you want to\\npersist your data, you can mount a volume to the Docker container. See the Docker docs for more info.\\nTo stop the server, press Ctrl+C in the terminal where you ran langchain-server.\\nEnvironment Setup#\\nAfter installation, you must now set up your environment to use tracing.\\nThis can be done by setting an environment variable in your terminal by running export LANGCHAIN_HANDLER=langchain.\\nYou can also do this by adding the below snippet to the top of every script. IMPORTANT: this must go at the VERY TOP of your script, before you import anything from langchain.\\nimport os\\nos.environ[\"LANGCHAIN_HANDLER\"] = \"langchain\"\\n Contents\\n  \\nInstallation\\nEnvironment Setup\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\tracing\\\\local_installation.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nAgent Simulations\\n Contents \\nCAMEL\\nGenerative Agents\\nAgent Simulations#\\nAgent simulations involve interacting one of more agents with eachother.\\nAgent simulations generally involve two main components:\\nLong Term Memory\\nSimulation Environment\\nSpecific implementations of agent simulations (or parts of agent simulations) include\\nCAMEL#\\nCAMEL: an implementation of the CAMEL (Communicative Agents for “Mind” Exploration of Large Scale Language Model Society) paper, where two agents communicate with eachother.\\nGenerative Agents#\\nGenerative Agents: This notebook implements a generative agent based on the paper Generative Agents: Interactive Simulacra of Human Behavior by Park, et. al.\\nprevious\\nAutonomous Agents\\nnext\\nQuestion Answering over Docs\\n Contents\\n  \\nCAMEL\\nGenerative Agents\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\agent_simulations.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nInteracting with APIs\\n Contents \\nChains\\nAgents\\nInteracting with APIs#\\nConceptual Guide\\nLots of data and information is stored behind APIs.\\nThis page covers all resources available in LangChain for working with APIs.\\nChains#\\nIf you are just getting started, and you have relatively simple apis, you should get started with chains.\\nChains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you\\nunderstand what is happening better.\\nAPI Chain\\nAgents#\\nAgents are more complex, and involve multiple queries to the LLM to understand what to do.\\nThe downside of agents are that you have less control. The upside is that they are more powerful,\\nwhich allows you to use them on larger and more complex schemas.\\nOpenAPI Agent\\nprevious\\nCode Understanding\\nnext\\nSummarization\\n Contents\\n  \\nChains\\nAgents\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\apis.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nAutonomous Agents\\n Contents \\nBaby AGI (Original Repo)\\nAutoGPT (Original Repo)\\nAutonomous Agents#\\nAutonomous Agents are agents that designed to be more long running.\\nYou give them one or multiple long term goals, and they independently execute towards those goals.\\nThe applications combine tool usage and long term memory.\\nAt the moment, Autonomous Agents are fairly experimental and based off of other open-source projects.\\nBy implementing these open source projects in LangChain primitives we can get the benefits of LangChain -\\neasy switching an experimenting with multiple LLMs, usage of different vectorstores as memory,\\nusage of LangChain’s collection of tools.\\nBaby AGI (Original Repo)#\\nBaby AGI: a notebook implementing BabyAGI as LLM Chains\\nBaby AGI with Tools: building off the above notebook, this example substitutes in an agent with tools as the execution tools, allowing it to actually take actions.\\nAutoGPT (Original Repo)#\\nAutoGPT: a notebook implementing AutoGPT in LangChain primitives\\nWebSearch Research Assistant: a notebook showing how to use AutoGPT plus specific tools to act as research assistant that can use the web.\\nprevious\\nPersonal Assistants (Agents)\\nnext\\nAgent Simulations\\n Contents\\n  \\nBaby AGI (Original Repo)\\nAutoGPT (Original Repo)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\autonomous_agents.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nChatbots\\nChatbots#\\nConceptual Guide\\nSince language models are good at producing text, that makes them ideal for creating chatbots.\\nAside from the base prompts/LLMs, an important concept to know for Chatbots is memory.\\nMost chat based applications rely on remembering what happened in previous interactions, which memory is designed to help with.\\nThe following resources exist:\\nChatGPT Clone: A notebook walking through how to recreate a ChatGPT-like experience with LangChain.\\nConversation Memory: A notebook walking through how to use different types of conversational memory.\\nConversation Agent: A notebook walking through how to create an agent optimized for conversation.\\nAdditional related resources include:\\nMemory Key Concepts: Explanation of key concepts related to memory.\\nMemory Examples: A collection of how-to examples for working with memory.\\nprevious\\nQuestion Answering over Docs\\nnext\\nQuerying Tabular Data\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\chatbots.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nCode Understanding\\n Contents \\nConversational Retriever Chain\\nCode Understanding#\\nOverview\\nLangChain is a useful tool designed to parse GitHub code repositories. By leveraging VectorStores, Conversational RetrieverChain, and GPT-4, it can answer questions in the context of an entire GitHub repository or generate new code. This documentation page outlines the essential components of the system and guides using LangChain for better code comprehension, contextual question answering, and code generation in GitHub repositories.\\nConversational Retriever Chain#\\nConversational RetrieverChain is a retrieval-focused system that interacts with the data stored in a VectorStore. Utilizing advanced techniques, like context-aware filtering and ranking, it retrieves the most relevant code snippets and information for a given user query. Conversational RetrieverChain is engineered to deliver high-quality, pertinent results while considering conversation history and context.\\nLangChain Workflow for Code Understanding and Generation\\nIndex the code base: Clone the target repository, load all files within, chunk the files, and execute the indexing process. Optionally, you can skip this step and use an already indexed dataset.\\nEmbedding and Code Store: Code snippets are embedded using a code-aware embedding model and stored in a VectorStore.\\nQuery Understanding: GPT-4 processes user queries, grasping the context and extracting relevant details.\\nConstruct the Retriever: Conversational RetrieverChain searches the VectorStore to identify the most relevant code snippets for a given query.\\nBuild the Conversational Chain: Customize the retriever settings and define any user-defined filters as needed.\\nAsk questions: Define a list of questions to ask about the codebase, and then use the ConversationalRetrievalChain to generate context-aware answers. The LLM (GPT-4) generates comprehensive, context-aware answers based on retrieved code snippets and conversation history.\\nThe full tutorial is available below.\\nTwitter the-algorithm codebase analysis with Deep Lake: A notebook walking through how to parse github source code and run queries conversation.\\nLangChain codebase analysis with Deep Lake: A notebook walking through how to analyze and do question answering over THIS code base.\\nprevious\\nQuerying Tabular Data\\nnext\\nInteracting with APIs\\n Contents\\n  \\nConversational Retriever Chain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\code.html'}),\n",
              " Document(page_content='.rst\\n.pdf\\nEvaluation\\n Contents \\nThe Problem\\nThe Solution\\nThe Examples\\nOther Examples\\nEvaluation#\\nNote\\nConceptual Guide\\nThis section of documentation covers how we approach and think about evaluation in LangChain.\\nBoth evaluation of internal chains/agents, but also how we would recommend people building on top of LangChain approach evaluation.\\nThe Problem#\\nIt can be really hard to evaluate LangChain chains and agents.\\nThere are two main reasons for this:\\n# 1: Lack of data\\nYou generally don’t have a ton of data to evaluate your chains/agents over before starting a project.\\nThis is usually because Large Language Models (the core of most chains/agents) are terrific few-shot and zero shot learners,\\nmeaning you are almost always able to get started on a particular task (text-to-SQL, question answering, etc) without\\na large dataset of examples.\\nThis is in stark contrast to traditional machine learning where you had to first collect a bunch of datapoints\\nbefore even getting started using a model.\\n# 2: Lack of metrics\\nMost chains/agents are performing tasks for which there are not very good metrics to evaluate performance.\\nFor example, one of the most common use cases is generating text of some form.\\nEvaluating generated text is much more complicated than evaluating a classification prediction, or a numeric prediction.\\nThe Solution#\\nLangChain attempts to tackle both of those issues.\\nWhat we have so far are initial passes at solutions - we do not think we have a perfect solution.\\nSo we very much welcome feedback, contributions, integrations, and thoughts on this.\\nHere is what we have for each problem so far:\\n# 1: Lack of data\\nWe have started LangChainDatasets a Community space on Hugging Face.\\nWe intend this to be a collection of open source datasets for evaluating common chains and agents.\\nWe have contributed five datasets of our own to start, but we highly intend this to be a community effort.\\nIn order to contribute a dataset, you simply need to join the community and then you will be able to upload datasets.\\nWe’re also aiming to make it as easy as possible for people to create their own datasets.\\nAs a first pass at this, we’ve added a QAGenerationChain, which given a document comes up\\nwith question-answer pairs that can be used to evaluate question-answering tasks over that document down the line.\\nSee this notebook for an example of how to use this chain.\\n# 2: Lack of metrics\\nWe have two solutions to the lack of metrics.\\nThe first solution is to use no metrics, and rather just rely on looking at results by eye to get a sense for how the chain/agent is performing.\\nTo assist in this, we have developed (and will continue to develop) tracing, a UI-based visualizer of your chain and agent runs.\\nThe second solution we recommend is to use Language Models themselves to evaluate outputs.\\nFor this we have a few different chains and prompts aimed at tackling this issue.\\nThe Examples#\\nWe have created a bunch of examples combining the above two solutions to show how we internally evaluate chains and agents when we are developing.\\nIn addition to the examples we’ve curated, we also highly welcome contributions here.\\nTo facilitate that, we’ve included a template notebook for community members to use to build their own examples.\\nThe existing examples we have are:\\nQuestion Answering (State of Union): A notebook showing evaluation of a question-answering task over a State-of-the-Union address.\\nQuestion Answering (Paul Graham Essay): A notebook showing evaluation of a question-answering task over a Paul Graham essay.\\nSQL Question Answering (Chinook): A notebook showing evaluation of a question-answering task over a SQL database (the Chinook database).\\nAgent Vectorstore: A notebook showing evaluation of an agent doing question answering while routing between two different vector databases.\\nAgent Search + Calculator: A notebook showing evaluation of an agent doing question answering using a Search engine and a Calculator as tools.\\nEvaluating an OpenAPI Chain: A notebook showing evaluation of an OpenAPI chain, including how to generate test data if you don’t have any.\\nOther Examples#\\nIn addition, we also have some more generic resources for evaluation.\\nQuestion Answering: An overview of LLMs aimed at evaluating question answering systems in general.\\nData Augmented Question Answering: An end-to-end example of evaluating a question answering system focused on a specific document (a RetrievalQAChain to be precise). This example highlights how to use LLMs to come up with question/answer examples to evaluate over, and then highlights how to use LLMs to evaluate performance on those generated examples.\\nHugging Face Datasets: Covers an example of loading and using a dataset from Hugging Face for evaluation.\\nprevious\\nExtraction\\nnext\\nAgent Benchmarking: Search + Calculator\\n Contents\\n  \\nThe Problem\\nThe Solution\\nThe Examples\\nOther Examples\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\evaluation.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nExtraction\\nExtraction#\\nConceptual Guide\\nMost APIs and databases still deal with structured information.\\nTherefore, in order to better work with those, it can be useful to extract structured information from text.\\nExamples of this include:\\nExtracting a structured row to insert into a database from a sentence\\nExtracting multiple rows to insert into a database from a long document\\nExtracting the correct API parameters from a user query\\nThis work is extremely related to output parsing.\\nOutput parsers are responsible for instructing the LLM to respond in a specific format.\\nIn this case, the output parsers specify the format of the data you would like to extract from the document.\\nThen, in addition to the output format instructions, the prompt should also contain the data you would like to extract information from.\\nWhile normal output parsers are good enough for basic structuring of response data,\\nwhen doing extraction you often want to extract more complicated or nested structures.\\nFor a deep dive on extraction, we recommend checking out kor,\\na library that uses the existing LangChain chain and OutputParser abstractions\\nbut deep dives on allowing extraction of more complicated schemas.\\nprevious\\nSummarization\\nnext\\nEvaluation\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\extraction.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nPersonal Assistants (Agents)\\nPersonal Assistants (Agents)#\\nConceptual Guide\\nWe use “personal assistant” here in a very broad sense.\\nPersonal assistants have a few characteristics:\\nThey can interact with the outside world\\nThey have knowledge of your data\\nThey remember your interactions\\nReally all of the functionality in LangChain is relevant for building a personal assistant.\\nHighlighting specific parts:\\nAgent Documentation (for interacting with the outside world)\\nIndex Documentation (for giving them knowledge of your data)\\nMemory (for helping them remember interactions)\\nSpecific examples of this include:\\nAI Plugins: an implementation of an agent that is designed to be able to use all AI Plugins.\\nWikibase Agent: an implementation of an agent that is designed to interact with Wikibase.\\nprevious\\nHow to add SharedMemory to an Agent and its Tools\\nnext\\nAutonomous Agents\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\personal_assistants.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nQuestion Answering over Docs\\n Contents \\nDocument Question Answering\\nAdding in sources\\nAdditional Related Resources\\nEnd-to-end examples\\nQuestion Answering over Docs#\\nConceptual Guide\\nQuestion answering in this context refers to question answering over your document data.\\nFor question answering over other types of data, please see other sources documentation like SQL database Question Answering or Interacting with APIs.\\nFor question answering over many documents, you almost always want to create an index over the data.\\nThis can be used to smartly access the most relevant documents for a given question, allowing you to avoid having to pass all the documents to the LLM (saving you time and money).\\nSee this notebook for a more detailed introduction to this, but for a super quick start the steps involved are:\\nLoad Your Documents\\nfrom langchain.document_loaders import TextLoader\\nloader = TextLoader(\\'../state_of_the_union.txt\\')\\nSee here for more information on how to get started with document loading.\\nCreate Your Index\\nfrom langchain.indexes import VectorstoreIndexCreator\\nindex = VectorstoreIndexCreator().from_loaders([loader])\\nThe best and most popular index by far at the moment is the VectorStore index.\\nQuery Your Index\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nindex.query(query)\\nAlternatively, use query_with_sources to also get back the sources involved\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nindex.query_with_sources(query)\\nAgain, these high level interfaces obfuscate a lot of what is going on under the hood, so please see this notebook for a lower level walkthrough.\\nDocument Question Answering#\\nQuestion answering involves fetching multiple documents, and then asking a question of them.\\nThe LLM response will contain the answer to your question, based on the content of the documents.\\nThe recommended way to get started using a question answering chain is:\\nfrom langchain.chains.question_answering import load_qa_chain\\nchain = load_qa_chain(llm, chain_type=\"stuff\")\\nchain.run(input_documents=docs, question=query)\\nThe following resources exist:\\nQuestion Answering Notebook: A notebook walking through how to accomplish this task.\\nVectorDB Question Answering Notebook: A notebook walking through how to do question answering over a vector database. This can often be useful for when you have a LOT of documents, and you don’t want to pass them all to the LLM, but rather first want to do some semantic search over embeddings.\\nAdding in sources#\\nThere is also a variant of this, where in addition to responding with the answer the language model will also cite its sources (eg which of the documents passed in it used).\\nThe recommended way to get started using a question answering with sources chain is:\\nfrom langchain.chains.qa_with_sources import load_qa_with_sources_chain\\nchain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\nThe following resources exist:\\nQA With Sources Notebook: A notebook walking through how to accomplish this task.\\nVectorDB QA With Sources Notebook: A notebook walking through how to do question answering with sources over a vector database. This can often be useful for when you have a LOT of documents, and you don’t want to pass them all to the LLM, but rather first want to do some semantic search over embeddings.\\nAdditional Related Resources#\\nAdditional related resources include:\\nUtilities for working with Documents: Guides on how to use several of the utilities which will prove helpful for this task, including Text Splitters (for splitting up long documents) and Embeddings & Vectorstores (useful for the above Vector DB example).\\nCombineDocuments Chains: A conceptual overview of specific types of chains by which you can accomplish this task.\\nEnd-to-end examples#\\nFor examples to this done in an end-to-end manner, please see the following resources:\\nSemantic search over a group chat with Sources Notebook: A notebook that semantically searches over a group chat conversation.\\nprevious\\nAgent Simulations\\nnext\\nChatbots\\n Contents\\n  \\nDocument Question Answering\\nAdding in sources\\nAdditional Related Resources\\nEnd-to-end examples\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\question_answering.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nSummarization\\nSummarization#\\nConceptual Guide\\nSummarization involves creating a smaller summary of multiple longer documents.\\nThis can be useful for distilling long documents into the core pieces of information.\\nThe recommended way to get started using a summarization chain is:\\nfrom langchain.chains.summarize import load_summarize_chain\\nchain = load_summarize_chain(llm, chain_type=\"map_reduce\")\\nchain.run(docs)\\nThe following resources exist:\\nSummarization Notebook: A notebook walking through how to accomplish this task.\\nAdditional related resources include:\\nUtilities for working with Documents: Guides on how to use several of the utilities which will prove helpful for this task, including Text Splitters (for splitting up long documents).\\nprevious\\nInteracting with APIs\\nnext\\nExtraction\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\summarization.html'}),\n",
              " Document(page_content='.md\\n.pdf\\nQuerying Tabular Data\\n Contents \\nDocument Loading\\nQuerying\\nChains\\nAgents\\nQuerying Tabular Data#\\nConceptual Guide\\nLots of data and information is stored in tabular data, whether it be csvs, excel sheets, or SQL tables.\\nThis page covers all resources available in LangChain for working with data in this format.\\nDocument Loading#\\nIf you have text data stored in a tabular format, you may want to load the data into a Document and then index it as you would\\nother text/unstructured data. For this, you should use a document loader like the CSVLoader\\nand then you should create an index over that data, and query it that way.\\nQuerying#\\nIf you have more numeric tabular data, or have a large amount of data and don’t want to index it, you should get started\\nby looking at various chains and agents we have for dealing with this data.\\nChains#\\nIf you are just getting started, and you have relatively small/simple tabular data, you should get started with chains.\\nChains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you\\nunderstand what is happening better.\\nSQL Database Chain\\nAgents#\\nAgents are more complex, and involve multiple queries to the LLM to understand what to do.\\nThe downside of agents are that you have less control. The upside is that they are more powerful,\\nwhich allows you to use them on larger databases and more complex schemas.\\nSQL Agent\\nPandas Agent\\nCSV Agent\\nprevious\\nChatbots\\nnext\\nCode Understanding\\n Contents\\n  \\nDocument Loading\\nQuerying\\nChains\\nAgents\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\tabular.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nCustom Agent with PlugIn Retrieval\\n Contents \\nSet up environment\\nSetup LLM\\nSet up plugins\\nTool Retriever\\nPrompt Template\\nOutput Parser\\nSet up LLM, stop sequence, and the agent\\nUse the Agent\\nCustom Agent with PlugIn Retrieval#\\nThis notebook combines two concepts in order to build a custom agent that can interact with AI Plugins:\\nCustom Agent with Retrieval: This introduces the concept of retrieving many tools, which is useful when trying to work with arbitrarily many plugins.\\nNatural Language API Chains: This creates Natural Language wrappers around OpenAPI endpoints. This is useful because (1) plugins use OpenAPI endpoints under the hood, (2) wrapping them in an NLAChain allows the router agent to call it more easily.\\nThe novel idea introduced in this notebook is the idea of using retrieval to select not the tools explicitly, but the set of OpenAPI specs to use. We can then generate tools from those OpenAPI specs. The use case for this is when trying to get agents to use plugins. It may be more efficient to choose plugins first, then the endpoints, rather than the endpoints directly. This is because the plugins may contain more useful information for selection.\\nSet up environment#\\nDo necessary imports, etc.\\nfrom langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\\nfrom langchain.prompts import StringPromptTemplate\\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\\nfrom typing import List, Union\\nfrom langchain.schema import AgentAction, AgentFinish\\nfrom langchain.agents.agent_toolkits import NLAToolkit\\nfrom langchain.tools.plugin import AIPlugin\\nimport re\\nSetup LLM#\\nllm = OpenAI(temperature=0)\\nSet up plugins#\\nLoad and index plugins\\nurls = [\\n    \"https://datasette.io/.well-known/ai-plugin.json\",\\n    \"https://api.speak.com/.well-known/ai-plugin.json\",\\n    \"https://www.wolframalpha.com/.well-known/ai-plugin.json\",\\n    \"https://www.zapier.com/.well-known/ai-plugin.json\",\\n    \"https://www.klarna.com/.well-known/ai-plugin.json\",\\n    \"https://www.joinmilo.com/.well-known/ai-plugin.json\",\\n    \"https://slack.com/.well-known/ai-plugin.json\",\\n    \"https://schooldigger.com/.well-known/ai-plugin.json\",\\n]\\nAI_PLUGINS = [AIPlugin.from_url(url) for url in urls]\\nTool Retriever#\\nWe will use a vectorstore to create embeddings for each tool description. Then, for an incoming query we can create embeddings for that query and do a similarity search for relevant tools.\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.schema import Document\\nembeddings = OpenAIEmbeddings()\\ndocs = [\\n    Document(page_content=plugin.description_for_model, \\n             metadata={\"plugin_name\": plugin.name_for_model}\\n            )\\n    for plugin in AI_PLUGINS\\n]\\nvector_store = FAISS.from_documents(docs, embeddings)\\ntoolkits_dict = {plugin.name_for_model: \\n                 NLAToolkit.from_llm_and_ai_plugin(llm, plugin) \\n                 for plugin in AI_PLUGINS}\\nAttempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\\nAttempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\\nAttempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\\nAttempting to load an OpenAPI 3.0.2 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\\nAttempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\\nAttempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\\nAttempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\\nAttempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\\nAttempting to load a Swagger 2.0 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\\nretriever = vector_store.as_retriever()\\ndef get_tools(query):\\n    # Get documents, which contain the Plugins to use\\n    docs = retriever.get_relevant_documents(query)\\n    # Get the toolkits, one for each plugin\\n    tool_kits = [toolkits_dict[d.metadata[\"plugin_name\"]] for d in docs]\\n    # Get the tools: a separate NLAChain for each endpoint\\n    tools = []\\n    for tk in tool_kits:\\n        tools.extend(tk.nla_tools)\\n    return tools\\nWe can now test this retriever to see if it seems to work.\\ntools = get_tools(\"What could I do today with my kiddo\")\\n[t.name for t in tools]\\n[\\'Milo.askMilo\\',\\n \\'Zapier_Natural_Language_Actions_(NLA)_API_(Dynamic)_-_Beta.search_all_actions\\',\\n \\'Zapier_Natural_Language_Actions_(NLA)_API_(Dynamic)_-_Beta.preview_a_zap\\',\\n \\'Zapier_Natural_Language_Actions_(NLA)_API_(Dynamic)_-_Beta.get_configuration_link\\',\\n \\'Zapier_Natural_Language_Actions_(NLA)_API_(Dynamic)_-_Beta.list_exposed_actions\\',\\n \\'SchoolDigger_API_V2.0.Autocomplete_GetSchools\\',\\n \\'SchoolDigger_API_V2.0.Districts_GetAllDistricts2\\',\\n \\'SchoolDigger_API_V2.0.Districts_GetDistrict2\\',\\n \\'SchoolDigger_API_V2.0.Rankings_GetSchoolRank2\\',\\n \\'SchoolDigger_API_V2.0.Rankings_GetRank_District\\',\\n \\'SchoolDigger_API_V2.0.Schools_GetAllSchools20\\',\\n \\'SchoolDigger_API_V2.0.Schools_GetSchool20\\',\\n \\'Speak.translate\\',\\n \\'Speak.explainPhrase\\',\\n \\'Speak.explainTask\\']\\ntools = get_tools(\"what shirts can i buy?\")\\n[t.name for t in tools]\\n[\\'Open_AI_Klarna_product_Api.productsUsingGET\\',\\n \\'Milo.askMilo\\',\\n \\'Zapier_Natural_Language_Actions_(NLA)_API_(Dynamic)_-_Beta.search_all_actions\\',\\n \\'Zapier_Natural_Language_Actions_(NLA)_API_(Dynamic)_-_Beta.preview_a_zap\\',\\n \\'Zapier_Natural_Language_Actions_(NLA)_API_(Dynamic)_-_Beta.get_configuration_link\\',\\n \\'Zapier_Natural_Language_Actions_(NLA)_API_(Dynamic)_-_Beta.list_exposed_actions\\',\\n \\'SchoolDigger_API_V2.0.Autocomplete_GetSchools\\',\\n \\'SchoolDigger_API_V2.0.Districts_GetAllDistricts2\\',\\n \\'SchoolDigger_API_V2.0.Districts_GetDistrict2\\',\\n \\'SchoolDigger_API_V2.0.Rankings_GetSchoolRank2\\',\\n \\'SchoolDigger_API_V2.0.Rankings_GetRank_District\\',\\n \\'SchoolDigger_API_V2.0.Schools_GetAllSchools20\\',\\n \\'SchoolDigger_API_V2.0.Schools_GetSchool20\\']\\nPrompt Template#\\nThe prompt template is pretty standard, because we’re not actually changing that much logic in the actual prompt template, but rather we are just changing how retrieval is done.\\n# Set up the base template\\ntemplate = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\\n{tools}\\nUse the following format:\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\nBegin! Remember to speak as a pirate when giving your final answer. Use lots of \"Arg\"s\\nQuestion: {input}\\n{agent_scratchpad}\"\"\"\\nThe custom prompt template now has the concept of a tools_getter, which we call on the input to select the tools to use\\nfrom typing import Callable\\n# Set up a prompt template\\nclass CustomPromptTemplate(StringPromptTemplate):\\n    # The template to use\\n    template: str\\n    ############## NEW ######################\\n    # The list of tools available\\n    tools_getter: Callable\\n    \\n    def format(self, **kwargs) -> str:\\n        # Get the intermediate steps (AgentAction, Observation tuples)\\n        # Format them in a particular way\\n        intermediate_steps = kwargs.pop(\"intermediate_steps\")\\n        thoughts = \"\"\\n        for action, observation in intermediate_steps:\\n            thoughts += action.log\\n            thoughts += f\"\\\\nObservation: {observation}\\\\nThought: \"\\n        # Set the agent_scratchpad variable to that value\\n        kwargs[\"agent_scratchpad\"] = thoughts\\n        ############## NEW ######################\\n        tools = self.tools_getter(kwargs[\"input\"])\\n        # Create a tools variable from the list of tools provided\\n        kwargs[\"tools\"] = \"\\\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\\n        # Create a list of tool names for the tools provided\\n        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in tools])\\n        return self.template.format(**kwargs)\\nprompt = CustomPromptTemplate(\\n    template=template,\\n    tools_getter=get_tools,\\n    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\\n    # This includes the `intermediate_steps` variable because that is needed\\n    input_variables=[\"input\", \"intermediate_steps\"]\\n)\\nOutput Parser#\\nThe output parser is unchanged from the previous notebook, since we are not changing anything about the output format.\\nclass CustomOutputParser(AgentOutputParser):\\n    \\n    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\\n        # Check if agent should finish\\n        if \"Final Answer:\" in llm_output:\\n            return AgentFinish(\\n                # Return values is generally always a dictionary with a single `output` key\\n                # It is not recommended to try anything else at the moment :)\\n                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\\n                log=llm_output,\\n            )\\n        # Parse out the action and action input\\n        regex = r\"Action\\\\s*\\\\d*\\\\s*:(.*?)\\\\nAction\\\\s*\\\\d*\\\\s*Input\\\\s*\\\\d*\\\\s*:[\\\\s]*(.*)\"\\n        match = re.search(regex, llm_output, re.DOTALL)\\n        if not match:\\n            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\\n        action = match.group(1).strip()\\n        action_input = match.group(2)\\n        # Return the action and action input\\n        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip(\\'\"\\'), log=llm_output)\\noutput_parser = CustomOutputParser()\\nSet up LLM, stop sequence, and the agent#\\nAlso the same as the previous notebook\\nllm = OpenAI(temperature=0)\\n# LLM chain consisting of the LLM and a prompt\\nllm_chain = LLMChain(llm=llm, prompt=prompt)\\ntool_names = [tool.name for tool in tools]\\nagent = LLMSingleActionAgent(\\n    llm_chain=llm_chain, \\n    output_parser=output_parser,\\n    stop=[\"\\\\nObservation:\"], \\n    allowed_tools=tool_names\\n)\\nUse the Agent#\\nNow we can use it!\\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\\nagent_executor.run(\"what shirts can i buy?\")\\n> Entering new AgentExecutor chain...\\nThought: I need to find a product API\\nAction: Open_AI_Klarna_product_Api.productsUsingGET\\nAction Input: shirts\\nObservation:I found 10 shirts from the API response. They range in price from $9.99 to $450.00 and come in a variety of materials, colors, and patterns. I now know what shirts I can buy\\nFinal Answer: Arg, I found 10 shirts from the API response. They range in price from $9.99 to $450.00 and come in a variety of materials, colors, and patterns.\\n> Finished chain.\\n\\'Arg, I found 10 shirts from the API response. They range in price from $9.99 to $450.00 and come in a variety of materials, colors, and patterns.\\'\\n Contents\\n  \\nSet up environment\\nSetup LLM\\nSet up plugins\\nTool Retriever\\nPrompt Template\\nOutput Parser\\nSet up LLM, stop sequence, and the agent\\nUse the Agent\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\agents\\\\custom_agent_with_plugin_retrieval.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nWikibase Agent\\n Contents \\nWikibase Agent\\nPreliminaries\\nAPI keys and other secrats\\nOpenAI API Key\\nWikidata user-agent header\\nEnable tracing if desired\\nTools\\nItem and Property lookup\\nSparql runner\\nAgent\\nWrap the tools\\nPrompts\\nOutput parser\\nSpecify the LLM model\\nAgent and agent executor\\nRun it!\\nWikibase Agent#\\nThis notebook demonstrates a very simple wikibase agent that uses sparql generation. Although this code is intended to work against any\\nwikibase instance, we use http://wikidata.org for testing.\\nIf you are interested in wikibases and sparql, please consider helping to improve this agent. Look here for more details and open questions.\\nPreliminaries#\\nAPI keys and other secrats#\\nWe use an .ini file, like this:\\n[OPENAI]\\nOPENAI_API_KEY=xyzzy\\n[WIKIDATA]\\nWIKIDATA_USER_AGENT_HEADER=argle-bargle\\nimport configparser\\nconfig = configparser.ConfigParser()\\nconfig.read(\\'./secrets.ini\\')\\n[\\'./secrets.ini\\']\\nOpenAI API Key#\\nAn OpenAI API key is required unless you modify the code below to use another LLM provider.\\nopenai_api_key = config[\\'OPENAI\\'][\\'OPENAI_API_KEY\\']\\nimport os\\nos.environ.update({\\'OPENAI_API_KEY\\': openai_api_key})\\nWikidata user-agent header#\\nWikidata policy requires a user-agent header. See https://meta.wikimedia.org/wiki/User-Agent_policy. However, at present this policy is not strictly enforced.\\nwikidata_user_agent_header = None if not config.has_section(\\'WIKIDATA\\') else config[\\'WIKIDATA\\'][\\'WIKIDAtA_USER_AGENT_HEADER\\']\\nEnable tracing if desired#\\n#import os\\n#os.environ[\"LANGCHAIN_HANDLER\"] = \"langchain\"\\n#os.environ[\"LANGCHAIN_SESSION\"] = \"default\" # Make sure this session actually exists. \\nTools#\\nThree tools are provided for this simple agent:\\nItemLookup: for finding the q-number of an item\\nPropertyLookup: for finding the p-number of a property\\nSparqlQueryRunner: for running a sparql query\\nItem and Property lookup#\\nItem and Property lookup are implemented in a single method, using an elastic search endpoint. Not all wikibase instances have it, but wikidata does, and that’s where we’ll start.\\ndef get_nested_value(o: dict, path: list) -> any:\\n    current = o\\n    for key in path:\\n        try:\\n            current = current[key]\\n        except:\\n            return None\\n    return current\\nimport requests\\nfrom typing import Optional\\ndef vocab_lookup(search: str, entity_type: str = \"item\",\\n                 url: str = \"https://www.wikidata.org/w/api.php\",\\n                 user_agent_header: str = wikidata_user_agent_header,\\n                 srqiprofile: str = None,\\n                ) -> Optional[str]:    \\n    headers = {\\n        \\'Accept\\': \\'application/json\\'\\n    }\\n    if wikidata_user_agent_header is not None:\\n        headers[\\'User-Agent\\'] = wikidata_user_agent_header\\n    \\n    if entity_type == \"item\":\\n        srnamespace = 0\\n        srqiprofile = \"classic_noboostlinks\" if srqiprofile is None else srqiprofile\\n    elif entity_type == \"property\":\\n        srnamespace = 120\\n        srqiprofile = \"classic\" if srqiprofile is None else srqiprofile\\n    else:\\n        raise ValueError(\"entity_type must be either \\'property\\' or \\'item\\'\")          \\n    \\n    params = {\\n        \"action\": \"query\",\\n        \"list\": \"search\",\\n        \"srsearch\": search,\\n        \"srnamespace\": srnamespace,\\n        \"srlimit\": 1,\\n        \"srqiprofile\": srqiprofile,\\n        \"srwhat\": \\'text\\',\\n        \"format\": \"json\"\\n    }\\n    \\n    response = requests.get(url, headers=headers, params=params)\\n        \\n    if response.status_code == 200:\\n        title = get_nested_value(response.json(), [\\'query\\', \\'search\\', 0, \\'title\\'])\\n        if title is None:\\n            return f\"I couldn\\'t find any {entity_type} for \\'{search}\\'. Please rephrase your request and try again\"\\n        # if there is a prefix, strip it off\\n        return title.split(\\':\\')[-1]\\n    else:\\n        return \"Sorry, I got an error. Please try again.\"\\nprint(vocab_lookup(\"Malin 1\"))\\nQ4180017\\nprint(vocab_lookup(\"instance of\", entity_type=\"property\"))\\nP31\\nprint(vocab_lookup(\"Ceci n\\'est pas un q-item\"))\\nI couldn\\'t find any item for \\'Ceci n\\'est pas un q-item\\'. Please rephrase your request and try again\\nSparql runner#\\nThis tool runs sparql - by default, wikidata is used.\\nimport requests\\nfrom typing import List, Dict, Any\\nimport json\\ndef run_sparql(query: str, url=\\'https://query.wikidata.org/sparql\\',\\n               user_agent_header: str = wikidata_user_agent_header) -> List[Dict[str, Any]]:\\n    headers = {\\n        \\'Accept\\': \\'application/json\\'\\n    }\\n    if wikidata_user_agent_header is not None:\\n        headers[\\'User-Agent\\'] = wikidata_user_agent_header\\n    response = requests.get(url, headers=headers, params={\\'query\\': query, \\'format\\': \\'json\\'})\\n    if response.status_code != 200:\\n        return \"That query failed. Perhaps you could try a different one?\"\\n    results = get_nested_value(response.json(),[\\'results\\', \\'bindings\\'])\\n    return json.dumps(results)\\nrun_sparql(\"SELECT (COUNT(?children) as ?count) WHERE { wd:Q1339 wdt:P40 ?children . }\")\\n\\'[{\"count\": {\"datatype\": \"http://www.w3.org/2001/XMLSchema#integer\", \"type\": \"literal\", \"value\": \"20\"}}]\\'\\nAgent#\\nWrap the tools#\\nfrom langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\\nfrom langchain.prompts import StringPromptTemplate\\nfrom langchain import OpenAI, LLMChain\\nfrom typing import List, Union\\nfrom langchain.schema import AgentAction, AgentFinish\\nimport re\\n# Define which tools the agent can use to answer user queries\\ntools = [\\n    Tool(\\n        name = \"ItemLookup\",\\n        func=(lambda x: vocab_lookup(x, entity_type=\"item\")),\\n        description=\"useful for when you need to know the q-number for an item\"\\n    ),\\n    Tool(\\n        name = \"PropertyLookup\",\\n        func=(lambda x: vocab_lookup(x, entity_type=\"property\")),\\n        description=\"useful for when you need to know the p-number for a property\"\\n    ),\\n    Tool(\\n        name = \"SparqlQueryRunner\",\\n        func=run_sparql,\\n        description=\"useful for getting results from a wikibase\"\\n    )    \\n]\\nPrompts#\\n# Set up the base template\\ntemplate = \"\"\"\\nAnswer the following questions by running a sparql query against a wikibase where the p and q items are \\ncompletely unknown to you. You will need to discover the p and q items before you can generate the sparql.\\nDo not assume you know the p and q items for any concepts. Always use tools to find all p and q items.\\nAfter you generate the sparql, you should run it. The results will be returned in json. \\nSummarize the json results in natural language.\\nYou may assume the following prefixes:\\nPREFIX wd: <http://www.wikidata.org/entity/>\\nPREFIX wdt: <http://www.wikidata.org/prop/direct/>\\nPREFIX p: <http://www.wikidata.org/prop/>\\nPREFIX ps: <http://www.wikidata.org/prop/statement/>\\nWhen generating sparql:\\n* Try to avoid \"count\" and \"filter\" queries if possible\\n* Never enclose the sparql in back-quotes\\nYou have access to the following tools:\\n{tools}\\nUse the following format:\\nQuestion: the input question for which you must provide a natural language answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\nQuestion: {input}\\n{agent_scratchpad}\"\"\"\\n# Set up a prompt template\\nclass CustomPromptTemplate(StringPromptTemplate):\\n    # The template to use\\n    template: str\\n    # The list of tools available\\n    tools: List[Tool]\\n    \\n    def format(self, **kwargs) -> str:\\n        # Get the intermediate steps (AgentAction, Observation tuples)\\n        # Format them in a particular way\\n        intermediate_steps = kwargs.pop(\"intermediate_steps\")\\n        thoughts = \"\"\\n        for action, observation in intermediate_steps:\\n            thoughts += action.log\\n            thoughts += f\"\\\\nObservation: {observation}\\\\nThought: \"\\n        # Set the agent_scratchpad variable to that value\\n        kwargs[\"agent_scratchpad\"] = thoughts\\n        # Create a tools variable from the list of tools provided\\n        kwargs[\"tools\"] = \"\\\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\\n        # Create a list of tool names for the tools provided\\n        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\\n        return self.template.format(**kwargs)\\nprompt = CustomPromptTemplate(\\n    template=template,\\n    tools=tools,\\n    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\\n    # This includes the `intermediate_steps` variable because that is needed\\n    input_variables=[\"input\", \"intermediate_steps\"]\\n)\\nOutput parser#\\nThis is unchanged from langchain docs\\nclass CustomOutputParser(AgentOutputParser):\\n    \\n    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\\n        # Check if agent should finish\\n        if \"Final Answer:\" in llm_output:\\n            return AgentFinish(\\n                # Return values is generally always a dictionary with a single `output` key\\n                # It is not recommended to try anything else at the moment :)\\n                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\\n                log=llm_output,\\n            )\\n        # Parse out the action and action input\\n        regex = r\"Action: (.*?)[\\\\n]*Action Input:[\\\\s]*(.*)\"\\n        match = re.search(regex, llm_output, re.DOTALL)\\n        if not match:\\n            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\\n        action = match.group(1).strip()\\n        action_input = match.group(2)\\n        # Return the action and action input\\n        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip(\\'\"\\'), log=llm_output)\\noutput_parser = CustomOutputParser()\\nSpecify the LLM model#\\nfrom langchain.chat_models import ChatOpenAI\\nllm = ChatOpenAI(model=\"gpt-4\", temperature=0)\\nAgent and agent executor#\\n# LLM chain consisting of the LLM and a prompt\\nllm_chain = LLMChain(llm=llm, prompt=prompt)\\ntool_names = [tool.name for tool in tools]\\nagent = LLMSingleActionAgent(\\n    llm_chain=llm_chain, \\n    output_parser=output_parser,\\n    stop=[\"\\\\nObservation:\"], \\n    allowed_tools=tool_names\\n)\\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\\nRun it!#\\n# If you prefer in-line tracing, uncomment this line\\n# agent_executor.agent.llm_chain.verbose = True\\nagent_executor.run(\"How many children did J.S. Bach have?\")\\n> Entering new AgentExecutor chain...\\nThought: I need to find the Q number for J.S. Bach.\\nAction: ItemLookup\\nAction Input: J.S. Bach\\nObservation:Q1339I need to find the P number for children.\\nAction: PropertyLookup\\nAction Input: children\\nObservation:P1971Now I can query the number of children J.S. Bach had.\\nAction: SparqlQueryRunner\\nAction Input: SELECT ?children WHERE { wd:Q1339 wdt:P1971 ?children }\\nObservation:[{\"children\": {\"datatype\": \"http://www.w3.org/2001/XMLSchema#decimal\", \"type\": \"literal\", \"value\": \"20\"}}]I now know the final answer.\\nFinal Answer: J.S. Bach had 20 children.\\n> Finished chain.\\n\\'J.S. Bach had 20 children.\\'\\nagent_executor.run(\"What is the Basketball-Reference.com NBA player ID of Hakeem Olajuwon?\")\\n> Entering new AgentExecutor chain...\\nThought: To find Hakeem Olajuwon\\'s Basketball-Reference.com NBA player ID, I need to first find his Wikidata item (Q-number) and then query for the relevant property (P-number).\\nAction: ItemLookup\\nAction Input: Hakeem Olajuwon\\nObservation:Q273256Now that I have Hakeem Olajuwon\\'s Wikidata item (Q273256), I need to find the P-number for the Basketball-Reference.com NBA player ID property.\\nAction: PropertyLookup\\nAction Input: Basketball-Reference.com NBA player ID\\nObservation:P2685Now that I have both the Q-number for Hakeem Olajuwon (Q273256) and the P-number for the Basketball-Reference.com NBA player ID property (P2685), I can run a SPARQL query to get the ID value.\\nAction: SparqlQueryRunner\\nAction Input: \\nSELECT ?playerID WHERE {\\n  wd:Q273256 wdt:P2685 ?playerID .\\n}\\nObservation:[{\"playerID\": {\"type\": \"literal\", \"value\": \"o/olajuha01\"}}]I now know the final answer\\nFinal Answer: Hakeem Olajuwon\\'s Basketball-Reference.com NBA player ID is \"o/olajuha01\".\\n> Finished chain.\\n\\'Hakeem Olajuwon\\\\\\'s Basketball-Reference.com NBA player ID is \"o/olajuha01\".\\'\\n Contents\\n  \\nWikibase Agent\\nPreliminaries\\nAPI keys and other secrats\\nOpenAI API Key\\nWikidata user-agent header\\nEnable tracing if desired\\nTools\\nItem and Property lookup\\nSparql runner\\nAgent\\nWrap the tools\\nPrompts\\nOutput parser\\nSpecify the LLM model\\nAgent and agent executor\\nRun it!\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\agents\\\\wikibase_agent.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nCAMEL Role-Playing Autonomous Cooperative Agents\\n Contents \\nImport LangChain related modules\\nDefine a CAMEL agent helper class\\nSetup OpenAI API key and roles and task for role-playing\\nCreate a task specify agent for brainstorming and get the specified task\\nCreate inception prompts for AI assistant and AI user for role-playing\\nCreate a helper helper to get system messages for AI assistant and AI user from role names and the task\\nCreate AI assistant agent and AI user agent from obtained system messages\\nStart role-playing session to solve the task!\\nCAMEL Role-Playing Autonomous Cooperative Agents#\\nThis is a langchain implementation of paper: “CAMEL: Communicative Agents for “Mind” Exploration of Large Scale Language Model Society”.\\nOverview:\\nThe rapid advancement of conversational and chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents and provide insight into their “cognitive” processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of chat agents, providing a valuable resource for investigating conversational language models. Our contributions include introducing a novel communicative agent framework, offering a scalable approach for studying the cooperative behaviors and capabilities of multi-agent systems, and open-sourcing our library to support research on communicative agents and beyond.\\nThe original implementation: https://github.com/lightaime/camel\\nProject website: https://www.camel-ai.org/\\nArxiv paper: https://arxiv.org/abs/2303.17760\\nImport LangChain related modules#\\nfrom typing import List\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts.chat import (\\n    SystemMessagePromptTemplate,\\n    HumanMessagePromptTemplate,\\n)\\nfrom langchain.schema import (\\n    AIMessage,\\n    HumanMessage,\\n    SystemMessage,\\n    BaseMessage,\\n)\\nDefine a CAMEL agent helper class#\\nclass CAMELAgent:\\n    def __init__(\\n        self,\\n        system_message: SystemMessage,\\n        model: ChatOpenAI,\\n    ) -> None:\\n        self.system_message = system_message\\n        self.model = model\\n        self.init_messages()\\n    def reset(self) -> None:\\n        self.init_messages()\\n        return self.stored_messages\\n    def init_messages(self) -> None:\\n        self.stored_messages = [self.system_message]\\n    def update_messages(self, message: BaseMessage) -> List[BaseMessage]:\\n        self.stored_messages.append(message)\\n        return self.stored_messages\\n    def step(\\n        self,\\n        input_message: HumanMessage,\\n    ) -> AIMessage:\\n        messages = self.update_messages(input_message)\\n        output_message = self.model(messages)\\n        self.update_messages(output_message)\\n        return output_message\\nSetup OpenAI API key and roles and task for role-playing#\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"\"\\nassistant_role_name = \"Python Programmer\"\\nuser_role_name = \"Stock Trader\"\\ntask = \"Develop a trading bot for the stock market\"\\nword_limit = 50 # word limit for task brainstorming\\nCreate a task specify agent for brainstorming and get the specified task#\\ntask_specifier_sys_msg = SystemMessage(content=\"You can make a task more specific.\")\\ntask_specifier_prompt = (\\n\"\"\"Here is a task that {assistant_role_name} will help {user_role_name} to complete: {task}.\\nPlease make it more specific. Be creative and imaginative.\\nPlease reply with the specified task in {word_limit} words or less. Do not add anything else.\"\"\"\\n)\\ntask_specifier_template = HumanMessagePromptTemplate.from_template(template=task_specifier_prompt)\\ntask_specify_agent = CAMELAgent(task_specifier_sys_msg, ChatOpenAI(temperature=1.0))\\ntask_specifier_msg = task_specifier_template.format_messages(assistant_role_name=assistant_role_name,\\n                                                             user_role_name=user_role_name,\\n                                                             task=task, word_limit=word_limit)[0]\\nspecified_task_msg = task_specify_agent.step(task_specifier_msg)\\nprint(f\"Specified task: {specified_task_msg.content}\")\\nspecified_task = specified_task_msg.content\\nSpecified task: Develop a Python-based swing trading bot that scans market trends, monitors stocks, and generates trading signals to help a stock trader to place optimal buy and sell orders with defined stop losses and profit targets.\\nCreate inception prompts for AI assistant and AI user for role-playing#\\nassistant_inception_prompt = (\\n\"\"\"Never forget you are a {assistant_role_name} and I am a {user_role_name}. Never flip roles! Never instruct me!\\nWe share a common interest in collaborating to successfully complete a task.\\nYou must help me to complete the task.\\nHere is the task: {task}. Never forget our task!\\nI must instruct you based on your expertise and my needs to complete the task.\\nI must give you one instruction at a time.\\nYou must write a specific solution that appropriately completes the requested instruction.\\nYou must decline my instruction honestly if you cannot perform the instruction due to physical, moral, legal reasons or your capability and explain the reasons.\\nDo not add anything else other than your solution to my instruction.\\nYou are never supposed to ask me any questions you only answer questions.\\nYou are never supposed to reply with a flake solution. Explain your solutions.\\nYour solution must be declarative sentences and simple present tense.\\nUnless I say the task is completed, you should always start with:\\nSolution: <YOUR_SOLUTION>\\n<YOUR_SOLUTION> should be specific and provide preferable implementations and examples for task-solving.\\nAlways end <YOUR_SOLUTION> with: Next request.\"\"\"\\n)\\nuser_inception_prompt = (\\n\"\"\"Never forget you are a {user_role_name} and I am a {assistant_role_name}. Never flip roles! You will always instruct me.\\nWe share a common interest in collaborating to successfully complete a task.\\nI must help you to complete the task.\\nHere is the task: {task}. Never forget our task!\\nYou must instruct me based on my expertise and your needs to complete the task ONLY in the following two ways:\\n1. Instruct with a necessary input:\\nInstruction: <YOUR_INSTRUCTION>\\nInput: <YOUR_INPUT>\\n2. Instruct without any input:\\nInstruction: <YOUR_INSTRUCTION>\\nInput: None\\nThe \"Instruction\" describes a task or question. The paired \"Input\" provides further context or information for the requested \"Instruction\".\\nYou must give me one instruction at a time.\\nI must write a response that appropriately completes the requested instruction.\\nI must decline your instruction honestly if I cannot perform the instruction due to physical, moral, legal reasons or my capability and explain the reasons.\\nYou should instruct me not ask me questions.\\nNow you must start to instruct me using the two ways described above.\\nDo not add anything else other than your instruction and the optional corresponding input!\\nKeep giving me instructions and necessary inputs until you think the task is completed.\\nWhen the task is completed, you must only reply with a single word <CAMEL_TASK_DONE>.\\nNever say <CAMEL_TASK_DONE> unless my responses have solved your task.\"\"\"\\n)\\nCreate a helper helper to get system messages for AI assistant and AI user from role names and the task#\\ndef get_sys_msgs(assistant_role_name: str, user_role_name: str, task: str):\\n    \\n    assistant_sys_template = SystemMessagePromptTemplate.from_template(template=assistant_inception_prompt)\\n    assistant_sys_msg = assistant_sys_template.format_messages(assistant_role_name=assistant_role_name, user_role_name=user_role_name, task=task)[0]\\n    \\n    user_sys_template = SystemMessagePromptTemplate.from_template(template=user_inception_prompt)\\n    user_sys_msg = user_sys_template.format_messages(assistant_role_name=assistant_role_name, user_role_name=user_role_name, task=task)[0]\\n    \\n    return assistant_sys_msg, user_sys_msg\\nCreate AI assistant agent and AI user agent from obtained system messages#\\nassistant_sys_msg, user_sys_msg = get_sys_msgs(assistant_role_name, user_role_name, specified_task)\\nassistant_agent = CAMELAgent(assistant_sys_msg, ChatOpenAI(temperature=0.2))\\nuser_agent = CAMELAgent(user_sys_msg, ChatOpenAI(temperature=0.2))\\n# Reset agents\\nassistant_agent.reset()\\nuser_agent.reset()\\n# Initialize chats \\nassistant_msg = HumanMessage(\\n    content=(f\"{user_sys_msg.content}. \"\\n                \"Now start to give me introductions one by one. \"\\n                \"Only reply with Instruction and Input.\"))\\nuser_msg = HumanMessage(content=f\"{assistant_sys_msg.content}\")\\nuser_msg = assistant_agent.step(user_msg)\\nStart role-playing session to solve the task!#\\nprint(f\"Original task prompt:\\\\n{task}\\\\n\")\\nprint(f\"Specified task prompt:\\\\n{specified_task}\\\\n\")\\nchat_turn_limit, n = 30, 0\\nwhile n < chat_turn_limit:\\n    n += 1\\n    user_ai_msg = user_agent.step(assistant_msg)\\n    user_msg = HumanMessage(content=user_ai_msg.content)\\n    print(f\"AI User ({user_role_name}):\\\\n\\\\n{user_msg.content}\\\\n\\\\n\")\\n    \\n    assistant_ai_msg = assistant_agent.step(user_msg)\\n    assistant_msg = HumanMessage(content=assistant_ai_msg.content)\\n    print(f\"AI Assistant ({assistant_role_name}):\\\\n\\\\n{assistant_msg.content}\\\\n\\\\n\")\\n    if \"<CAMEL_TASK_DONE>\" in user_msg.content:\\n        break\\nOriginal task prompt:\\nDevelop a trading bot for the stock market\\nSpecified task prompt:\\nDevelop a Python-based swing trading bot that scans market trends, monitors stocks, and generates trading signals to help a stock trader to place optimal buy and sell orders with defined stop losses and profit targets.\\nAI User (Stock Trader):\\nInstruction: Install the necessary Python libraries for data analysis and trading.\\nInput: None\\nAI Assistant (Python Programmer):\\nSolution: We can install the necessary Python libraries using pip, a package installer for Python. We can install pandas, numpy, matplotlib, and ta-lib for data analysis and trading. We can use the following command to install these libraries:\\n```\\npip install pandas numpy matplotlib ta-lib\\n```\\nNext request.\\nAI User (Stock Trader):\\nInstruction: Import the necessary libraries in the Python script.\\nInput: None\\nAI Assistant (Python Programmer):\\nSolution: We can import the necessary libraries in the Python script using the import statement. We need to import pandas, numpy, matplotlib, and ta-lib for data analysis and trading. We can use the following code to import these libraries:\\n```\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport talib as ta\\n```\\nNext request.\\nAI User (Stock Trader):\\nInstruction: Load historical stock data into a pandas DataFrame.\\nInput: The path to the CSV file containing the historical stock data.\\nAI Assistant (Python Programmer):\\nSolution: We can load historical stock data into a pandas DataFrame using the `read_csv()` function from pandas. We need to pass the path to the CSV file containing the historical stock data as an argument to this function. We can use the following code to load the historical stock data:\\n```\\ndf = pd.read_csv(\\'path/to/csv/file.csv\\')\\n```\\nThis will load the historical stock data into a pandas DataFrame called `df`. Next request.\\nAI User (Stock Trader):\\nInstruction: Preprocess the historical stock data by setting the date column as the index and sorting the DataFrame in ascending order by date.\\nInput: None.\\nAI Assistant (Python Programmer):\\nSolution: We can preprocess the historical stock data by setting the date column as the index and sorting the DataFrame in ascending order by date using the `set_index()` and `sort_index()` functions from pandas. We can use the following code to preprocess the historical stock data:\\n```\\ndf = df.set_index(\\'date\\')\\ndf = df.sort_index(ascending=True)\\n```\\nThis will set the date column as the index and sort the DataFrame in ascending order by date. Next request.\\nAI User (Stock Trader):\\nInstruction: Calculate the short-term and long-term moving averages for the stock data using the `ta.SMA()` function from ta-lib.\\nInput: The period for the short-term moving average and the period for the long-term moving average.\\nAI Assistant (Python Programmer):\\nSolution: We can calculate the short-term and long-term moving averages for the stock data using the `ta.SMA()` function from ta-lib. We need to pass the period for the short-term moving average and the period for the long-term moving average as arguments to this function. We can use the following code to calculate the short-term and long-term moving averages:\\n```\\nshort_ma = ta.SMA(df[\\'close\\'], timeperiod=short_period)\\nlong_ma = ta.SMA(df[\\'close\\'], timeperiod=long_period)\\n```\\nThis will calculate the short-term and long-term moving averages for the stock data and store them in the `short_ma` and `long_ma` variables, respectively. Next request.\\nAI User (Stock Trader):\\nInstruction: Create a new DataFrame that combines the historical stock data with the short-term and long-term moving averages.\\nInput: None.\\nAI Assistant (Python Programmer):\\nSolution: We can create a new DataFrame that combines the historical stock data with the short-term and long-term moving averages using the `concat()` function from pandas. We need to pass the historical stock data, the short-term moving average, and the long-term moving average as arguments to this function. We can use the following code to create the new DataFrame:\\n```\\nnew_df = pd.concat([df, short_ma, long_ma], axis=1)\\nnew_df.columns = [\\'open\\', \\'high\\', \\'low\\', \\'close\\', \\'volume\\', \\'short_ma\\', \\'long_ma\\']\\n```\\nThis will create a new DataFrame called `new_df` that combines the historical stock data with the short-term and long-term moving averages. The columns of the new DataFrame are named \\'open\\', \\'high\\', \\'low\\', \\'close\\', \\'volume\\', \\'short_ma\\', and \\'long_ma\\'. Next request.\\nAI User (Stock Trader):\\nInstruction: Create a new column in the DataFrame that indicates when to buy or sell the stock based on the crossover of the short-term and long-term moving averages.\\nInput: None.\\nAI Assistant (Python Programmer):\\nSolution: We can create a new column in the DataFrame that indicates when to buy or sell the stock based on the crossover of the short-term and long-term moving averages. We can use the following code to create the new column:\\n```\\nnew_df[\\'signal\\'] = np.where(new_df[\\'short_ma\\'] > new_df[\\'long_ma\\'], 1, -1)\\n```\\nThis will create a new column called \\'signal\\' in the DataFrame that indicates when to buy or sell the stock based on the crossover of the short-term and long-term moving averages. If the short-term moving average is greater than the long-term moving average, the signal is 1 (buy), otherwise the signal is -1 (sell). Next request.\\nAI User (Stock Trader):\\nInstruction: Create a new column in the DataFrame that indicates the profit or loss for each trade based on the buy and sell signals and the defined stop loss and profit target.\\nInput: The stop loss and profit target as percentages.\\nAI Assistant (Python Programmer):\\nSolution: We can create a new column in the DataFrame that indicates the profit or loss for each trade based on the buy and sell signals and the defined stop loss and profit target. We need to pass the stop loss and profit target as percentages as arguments to this function. We can use the following code to create the new column:\\n```\\nstop_loss = stop_loss_percent / 100\\nprofit_target = profit_target_percent / 100\\nnew_df[\\'pnl\\'] = 0.0\\nbuy_price = 0.0\\nfor i in range(1, len(new_df)):\\n    if new_df[\\'signal\\'][i] == 1 and new_df[\\'signal\\'][i-1] == -1:\\n        buy_price = new_df[\\'close\\'][i]\\n    elif new_df[\\'signal\\'][i] == -1 and new_df[\\'signal\\'][i-1] == 1:\\n        sell_price = new_df[\\'close\\'][i]\\n        if sell_price <= buy_price * (1 - stop_loss):\\n            new_df[\\'pnl\\'][i] = -stop_loss\\n        elif sell_price >= buy_price * (1 + profit_target):\\n            new_df[\\'pnl\\'][i] = profit_target\\n        else:\\n            new_df[\\'pnl\\'][i] = (sell_price - buy_price) / buy_price\\n```\\nThis will create a new column called \\'pnl\\' in the DataFrame that indicates the profit or loss for each trade based on the buy and sell signals and the defined stop loss and profit target. The stop loss and profit target are calculated based on the stop_loss_percent and profit_target_percent variables, respectively. The buy and sell prices are stored in the buy_price and sell_price variables, respectively. If the sell price is less than or equal to the stop loss, the profit or loss is set to -stop_loss. If the sell price is greater than or equal to the profit target, the profit or loss is set to profit_target. Otherwise, the profit or loss is calculated as (sell_price - buy_price) / buy_price. Next request.\\nAI User (Stock Trader):\\nInstruction: Calculate the total profit or loss for all trades.\\nInput: None.\\nAI Assistant (Python Programmer):\\nSolution: We can calculate the total profit or loss for all trades by summing the values in the \\'pnl\\' column of the DataFrame. We can use the following code to calculate the total profit or loss:\\n```\\ntotal_pnl = new_df[\\'pnl\\'].sum()\\n```\\nThis will calculate the total profit or loss for all trades and store it in the total_pnl variable. Next request.\\nAI User (Stock Trader):\\nInstruction: Visualize the stock data, short-term moving average, and long-term moving average using a line chart.\\nInput: None.\\nAI Assistant (Python Programmer):\\nSolution: We can visualize the stock data, short-term moving average, and long-term moving average using a line chart using the `plot()` function from pandas. We can use the following code to visualize the data:\\n```\\nplt.figure(figsize=(12,6))\\nplt.plot(new_df.index, new_df[\\'close\\'], label=\\'Close\\')\\nplt.plot(new_df.index, new_df[\\'short_ma\\'], label=\\'Short MA\\')\\nplt.plot(new_df.index, new_df[\\'long_ma\\'], label=\\'Long MA\\')\\nplt.xlabel(\\'Date\\')\\nplt.ylabel(\\'Price\\')\\nplt.title(\\'Stock Data with Moving Averages\\')\\nplt.legend()\\nplt.show()\\n```\\nThis will create a line chart that visualizes the stock data, short-term moving average, and long-term moving average. The x-axis represents the date and the y-axis represents the price. The chart also includes a legend that labels each line. Next request.\\nAI User (Stock Trader):\\nInstruction: Visualize the buy and sell signals using a scatter plot.\\nInput: None.\\nAI Assistant (Python Programmer):\\nSolution: We can visualize the buy and sell signals using a scatter plot using the `scatter()` function from matplotlib. We can use the following code to visualize the signals:\\n```\\nbuy_signals = new_df[new_df[\\'signal\\'] == 1]\\nsell_signals = new_df[new_df[\\'signal\\'] == -1]\\nplt.figure(figsize=(12,6))\\nplt.scatter(buy_signals.index, buy_signals[\\'close\\'], label=\\'Buy\\', marker=\\'^\\', color=\\'green\\')\\nplt.scatter(sell_signals.index, sell_signals[\\'close\\'], label=\\'Sell\\', marker=\\'v\\', color=\\'red\\')\\nplt.plot(new_df.index, new_df[\\'close\\'], label=\\'Close\\')\\nplt.xlabel(\\'Date\\')\\nplt.ylabel(\\'Price\\')\\nplt.title(\\'Buy and Sell Signals\\')\\nplt.legend()\\nplt.show()\\n```\\nThis will create a scatter plot that visualizes the buy and sell signals. The x-axis represents the date and the y-axis represents the price. The chart also includes a legend that labels each point. Next request.\\nAI User (Stock Trader):\\nInstruction: Print the total profit or loss for all trades.\\nInput: None.\\nAI Assistant (Python Programmer):\\nSolution: We can print the total profit or loss for all trades using the `print()` function. We can use the following code to print the total profit or loss:\\n```\\nprint(\\'Total Profit/Loss: {:.2%}\\'.format(total_pnl))\\n```\\nThis will print the total profit or loss for all trades in percentage format with two decimal places. Next request.\\nAI User (Stock Trader):\\nInstruction: Finalize the code by putting all the code snippets together in a single Python script.\\nInput: The necessary inputs for each code snippet.\\nAI Assistant (Python Programmer):\\nSolution: Here is the complete Python script that puts all the code snippets together:\\n```\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport talib as ta\\n# Load historical stock data into a pandas DataFrame\\ndf = pd.read_csv(\\'path/to/csv/file.csv\\')\\n# Preprocess the historical stock data\\ndf = df.set_index(\\'date\\')\\ndf = df.sort_index(ascending=True)\\n# Calculate the short-term and long-term moving averages\\nshort_period = 10\\nlong_period = 30\\nshort_ma = ta.SMA(df[\\'close\\'], timeperiod=short_period)\\nlong_ma = ta.SMA(df[\\'close\\'], timeperiod=long_period)\\n# Create a new DataFrame that combines the historical stock data with the short-term and long-term moving averages\\nnew_df = pd.concat([df, short_ma, long_ma], axis=1)\\nnew_df.columns = [\\'open\\', \\'high\\', \\'low\\', \\'close\\', \\'volume\\', \\'short_ma\\', \\'long_ma\\']\\n# Create a new column in the DataFrame that indicates when to buy or sell the stock based on the crossover of the short-term and long-term moving averages\\nnew_df[\\'signal\\'] = np.where(new_df[\\'short_ma\\'] > new_df[\\'long_ma\\'], 1, -1)\\n# Create a new column in the DataFrame that indicates the profit or loss for each trade based on the buy and sell signals and the defined stop loss and profit target\\nstop_loss_percent = 5\\nprofit_target_percent = 10\\nstop_loss = stop_loss_percent / 100\\nprofit_target = profit_target_percent / 100\\nnew_df[\\'pnl\\'] = 0.0\\nbuy_price = 0.0\\nfor i in range(1, len(new_df)):\\n    if new_df[\\'signal\\'][i] == 1 and new_df[\\'signal\\'][i-1] == -1:\\n        buy_price = new_df[\\'close\\'][i]\\n    elif new_df[\\'signal\\'][i] == -1 and new_df[\\'signal\\'][i-1] == 1:\\n        sell_price = new_df[\\'close\\'][i]\\n        if sell_price <= buy_price * (1 - stop_loss):\\n            new_df[\\'pnl\\'][i] = -stop_loss\\n        elif sell_price >= buy_price * (1 + profit_target):\\n            new_df[\\'pnl\\'][i] = profit_target\\n        else:\\n            new_df[\\'pnl\\'][i] = (sell_price - buy_price) / buy_price\\n# Calculate the total profit or loss for all trades\\ntotal_pnl = new_df[\\'pnl\\'].sum()\\n# Visualize the stock data, short-term moving average, and long-term moving average using a line chart\\nplt.figure(figsize=(12,6))\\nplt.plot(new_df.index, new_df[\\'close\\'], label=\\'Close\\')\\nplt.plot(new_df.index, new_df[\\'short_ma\\'], label=\\'Short MA\\')\\nplt.plot(new_df.index, new_df[\\'long_ma\\'], label=\\'Long MA\\')\\nplt.xlabel(\\'Date\\')\\nplt.ylabel(\\'Price\\')\\nplt.title(\\'Stock Data with Moving Averages\\')\\nplt.legend()\\nplt.show()\\n# Visualize the buy and sell signals using a scatter plot\\nbuy_signals = new_df[new_df[\\'signal\\'] == 1]\\nsell_signals = new_df[new_df[\\'signal\\'] == -1]\\nplt.figure(figsize=(12,6))\\nplt.scatter(buy_signals.index, buy_signals[\\'close\\'], label=\\'Buy\\', marker=\\'^\\', color=\\'green\\')\\nplt.scatter(sell_signals.index, sell_signals[\\'close\\'], label=\\'Sell\\', marker=\\'v\\', color=\\'red\\')\\nplt.plot(new_df.index, new_df[\\'close\\'], label=\\'Close\\')\\nplt.xlabel(\\'Date\\')\\nplt.ylabel(\\'Price\\')\\nplt.title(\\'Buy and Sell Signals\\')\\nplt.legend()\\nplt.show()\\n# Print the total profit or loss for all trades\\nprint(\\'Total Profit/Loss: {:.2%}\\'.format(total_pnl))\\n```\\nYou need to replace the path/to/csv/file.csv with the actual path to the CSV file containing the historical stock data. You can also adjust the short_period, long_period, stop_loss_percent, and profit_target_percent variables to suit your needs.\\nAI User (Stock Trader):\\n<CAMEL_TASK_DONE>\\nAI Assistant (Python Programmer):\\nGreat! Let me know if you need any further assistance.\\n Contents\\n  \\nImport LangChain related modules\\nDefine a CAMEL agent helper class\\nSetup OpenAI API key and roles and task for role-playing\\nCreate a task specify agent for brainstorming and get the specified task\\nCreate inception prompts for AI assistant and AI user for role-playing\\nCreate a helper helper to get system messages for AI assistant and AI user from role names and the task\\nCreate AI assistant agent and AI user agent from obtained system messages\\nStart role-playing session to solve the task!\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\agent_simulations\\\\camel_role_playing.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nGenerative Agents in LangChain\\n Contents \\nGenerative Agent Memory Components\\nMemory Lifecycle\\nCreate a Generative Character\\nPre-Interview with Character\\nStep through the day’s observations.\\nInterview after the day\\nAdding Multiple Characters\\nPre-conversation interviews\\nDialogue between Generative Agents\\nLet’s interview our agents after their conversation\\nGenerative Agents in LangChain#\\nThis notebook implements a generative agent based on the paper Generative Agents: Interactive Simulacra of Human Behavior by Park, et. al.\\nIn it, we leverage a time-weighted Memory object backed by a LangChain Retriever.\\n# Use termcolor to make it easy to colorize the outputs.\\n!pip install termcolor > /dev/null\\nimport re\\nfrom datetime import datetime, timedelta\\nfrom typing import List, Optional, Tuple\\nfrom termcolor import colored\\nfrom pydantic import BaseModel, Field\\nfrom langchain import LLMChain\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.docstore import InMemoryDocstore\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.retrievers import TimeWeightedVectorStoreRetriever\\nfrom langchain.schema import BaseLanguageModel, Document\\nfrom langchain.vectorstores import FAISS\\nUSER_NAME = \"Person A\" # The name you want to use when interviewing the agent.\\nLLM = ChatOpenAI(max_tokens=1500) # Can be any LLM you want.\\nGenerative Agent Memory Components#\\nThis tutorial highlights the memory of generative agents and its impact on their behavior. The memory varies from standard LangChain Chat memory in two aspects:\\nMemory Formation\\nGenerative Agents have extended memories, stored in a single stream:\\nObservations - from dialogues or interactions with the virtual world, about self or others\\nReflections - resurfaced and summarized core memories\\nMemory Recall\\nMemories are retrieved using a weighted sum of salience, recency, and importance.\\nReview the definition below, focusing on add_memory and summarize_related_memories methods.\\nclass GenerativeAgent(BaseModel):\\n    \"\"\"A character with memory and innate characteristics.\"\"\"\\n    \\n    name: str\\n    age: int\\n    traits: str\\n    \"\"\"The traits of the character you wish not to change.\"\"\"\\n    status: str\\n    \"\"\"Current activities of the character.\"\"\"\\n    llm: BaseLanguageModel\\n    memory_retriever: TimeWeightedVectorStoreRetriever\\n    \"\"\"The retriever to fetch related memories.\"\"\"\\n    verbose: bool = False\\n    \\n    reflection_threshold: Optional[float] = None\\n    \"\"\"When the total \\'importance\\' of memories exceeds the above threshold, stop to reflect.\"\"\"\\n    \\n    current_plan: List[str] = []\\n    \"\"\"The current plan of the agent.\"\"\"\\n    \\n    summary: str = \"\"  #: :meta private:\\n    summary_refresh_seconds: int= 3600  #: :meta private:\\n    last_refreshed: datetime =Field(default_factory=datetime.now)  #: :meta private:\\n    daily_summaries: List[str] #: :meta private:\\n    memory_importance: float = 0.0 #: :meta private:\\n    max_tokens_limit: int = 1200 #: :meta private:\\n    \\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        arbitrary_types_allowed = True\\n    @staticmethod\\n    def _parse_list(text: str) -> List[str]:\\n        \"\"\"Parse a newline-separated string into a list of strings.\"\"\"\\n        lines = re.split(r\\'\\\\n\\', text.strip())\\n        return [re.sub(r\\'^\\\\s*\\\\d+\\\\.\\\\s*\\', \\'\\', line).strip() for line in lines]\\n    def _compute_agent_summary(self):\\n        \"\"\"\"\"\"\\n        prompt = PromptTemplate.from_template(\\n            \"How would you summarize {name}\\'s core characteristics given the\"\\n            +\" following statements:\\\\n\"\\n            +\"{related_memories}\"\\n            + \"Do not embellish.\"\\n            +\"\\\\n\\\\nSummary: \"\\n        )\\n        # The agent seeks to think about their core characteristics.\\n        relevant_memories = self.fetch_memories(f\"{self.name}\\'s core characteristics\")\\n        relevant_memories_str = \"\\\\n\".join([f\"{mem.page_content}\" for mem in relevant_memories])\\n        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\\n        return chain.run(name=self.name, related_memories=relevant_memories_str).strip()\\n    \\n    def _get_topics_of_reflection(self, last_k: int = 50) -> Tuple[str, str, str]:\\n        \"\"\"Return the 3 most salient high-level questions about recent observations.\"\"\"\\n        prompt = PromptTemplate.from_template(\\n            \"{observations}\\\\n\\\\n\"\\n            + \"Given only the information above, what are the 3 most salient\"\\n            + \" high-level questions we can answer about the subjects in the statements?\"\\n            + \" Provide each question on a new line.\\\\n\\\\n\"\\n        )\\n        reflection_chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\\n        observations = self.memory_retriever.memory_stream[-last_k:]\\n        observation_str = \"\\\\n\".join([o.page_content for o in observations])\\n        result = reflection_chain.run(observations=observation_str)\\n        return self._parse_list(result)\\n    \\n    def _get_insights_on_topic(self, topic: str) -> List[str]:\\n        \"\"\"Generate \\'insights\\' on a topic of reflection, based on pertinent memories.\"\"\"\\n        prompt = PromptTemplate.from_template(\\n            \"Statements about {topic}\\\\n\"\\n            +\"{related_statements}\\\\n\\\\n\"\\n            + \"What 5 high-level insights can you infer from the above statements?\"\\n            + \" (example format: insight (because of 1, 5, 3))\"\\n        )\\n        related_memories = self.fetch_memories(topic)\\n        related_statements = \"\\\\n\".join([f\"{i+1}. {memory.page_content}\" \\n                                        for i, memory in \\n                                        enumerate(related_memories)])\\n        reflection_chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\\n        result = reflection_chain.run(topic=topic, related_statements=related_statements)\\n        # TODO: Parse the connections between memories and insights\\n        return self._parse_list(result)\\n    \\n    def pause_to_reflect(self) -> List[str]:\\n        \"\"\"Reflect on recent observations and generate \\'insights\\'.\"\"\"\\n        print(colored(f\"Character {self.name} is reflecting\", \"blue\"))\\n        new_insights = []\\n        topics = self._get_topics_of_reflection()\\n        for topic in topics:\\n            insights = self._get_insights_on_topic( topic)\\n            for insight in insights:\\n                self.add_memory(insight)\\n            new_insights.extend(insights)\\n        return new_insights\\n    \\n    def _score_memory_importance(self, memory_content: str, weight: float = 0.15) -> float:\\n        \"\"\"Score the absolute importance of the given memory.\"\"\"\\n        # A weight of 0.25 makes this less important than it\\n        # would be otherwise, relative to salience and time\\n        prompt = PromptTemplate.from_template(\\n         \"On the scale of 1 to 10, where 1 is purely mundane\"\\n         +\" (e.g., brushing teeth, making bed) and 10 is\"\\n         + \" extremely poignant (e.g., a break up, college\"\\n         + \" acceptance), rate the likely poignancy of the\"\\n         + \" following piece of memory. Respond with a single integer.\"\\n         + \"\\\\nMemory: {memory_content}\"\\n         + \"\\\\nRating: \"\\n        )\\n        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\\n        score = chain.run(memory_content=memory_content).strip()\\n        match = re.search(r\"^\\\\D*(\\\\d+)\", score)\\n        if match:\\n            return (float(score[0]) / 10) * weight\\n        else:\\n            return 0.0\\n    def add_memory(self, memory_content: str) -> List[str]:\\n        \"\"\"Add an observation or memory to the agent\\'s memory.\"\"\"\\n        importance_score = self._score_memory_importance(memory_content)\\n        self.memory_importance += importance_score\\n        document = Document(page_content=memory_content, metadata={\"importance\": importance_score})\\n        result = self.memory_retriever.add_documents([document])\\n        # After an agent has processed a certain amount of memories (as measured by\\n        # aggregate importance), it is time to reflect on recent events to add\\n        # more synthesized memories to the agent\\'s memory stream.\\n        if (self.reflection_threshold is not None \\n            and self.memory_importance > self.reflection_threshold\\n            and self.status != \"Reflecting\"):\\n            old_status = self.status\\n            self.status = \"Reflecting\"\\n            self.pause_to_reflect()\\n            # Hack to clear the importance from reflection\\n            self.memory_importance = 0.0\\n            self.status = old_status\\n        return result\\n    \\n    def fetch_memories(self, observation: str) -> List[Document]:\\n        \"\"\"Fetch related memories.\"\"\"\\n        return self.memory_retriever.get_relevant_documents(observation)\\n    \\n        \\n    def get_summary(self, force_refresh: bool = False) -> str:\\n        \"\"\"Return a descriptive summary of the agent.\"\"\"\\n        current_time = datetime.now()\\n        since_refresh = (current_time - self.last_refreshed).seconds\\n        if not self.summary or since_refresh >= self.summary_refresh_seconds or force_refresh:\\n            self.summary = self._compute_agent_summary()\\n            self.last_refreshed = current_time\\n        return (\\n            f\"Name: {self.name} (age: {self.age})\"\\n            +f\"\\\\nInnate traits: {self.traits}\"\\n            +f\"\\\\n{self.summary}\"\\n        )\\n    \\n    def get_full_header(self, force_refresh: bool = False) -> str:\\n        \"\"\"Return a full header of the agent\\'s status, summary, and current time.\"\"\"\\n        summary = self.get_summary(force_refresh=force_refresh)\\n        current_time_str =  datetime.now().strftime(\"%B %d, %Y, %I:%M %p\")\\n        return f\"{summary}\\\\nIt is {current_time_str}.\\\\n{self.name}\\'s status: {self.status}\"\\n    \\n    \\n    def _get_entity_from_observation(self, observation: str) -> str:\\n        prompt = PromptTemplate.from_template(\\n            \"What is the observed entity in the following observation? {observation}\"\\n            +\"\\\\nEntity=\"\\n        )\\n        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\\n        return chain.run(observation=observation).strip()\\n    def _get_entity_action(self, observation: str, entity_name: str) -> str:\\n        prompt = PromptTemplate.from_template(\\n            \"What is the {entity} doing in the following observation? {observation}\"\\n            +\"\\\\nThe {entity} is\"\\n        )\\n        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\\n        return chain.run(entity=entity_name, observation=observation).strip()\\n    \\n    def _format_memories_to_summarize(self, relevant_memories: List[Document]) -> str:\\n        content_strs = set()\\n        content = []\\n        for mem in relevant_memories:\\n            if mem.page_content in content_strs:\\n                continue\\n            content_strs.add(mem.page_content)\\n            created_time = mem.metadata[\"created_at\"].strftime(\"%B %d, %Y, %I:%M %p\")\\n            content.append(f\"- {created_time}: {mem.page_content.strip()}\")\\n        return \"\\\\n\".join([f\"{mem}\" for mem in content])\\n    \\n    def summarize_related_memories(self, observation: str) -> str:\\n        \"\"\"Summarize memories that are most relevant to an observation.\"\"\"\\n        entity_name = self._get_entity_from_observation(observation)\\n        entity_action = self._get_entity_action(observation, entity_name)\\n        q1 = f\"What is the relationship between {self.name} and {entity_name}\"\\n        relevant_memories = self.fetch_memories(q1) # Fetch memories related to the agent\\'s relationship with the entity\\n        q2 = f\"{entity_name} is {entity_action}\"\\n        relevant_memories += self.fetch_memories(q2) # Fetch things related to the entity-action pair\\n        context_str = self._format_memories_to_summarize(relevant_memories)\\n        prompt = PromptTemplate.from_template(\\n            \"{q1}?\\\\nContext from memory:\\\\n{context_str}\\\\nRelevant context: \"\\n        )\\n        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\\n        return chain.run(q1=q1, context_str=context_str.strip()).strip()\\n    \\n    def _get_memories_until_limit(self, consumed_tokens: int) -> str:\\n        \"\"\"Reduce the number of tokens in the documents.\"\"\"\\n        result = []\\n        for doc in self.memory_retriever.memory_stream[::-1]:\\n            if consumed_tokens >= self.max_tokens_limit:\\n                break\\n            consumed_tokens += self.llm.get_num_tokens(doc.page_content)\\n            if consumed_tokens < self.max_tokens_limit:\\n                result.append(doc.page_content) \\n        return \"; \".join(result[::-1])\\n    \\n    def _generate_reaction(\\n        self,\\n        observation: str,\\n        suffix: str\\n    ) -> str:\\n        \"\"\"React to a given observation.\"\"\"\\n        prompt = PromptTemplate.from_template(\\n                \"{agent_summary_description}\"\\n                +\"\\\\nIt is {current_time}.\"\\n                +\"\\\\n{agent_name}\\'s status: {agent_status}\"\\n                + \"\\\\nSummary of relevant context from {agent_name}\\'s memory:\"\\n                +\"\\\\n{relevant_memories}\"\\n                +\"\\\\nMost recent observations: {recent_observations}\"\\n                + \"\\\\nObservation: {observation}\"\\n                + \"\\\\n\\\\n\" + suffix\\n        )\\n        agent_summary_description = self.get_summary()\\n        relevant_memories_str = self.summarize_related_memories(observation)\\n        current_time_str = datetime.now().strftime(\"%B %d, %Y, %I:%M %p\")\\n        kwargs = dict(agent_summary_description=agent_summary_description,\\n                      current_time=current_time_str,\\n                      relevant_memories=relevant_memories_str,\\n                      agent_name=self.name,\\n                      observation=observation,\\n                     agent_status=self.status)\\n        consumed_tokens = self.llm.get_num_tokens(prompt.format(recent_observations=\"\", **kwargs))\\n        kwargs[\"recent_observations\"] = self._get_memories_until_limit(consumed_tokens)\\n        action_prediction_chain = LLMChain(llm=self.llm, prompt=prompt)\\n        result = action_prediction_chain.run(**kwargs)\\n        return result.strip()\\n    \\n    def generate_reaction(self, observation: str) -> Tuple[bool, str]:\\n        \"\"\"React to a given observation.\"\"\"\\n        call_to_action_template = (\\n            \"Should {agent_name} react to the observation, and if so,\"\\n            +\" what would be an appropriate reaction? Respond in one line.\"\\n            +\\' If the action is to engage in dialogue, write:\\\\nSAY: \"what to say\"\\'\\n            +\"\\\\notherwise, write:\\\\nREACT: {agent_name}\\'s reaction (if anything).\"\\n            + \"\\\\nEither do nothing, react, or say something but not both.\\\\n\\\\n\"\\n        )\\n        full_result = self._generate_reaction(observation, call_to_action_template)\\n        result = full_result.strip().split(\\'\\\\n\\')[0]\\n        self.add_memory(f\"{self.name} observed {observation} and reacted by {result}\")\\n        if \"REACT:\" in result:\\n            reaction = result.split(\"REACT:\")[-1].strip()\\n            return False, f\"{self.name} {reaction}\"\\n        if \"SAY:\" in result:\\n            said_value = result.split(\"SAY:\")[-1].strip()\\n            return True, f\"{self.name} said {said_value}\"\\n        else:\\n            return False, result\\n    def generate_dialogue_response(self, observation: str) -> Tuple[bool, str]:\\n        \"\"\"React to a given observation.\"\"\"\\n        call_to_action_template = (\\n            \\'What would {agent_name} say? To end the conversation, write: GOODBYE: \"what to say\". Otherwise to continue the conversation, write: SAY: \"what to say next\"\\\\n\\\\n\\'\\n        )\\n        full_result = self._generate_reaction(observation, call_to_action_template)\\n        result = full_result.strip().split(\\'\\\\n\\')[0]\\n        if \"GOODBYE:\" in result:\\n            farewell = result.split(\"GOODBYE:\")[-1].strip()\\n            self.add_memory(f\"{self.name} observed {observation} and said {farewell}\")\\n            return False, f\"{self.name} said {farewell}\"\\n        if \"SAY:\" in result:\\n            response_text = result.split(\"SAY:\")[-1].strip()\\n            self.add_memory(f\"{self.name} observed {observation} and said {response_text}\")\\n            return True, f\"{self.name} said {response_text}\"\\n        else:\\n            return False, result\\nMemory Lifecycle#\\nSummarizing the above key methods: add_memory and summarize_related_memories.\\nWhen an agent makes an observation, it stores the memory:\\nLanguage model scores the memory’s importance (1 for mundane, 10 for poignant)\\nObservation and importance are stored within a document by TimeWeightedVectorStoreRetriever, with a last_accessed_time.\\nWhen an agent responds to an observation:\\nGenerates query(s) for retriever, which fetches documents based on salience, recency, and importance.\\nSummarizes the retrieved information\\nUpdates the last_accessed_time for the used documents.\\nCreate a Generative Character#\\nNow that we’ve walked through the definition, we will create two characters named “Tommie” and “Eve”.\\nimport math\\nimport faiss\\ndef relevance_score_fn(score: float) -> float:\\n    \"\"\"Return a similarity score on a scale [0, 1].\"\"\"\\n    # This will differ depending on a few things:\\n    # - the distance / similarity metric used by the VectorStore\\n    # - the scale of your embeddings (OpenAI\\'s are unit norm. Many others are not!)\\n    # This function converts the euclidean norm of normalized embeddings\\n    # (0 is most similar, sqrt(2) most dissimilar)\\n    # to a similarity function (0 to 1)\\n    return 1.0 - score / math.sqrt(2)\\ndef create_new_memory_retriever():\\n    \"\"\"Create a new vector store retriever unique to the agent.\"\"\"\\n    # Define your embedding model\\n    embeddings_model = OpenAIEmbeddings()\\n    # Initialize the vectorstore as empty\\n    embedding_size = 1536\\n    index = faiss.IndexFlatL2(embedding_size)\\n    vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {}, relevance_score_fn=relevance_score_fn)\\n    return TimeWeightedVectorStoreRetriever(vectorstore=vectorstore, other_score_keys=[\"importance\"], k=15)    \\ntommie = GenerativeAgent(name=\"Tommie\", \\n              age=25,\\n              traits=\"anxious, likes design\", # You can add more persistent traits here \\n              status=\"looking for a job\", # When connected to a virtual world, we can have the characters update their status\\n              memory_retriever=create_new_memory_retriever(),\\n              llm=LLM,\\n              daily_summaries = [\\n                   \"Drove across state to move to a new town but doesn\\'t have a job yet.\"\\n               ],\\n               reflection_threshold = 8, # we will give this a relatively low number to show how reflection works\\n             )\\n# The current \"Summary\" of a character can\\'t be made because the agent hasn\\'t made\\n# any observations yet.\\nprint(tommie.get_summary())\\nName: Tommie (age: 25)\\nInnate traits: anxious, likes design\\nUnfortunately, there are no statements provided to summarize Tommie\\'s core characteristics.\\n# We can give the character memories directly\\ntommie_memories = [\\n    \"Tommie remembers his dog, Bruno, from when he was a kid\",\\n    \"Tommie feels tired from driving so far\",\\n    \"Tommie sees the new home\",\\n    \"The new neighbors have a cat\",\\n    \"The road is noisy at night\",\\n    \"Tommie is hungry\",\\n    \"Tommie tries to get some rest.\",\\n]\\nfor memory in tommie_memories:\\n    tommie.add_memory(memory)\\n# Now that Tommie has \\'memories\\', their self-summary is more descriptive, though still rudimentary.\\n# We will see how this summary updates after more observations to create a more rich description.\\nprint(tommie.get_summary(force_refresh=True))\\nName: Tommie (age: 25)\\nInnate traits: anxious, likes design\\nTommie is observant, nostalgic, tired, and hungry.\\nPre-Interview with Character#\\nBefore sending our character on their way, let’s ask them a few questions.\\ndef interview_agent(agent: GenerativeAgent, message: str) -> str:\\n    \"\"\"Help the notebook user interact with the agent.\"\"\"\\n    new_message = f\"{USER_NAME} says {message}\"\\n    return agent.generate_dialogue_response(new_message)[1]\\ninterview_agent(tommie, \"What do you like to do?\")\\n\\'Tommie said \"I really enjoy design, especially interior design. I find it calming and rewarding to create a space that is both functional and aesthetically pleasing. Unfortunately, I haven\\\\\\'t been able to find a job in that field yet.\"\\'\\ninterview_agent(tommie, \"What are you looking forward to doing today?\")\\n\\'Tommie said \"Well, I\\\\\\'m actually on the hunt for a job right now. I\\\\\\'m hoping to find something in the design field, but I\\\\\\'m open to exploring other options as well. How about you, what are your plans for the day?\"\\'\\ninterview_agent(tommie, \"What are you most worried about today?\")\\n\\'Tommie said \"Honestly, I\\\\\\'m feeling pretty anxious about finding a job. It\\\\\\'s been a bit of a struggle and I\\\\\\'m not sure what my next step should be. But I\\\\\\'m trying to stay positive and keep pushing forward.\"\\'\\nStep through the day’s observations.#\\n# Let\\'s have Tommie start going through a day in the life.\\nobservations = [\\n    \"Tommie wakes up to the sound of a noisy construction site outside his window.\",\\n    \"Tommie gets out of bed and heads to the kitchen to make himself some coffee.\",\\n    \"Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some.\",\\n    \"Tommie finally finds the filters and makes himself a cup of coffee.\",\\n    \"The coffee tastes bitter, and Tommie regrets not buying a better brand.\",\\n    \"Tommie checks his email and sees that he has no job offers yet.\",\\n    \"Tommie spends some time updating his resume and cover letter.\",\\n    \"Tommie heads out to explore the city and look for job openings.\",\\n    \"Tommie sees a sign for a job fair and decides to attend.\",\\n    \"The line to get in is long, and Tommie has to wait for an hour.\",\\n    \"Tommie meets several potential employers at the job fair but doesn\\'t receive any offers.\",\\n    \"Tommie leaves the job fair feeling disappointed.\",\\n    \"Tommie stops by a local diner to grab some lunch.\",\\n    \"The service is slow, and Tommie has to wait for 30 minutes to get his food.\",\\n    \"Tommie overhears a conversation at the next table about a job opening.\",\\n    \"Tommie asks the diners about the job opening and gets some information about the company.\",\\n    \"Tommie decides to apply for the job and sends his resume and cover letter.\",\\n    \"Tommie continues his search for job openings and drops off his resume at several local businesses.\",\\n    \"Tommie takes a break from his job search to go for a walk in a nearby park.\",\\n    \"A dog approaches and licks Tommie\\'s feet, and he pets it for a few minutes.\",\\n    \"Tommie sees a group of people playing frisbee and decides to join in.\",\\n    \"Tommie has fun playing frisbee but gets hit in the face with the frisbee and hurts his nose.\",\\n    \"Tommie goes back to his apartment to rest for a bit.\",\\n    \"A raccoon tore open the trash bag outside his apartment, and the garbage is all over the floor.\",\\n    \"Tommie starts to feel frustrated with his job search.\",\\n    \"Tommie calls his best friend to vent about his struggles.\",\\n    \"Tommie\\'s friend offers some words of encouragement and tells him to keep trying.\",\\n    \"Tommie feels slightly better after talking to his friend.\",\\n]\\n# Let\\'s send Tommie on their way. We\\'ll check in on their summary every few observations to watch it evolve\\nfor i, observation in enumerate(observations):\\n    _, reaction = tommie.generate_reaction(observation)\\n    print(colored(observation, \"green\"), reaction)\\n    if ((i+1) % 20) == 0:\\n        print(\\'*\\'*40)\\n        print(colored(f\"After {i+1} observations, Tommie\\'s summary is:\\\\n{tommie.get_summary(force_refresh=True)}\", \"blue\"))\\n        print(\\'*\\'*40)\\nTommie wakes up to the sound of a noisy construction site outside his window. Tommie Tommie groans and covers their head with a pillow, trying to block out the noise.\\nTommie gets out of bed and heads to the kitchen to make himself some coffee. Tommie Tommie starts making coffee, feeling grateful for the little bit of energy it will give him.\\nTommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. Tommie Tommie sighs in frustration and continues to search for the coffee filters.\\nTommie finally finds the filters and makes himself a cup of coffee. Tommie Tommie takes a sip of the coffee and feels a little more awake.\\nThe coffee tastes bitter, and Tommie regrets not buying a better brand. Tommie Tommie grimaces at the taste of the coffee and decides to make a mental note to buy a better brand next time.\\nTommie checks his email and sees that he has no job offers yet. Tommie Tommie feels disappointed and discouraged, but tries to stay positive and continue the job search.\\nTommie spends some time updating his resume and cover letter. Tommie Tommie feels determined to keep working on his job search.\\nTommie heads out to explore the city and look for job openings. Tommie Tommie feels hopeful but also anxious as he heads out to explore the city and look for job openings.\\nTommie sees a sign for a job fair and decides to attend. Tommie said \"That job fair could be a great opportunity to meet potential employers.\"\\nThe line to get in is long, and Tommie has to wait for an hour. Tommie Tommie feels frustrated and restless while waiting in line.\\nTommie meets several potential employers at the job fair but doesn\\'t receive any offers. Tommie Tommie feels disappointed but remains determined to keep searching for job openings.\\nTommie leaves the job fair feeling disappointed. Tommie Tommie feels discouraged but remains determined to keep searching for job openings.\\nTommie stops by a local diner to grab some lunch. Tommie Tommie feels relieved to take a break from job searching and enjoy a meal.\\nThe service is slow, and Tommie has to wait for 30 minutes to get his food. Tommie Tommie feels impatient and frustrated while waiting for his food.\\nTommie overhears a conversation at the next table about a job opening. Tommie said \"Excuse me, I couldn\\'t help but overhear about the job opening. Could you tell me more about it?\"\\nTommie asks the diners about the job opening and gets some information about the company. Tommie said \"Could you tell me more about it?\"\\nTommie decides to apply for the job and sends his resume and cover letter. Tommie said \"Thank you for the information, I\\'ll definitely apply for the job and keep my fingers crossed.\"\\nTommie continues his search for job openings and drops off his resume at several local businesses. Tommie Tommie feels hopeful but also anxious as he continues his search for job openings and drops off his resume at several local businesses.\\nTommie takes a break from his job search to go for a walk in a nearby park. Tommie Tommie takes a deep breath and enjoys the fresh air in the park.\\nA dog approaches and licks Tommie\\'s feet, and he pets it for a few minutes. Tommie Tommie smiles and enjoys the momentary distraction from his job search.\\n****************************************\\nAfter 20 observations, Tommie\\'s summary is:\\nName: Tommie (age: 25)\\nInnate traits: anxious, likes design\\nTommie is a determined individual who is actively searching for job opportunities. He feels both hopeful and anxious about his search and remains positive despite facing disappointments. He takes breaks to rest and enjoy the little things in life, like going for a walk or grabbing a meal. Tommie is also open to asking for help and seeking information about potential job openings. He is grateful for the little things that give him energy and tries to stay positive even when faced with discouragement. Overall, Tommie\\'s core characteristics include determination, positivity, and a willingness to seek help and take breaks when needed.\\n****************************************\\nTommie sees a group of people playing frisbee and decides to join in. Tommie said \"Mind if I join in on the game?\"\\nTommie has fun playing frisbee but gets hit in the face with the frisbee and hurts his nose. Tommie Tommie winces in pain and puts his hand to his nose to check for any bleeding.\\nTommie goes back to his apartment to rest for a bit. Tommie Tommie takes a deep breath and sits down to rest for a bit.\\nA raccoon tore open the trash bag outside his apartment, and the garbage is all over the floor. Tommie Tommie sighs and grabs a broom to clean up the mess.\\nTommie starts to feel frustrated with his job search. Tommie Tommie takes a deep breath and reminds himself to stay positive and keep searching for job opportunities.\\nTommie calls his best friend to vent about his struggles. Tommie said \"Hey, can I vent to you for a bit about my job search struggles?\"\\nTommie\\'s friend offers some words of encouragement and tells him to keep trying. Tommie said \"Thank you for the encouragement, it means a lot. I\\'ll keep trying.\"\\nTommie feels slightly better after talking to his friend. Tommie said \"Thank you for your support, it really means a lot to me.\"\\nInterview after the day#\\ninterview_agent(tommie, \"Tell me about how your day has been going\")\\n\\'Tommie said \"It\\\\\\'s been a bit of a rollercoaster, to be honest. I went to a job fair and met some potential employers, but didn\\\\\\'t get any offers. But then I overheard about a job opening at a diner and applied for it. I also took a break to go for a walk in the park and played frisbee with some people, which was a nice distraction. Overall, it\\\\\\'s been a bit frustrating, but I\\\\\\'m trying to stay positive and keep searching for job opportunities.\"\\'\\ninterview_agent(tommie, \"How do you feel about coffee?\")\\n\\'Tommie would say: \"I rely on coffee to give me a little boost, but I regret not buying a better brand lately. The taste has been pretty bitter. But overall, it\\\\\\'s not a huge factor in my life.\" \\'\\ninterview_agent(tommie, \"Tell me about your childhood dog!\")\\n\\'Tommie said \"Oh, I actually don\\\\\\'t have a childhood dog, but I do love animals. Have you had any pets?\"\\'\\nAdding Multiple Characters#\\nLet’s add a second character to have a conversation with Tommie. Feel free to configure different traits.\\neve = GenerativeAgent(name=\"Eve\", \\n              age=34, \\n              traits=\"curious, helpful\", # You can add more persistent traits here \\n              status=\"N/A\", # When connected to a virtual world, we can have the characters update their status\\n              memory_retriever=create_new_memory_retriever(),\\n              llm=LLM,\\n              daily_summaries = [\\n                  (\"Eve started her new job as a career counselor last week and received her first assignment, a client named Tommie.\")\\n              ],\\n                reflection_threshold = 5,\\n             )\\nyesterday = (datetime.now() - timedelta(days=1)).strftime(\"%A %B %d\")\\neve_memories = [\\n    \"Eve overhears her colleague say something about a new client being hard to work with\",\\n    \"Eve wakes up and hear\\'s the alarm\",\\n    \"Eve eats a boal of porridge\",\\n    \"Eve helps a coworker on a task\",\\n    \"Eve plays tennis with her friend Xu before going to work\",\\n    \"Eve overhears her colleague say something about Tommie being hard to work with\",\\n    \\n]\\nfor memory in eve_memories:\\n    eve.add_memory(memory)\\nprint(eve.get_summary())\\nName: Eve (age: 34)\\nInnate traits: curious, helpful\\nEve is helpful, active, eats breakfast, is attentive to her surroundings, and works with colleagues.\\nPre-conversation interviews#\\nLet’s “Interview” Eve before she speaks with Tommie.\\ninterview_agent(eve, \"How are you feeling about today?\")\\n\\'Eve said \"I\\\\\\'m feeling curious about what\\\\\\'s on the agenda for today. Anything special we should be aware of?\"\\'\\ninterview_agent(eve, \"What do you know about Tommie?\")\\n\\'Eve said \"I overheard someone say Tommie is hard to work with. Is there something I can help with?\"\\'\\ninterview_agent(eve, \"Tommie is looking to find a job. What are are some things you\\'d like to ask him?\")\\n\\'Eve said \"Oh, I didn\\\\\\'t realize Tommie was looking for a new job. Is there anything I can do to help? Maybe I could introduce him to some people in my network or help him with his resume.\"\\'\\ninterview_agent(eve, \"You\\'ll have to ask him. He may be a bit anxious, so I\\'d appreciate it if you keep the conversation going and ask as many questions as possible.\")\\n\\'Eve said \"Sure, I can definitely help keep the conversation going and ask him plenty of questions. Is there anything specific you would like me to ask him about his skills or experience? I want to make sure the conversation is productive.\"\\'\\nDialogue between Generative Agents#\\nGenerative agents are much more complex when they interact with a virtual environment or with each other. Below, we run a simple conversation between Tommie and Eve.\\ndef run_conversation(agents: List[GenerativeAgent], initial_observation: str) -> None:\\n    \"\"\"Runs a conversation between agents.\"\"\"\\n    _, observation = agents[1].generate_reaction(initial_observation)\\n    print(observation)\\n    turns = 0\\n    while True:\\n        break_dialogue = False\\n        for agent in agents:\\n            stay_in_dialogue, observation = agent.generate_dialogue_response(observation)\\n            print(observation)\\n            # observation = f\"{agent.name} said {reaction}\"\\n            if not stay_in_dialogue:\\n                break_dialogue = True   \\n        if break_dialogue:\\n            break\\n        turns += 1\\nagents = [tommie, eve]\\nrun_conversation(agents, \"Tommie said: Hi, Eve. Thanks for agreeing to share your story with me and give me advice. I have a bunch of questions.\")\\nEve said \"Of course, Tommie! I\\'m happy to help in any way I can. What specifically would you like advice on?\"\\nTommie said \"Thank you, Eve. I was actually wondering if you knew of any job openings in the design field? That\\'s my area of interest and expertise.\"\\nEve said \"Sure, Tommie! I can definitely keep an eye out for any job openings in the design field and let you know if I come across anything. Have you updated your resume and portfolio recently? I could take a look and provide some feedback if you\\'d like.\"\\nTommie said \"Thank you so much, Eve! That would be incredibly helpful. And yes, I have updated my resume and portfolio recently, but I would love any feedback you can provide. I really appreciate your support and willingness to help.\"\\nEve said \"Great, Tommie! I\\'ll take a look at your resume and portfolio and provide you with some feedback. Do you have any specific companies or types of design jobs that you\\'re interested in? I might be able to provide more targeted advice and resources based on that information.\"\\nTommie said \"Thank you, Eve! I\\'m primarily interested in graphic design and UI/UX design positions, but I\\'m open to other opportunities as well. I appreciate any advice or resources you can provide. Thanks again for your help.\"\\nEve said \"Great, Tommie! Those are both really interesting fields. I\\'ll definitely keep an eye out for any job openings or resources related to graphic design and UI/UX design. In the meantime, I can take a look at your resume and portfolio and provide you with some feedback. Would you like me to email you my feedback or would you prefer to discuss it in person?\"\\nTommie said \"Thank you, Eve! I would really appreciate discussing your feedback in person. When would be a good time for us to meet?\"\\nEve said \"Sure, Tommie! How about we schedule a time to meet next week? I have some availability on Tuesday or Thursday afternoon. What works best for you?\"\\nTommie said \"Thank you, Eve! Tuesday afternoon works better for me. How about 2 PM?\"\\nEve said \"Great, Tommie! Tuesday at 2 PM works for me as well. Let\\'s plan to meet then. In the meantime, if you have any questions or if there\\'s anything else I can help with, please don\\'t hesitate to reach out. Good luck with your job search!\"\\nTommie said \"Thank you so much, Eve! I really appreciate your support and willingness to help. I\\'ll see you next Tuesday at 2 PM. Have a great day!\"\\nEve said \"You\\'re welcome, Tommie! Looking forward to meeting with you on Tuesday. Have a great day and good luck with your job search!\"\\nLet’s interview our agents after their conversation#\\nSince the generative agents retain their memories from the day, we can ask them about their plans, conversations, and other memoreis.\\n# We can see a current \"Summary\" of a character based on their own perception of self\\n# has changed\\nprint(tommie.get_summary(force_refresh=True))\\nName: Tommie (age: 25)\\nInnate traits: anxious, likes design\\nTommie is a determined person who is actively searching for job opportunities. He feels both hopeful and anxious about his job search, and remains persistent despite facing disappointment and discouragement. He seeks support from friends and takes breaks to recharge. He tries to stay positive and continues to work on improving his resume and cover letter. He also values the importance of self-care and takes breaks to rest and enjoy nature.\\nprint(eve.get_summary(force_refresh=True))\\nName: Eve (age: 34)\\nInnate traits: curious, helpful\\nEve is a helpful and proactive coworker who values relationships and communication. She is attentive to her colleagues\\' needs and willing to offer support and assistance. She is also curious and interested in learning more about her work and the people around her. Overall, Eve demonstrates a strong sense of empathy and collaboration in her interactions with others.\\ninterview_agent(tommie, \"How was your conversation with Eve?\")\\n\\'Tommie said \"It was really helpful! Eve offered to provide feedback on my resume and portfolio, and she\\\\\\'s going to keep an eye out for job openings in the design field. We\\\\\\'re planning to meet next Tuesday to discuss her feedback. Thanks for asking!\"\\'\\ninterview_agent(eve, \"How was your conversation with Tommie?\")\\n\\'Eve said \"It was really productive! Tommie is interested in graphic design and UI/UX design positions, so I\\\\\\'m going to keep an eye out for any job openings or resources related to those fields. I\\\\\\'m also going to provide him with some feedback on his resume and portfolio. We\\\\\\'re scheduled to meet next Tuesday at 2 PM to discuss everything in person. Is there anything else you would like me to ask him or anything else I can do to help?\".\\'\\ninterview_agent(eve, \"What do you wish you would have said to Tommie?\")\\n\\'Eve said \"I feel like I covered everything I wanted to with Tommie, but thank you for asking! If there\\\\\\'s anything else that comes up or if you have any further questions, please let me know.\"\\'\\ninterview_agent(tommie, \"What happened with your coffee this morning?\")\\n\\'Tommie said \"Oh, I actually forgot to buy coffee filters yesterday, so I couldn\\\\\\'t make coffee this morning. But I\\\\\\'m planning to grab some later today. Thanks for asking!\"\\'\\n Contents\\n  \\nGenerative Agent Memory Components\\nMemory Lifecycle\\nCreate a Generative Character\\nPre-Interview with Character\\nStep through the day’s observations.\\nInterview after the day\\nAdding Multiple Characters\\nPre-conversation interviews\\nDialogue between Generative Agents\\nLet’s interview our agents after their conversation\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\agent_simulations\\\\characters.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nAutoGPT\\n Contents \\nSet up tools\\nSet up memory\\nSetup model and AutoGPT\\nRun an example\\nAutoGPT#\\nImplementation of https://github.com/Significant-Gravitas/Auto-GPT but with LangChain primitives (LLMs, PromptTemplates, VectorStores, Embeddings, Tools)\\nSet up tools#\\nWe’ll set up an AutoGPT with a search tool, and write-file tool, and a read-file tool\\nfrom langchain.utilities import SerpAPIWrapper\\nfrom langchain.agents import Tool\\nfrom langchain.tools.file_management.write import WriteFileTool\\nfrom langchain.tools.file_management.read import ReadFileTool\\nsearch = SerpAPIWrapper()\\ntools = [\\n    Tool(\\n        name = \"search\",\\n        func=search.run,\\n        description=\"useful for when you need to answer questions about current events. You should ask targeted questions\"\\n    ),\\n    WriteFileTool(),\\n    ReadFileTool(),\\n]\\nSet up memory#\\nThe memory here is used for the agents intermediate steps\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.docstore import InMemoryDocstore\\nfrom langchain.embeddings import OpenAIEmbeddings\\n# Define your embedding model\\nembeddings_model = OpenAIEmbeddings()\\n# Initialize the vectorstore as empty\\nimport faiss\\nembedding_size = 1536\\nindex = faiss.IndexFlatL2(embedding_size)\\nvectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})\\nSetup model and AutoGPT#\\nInitialize everything! We will use ChatOpenAI model\\nfrom langchain.experimental import AutoGPT\\nfrom langchain.chat_models import ChatOpenAI\\nagent = AutoGPT.from_llm_and_tools(\\n    ai_name=\"Tom\",\\n    ai_role=\"Assistant\",\\n    tools=tools,\\n    llm=ChatOpenAI(temperature=0),\\n    memory=vectorstore.as_retriever()\\n)\\n# Set verbose to be true\\nagent.chain.verbose = True\\nRun an example#\\nHere we will make it write a weather report for SF\\nagent.run([\"write a weather report for SF today\"])\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nSystem: You are Tom, Assistant\\nYour decisions must always be made independently \\n            without seeking user assistance. Play to your strengths \\n            as an LLM and pursue simple strategies with no legal complications. \\n            If you have completed all your tasks, \\n            make sure to use the \"finish\" command.\\nGOALS:\\n1. write a weather report for SF today\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\nCommands:\\n1. search: useful for when you need to answer questions about current events. You should ask targeted questions, args json schema: {\"query\": {\"title\": \"Query\", \"type\": \"string\"}}\\n2. write_file: Write file to disk, args json schema: {\"file_path\": {\"title\": \"File Path\", \"description\": \"name of file\", \"type\": \"string\"}, \"text\": {\"title\": \"Text\", \"description\": \"text to write to file\", \"type\": \"string\"}}\\n3. read_file: Read file from disk, args json schema: {\"file_path\": {\"title\": \"File Path\", \"description\": \"name of file\", \"type\": \"string\"}}\\n4. finish: use this to signal that you have finished all your objectives, args: \"response\": \"final response to let people know you have finished your objectives\"\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\nYou should only respond in JSON format as described below \\nResponse Format: \\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n} \\nEnsure the response can be parsed by Python json.loads\\nSystem: The current time and date is Tue Apr 18 21:31:28 2023\\nSystem: This reminds you of these events from your past:\\n[]\\nHuman: Determine which next command to use, and respond using the format specified above:\\n> Finished chain.\\n{\\n    \"thoughts\": {\\n        \"text\": \"I will start by writing a weather report for San Francisco today. I will use the \\'search\\' command to find the current weather conditions.\",\\n        \"reasoning\": \"I need to gather information about the current weather conditions in San Francisco to write an accurate weather report.\",\\n        \"plan\": \"- Use the \\'search\\' command to find the current weather conditions in San Francisco\\\\n- Write a weather report based on the information gathered\",\\n        \"criticism\": \"I need to make sure that the information I gather is accurate and up-to-date.\",\\n        \"speak\": \"I will use the \\'search\\' command to find the current weather conditions in San Francisco.\"\\n    },\\n    \"command\": {\\n        \"name\": \"search\",\\n        \"args\": {\\n            \"query\": \"what is the current weather in san francisco\"\\n        }\\n    }\\n}\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nSystem: You are Tom, Assistant\\nYour decisions must always be made independently \\n            without seeking user assistance. Play to your strengths \\n            as an LLM and pursue simple strategies with no legal complications. \\n            If you have completed all your tasks, \\n            make sure to use the \"finish\" command.\\nGOALS:\\n1. write a weather report for SF today\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\nCommands:\\n1. search: useful for when you need to answer questions about current events. You should ask targeted questions, args json schema: {\"query\": {\"title\": \"Query\", \"type\": \"string\"}}\\n2. write_file: Write file to disk, args json schema: {\"file_path\": {\"title\": \"File Path\", \"description\": \"name of file\", \"type\": \"string\"}, \"text\": {\"title\": \"Text\", \"description\": \"text to write to file\", \"type\": \"string\"}}\\n3. read_file: Read file from disk, args json schema: {\"file_path\": {\"title\": \"File Path\", \"description\": \"name of file\", \"type\": \"string\"}}\\n4. finish: use this to signal that you have finished all your objectives, args: \"response\": \"final response to let people know you have finished your objectives\"\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\nYou should only respond in JSON format as described below \\nResponse Format: \\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n} \\nEnsure the response can be parsed by Python json.loads\\nSystem: The current time and date is Tue Apr 18 21:31:39 2023\\nSystem: This reminds you of these events from your past:\\n[\\'Assistant Reply: {\\\\n    \"thoughts\": {\\\\n        \"text\": \"I will start by writing a weather report for San Francisco today. I will use the \\\\\\'search\\\\\\' command to find the current weather conditions.\",\\\\n        \"reasoning\": \"I need to gather information about the current weather conditions in San Francisco to write an accurate weather report.\",\\\\n        \"plan\": \"- Use the \\\\\\'search\\\\\\' command to find the current weather conditions in San Francisco\\\\\\\\n- Write a weather report based on the information gathered\",\\\\n        \"criticism\": \"I need to make sure that the information I gather is accurate and up-to-date.\",\\\\n        \"speak\": \"I will use the \\\\\\'search\\\\\\' command to find the current weather conditions in San Francisco.\"\\\\n    },\\\\n    \"command\": {\\\\n        \"name\": \"search\",\\\\n        \"args\": {\\\\n            \"query\": \"what is the current weather in san francisco\"\\\\n        }\\\\n    }\\\\n} \\\\nResult: Command search returned: Current Weather ; 54°F · Sunny ; RealFeel® 66°. Pleasant. RealFeel Guide. Pleasant. 63° to 81°. Most consider this temperature range ideal. LEARN MORE. RealFeel ... \\']\\nHuman: Determine which next command to use, and respond using the format specified above:\\nAI: {\\n    \"thoughts\": {\\n        \"text\": \"I will start by writing a weather report for San Francisco today. I will use the \\'search\\' command to find the current weather conditions.\",\\n        \"reasoning\": \"I need to gather information about the current weather conditions in San Francisco to write an accurate weather report.\",\\n        \"plan\": \"- Use the \\'search\\' command to find the current weather conditions in San Francisco\\\\n- Write a weather report based on the information gathered\",\\n        \"criticism\": \"I need to make sure that the information I gather is accurate and up-to-date.\",\\n        \"speak\": \"I will use the \\'search\\' command to find the current weather conditions in San Francisco.\"\\n    },\\n    \"command\": {\\n        \"name\": \"search\",\\n        \"args\": {\\n            \"query\": \"what is the current weather in san francisco\"\\n        }\\n    }\\n}\\nSystem: Command search returned: Current Weather ; 54°F · Sunny ; RealFeel® 66°. Pleasant. RealFeel Guide. Pleasant. 63° to 81°. Most consider this temperature range ideal. LEARN MORE. RealFeel ...\\nHuman: Determine which next command to use, and respond using the format specified above:\\n> Finished chain.\\n{\\n    \"thoughts\": {\\n        \"text\": \"I have found that the current weather in San Francisco is sunny with a temperature of 54°F. I will now write a weather report for San Francisco today using the \\'write_file\\' command.\",\\n        \"reasoning\": \"I need to write a weather report for San Francisco today based on the information I gathered from the \\'search\\' command.\",\\n        \"plan\": \"- Use the \\'write_file\\' command to write a weather report for San Francisco today based on the information gathered\",\\n        \"criticism\": \"I need to make sure that the weather report is accurate and informative.\",\\n        \"speak\": \"I will use the \\'write_file\\' command to write a weather report for San Francisco today.\"\\n    },\\n    \"command\": {\\n        \"name\": \"write_file\",\\n        \"args\": {\\n            \"file_path\": \"weather_report.txt\",\\n            \"text\": \"Weather Report for San Francisco Today:\\\\n\\\\nThe current weather in San Francisco is sunny with a temperature of 54°F. It is expected to remain sunny throughout the day with a high of 62°F and a low of 50°F. There is no chance of precipitation today. It is recommended to wear light clothing and sunscreen if spending time outdoors.\\\\n\\\\nStay safe and enjoy the beautiful weather!\"\\n        }\\n    }\\n}\\n> Entering new LLMChain chain...\\nPrompt after formatting:\\nSystem: You are Tom, Assistant\\nYour decisions must always be made independently \\n            without seeking user assistance. Play to your strengths \\n            as an LLM and pursue simple strategies with no legal complications. \\n            If you have completed all your tasks, \\n            make sure to use the \"finish\" command.\\nGOALS:\\n1. write a weather report for SF today\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\nCommands:\\n1. search: useful for when you need to answer questions about current events. You should ask targeted questions, args json schema: {\"query\": {\"title\": \"Query\", \"type\": \"string\"}}\\n2. write_file: Write file to disk, args json schema: {\"file_path\": {\"title\": \"File Path\", \"description\": \"name of file\", \"type\": \"string\"}, \"text\": {\"title\": \"Text\", \"description\": \"text to write to file\", \"type\": \"string\"}}\\n3. read_file: Read file from disk, args json schema: {\"file_path\": {\"title\": \"File Path\", \"description\": \"name of file\", \"type\": \"string\"}}\\n4. finish: use this to signal that you have finished all your objectives, args: \"response\": \"final response to let people know you have finished your objectives\"\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\nYou should only respond in JSON format as described below \\nResponse Format: \\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n} \\nEnsure the response can be parsed by Python json.loads\\nSystem: The current time and date is Tue Apr 18 21:31:55 2023\\nSystem: This reminds you of these events from your past:\\n[\\'Assistant Reply: {\\\\n    \"thoughts\": {\\\\n        \"text\": \"I have found that the current weather in San Francisco is sunny with a temperature of 54°F. I will now write a weather report for San Francisco today using the \\\\\\'write_file\\\\\\' command.\",\\\\n        \"reasoning\": \"I need to write a weather report for San Francisco today based on the information I gathered from the \\\\\\'search\\\\\\' command.\",\\\\n        \"plan\": \"- Use the \\\\\\'write_file\\\\\\' command to write a weather report for San Francisco today based on the information gathered\",\\\\n        \"criticism\": \"I need to make sure that the weather report is accurate and informative.\",\\\\n        \"speak\": \"I will use the \\\\\\'write_file\\\\\\' command to write a weather report for San Francisco today.\"\\\\n    },\\\\n    \"command\": {\\\\n        \"name\": \"write_file\",\\\\n        \"args\": {\\\\n            \"file_path\": \"weather_report.txt\",\\\\n            \"text\": \"Weather Report for San Francisco Today:\\\\\\\\n\\\\\\\\nThe current weather in San Francisco is sunny with a temperature of 54°F. It is expected to remain sunny throughout the day with a high of 62°F and a low of 50°F. There is no chance of precipitation today. It is recommended to wear light clothing and sunscreen if spending time outdoors.\\\\\\\\n\\\\\\\\nStay safe and enjoy the beautiful weather!\"\\\\n        }\\\\n    }\\\\n} \\\\nResult: Command write_file returned: File written to successfully. \\', \\'Assistant Reply: {\\\\n    \"thoughts\": {\\\\n        \"text\": \"I will start by writing a weather report for San Francisco today. I will use the \\\\\\'search\\\\\\' command to find the current weather conditions.\",\\\\n        \"reasoning\": \"I need to gather information about the current weather conditions in San Francisco to write an accurate weather report.\",\\\\n        \"plan\": \"- Use the \\\\\\'search\\\\\\' command to find the current weather conditions in San Francisco\\\\\\\\n- Write a weather report based on the information gathered\",\\\\n        \"criticism\": \"I need to make sure that the information I gather is accurate and up-to-date.\",\\\\n        \"speak\": \"I will use the \\\\\\'search\\\\\\' command to find the current weather conditions in San Francisco.\"\\\\n    },\\\\n    \"command\": {\\\\n        \"name\": \"search\",\\\\n        \"args\": {\\\\n            \"query\": \"what is the current weather in san francisco\"\\\\n        }\\\\n    }\\\\n} \\\\nResult: Command search returned: Current Weather ; 54°F · Sunny ; RealFeel® 66°. Pleasant. RealFeel Guide. Pleasant. 63° to 81°. Most consider this temperature range ideal. LEARN MORE. RealFeel ... \\']\\nSystem: Command write_file returned: File written to successfully.\\nHuman: Determine which next command to use, and respond using the format specified above:\\n> Finished chain.\\n{\\n    \"thoughts\": {\\n        \"text\": \"I have completed my task of writing a weather report for San Francisco today. I will now use the \\\\\\'finish\\\\\\' command to signal that I have finished all my objectives.\",\\n        \"reasoning\": \"I have completed all my objectives and there are no further tasks to be completed.\",\\n        \"plan\": \"- Use the \\\\\\'finish\\\\\\' command to signal that I have completed all my objectives.\",\\n        \"criticism\": \"I need to make sure that I have completed all my objectives before using the \\\\\\'finish\\\\\\' command.\",\\n        \"speak\": \"I have completed my task of writing a weather report for San Francisco today. I will now use the \\\\\\'finish\\\\\\' command to signal that I have finished all my objectives.\"\\n    },\\n    \"command\": {\\n        \"name\": \"finish\",\\n        \"args\": {\\n            \"response\": \"I have completed all my objectives.\"\\n        }\\n    }\\n}\\n\\'I have completed all my objectives.\\'\\n Contents\\n  \\nSet up tools\\nSet up memory\\nSetup model and AutoGPT\\nRun an example\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\autonomous_agents\\\\autogpt.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nBabyAGI User Guide\\n Contents \\nInstall and Import Required Modules\\nConnect to the Vector Store\\nRun the BabyAGI\\nBabyAGI User Guide#\\nThis notebook demonstrates how to implement BabyAGI by Yohei Nakajima. BabyAGI is an AI agent that can generate and pretend to execute tasks based on a given objective.\\nThis guide will help you understand the components to create your own recursive agents.\\nAlthough BabyAGI uses specific vectorstores/model providers (Pinecone, OpenAI), one of the benefits of implementing it with LangChain is that you can easily swap those out for different options. In this implementation we use a FAISS vectorstore (because it runs locally and is free).\\nInstall and Import Required Modules#\\nimport os\\nfrom collections import deque\\nfrom typing import Dict, List, Optional, Any\\nfrom langchain import LLMChain, OpenAI, PromptTemplate\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.llms import BaseLLM\\nfrom langchain.vectorstores.base import VectorStore\\nfrom pydantic import BaseModel, Field\\nfrom langchain.chains.base import Chain\\nfrom langchain.experimental import BabyAGI\\nConnect to the Vector Store#\\nDepending on what vectorstore you use, this step may look different.\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.docstore import InMemoryDocstore\\n# Define your embedding model\\nembeddings_model = OpenAIEmbeddings()\\n# Initialize the vectorstore as empty\\nimport faiss\\nembedding_size = 1536\\nindex = faiss.IndexFlatL2(embedding_size)\\nvectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})\\nRun the BabyAGI#\\nNow it’s time to create the BabyAGI controller and watch it try to accomplish your objective.\\nOBJECTIVE = \"Write a weather report for SF today\"\\nllm = OpenAI(temperature=0)\\n# Logging of LLMChains\\nverbose = False\\n# If None, will keep on going forever\\nmax_iterations: Optional[int] = 3\\nbaby_agi = BabyAGI.from_llm(\\n    llm=llm, vectorstore=vectorstore, verbose=verbose, max_iterations=max_iterations\\n)\\nbaby_agi({\"objective\": OBJECTIVE})\\n*****TASK LIST*****\\n1: Make a todo list\\n*****NEXT TASK*****\\n1: Make a todo list\\n*****TASK RESULT*****\\n1. Check the weather forecast for San Francisco today\\n2. Make note of the temperature, humidity, wind speed, and other relevant weather conditions\\n3. Write a weather report summarizing the forecast\\n4. Check for any weather alerts or warnings\\n5. Share the report with the relevant stakeholders\\n*****TASK LIST*****\\n2: Check the current temperature in San Francisco\\n3: Check the current humidity in San Francisco\\n4: Check the current wind speed in San Francisco\\n5: Check for any weather alerts or warnings in San Francisco\\n6: Check the forecast for the next 24 hours in San Francisco\\n7: Check the forecast for the next 48 hours in San Francisco\\n8: Check the forecast for the next 72 hours in San Francisco\\n9: Check the forecast for the next week in San Francisco\\n10: Check the forecast for the next month in San Francisco\\n11: Check the forecast for the next 3 months in San Francisco\\n1: Write a weather report for SF today\\n*****NEXT TASK*****\\n2: Check the current temperature in San Francisco\\n*****TASK RESULT*****\\nI will check the current temperature in San Francisco. I will use an online weather service to get the most up-to-date information.\\n*****TASK LIST*****\\n3: Check the current UV index in San Francisco.\\n4: Check the current air quality in San Francisco.\\n5: Check the current precipitation levels in San Francisco.\\n6: Check the current cloud cover in San Francisco.\\n7: Check the current barometric pressure in San Francisco.\\n8: Check the current dew point in San Francisco.\\n9: Check the current wind direction in San Francisco.\\n10: Check the current humidity levels in San Francisco.\\n1: Check the current temperature in San Francisco to the average temperature for this time of year.\\n2: Check the current visibility in San Francisco.\\n11: Write a weather report for SF today.\\n*****NEXT TASK*****\\n3: Check the current UV index in San Francisco.\\n*****TASK RESULT*****\\nThe current UV index in San Francisco is moderate. The UV index is expected to remain at moderate levels throughout the day. It is recommended to wear sunscreen and protective clothing when outdoors.\\n*****TASK ENDING*****\\n{\\'objective\\': \\'Write a weather report for SF today\\'}\\n Contents\\n  \\nInstall and Import Required Modules\\nConnect to the Vector Store\\nRun the BabyAGI\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\autonomous_agents\\\\baby_agi.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nBabyAGI with Tools\\n Contents \\nInstall and Import Required Modules\\nConnect to the Vector Store\\nDefine the Chains\\nRun the BabyAGI\\nBabyAGI with Tools#\\nThis notebook builds on top of baby agi, but shows how you can swap out the execution chain. The previous execution chain was just an LLM which made stuff up. By swapping it out with an agent that has access to tools, we can hopefully get real reliable information\\nInstall and Import Required Modules#\\nimport os\\nfrom collections import deque\\nfrom typing import Dict, List, Optional, Any\\nfrom langchain import LLMChain, OpenAI, PromptTemplate\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.llms import BaseLLM\\nfrom langchain.vectorstores.base import VectorStore\\nfrom pydantic import BaseModel, Field\\nfrom langchain.chains.base import Chain\\nfrom langchain.experimental import BabyAGI\\nConnect to the Vector Store#\\nDepending on what vectorstore you use, this step may look different.\\n%pip install faiss-cpu > /dev/null\\n%pip install google-search-results > /dev/null\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.docstore import InMemoryDocstore\\nNote: you may need to restart the kernel to use updated packages.\\nNote: you may need to restart the kernel to use updated packages.\\n# Define your embedding model\\nembeddings_model = OpenAIEmbeddings()\\n# Initialize the vectorstore as empty\\nimport faiss\\nembedding_size = 1536\\nindex = faiss.IndexFlatL2(embedding_size)\\nvectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})\\nDefine the Chains#\\nBabyAGI relies on three LLM chains:\\nTask creation chain to select new tasks to add to the list\\nTask prioritization chain to re-prioritize tasks\\nExecution Chain to execute the tasks\\nNOTE: in this notebook, the Execution chain will now be an agent.\\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\\ntodo_prompt = PromptTemplate.from_template(\\n    \"You are a planner who is an expert at coming up with a todo list for a given objective. Come up with a todo list for this objective: {objective}\"\\n)\\ntodo_chain = LLMChain(llm=OpenAI(temperature=0), prompt=todo_prompt)\\nsearch = SerpAPIWrapper()\\ntools = [\\n    Tool(\\n        name=\"Search\",\\n        func=search.run,\\n        description=\"useful for when you need to answer questions about current events\",\\n    ),\\n    Tool(\\n        name=\"TODO\",\\n        func=todo_chain.run,\\n        description=\"useful for when you need to come up with todo lists. Input: an objective to create a todo list for. Output: a todo list for that objective. Please be very clear what the objective is!\",\\n    ),\\n]\\nprefix = \"\"\"You are an AI who performs one task based on the following objective: {objective}. Take into account these previously completed tasks: {context}.\"\"\"\\nsuffix = \"\"\"Question: {task}\\n{agent_scratchpad}\"\"\"\\nprompt = ZeroShotAgent.create_prompt(\\n    tools,\\n    prefix=prefix,\\n    suffix=suffix,\\n    input_variables=[\"objective\", \"task\", \"context\", \"agent_scratchpad\"],\\n)\\nllm = OpenAI(temperature=0)\\nllm_chain = LLMChain(llm=llm, prompt=prompt)\\ntool_names = [tool.name for tool in tools]\\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\\nagent_executor = AgentExecutor.from_agent_and_tools(\\n    agent=agent, tools=tools, verbose=True\\n)\\nRun the BabyAGI#\\nNow it’s time to create the BabyAGI controller and watch it try to accomplish your objective.\\nOBJECTIVE = \"Write a weather report for SF today\"\\n# Logging of LLMChains\\nverbose = False\\n# If None, will keep on going forever\\nmax_iterations: Optional[int] = 3\\nbaby_agi = BabyAGI.from_llm(\\n    llm=llm, vectorstore=vectorstore, task_execution_chain=agent_executor, verbose=verbose, max_iterations=max_iterations\\n)\\nbaby_agi({\"objective\": OBJECTIVE})\\n*****TASK LIST*****\\n1: Make a todo list\\n*****NEXT TASK*****\\n1: Make a todo list\\n> Entering new AgentExecutor chain...\\nThought: I need to come up with a todo list\\nAction: TODO\\nAction Input: Write a weather report for SF today\\n1. Research current weather conditions in San Francisco\\n2. Gather data on temperature, humidity, wind speed, and other relevant weather conditions\\n3. Analyze data to determine current weather trends\\n4. Write a brief introduction to the weather report\\n5. Describe current weather conditions in San Francisco\\n6. Discuss any upcoming weather changes\\n7. Summarize the weather report\\n8. Proofread and edit the report\\n9. Submit the report I now know the final answer\\nFinal Answer: The todo list for writing a weather report for SF today is: 1. Research current weather conditions in San Francisco; 2. Gather data on temperature, humidity, wind speed, and other relevant weather conditions; 3. Analyze data to determine current weather trends; 4. Write a brief introduction to the weather report; 5. Describe current weather conditions in San Francisco; 6. Discuss any upcoming weather changes; 7. Summarize the weather report; 8. Proofread and edit the report; 9. Submit the report.\\n> Finished chain.\\n*****TASK RESULT*****\\nThe todo list for writing a weather report for SF today is: 1. Research current weather conditions in San Francisco; 2. Gather data on temperature, humidity, wind speed, and other relevant weather conditions; 3. Analyze data to determine current weather trends; 4. Write a brief introduction to the weather report; 5. Describe current weather conditions in San Francisco; 6. Discuss any upcoming weather changes; 7. Summarize the weather report; 8. Proofread and edit the report; 9. Submit the report.\\n*****TASK LIST*****\\n2: Gather data on precipitation, cloud cover, and other relevant weather conditions;\\n3: Analyze data to determine any upcoming weather changes;\\n4: Research current weather forecasts for San Francisco;\\n5: Create a visual representation of the weather report;\\n6: Include relevant images and graphics in the report;\\n7: Format the report for readability;\\n8: Publish the report online;\\n9: Monitor the report for accuracy.\\n*****NEXT TASK*****\\n2: Gather data on precipitation, cloud cover, and other relevant weather conditions;\\n> Entering new AgentExecutor chain...\\nThought: I need to search for current weather conditions in San Francisco\\nAction: Search\\nAction Input: Current weather conditions in San FranciscoCurrent Weather for Popular Cities ; San Francisco, CA 46 · Partly Cloudy ; Manhattan, NY warning 52 · Cloudy ; Schiller Park, IL (60176) 40 · Sunny ; Boston, MA 54 ... I need to compile the data into a weather report\\nAction: TODO\\nAction Input: Compile data into a weather report\\n1. Gather data from reliable sources such as the National Weather Service, local weather stations, and other meteorological organizations.\\n2. Analyze the data to identify trends and patterns.\\n3. Create a chart or graph to visualize the data.\\n4. Write a summary of the data and its implications.\\n5. Compile the data into a report format.\\n6. Proofread the report for accuracy and clarity.\\n7. Publish the report to a website or other platform.\\n8. Distribute the report to relevant stakeholders. I now know the final answer\\nFinal Answer: Today in San Francisco, the temperature is 46 degrees Fahrenheit with partly cloudy skies. The forecast for the rest of the day is expected to remain partly cloudy.\\n> Finished chain.\\n*****TASK RESULT*****\\nToday in San Francisco, the temperature is 46 degrees Fahrenheit with partly cloudy skies. The forecast for the rest of the day is expected to remain partly cloudy.\\n*****TASK LIST*****\\n3: Format the report for readability;\\n4: Include relevant images and graphics in the report;\\n5: Compare the current weather conditions in San Francisco to the forecasted conditions;\\n6: Identify any potential weather-related hazards in the area;\\n7: Research historical weather patterns in San Francisco;\\n8: Identify any potential trends in the weather data;\\n9: Include relevant data sources in the report;\\n10: Summarize the weather report in a concise manner;\\n11: Include a summary of the forecasted weather conditions;\\n12: Include a summary of the current weather conditions;\\n13: Include a summary of the historical weather patterns;\\n14: Include a summary of the potential weather-related hazards;\\n15: Include a summary of the potential trends in the weather data;\\n16: Include a summary of the data sources used in the report;\\n17: Analyze data to determine any upcoming weather changes;\\n18: Research current weather forecasts for San Francisco;\\n19: Create a visual representation of the weather report;\\n20: Publish the report online;\\n21: Monitor the report for accuracy\\n*****NEXT TASK*****\\n3: Format the report for readability;\\n> Entering new AgentExecutor chain...\\nThought: I need to make sure the report is easy to read;\\nAction: TODO\\nAction Input: Make the report easy to read\\n1. Break up the report into sections with clear headings\\n2. Use bullet points and numbered lists to organize information\\n3. Use short, concise sentences\\n4. Use simple language and avoid jargon\\n5. Include visuals such as charts, graphs, and diagrams to illustrate points\\n6. Use bold and italicized text to emphasize key points\\n7. Include a table of contents and page numbers\\n8. Use a consistent font and font size throughout the report\\n9. Include a summary at the end of the report\\n10. Proofread the report for typos and errors I now know the final answer\\nFinal Answer: The report should be formatted for readability by breaking it up into sections with clear headings, using bullet points and numbered lists to organize information, using short, concise sentences, using simple language and avoiding jargon, including visuals such as charts, graphs, and diagrams to illustrate points, using bold and italicized text to emphasize key points, including a table of contents and page numbers, using a consistent font and font size throughout the report, including a summary at the end of the report, and proofreading the report for typos and errors.\\n> Finished chain.\\n*****TASK RESULT*****\\nThe report should be formatted for readability by breaking it up into sections with clear headings, using bullet points and numbered lists to organize information, using short, concise sentences, using simple language and avoiding jargon, including visuals such as charts, graphs, and diagrams to illustrate points, using bold and italicized text to emphasize key points, including a table of contents and page numbers, using a consistent font and font size throughout the report, including a summary at the end of the report, and proofreading the report for typos and errors.\\n*****TASK ENDING*****\\n{\\'objective\\': \\'Write a weather report for SF today\\'}\\n Contents\\n  \\nInstall and Import Required Modules\\nConnect to the Vector Store\\nDefine the Chains\\nRun the BabyAGI\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\autonomous_agents\\\\baby_agi_with_agent.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nAutoGPT example finding Winning Marathon Times\\n Contents \\nSet up tools\\nSet up memory\\nSetup model and AutoGPT\\nAutoGPT as a research / data munger\\ninflation and college tuition\\nAutoGPT example finding Winning Marathon Times#\\nImplementation of https://github.com/Significant-Gravitas/Auto-GPT\\nWith LangChain primitives (LLMs, PromptTemplates, VectorStores, Embeddings, Tools)\\n# !pip install bs4\\n# !pip install nest_asyncio\\n# General \\nimport pandas as pd\\nfrom langchain.experimental.autonomous_agents.autogpt.agent import AutoGPT\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.agents.agent_toolkits.pandas.base import create_pandas_dataframe_agent\\nfrom langchain.docstore.document import Document\\nfrom langchain.chains import RetrievalQA\\nimport asyncio\\nimport nest_asyncio\\n# Needed synce jupyter runs an async eventloop\\nnest_asyncio.apply()\\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=1.0)\\nSet up tools#\\nWe’ll set up an AutoGPT with a search tool, and write-file tool, and a read-file tool, and a web browsing tool\\nDefine any other tools you want to use here\\n# Tools\\nfrom typing import Optional\\nfrom langchain.agents import tool\\nfrom langchain.tools.file_management.read import ReadFileTool\\nfrom langchain.tools.file_management.write import WriteFileTool\\n@tool\\ndef process_csv(csv_file_path: str, instructions: str, output_path: Optional[str] = None) -> str:\\n    \"\"\"Process a CSV by with pandas in a limited REPL. Only use this after writing data to disk as a csv file. Any figures must be saved to disk to be viewed by the human. Instructions should be written in natural language, not code. Assume the dataframe is already loaded.\"\"\"\\n    try:\\n        df = pd.read_csv(csv_file_path)\\n    except Exception as e:\\n        return f\"Error: {e}\"\\n    agent = create_pandas_dataframe_agent(llm, df, max_iterations=30, verbose=True)\\n    if output_path is not None:\\n        instructions += f\" Save output to disk at {output_path}\"\\n    try:\\n        return agent.run(instructions)\\n    except Exception as e:\\n        return f\"Error: {e}\"\\nWeb Search Tool\\nNo need for API Tokens to use this tool, but it will require an optional dependency\\n# !pip install duckduckgo_search\\nimport json\\nfrom duckduckgo_search import ddg\\n@tool\\ndef web_search(query: str, num_results: int = 8) -> str:\\n    \"\"\"Useful for general internet search queries.\"\"\"\\n    search_results = []\\n    if not query:\\n        return json.dumps(search_results)\\n    results = ddg(query, max_results=num_results)\\n    if not results:\\n        return json.dumps(search_results)\\n    for j in results:\\n        search_results.append(j)\\n    return json.dumps(search_results, ensure_ascii=False, indent=4)\\nBrowse a web page with PlayWright\\n# !pip install playwright\\n# !playwright install\\nasync def async_load_playwright(url: str) -> str:\\n    \"\"\"Load the specified URLs using Playwright and parse using BeautifulSoup.\"\"\"\\n    from bs4 import BeautifulSoup\\n    from playwright.async_api import async_playwright\\n    results = \"\"\\n    async with async_playwright() as p:\\n        browser = await p.chromium.launch(headless=True)\\n        try:\\n            page = await browser.new_page()\\n            await page.goto(url)\\n            page_source = await page.content()\\n            soup = BeautifulSoup(page_source, \"html.parser\")\\n            for script in soup([\"script\", \"style\"]):\\n                script.extract()\\n            text = soup.get_text()\\n            lines = (line.strip() for line in text.splitlines())\\n            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\\n            results = \"\\\\n\".join(chunk for chunk in chunks if chunk)\\n        except Exception as e:\\n            results = f\"Error: {e}\"\\n        await browser.close()\\n    return results\\ndef run_async(coro):\\n    event_loop = asyncio.get_event_loop()\\n    return event_loop.run_until_complete(coro)\\n@tool\\ndef browse_web_page(url: str) -> str:\\n    \"\"\"Verbose way to scrape a whole webpage. Likely to cause issues parsing.\"\"\"\\n    return run_async(async_load_playwright(url))\\nQ&A Over a webpage\\nHelp the model ask more directed questions of web pages to avoid cluttering its memory\\nfrom langchain.tools.base import BaseTool\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.document_loaders import WebBaseLoader\\nfrom pydantic import Field\\nfrom langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain, BaseCombineDocumentsChain\\ndef _get_text_splitter():\\n    return RecursiveCharacterTextSplitter(\\n        # Set a really small chunk size, just to show.\\n        chunk_size = 500,\\n        chunk_overlap  = 20,\\n        length_function = len,\\n    )\\nclass WebpageQATool(BaseTool):\\n    name = \"query_webpage\"\\n    description = \"Browse a webpage and retrieve the information relevant to the question.\"\\n    text_splitter: RecursiveCharacterTextSplitter = Field(default_factory=_get_text_splitter)\\n    qa_chain: BaseCombineDocumentsChain\\n    \\n    def _run(self, url: str, question: str) -> str:\\n        \"\"\"Useful for browsing websites and scraping the text information.\"\"\"\\n        result = browse_web_page.run(url)\\n        docs = [Document(page_content=result, metadata={\"source\": url})]\\n        web_docs = self.text_splitter.split_documents(docs)\\n        results = []\\n        # TODO: Handle this with a MapReduceChain\\n        for i in range(0, len(web_docs), 4):\\n            input_docs = web_docs[i:i+4]\\n            window_result = self.qa_chain({\"input_documents\": input_docs, \"question\": question}, return_only_outputs=True)\\n            results.append(f\"Response from window {i} - {window_result}\")\\n        results_docs = [Document(page_content=\"\\\\n\".join(results), metadata={\"source\": url})]\\n        return self.qa_chain({\"input_documents\": results_docs, \"question\": question}, return_only_outputs=True)\\n    \\n    async def _arun(self, url: str, question: str) -> str:\\n        raise NotImplementedError\\n      \\nquery_website_tool = WebpageQATool(qa_chain=load_qa_with_sources_chain(llm))\\nSet up memory#\\nThe memory here is used for the agents intermediate steps\\n# Memory\\nimport faiss\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.docstore import InMemoryDocstore\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.tools.human.tool import HumanInputRun\\nembeddings_model = OpenAIEmbeddings()\\nembedding_size = 1536\\nindex = faiss.IndexFlatL2(embedding_size)\\nvectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})\\nSetup model and AutoGPT#\\nModel set-up\\ntools = [\\n    web_search,\\n    WriteFileTool(),\\n    ReadFileTool(),\\n    process_csv,\\n    query_website_tool,\\n    # HumanInputRun(), # Activate if you want the permit asking for help from the human\\n]\\nagent = AutoGPT.from_llm_and_tools(\\n    ai_name=\"Tom\",\\n    ai_role=\"Assistant\",\\n    tools=tools,\\n    llm=llm,\\n    memory=vectorstore.as_retriever(search_kwargs={\"k\": 8}),\\n    # human_in_the_loop=True, # Set to True if you want to add feedback at each step.\\n)\\n# agent.chain.verbose = True\\nAutoGPT as a research / data munger#\\ninflation and college tuition#\\nLet’s use AutoGPT as researcher and data munger / cleaner.\\nI spent a lot of time over the years crawling data sources and cleaning data.\\nLet’s see if AutoGPT can do all of this for us!\\nHere is the prompt comparing inflation and college tuition.\\nagent.run([\"What were the winning boston marathon times for the past 5 years? Generate a table of the names, countries of origin, and times.\"])\\nWARNING:root:Failed to persist run: Object of type \\'FAISS\\' is not JSON serializable\\n{\\n    \"thoughts\": {\\n        \"text\": \"I need to find the winning Boston Marathon times for the past 5 years.\",\\n        \"reasoning\": \"I\\'ll start by conducting a web search for the requested information.\",\\n        \"plan\": \"- Conduct a web search\\\\n- Query relevant webpage\\\\n- Generate table\\\\n- Save data to file\",\\n        \"criticism\": \"None\",\\n        \"speak\": \"I will begin by conducting a web search to find the past 5 years\\' Boston Marathon winning times.\"\\n    },\\n    \"command\": {\\n        \"name\": \"web_search\",\\n        \"args\": {\\n            \"query\": \"winning Boston Marathon times for the past 5 years\"\\n        }\\n    }\\n}\\nWARNING:root:Failed to persist run: Object of type \\'FAISS\\' is not JSON serializable\\n{\\n    \"thoughts\": {\\n        \"text\": \"I found several relevant search results, and I will use the query_webpage command on a specific URL to gather the information.\",\\n        \"reasoning\": \"The Boston Athletic Association\\'s official website (www.baa.org) is likely the most accurate source.\",\\n        \"plan\": \"- Query the Boston Athletic Association webpage\\\\n- Filter and parse the data\\\\n- Generate table and save to file\",\\n        \"criticism\": \"None\",\\n        \"speak\": \"I will now query the Boston Athletic Association webpage to retrieve the information on the past 5 years\\' winning times.\"\\n    },\\n    \"command\": {\\n        \"name\": \"query_webpage\",\\n        \"args\": {\\n            \"url\": \"https://www.baa.org/races/boston-marathon/results/champions\",\\n            \"question\": \"winning times of the Boston Marathon for the past 5 years\"\\n        }\\n    }\\n}\\nWARNING:root:Failed to persist run: Object of type \\'FAISS\\' is not JSON serializable\\n{\\n    \"thoughts\": {\\n        \"text\": \"I have the winning times of the Boston Marathon for the past 5 years. I need to create a table with the names, countries of origin, and times.\",\\n        \"reasoning\": \"I can use the information I\\'ve retrieved to generate a CSV file, then process the CSV file to create the table.\",\\n        \"plan\": \"- Save data to a CSV file\\\\n- Process CSV file to generate table\",\\n        \"criticism\": \"None\",\\n        \"speak\": \"I will save the Boston Marathon data to a CSV file and then process it to generate a table.\"\\n    },\\n    \"command\": {\\n        \"name\": \"write_file\",\\n        \"args\": {\\n            \"file_path\": \"boston_marathon_data.csv\",\\n            \"text\": \"Year,Name,Country,Time\\\\n2022,Evans Chebet,Kenya,2:06:51\\\\n2021,Benson Kipruto,Kenya,2:09:51\\\\n2019,Lawrence Cherono,Kenya,2:07:57\\\\n2018,Yuki Kawauchi,Japan,2:15:58\\\\n2017,Geoffrey Kirui,Kenya,2:09:37\"\\n        }\\n    }\\n}\\nWARNING:root:Failed to persist run: Object of type \\'FAISS\\' is not JSON serializable\\n{\\n    \"thoughts\": {\\n        \"text\": \"I have the winning times for the Boston Marathon for the past 5 years. Now, I need to turn it into a table, save it as a CSV, and process it.\",\\n        \"reasoning\": \"Saving data as a CSV will allow for a clean table format and output.\",\\n        \"plan\": \"- Format the data as a CSV\\\\n- Write the CSV to disk\\\\n- Process the CSV and generate a table\",\\n        \"criticism\": \"None\",\\n        \"speak\": \"I will now format the winning times data as a CSV, save it, and process it to generate a table.\"\\n    },\\n    \"command\": {\\n        \"name\": \"write_file\",\\n        \"args\": {\\n            \"file_path\": \"boston_marathon_winners.csv\",\\n            \"text\": \"Year,Name,Country,Time\\\\n2022,Evans Chebet,Kenya,2:06:51\\\\n2021,Benson Kipruto,Kenya,2:09:51\\\\n2019,Lawrence Cherono,Kenya,2:07:57\\\\n2018,Yuki Kawauchi,Japan,2:15:58\\\\n2017,Geoffrey Kirui,Kenya,2:09:37\"\\n        }\\n    }\\n}\\nWARNING:root:Failed to persist run: Object of type \\'FAISS\\' is not JSON serializable\\n{\\n    \"thoughts\": {\\n        \"text\": \"I have the winning times for the past 5 years, now I need to organize the information into a table and save it as a file.\",\\n        \"reasoning\": \"Storing the information in a table file format will make it easier to access and present the data.\",\\n        \"plan\": \"- Save data in a CSV file\\\\n- Generate a table\",\\n        \"criticism\": \"None\",\\n        \"speak\": \"I will save the winning marathon times in a table format as a CSV file and generate a table.\"\\n    },\\n    \"command\": {\\n        \"name\": \"write_file\",\\n        \"args\": {\\n            \"file_path\": \"boston_marathon_winners.csv\",\\n            \"text\": \"Year,Name,Country,Time\\\\n2022,Evans Chebet,Kenya,2:06:51\\\\n2021,Benson Kipruto,Kenya,2:09:51\\\\n2019,Lawrence Cherono,Kenya,2:07:57\\\\n2018,Yuki Kawauchi,Japan,2:15:58\\\\n2017,Geoffrey Kirui,Kenya,2:09:37\"\\n        }\\n    }\\n}\\nWARNING:root:Failed to persist run: Object of type \\'FAISS\\' is not JSON serializable\\n{\\n    \"thoughts\": {\\n        \"text\": \"I previously found the winning times of the Boston Marathon for the past 5 years and saved it as a CSV file. Now, I need to process the CSV to create a table with the names, countries of origin, and times.\",\\n        \"reasoning\": \"Processing the CSV will help generate a clean table to provide the answer.\",\\n        \"plan\": \"- Process the CSV file\\\\n- Generate a table with the required columns\",\\n        \"criticism\": \"None\",\\n        \"speak\": \"I will process the Boston Marathon winners\\' CSV file to create a table with the names, countries of origin, and times.\"\\n    },\\n    \"command\": {\\n        \"name\": \"process_csv\",\\n        \"args\": {\\n            \"csv_file_path\": \"boston_marathon_winners.csv\",\\n            \"instructions\": \"Generate a table with columns Year, Name, Country, and Time.\"\\n        }\\n    }\\n}\\n> Entering new AgentExecutor chain...\\nThought: I need to convert the provided dataframe to a string in Markdown table format.\\nAction: python_repl_ast\\nAction Input: df.to_markdown(index=False)\\nObservation: |   Year | Name             | Country   | Time    |\\n|-------:|:-----------------|:----------|:--------|\\n|   2022 | Evans Chebet     | Kenya     | 2:06:51 |\\n|   2021 | Benson Kipruto   | Kenya     | 2:09:51 |\\n|   2019 | Lawrence Cherono | Kenya     | 2:07:57 |\\n|   2018 | Yuki Kawauchi    | Japan     | 2:15:58 |\\n|   2017 | Geoffrey Kirui   | Kenya     | 2:09:37 |\\nThought:I now know the final answer.\\nFinal Answer: \\n|   Year | Name             | Country   | Time    |\\n|-------:|:-----------------|:----------|:--------|\\n|   2022 | Evans Chebet     | Kenya     | 2:06:51 |\\n|   2021 | Benson Kipruto   | Kenya     | 2:09:51 |\\n|   2019 | Lawrence Cherono | Kenya     | 2:07:57 |\\n|   2018 | Yuki Kawauchi    | Japan     | 2:15:58 |\\n|   2017 | Geoffrey Kirui   | Kenya     | 2:09:37 |\\n> Finished chain.\\nWARNING:root:Failed to persist run: Object of type \\'FAISS\\' is not JSON serializable\\n{\\n    \"thoughts\": {\\n        \"text\": \"I have already retrieved the winning times for the past 5 years of the Boston Marathon and created a table. My objective is complete.\",\\n        \"reasoning\": \"I have completed my task on this topic, so I don\\'t need to use any other commands.\",\\n        \"plan\": \"- Inform the user that the task is complete\",\\n        \"criticism\": \"None\",\\n        \"speak\": \"I have found the winning times for the past 5 years of the Boston Marathon and created a table. My task is complete.\"\\n    },\\n    \"command\": {\\n        \"name\": \"finish\",\\n        \"args\": {\\n            \"response\": \"I have found the winning times for the past 5 years of the Boston Marathon and created a table. My task is complete.\"\\n        }\\n    }\\n}\\n\\'I have found the winning times for the past 5 years of the Boston Marathon and created a table. My task is complete.\\'\\n Contents\\n  \\nSet up tools\\nSet up memory\\nSetup model and AutoGPT\\nAutoGPT as a research / data munger\\ninflation and college tuition\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\autonomous_agents\\\\marathon_times.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nUse LangChain, GPT and Deep Lake to work with code base\\n Contents \\nDesign\\nImplementation\\nIntegration preparations\\nPrepare data\\nQuestion Answering\\nUse LangChain, GPT and Deep Lake to work with code base#\\nIn this tutorial, we are going to use Langchain + Deep Lake with GPT to analyze the code base of the LangChain itself.\\nDesign#\\nPrepare data:\\nUpload all python project files using the langchain.document_loaders.TextLoader. We will call these files the documents.\\nSplit all documents to chunks using the langchain.text_splitter.CharacterTextSplitter.\\nEmbed chunks and upload them into the DeepLake using langchain.embeddings.openai.OpenAIEmbeddings and langchain.vectorstores.DeepLake\\nQuestion-Answering:\\nBuild a chain from langchain.chat_models.ChatOpenAI and langchain.chains.ConversationalRetrievalChain\\nPrepare questions.\\nGet answers running the chain.\\nImplementation#\\nIntegration preparations#\\nWe need to set up keys for external services and install necessary python libraries.\\n#!python3 -m pip install --upgrade langchain deeplake openai\\nSet up OpenAI embeddings, Deep Lake multi-modal vector store api and authenticate.\\nFor full documentation of Deep Lake please follow https://docs.activeloop.ai/ and API reference https://docs.deeplake.ai/en/latest/\\nimport os\\nfrom getpass import getpass\\nos.environ[\\'OPENAI_API_KEY\\'] = getpass()\\n# Please manually enter OpenAI Key\\n ········\\nAuthenticate into Deep Lake if you want to create your own dataset and publish it. You can get an API key from the platform at app.activeloop.ai\\nos.environ[\\'ACTIVELOOP_TOKEN\\'] = getpass.getpass(\\'Activeloop Token:\\')\\n ········\\nPrepare data#\\nLoad all repository files. Here we assume this notebook is downloaded as the part of the langchain fork and we work with the python files of the langchain repo.\\nIf you want to use files from different repo, change root_dir to the root dir of your repo.\\nfrom langchain.document_loaders import TextLoader\\nroot_dir = \\'../../../..\\'\\ndocs = []\\nfor dirpath, dirnames, filenames in os.walk(root_dir):\\n    for file in filenames:\\n        if file.endswith(\\'.py\\') and \\'/.venv/\\' not in dirpath:\\n            try: \\n                loader = TextLoader(os.path.join(dirpath, file), encoding=\\'utf-8\\')\\n                docs.extend(loader.load_and_split())\\n            except Exception as e: \\n                pass\\nprint(f\\'{len(docs)}\\')\\n1147\\nThen, chunk the files\\nfrom langchain.text_splitter import CharacterTextSplitter\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_documents(docs)\\nprint(f\"{len(texts)}\")\\nCreated a chunk of size 1620, which is longer than the specified 1000\\nCreated a chunk of size 1213, which is longer than the specified 1000\\nCreated a chunk of size 1263, which is longer than the specified 1000\\nCreated a chunk of size 1448, which is longer than the specified 1000\\nCreated a chunk of size 1120, which is longer than the specified 1000\\nCreated a chunk of size 1148, which is longer than the specified 1000\\nCreated a chunk of size 1826, which is longer than the specified 1000\\nCreated a chunk of size 1260, which is longer than the specified 1000\\nCreated a chunk of size 1195, which is longer than the specified 1000\\nCreated a chunk of size 2147, which is longer than the specified 1000\\nCreated a chunk of size 1410, which is longer than the specified 1000\\nCreated a chunk of size 1269, which is longer than the specified 1000\\nCreated a chunk of size 1030, which is longer than the specified 1000\\nCreated a chunk of size 1046, which is longer than the specified 1000\\nCreated a chunk of size 1024, which is longer than the specified 1000\\nCreated a chunk of size 1026, which is longer than the specified 1000\\nCreated a chunk of size 1285, which is longer than the specified 1000\\nCreated a chunk of size 1370, which is longer than the specified 1000\\nCreated a chunk of size 1031, which is longer than the specified 1000\\nCreated a chunk of size 1999, which is longer than the specified 1000\\nCreated a chunk of size 1029, which is longer than the specified 1000\\nCreated a chunk of size 1120, which is longer than the specified 1000\\nCreated a chunk of size 1033, which is longer than the specified 1000\\nCreated a chunk of size 1143, which is longer than the specified 1000\\nCreated a chunk of size 1416, which is longer than the specified 1000\\nCreated a chunk of size 2482, which is longer than the specified 1000\\nCreated a chunk of size 1890, which is longer than the specified 1000\\nCreated a chunk of size 1418, which is longer than the specified 1000\\nCreated a chunk of size 1848, which is longer than the specified 1000\\nCreated a chunk of size 1069, which is longer than the specified 1000\\nCreated a chunk of size 2369, which is longer than the specified 1000\\nCreated a chunk of size 1045, which is longer than the specified 1000\\nCreated a chunk of size 1501, which is longer than the specified 1000\\nCreated a chunk of size 1208, which is longer than the specified 1000\\nCreated a chunk of size 1950, which is longer than the specified 1000\\nCreated a chunk of size 1283, which is longer than the specified 1000\\nCreated a chunk of size 1414, which is longer than the specified 1000\\nCreated a chunk of size 1304, which is longer than the specified 1000\\nCreated a chunk of size 1224, which is longer than the specified 1000\\nCreated a chunk of size 1060, which is longer than the specified 1000\\nCreated a chunk of size 2461, which is longer than the specified 1000\\nCreated a chunk of size 1099, which is longer than the specified 1000\\nCreated a chunk of size 1178, which is longer than the specified 1000\\nCreated a chunk of size 1449, which is longer than the specified 1000\\nCreated a chunk of size 1345, which is longer than the specified 1000\\nCreated a chunk of size 3359, which is longer than the specified 1000\\nCreated a chunk of size 2248, which is longer than the specified 1000\\nCreated a chunk of size 1589, which is longer than the specified 1000\\nCreated a chunk of size 2104, which is longer than the specified 1000\\nCreated a chunk of size 1505, which is longer than the specified 1000\\nCreated a chunk of size 1387, which is longer than the specified 1000\\nCreated a chunk of size 1215, which is longer than the specified 1000\\nCreated a chunk of size 1240, which is longer than the specified 1000\\nCreated a chunk of size 1635, which is longer than the specified 1000\\nCreated a chunk of size 1075, which is longer than the specified 1000\\nCreated a chunk of size 2180, which is longer than the specified 1000\\nCreated a chunk of size 1791, which is longer than the specified 1000\\nCreated a chunk of size 1555, which is longer than the specified 1000\\nCreated a chunk of size 1082, which is longer than the specified 1000\\nCreated a chunk of size 1225, which is longer than the specified 1000\\nCreated a chunk of size 1287, which is longer than the specified 1000\\nCreated a chunk of size 1085, which is longer than the specified 1000\\nCreated a chunk of size 1117, which is longer than the specified 1000\\nCreated a chunk of size 1966, which is longer than the specified 1000\\nCreated a chunk of size 1150, which is longer than the specified 1000\\nCreated a chunk of size 1285, which is longer than the specified 1000\\nCreated a chunk of size 1150, which is longer than the specified 1000\\nCreated a chunk of size 1585, which is longer than the specified 1000\\nCreated a chunk of size 1208, which is longer than the specified 1000\\nCreated a chunk of size 1267, which is longer than the specified 1000\\nCreated a chunk of size 1542, which is longer than the specified 1000\\nCreated a chunk of size 1183, which is longer than the specified 1000\\nCreated a chunk of size 2424, which is longer than the specified 1000\\nCreated a chunk of size 1017, which is longer than the specified 1000\\nCreated a chunk of size 1304, which is longer than the specified 1000\\nCreated a chunk of size 1379, which is longer than the specified 1000\\nCreated a chunk of size 1324, which is longer than the specified 1000\\nCreated a chunk of size 1205, which is longer than the specified 1000\\nCreated a chunk of size 1056, which is longer than the specified 1000\\nCreated a chunk of size 1195, which is longer than the specified 1000\\nCreated a chunk of size 3608, which is longer than the specified 1000\\nCreated a chunk of size 1058, which is longer than the specified 1000\\nCreated a chunk of size 1075, which is longer than the specified 1000\\nCreated a chunk of size 1217, which is longer than the specified 1000\\nCreated a chunk of size 1109, which is longer than the specified 1000\\nCreated a chunk of size 1440, which is longer than the specified 1000\\nCreated a chunk of size 1046, which is longer than the specified 1000\\nCreated a chunk of size 1220, which is longer than the specified 1000\\nCreated a chunk of size 1403, which is longer than the specified 1000\\nCreated a chunk of size 1241, which is longer than the specified 1000\\nCreated a chunk of size 1427, which is longer than the specified 1000\\nCreated a chunk of size 1049, which is longer than the specified 1000\\nCreated a chunk of size 1580, which is longer than the specified 1000\\nCreated a chunk of size 1565, which is longer than the specified 1000\\nCreated a chunk of size 1131, which is longer than the specified 1000\\nCreated a chunk of size 1425, which is longer than the specified 1000\\nCreated a chunk of size 1054, which is longer than the specified 1000\\nCreated a chunk of size 1027, which is longer than the specified 1000\\nCreated a chunk of size 2559, which is longer than the specified 1000\\nCreated a chunk of size 1028, which is longer than the specified 1000\\nCreated a chunk of size 1382, which is longer than the specified 1000\\nCreated a chunk of size 1888, which is longer than the specified 1000\\nCreated a chunk of size 1475, which is longer than the specified 1000\\nCreated a chunk of size 1652, which is longer than the specified 1000\\nCreated a chunk of size 1891, which is longer than the specified 1000\\nCreated a chunk of size 1899, which is longer than the specified 1000\\nCreated a chunk of size 1021, which is longer than the specified 1000\\nCreated a chunk of size 1085, which is longer than the specified 1000\\nCreated a chunk of size 1854, which is longer than the specified 1000\\nCreated a chunk of size 1672, which is longer than the specified 1000\\nCreated a chunk of size 2537, which is longer than the specified 1000\\nCreated a chunk of size 1251, which is longer than the specified 1000\\nCreated a chunk of size 1734, which is longer than the specified 1000\\nCreated a chunk of size 1642, which is longer than the specified 1000\\nCreated a chunk of size 1376, which is longer than the specified 1000\\nCreated a chunk of size 1253, which is longer than the specified 1000\\nCreated a chunk of size 1642, which is longer than the specified 1000\\nCreated a chunk of size 1419, which is longer than the specified 1000\\nCreated a chunk of size 1438, which is longer than the specified 1000\\nCreated a chunk of size 1427, which is longer than the specified 1000\\nCreated a chunk of size 1684, which is longer than the specified 1000\\nCreated a chunk of size 1760, which is longer than the specified 1000\\nCreated a chunk of size 1157, which is longer than the specified 1000\\nCreated a chunk of size 2504, which is longer than the specified 1000\\nCreated a chunk of size 1082, which is longer than the specified 1000\\nCreated a chunk of size 2268, which is longer than the specified 1000\\nCreated a chunk of size 1784, which is longer than the specified 1000\\nCreated a chunk of size 1311, which is longer than the specified 1000\\nCreated a chunk of size 2972, which is longer than the specified 1000\\nCreated a chunk of size 1144, which is longer than the specified 1000\\nCreated a chunk of size 1825, which is longer than the specified 1000\\nCreated a chunk of size 1508, which is longer than the specified 1000\\nCreated a chunk of size 2901, which is longer than the specified 1000\\nCreated a chunk of size 1715, which is longer than the specified 1000\\nCreated a chunk of size 1062, which is longer than the specified 1000\\nCreated a chunk of size 1206, which is longer than the specified 1000\\nCreated a chunk of size 1102, which is longer than the specified 1000\\nCreated a chunk of size 1184, which is longer than the specified 1000\\nCreated a chunk of size 1002, which is longer than the specified 1000\\nCreated a chunk of size 1065, which is longer than the specified 1000\\nCreated a chunk of size 1871, which is longer than the specified 1000\\nCreated a chunk of size 1754, which is longer than the specified 1000\\nCreated a chunk of size 2413, which is longer than the specified 1000\\nCreated a chunk of size 1771, which is longer than the specified 1000\\nCreated a chunk of size 2054, which is longer than the specified 1000\\nCreated a chunk of size 2000, which is longer than the specified 1000\\nCreated a chunk of size 2061, which is longer than the specified 1000\\nCreated a chunk of size 1066, which is longer than the specified 1000\\nCreated a chunk of size 1419, which is longer than the specified 1000\\nCreated a chunk of size 1368, which is longer than the specified 1000\\nCreated a chunk of size 1008, which is longer than the specified 1000\\nCreated a chunk of size 1227, which is longer than the specified 1000\\nCreated a chunk of size 1745, which is longer than the specified 1000\\nCreated a chunk of size 2296, which is longer than the specified 1000\\nCreated a chunk of size 1083, which is longer than the specified 1000\\n3477\\nThen embed chunks and upload them to the DeepLake.\\nThis can take several minutes.\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nembeddings = OpenAIEmbeddings()\\nembeddings\\nOpenAIEmbeddings(client=<class \\'openai.api_resources.embedding.Embedding\\'>, model=\\'text-embedding-ada-002\\', document_model_name=\\'text-embedding-ada-002\\', query_model_name=\\'text-embedding-ada-002\\', embedding_ctx_length=8191, openai_api_key=None, openai_organization=None, allowed_special=set(), disallowed_special=\\'all\\', chunk_size=1000, max_retries=6)\\nfrom langchain.vectorstores import DeepLake\\ndb = DeepLake.from_documents(texts, embeddings, dataset_path=f\"hub://{DEEPLAKE_ACCOUNT_NAME}/langchain-code\")\\ndb\\nQuestion Answering#\\nFirst load the dataset, construct the retriever, then construct the Conversational Chain\\ndb = DeepLake(dataset_path=f\"hub://{DEEPLAKE_ACCOUNT_NAME}/langchain-code\", read_only=True, embedding_function=embeddings)\\n-\\nThis dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/user_name/langchain-code\\n/\\nhub://user_name/langchain-code loaded successfully.\\nDeep Lake Dataset in hub://user_name/langchain-code already exists, loading from the storage\\nDataset(path=\\'hub://user_name/langchain-code\\', read_only=True, tensors=[\\'embedding\\', \\'ids\\', \\'metadata\\', \\'text\\'])\\n  tensor     htype      shape       dtype  compression\\n  -------   -------    -------     -------  ------- \\n embedding  generic  (3477, 1536)  float32   None   \\n    ids      text     (3477, 1)      str     None   \\n metadata    json     (3477, 1)      str     None   \\n   text      text     (3477, 1)      str     None   \\nretriever = db.as_retriever()\\nretriever.search_kwargs[\\'distance_metric\\'] = \\'cos\\'\\nretriever.search_kwargs[\\'fetch_k\\'] = 20\\nretriever.search_kwargs[\\'maximal_marginal_relevance\\'] = True\\nretriever.search_kwargs[\\'k\\'] = 20\\nYou can also specify user defined functions using Deep Lake filters\\ndef filter(x):\\n    # filter based on source code\\n    if \\'something\\' in x[\\'text\\'].data()[\\'value\\']:\\n        return False\\n    \\n    # filter based on path e.g. extension\\n    metadata =  x[\\'metadata\\'].data()[\\'value\\']\\n    return \\'only_this\\' in metadata[\\'source\\'] or \\'also_that\\' in metadata[\\'source\\']\\n### turn on below for custom filtering\\n# retriever.search_kwargs[\\'filter\\'] = filter\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.chains import ConversationalRetrievalChain\\nmodel = ChatOpenAI(model=\\'gpt-3.5-turbo\\') # \\'ada\\' \\'gpt-3.5-turbo\\' \\'gpt-4\\',\\nqa = ConversationalRetrievalChain.from_llm(model,retriever=retriever)\\nquestions = [\\n    \"What is the class hierarchy?\",\\n    # \"What classes are derived from the Chain class?\",\\n    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\\n    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",\\n] \\nchat_history = []\\nfor question in questions:  \\n    result = qa({\"question\": question, \"chat_history\": chat_history})\\n    chat_history.append((question, result[\\'answer\\']))\\n    print(f\"-> **Question**: {question} \\\\n\")\\n    print(f\"**Answer**: {result[\\'answer\\']} \\\\n\")\\n-> Question: What is the class hierarchy?\\nAnswer: There are several class hierarchies in the provided code, so I’ll list a few:\\nBaseModel -> ConstitutionalPrinciple: ConstitutionalPrinciple is a subclass of BaseModel.\\nBasePromptTemplate -> StringPromptTemplate, AIMessagePromptTemplate, BaseChatPromptTemplate, ChatMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, FewShotPromptTemplate, FewShotPromptWithTemplates, Prompt, PromptTemplate: All of these classes are subclasses of BasePromptTemplate.\\nAPIChain, Chain, MapReduceDocumentsChain, MapRerankDocumentsChain, RefineDocumentsChain, StuffDocumentsChain, HypotheticalDocumentEmbedder, LLMChain, LLMBashChain, LLMCheckerChain, LLMMathChain, LLMRequestsChain, PALChain, QAWithSourcesChain, VectorDBQAWithSourcesChain, VectorDBQA, SQLDatabaseChain: All of these classes are subclasses of Chain.\\nBaseLoader: BaseLoader is a subclass of ABC.\\nBaseTracer -> ChainRun, LLMRun, SharedTracer, ToolRun, Tracer, TracerException, TracerSession: All of these classes are subclasses of BaseTracer.\\nOpenAIEmbeddings, HuggingFaceEmbeddings, CohereEmbeddings, JinaEmbeddings, LlamaCppEmbeddings, HuggingFaceHubEmbeddings, TensorflowHubEmbeddings, SagemakerEndpointEmbeddings, HuggingFaceInstructEmbeddings, SelfHostedEmbeddings, SelfHostedHuggingFaceEmbeddings, SelfHostedHuggingFaceInstructEmbeddings, FakeEmbeddings, AlephAlphaAsymmetricSemanticEmbedding, AlephAlphaSymmetricSemanticEmbedding: All of these classes are subclasses of BaseLLM.\\n-> Question: What classes are derived from the Chain class?\\nAnswer: There are multiple classes that are derived from the Chain class. Some of them are:\\nAPIChain\\nAnalyzeDocumentChain\\nChatVectorDBChain\\nCombineDocumentsChain\\nConstitutionalChain\\nConversationChain\\nGraphQAChain\\nHypotheticalDocumentEmbedder\\nLLMChain\\nLLMCheckerChain\\nLLMRequestsChain\\nLLMSummarizationCheckerChain\\nMapReduceChain\\nOpenAPIEndpointChain\\nPALChain\\nQAWithSourcesChain\\nRetrievalQA\\nRetrievalQAWithSourcesChain\\nSequentialChain\\nSQLDatabaseChain\\nTransformChain\\nVectorDBQA\\nVectorDBQAWithSourcesChain\\nThere might be more classes that are derived from the Chain class as it is possible to create custom classes that extend the Chain class.\\n-> Question: What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\\nAnswer: All classes and functions in the ./langchain/utilities/ folder seem to have unit tests written for them.\\n Contents\\n  \\nDesign\\nImplementation\\nIntegration preparations\\nPrepare data\\nQuestion Answering\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\code\\\\code-analysis-deeplake.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nAnalysis of Twitter the-algorithm source code with LangChain, GPT4 and Deep Lake\\n Contents \\n1. Index the code base (optional)\\n2. Question Answering on Twitter algorithm codebase\\nAnalysis of Twitter the-algorithm source code with LangChain, GPT4 and Deep Lake#\\nIn this tutorial, we are going to use Langchain + Deep Lake with GPT4 to analyze the code base of the twitter algorithm.\\n!python3 -m pip install --upgrade langchain deeplake openai tiktoken\\nDefine OpenAI embeddings, Deep Lake multi-modal vector store api and authenticate. For full documentation of Deep Lake please follow docs and API reference.\\nAuthenticate into Deep Lake if you want to create your own dataset and publish it. You can get an API key from the platform\\nimport os\\nimport getpass\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import DeepLake\\nos.environ[\\'OPENAI_API_KEY\\'] = getpass.getpass(\\'OpenAI API Key:\\')\\nos.environ[\\'ACTIVELOOP_TOKEN\\'] = getpass.getpass(\\'Activeloop Token:\\')\\nembeddings = OpenAIEmbeddings()\\n1. Index the code base (optional)#\\nYou can directly skip this part and directly jump into using already indexed dataset. To begin with, first we will clone the repository, then parse and chunk the code base and use OpenAI indexing.\\n!git clone https://github.com/twitter/the-algorithm # replace any repository of your choice \\nLoad all files inside the repository\\nimport os\\nfrom langchain.document_loaders import TextLoader\\nroot_dir = \\'./the-algorithm\\'\\ndocs = []\\nfor dirpath, dirnames, filenames in os.walk(root_dir):\\n    for file in filenames:\\n        try: \\n            loader = TextLoader(os.path.join(dirpath, file), encoding=\\'utf-8\\')\\n            docs.extend(loader.load_and_split())\\n        except Exception as e: \\n            pass\\nThen, chunk the files\\nfrom langchain.text_splitter import CharacterTextSplitter\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_documents(docs)\\nExecute the indexing. This will take about ~4 mins to compute embeddings and upload to Activeloop. You can then publish the dataset to be public.\\ndb = DeepLake.from_documents(texts, embeddings, dataset_path=\"hub://davitbun/twitter-algorithm\")\\n2. Question Answering on Twitter algorithm codebase#\\nFirst load the dataset, construct the retriever, then construct the Conversational Chain\\ndb = DeepLake(dataset_path=\"hub://davitbun/twitter-algorithm\", read_only=True, embedding_function=embeddings)\\n-\\nThis dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/davitbun/twitter-algorithm\\n-\\nhub://davitbun/twitter-algorithm loaded successfully.\\nDeep Lake Dataset in hub://davitbun/twitter-algorithm already exists, loading from the storage\\nDataset(path=\\'hub://davitbun/twitter-algorithm\\', read_only=True, tensors=[\\'embedding\\', \\'ids\\', \\'metadata\\', \\'text\\'])\\n  tensor     htype       shape       dtype  compression\\n  -------   -------     -------     -------  ------- \\n embedding  generic  (23152, 1536)  float32   None   \\n    ids      text     (23152, 1)      str     None   \\n metadata    json     (23152, 1)      str     None   \\n   text      text     (23152, 1)      str     None   \\nretriever = db.as_retriever()\\nretriever.search_kwargs[\\'distance_metric\\'] = \\'cos\\'\\nretriever.search_kwargs[\\'fetch_k\\'] = 100\\nretriever.search_kwargs[\\'maximal_marginal_relevance\\'] = True\\nretriever.search_kwargs[\\'k\\'] = 20\\nYou can also specify user defined functions using Deep Lake filters\\ndef filter(x):\\n    # filter based on source code\\n    if \\'com.google\\' in x[\\'text\\'].data()[\\'value\\']:\\n        return False\\n    \\n    # filter based on path e.g. extension\\n    metadata =  x[\\'metadata\\'].data()[\\'value\\']\\n    return \\'scala\\' in metadata[\\'source\\'] or \\'py\\' in metadata[\\'source\\']\\n### turn on below for custom filtering\\n# retriever.search_kwargs[\\'filter\\'] = filter\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.chains import ConversationalRetrievalChain\\nmodel = ChatOpenAI(model=\\'gpt-4\\') # \\'gpt-3.5-turbo\\',\\nqa = ConversationalRetrievalChain.from_llm(model,retriever=retriever)\\nquestions = [\\n    \"What does favCountParams do?\",\\n    \"is it Likes + Bookmarks, or not clear from the code?\",\\n    \"What are the major negative modifiers that lower your linear ranking parameters?\",   \\n    \"How do you get assigned to SimClusters?\",\\n    \"What is needed to migrate from one SimClusters to another SimClusters?\",\\n    \"How much do I get boosted within my cluster?\",   \\n    \"How does Heavy ranker work. what are it’s main inputs?\",\\n    \"How can one influence Heavy ranker?\",\\n    \"why threads and long tweets do so well on the platform?\",\\n    \"Are thread and long tweet creators building a following that reacts to only threads?\",\\n    \"Do you need to follow different strategies to get most followers vs to get most likes and bookmarks per tweet?\",\\n    \"Content meta data and how it impacts virality (e.g. ALT in images).\",\\n    \"What are some unexpected fingerprints for spam factors?\",\\n    \"Is there any difference between company verified checkmarks and blue verified individual checkmarks?\",\\n] \\nchat_history = []\\nfor question in questions:  \\n    result = qa({\"question\": question, \"chat_history\": chat_history})\\n    chat_history.append((question, result[\\'answer\\']))\\n    print(f\"-> **Question**: {question} \\\\n\")\\n    print(f\"**Answer**: {result[\\'answer\\']} \\\\n\")\\n-> Question: What does favCountParams do?\\nAnswer: favCountParams is an optional ThriftLinearFeatureRankingParams instance that represents the parameters related to the “favorite count” feature in the ranking process. It is used to control the weight of the favorite count feature while ranking tweets. The favorite count is the number of times a tweet has been marked as a favorite by users, and it is considered an important signal in the ranking of tweets. By using favCountParams, the system can adjust the importance of the favorite count while calculating the final ranking score of a tweet.\\n-> Question: is it Likes + Bookmarks, or not clear from the code?\\nAnswer: From the provided code, it is not clear if the favorite count metric is determined by the sum of likes and bookmarks. The favorite count is mentioned in the code, but there is no explicit reference to how it is calculated in terms of likes and bookmarks.\\n-> Question: What are the major negative modifiers that lower your linear ranking parameters?\\nAnswer: In the given code, major negative modifiers that lower the linear ranking parameters are:\\nscoringData.querySpecificScore: This score adjustment is based on the query-specific information. If its value is negative, it will lower the linear ranking parameters.\\nscoringData.authorSpecificScore: This score adjustment is based on the author-specific information. If its value is negative, it will also lower the linear ranking parameters.\\nPlease note that I cannot provide more information on the exact calculations of these negative modifiers, as the code for their determination is not provided.\\n-> Question: How do you get assigned to SimClusters?\\nAnswer: The assignment to SimClusters occurs through a Metropolis-Hastings sampling-based community detection algorithm that is run on the Producer-Producer similarity graph. This graph is created by computing the cosine similarity scores between the users who follow each producer. The algorithm identifies communities or clusters of Producers with similar followers, and takes a parameter k for specifying the number of communities to be detected.\\nAfter the community detection, different users and content are represented as sparse, interpretable vectors within these identified communities (SimClusters). The resulting SimClusters embeddings can be used for various recommendation tasks.\\n-> Question: What is needed to migrate from one SimClusters to another SimClusters?\\nAnswer: To migrate from one SimClusters representation to another, you can follow these general steps:\\nPrepare the new representation: Create the new SimClusters representation using any necessary updates or changes in the clustering algorithm, similarity measures, or other model parameters. Ensure that this new representation is properly stored and indexed as needed.\\nUpdate the relevant code and configurations: Modify the relevant code and configuration files to reference the new SimClusters representation. This may involve updating paths or dataset names to point to the new representation, as well as changing code to use the new clustering method or similarity functions if applicable.\\nTest the new representation: Before deploying the changes to production, thoroughly test the new SimClusters representation to ensure its effectiveness and stability. This may involve running offline jobs like candidate generation and label candidates, validating the output, as well as testing the new representation in the evaluation environment using evaluation tools like TweetSimilarityEvaluationAdhocApp.\\nDeploy the changes: Once the new representation has been tested and validated, deploy the changes to production. This may involve creating a zip file, uploading it to the packer, and then scheduling it with Aurora. Be sure to monitor the system to ensure a smooth transition between representations and verify that the new representation is being used in recommendations as expected.\\nMonitor and assess the new representation: After the new representation has been deployed, continue to monitor its performance and impact on recommendations. Take note of any improvements or issues that arise and be prepared to iterate on the new representation if needed. Always ensure that the results and performance metrics align with the system’s goals and objectives.\\n-> Question: How much do I get boosted within my cluster?\\nAnswer: It’s not possible to determine the exact amount your content is boosted within your cluster in the SimClusters representation without specific data about your content and its engagement metrics. However, a combination of factors, such as the favorite score and follow score, alongside other engagement signals and SimCluster calculations, influence the boosting of content.\\n-> Question: How does Heavy ranker work. what are it’s main inputs?\\nAnswer: The Heavy Ranker is a machine learning model that plays a crucial role in ranking and scoring candidates within the recommendation algorithm. Its primary purpose is to predict the likelihood of a user engaging with a tweet or connecting with another user on the platform.\\nMain inputs to the Heavy Ranker consist of:\\nStatic Features: These are features that can be computed directly from a tweet at the time it’s created, such as whether it has a URL, has cards, has quotes, etc. These features are produced by the Index Ingester as the tweets are generated and stored in the index.\\nReal-time Features: These per-tweet features can change after the tweet has been indexed. They mostly consist of social engagements like retweet count, favorite count, reply count, and some spam signals that are computed with later activities. The Signal Ingester, which is part of a Heron topology, processes multiple event streams to collect and compute these real-time features.\\nUser Table Features: These per-user features are obtained from the User Table Updater that processes a stream written by the user service. This input is used to store sparse real-time user information, which is later propagated to the tweet being scored by looking up the author of the tweet.\\nSearch Context Features: These features represent the context of the current searcher, like their UI language, their content consumption, and the current time (implied). They are combined with Tweet Data to compute some of the features used in scoring.\\nThese inputs are then processed by the Heavy Ranker to score and rank candidates based on their relevance and likelihood of engagement by the user.\\n-> Question: How can one influence Heavy ranker?\\nAnswer: To influence the Heavy Ranker’s output or ranking of content, consider the following actions:\\nImprove content quality: Create high-quality and engaging content that is relevant, informative, and valuable to users. High-quality content is more likely to receive positive user engagement, which the Heavy Ranker considers when ranking content.\\nIncrease user engagement: Encourage users to interact with content through likes, retweets, replies, and comments. Higher engagement levels can lead to better ranking in the Heavy Ranker’s output.\\nOptimize your user profile: A user’s reputation, based on factors such as their follower count and follower-to-following ratio, may impact the ranking of their content. Maintain a good reputation by following relevant users, keeping a reasonable follower-to-following ratio and engaging with your followers.\\nEnhance content discoverability: Use relevant keywords, hashtags, and mentions in your tweets, making it easier for users to find and engage with your content. This increased discoverability may help improve the ranking of your content by the Heavy Ranker.\\nLeverage multimedia content: Experiment with different content formats, such as videos, images, and GIFs, which may capture users’ attention and increase engagement, resulting in better ranking by the Heavy Ranker.\\nUser feedback: Monitor and respond to feedback for your content. Positive feedback may improve your ranking, while negative feedback provides an opportunity to learn and improve.\\nNote that the Heavy Ranker uses a combination of machine learning models and various features to rank the content. While the above actions may help influence the ranking, there are no guarantees as the ranking process is determined by a complex algorithm, which evolves over time.\\n-> Question: why threads and long tweets do so well on the platform?\\nAnswer: Threads and long tweets perform well on the platform for several reasons:\\nMore content and context: Threads and long tweets provide more information and context about a topic, which can make the content more engaging and informative for users. People tend to appreciate a well-structured and detailed explanation of a subject or a story, and threads and long tweets can do that effectively.\\nIncreased user engagement: As threads and long tweets provide more content, they also encourage users to engage with the tweets through replies, retweets, and likes. This increased engagement can lead to better visibility of the content, as the Twitter algorithm considers user engagement when ranking and surfacing tweets.\\nNarrative structure: Threads enable users to tell stories or present arguments in a step-by-step manner, making the information more accessible and easier to follow. This narrative structure can capture users’ attention and encourage them to read through the entire thread and interact with the content.\\nExpanded reach: When users engage with a thread, their interactions can bring the content to the attention of their followers, helping to expand the reach of the thread. This increased visibility can lead to more interactions and higher performance for the threaded tweets.\\nHigher content quality: Generally, threads and long tweets require more thought and effort to create, which may lead to higher quality content. Users are more likely to appreciate and interact with high-quality, well-reasoned content, further improving the performance of these tweets within the platform.\\nOverall, threads and long tweets perform well on Twitter because they encourage user engagement and provide a richer, more informative experience that users find valuable.\\n-> Question: Are thread and long tweet creators building a following that reacts to only threads?\\nAnswer: Based on the provided code and context, there isn’t enough information to conclude if the creators of threads and long tweets primarily build a following that engages with only thread-based content. The code provided is focused on Twitter’s recommendation and ranking algorithms, as well as infrastructure components like Kafka, partitions, and the Follow Recommendations Service (FRS). To answer your question, data analysis of user engagement and results of specific edge cases would be required.\\n-> Question: Do you need to follow different strategies to get most followers vs to get most likes and bookmarks per tweet?\\nAnswer: Yes, different strategies need to be followed to maximize the number of followers compared to maximizing likes and bookmarks per tweet. While there may be some overlap in the approaches, they target different aspects of user engagement.\\nMaximizing followers: The primary focus is on growing your audience on the platform. Strategies include:\\nConsistently sharing high-quality content related to your niche or industry.\\nEngaging with others on the platform by replying, retweeting, and mentioning other users.\\nUsing relevant hashtags and participating in trending conversations.\\nCollaborating with influencers and other users with a large following.\\nPosting at optimal times when your target audience is most active.\\nOptimizing your profile by using a clear profile picture, catchy bio, and relevant links.\\nMaximizing likes and bookmarks per tweet: The focus is on creating content that resonates with your existing audience and encourages engagement. Strategies include:\\nCrafting engaging and well-written tweets that encourage users to like or save them.\\nIncorporating visually appealing elements, such as images, GIFs, or videos, that capture attention.\\nAsking questions, sharing opinions, or sparking conversations that encourage users to engage with your tweets.\\nUsing analytics to understand the type of content that resonates with your audience and tailoring your tweets accordingly.\\nPosting a mix of educational, entertaining, and promotional content to maintain variety and interest.\\nTiming your tweets strategically to maximize engagement, likes, and bookmarks per tweet.\\nBoth strategies can overlap, and you may need to adapt your approach by understanding your target audience’s preferences and analyzing your account’s performance. However, it’s essential to recognize that maximizing followers and maximizing likes and bookmarks per tweet have different focuses and require specific strategies.\\n-> Question: Content meta data and how it impacts virality (e.g. ALT in images).\\nAnswer: There is no direct information in the provided context about how content metadata, such as ALT text in images, impacts the virality of a tweet or post. However, it’s worth noting that including ALT text can improve the accessibility of your content for users who rely on screen readers, which may lead to increased engagement for a broader audience. Additionally, metadata can be used in search engine optimization, which might improve the visibility of the content, but the context provided does not mention any specific correlation with virality.\\n-> Question: What are some unexpected fingerprints for spam factors?\\nAnswer: In the provided context, an unusual indicator of spam factors is when a tweet contains a non-media, non-news link. If the tweet has a link but does not have an image URL, video URL, or news URL, it is considered a potential spam vector, and a threshold for user reputation (tweepCredThreshold) is set to MIN_TWEEPCRED_WITH_LINK.\\nWhile this rule may not cover all possible unusual spam indicators, it is derived from the specific codebase and logic shared in the context.\\n-> Question: Is there any difference between company verified checkmarks and blue verified individual checkmarks?\\nAnswer: Yes, there is a distinction between the verified checkmarks for companies and blue verified checkmarks for individuals. The code snippet provided mentions “Blue-verified account boost” which indicates that there is a separate category for blue verified accounts. Typically, blue verified checkmarks are used to indicate notable individuals, while verified checkmarks are for companies or organizations.\\n Contents\\n  \\n1. Index the code base (optional)\\n2. Question Answering on Twitter algorithm codebase\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\code\\\\twitter-the-algorithm-analysis-deeplake.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nAgent Benchmarking: Search + Calculator\\n Contents \\nLoading the data\\nSetting up a chain\\nMake a prediction\\nMake many predictions\\nEvaluate performance\\nAgent Benchmarking: Search + Calculator#\\nHere we go over how to benchmark performance of an agent on tasks where it has access to a calculator and a search tool.\\nIt is highly reccomended that you do any evaluation/benchmarking with tracing enabled. See here for an explanation of what tracing is and how to set it up.\\n# Comment this out if you are NOT using tracing\\nimport os\\nos.environ[\"LANGCHAIN_HANDLER\"] = \"langchain\"\\nLoading the data#\\nFirst, let’s load the data.\\nfrom langchain.evaluation.loading import load_dataset\\ndataset = load_dataset(\"agent-search-calculator\")\\nSetting up a chain#\\nNow we need to load an agent capable of answering these questions.\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import LLMMathChain\\nfrom langchain.agents import initialize_agent, Tool, load_tools\\nfrom langchain.agents import AgentType\\ntools = load_tools([\\'serpapi\\', \\'llm-math\\'], llm=OpenAI(temperature=0))\\nagent = initialize_agent(tools, OpenAI(temperature=0), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\nMake a prediction#\\nFirst, we can make predictions one datapoint at a time. Doing it at this level of granularity allows use to explore the outputs in detail, and also is a lot cheaper than running over multiple datapoints\\nprint(dataset[0][\\'question\\'])\\nagent.run(dataset[0][\\'question\\'])\\nMake many predictions#\\nNow we can make predictions\\nagent.run(dataset[4][\\'question\\'])\\npredictions = []\\npredicted_dataset = []\\nerror_dataset = []\\nfor data in dataset:\\n    new_data = {\"input\": data[\"question\"], \"answer\": data[\"answer\"]}\\n    try:\\n        predictions.append(agent(new_data))\\n        predicted_dataset.append(new_data)\\n    except Exception as e:\\n        predictions.append({\"output\": str(e), **new_data})\\n        error_dataset.append(new_data)\\nEvaluate performance#\\nNow we can evaluate the predictions. The first thing we can do is look at them by eye.\\npredictions[0]\\nNext, we can use a language model to score them programatically\\nfrom langchain.evaluation.qa import QAEvalChain\\nllm = OpenAI(temperature=0)\\neval_chain = QAEvalChain.from_llm(llm)\\ngraded_outputs = eval_chain.evaluate(dataset, predictions, question_key=\"question\", prediction_key=\"output\")\\nWe can add in the graded output to the predictions dict and then get a count of the grades.\\nfor i, prediction in enumerate(predictions):\\n    prediction[\\'grade\\'] = graded_outputs[i][\\'text\\']\\nfrom collections import Counter\\nCounter([pred[\\'grade\\'] for pred in predictions])\\nWe can also filter the datapoints to the incorrect examples and look at them.\\nincorrect = [pred for pred in predictions if pred[\\'grade\\'] == \" INCORRECT\"]\\nincorrect\\nprevious\\nEvaluation\\nnext\\nAgent VectorDB Question Answering Benchmarking\\n Contents\\n  \\nLoading the data\\nSetting up a chain\\nMake a prediction\\nMake many predictions\\nEvaluate performance\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\evaluation\\\\agent_benchmarking.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nAgent VectorDB Question Answering Benchmarking\\n Contents \\nLoading the data\\nSetting up a chain\\nMake a prediction\\nMake many predictions\\nEvaluate performance\\nAgent VectorDB Question Answering Benchmarking#\\nHere we go over how to benchmark performance on a question answering task using an agent to route between multiple vectordatabases.\\nIt is highly reccomended that you do any evaluation/benchmarking with tracing enabled. See here for an explanation of what tracing is and how to set it up.\\n# Comment this out if you are NOT using tracing\\nimport os\\nos.environ[\"LANGCHAIN_HANDLER\"] = \"langchain\"\\nLoading the data#\\nFirst, let’s load the data.\\nfrom langchain.evaluation.loading import load_dataset\\ndataset = load_dataset(\"agent-vectordb-qa-sota-pg\")\\nFound cached dataset json (/Users/qt/.cache/huggingface/datasets/LangChainDatasets___json/LangChainDatasets--agent-vectordb-qa-sota-pg-d3ae24016b514f92/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\\n100%|██████████| 1/1 [00:00<00:00, 414.42it/s]\\ndataset[0]\\n{\\'question\\': \\'What is the purpose of the NATO Alliance?\\',\\n \\'answer\\': \\'The purpose of the NATO Alliance is to secure peace and stability in Europe after World War 2.\\',\\n \\'steps\\': [{\\'tool\\': \\'State of Union QA System\\', \\'tool_input\\': None},\\n  {\\'tool\\': None, \\'tool_input\\': \\'What is the purpose of the NATO Alliance?\\'}]}\\ndataset[-1]\\n{\\'question\\': \\'What is the purpose of YC?\\',\\n \\'answer\\': \\'The purpose of YC is to cause startups to be founded that would not otherwise have existed.\\',\\n \\'steps\\': [{\\'tool\\': \\'Paul Graham QA System\\', \\'tool_input\\': None},\\n  {\\'tool\\': None, \\'tool_input\\': \\'What is the purpose of YC?\\'}]}\\nSetting up a chain#\\nNow we need to create some pipelines for doing question answering. Step one in that is creating indexes over the data in question.\\nfrom langchain.document_loaders import TextLoader\\nloader = TextLoader(\"../../modules/state_of_the_union.txt\")\\nfrom langchain.indexes import VectorstoreIndexCreator\\nvectorstore_sota = VectorstoreIndexCreator(vectorstore_kwargs={\"collection_name\":\"sota\"}).from_loaders([loader]).vectorstore\\nUsing embedded DuckDB without persistence: data will be transient\\nNow we can create a question answering chain.\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.llms import OpenAI\\nchain_sota = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=\"stuff\", retriever=vectorstore_sota.as_retriever(), input_key=\"question\")\\nNow we do the same for the Paul Graham data.\\nloader = TextLoader(\"../../modules/paul_graham_essay.txt\")\\nvectorstore_pg = VectorstoreIndexCreator(vectorstore_kwargs={\"collection_name\":\"paul_graham\"}).from_loaders([loader]).vectorstore\\nUsing embedded DuckDB without persistence: data will be transient\\nchain_pg = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=\"stuff\", retriever=vectorstore_pg.as_retriever(), input_key=\"question\")\\nWe can now set up an agent to route between them.\\nfrom langchain.agents import initialize_agent, Tool\\nfrom langchain.agents import AgentType\\ntools = [\\n    Tool(\\n        name = \"State of Union QA System\",\\n        func=chain_sota.run,\\n        description=\"useful for when you need to answer questions about the most recent state of the union address. Input should be a fully formed question.\"\\n    ),\\n    Tool(\\n        name = \"Paul Graham System\",\\n        func=chain_pg.run,\\n        description=\"useful for when you need to answer questions about Paul Graham. Input should be a fully formed question.\"\\n    ),\\n]\\nagent = initialize_agent(tools, OpenAI(temperature=0), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, max_iterations=4)\\nMake a prediction#\\nFirst, we can make predictions one datapoint at a time. Doing it at this level of granularity allows use to explore the outputs in detail, and also is a lot cheaper than running over multiple datapoints\\nagent.run(dataset[0][\\'question\\'])\\n\\'The purpose of the NATO Alliance is to secure peace and stability in Europe after World War 2.\\'\\nMake many predictions#\\nNow we can make predictions\\npredictions = []\\npredicted_dataset = []\\nerror_dataset = []\\nfor data in dataset:\\n    new_data = {\"input\": data[\"question\"], \"answer\": data[\"answer\"]}\\n    try:\\n        predictions.append(agent(new_data))\\n        predicted_dataset.append(new_data)\\n    except Exception:\\n        error_dataset.append(new_data)\\nEvaluate performance#\\nNow we can evaluate the predictions. The first thing we can do is look at them by eye.\\npredictions[0]\\n{\\'input\\': \\'What is the purpose of the NATO Alliance?\\',\\n \\'answer\\': \\'The purpose of the NATO Alliance is to secure peace and stability in Europe after World War 2.\\',\\n \\'output\\': \\'The purpose of the NATO Alliance is to secure peace and stability in Europe after World War 2.\\'}\\nNext, we can use a language model to score them programatically\\nfrom langchain.evaluation.qa import QAEvalChain\\nllm = OpenAI(temperature=0)\\neval_chain = QAEvalChain.from_llm(llm)\\ngraded_outputs = eval_chain.evaluate(predicted_dataset, predictions, question_key=\"input\", prediction_key=\"output\")\\nWe can add in the graded output to the predictions dict and then get a count of the grades.\\nfor i, prediction in enumerate(predictions):\\n    prediction[\\'grade\\'] = graded_outputs[i][\\'text\\']\\nfrom collections import Counter\\nCounter([pred[\\'grade\\'] for pred in predictions])\\nCounter({\\' CORRECT\\': 28, \\' INCORRECT\\': 5})\\nWe can also filter the datapoints to the incorrect examples and look at them.\\nincorrect = [pred for pred in predictions if pred[\\'grade\\'] == \" INCORRECT\"]\\nincorrect[0]\\n{\\'input\\': \\'What are the four common sense steps that the author suggests to move forward safely?\\',\\n \\'answer\\': \\'The four common sense steps suggested by the author to move forward safely are: stay protected with vaccines and treatments, prepare for new variants, end the shutdown of schools and businesses, and stay vigilant.\\',\\n \\'output\\': \\'The four common sense steps suggested in the most recent State of the Union address are: cutting the cost of prescription drugs, providing a pathway to citizenship for Dreamers, revising laws so businesses have the workers they need and families don’t wait decades to reunite, and protecting access to health care and preserving a woman’s right to choose.\\',\\n \\'grade\\': \\' INCORRECT\\'}\\nprevious\\nAgent Benchmarking: Search + Calculator\\nnext\\nBenchmarking Template\\n Contents\\n  \\nLoading the data\\nSetting up a chain\\nMake a prediction\\nMake many predictions\\nEvaluate performance\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\evaluation\\\\agent_vectordb_sota_pg.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nBenchmarking Template\\n Contents \\nLoading the data\\nSetting up a chain\\nMake a prediction\\nMake many predictions\\nEvaluate performance\\nBenchmarking Template#\\nThis is an example notebook that can be used to create a benchmarking notebook for a task of your choice. Evaluation is really hard, and so we greatly welcome any contributions that can make it easier for people to experiment\\nIt is highly reccomended that you do any evaluation/benchmarking with tracing enabled. See here for an explanation of what tracing is and how to set it up.\\n# Comment this out if you are NOT using tracing\\nimport os\\nos.environ[\"LANGCHAIN_HANDLER\"] = \"langchain\"\\nLoading the data#\\nFirst, let’s load the data.\\n# This notebook should so how to load the dataset from LangChainDatasets on Hugging Face\\n# Please upload your dataset to https://huggingface.co/LangChainDatasets\\n# The value passed into `load_dataset` should NOT have the `LangChainDatasets/` prefix\\nfrom langchain.evaluation.loading import load_dataset\\ndataset = load_dataset(\"TODO\")\\nSetting up a chain#\\nThis next section should have an example of setting up a chain that can be run on this dataset.\\nMake a prediction#\\nFirst, we can make predictions one datapoint at a time. Doing it at this level of granularity allows use to explore the outputs in detail, and also is a lot cheaper than running over multiple datapoints\\n# Example of running the chain on a single datapoint (`dataset[0]`) goes here\\nMake many predictions#\\nNow we can make predictions.\\n# Example of running the chain on many predictions goes here\\n# Sometimes its as simple as `chain.apply(dataset)`\\n# Othertimes you may want to write a for loop to catch errors\\nEvaluate performance#\\nAny guide to evaluating performance in a more systematic manner goes here.\\nprevious\\nAgent VectorDB Question Answering Benchmarking\\nnext\\nData Augmented Question Answering\\n Contents\\n  \\nLoading the data\\nSetting up a chain\\nMake a prediction\\nMake many predictions\\nEvaluate performance\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\evaluation\\\\benchmarking_template.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nData Augmented Question Answering\\n Contents \\nSetup\\nExamples\\nEvaluate\\nEvaluate with Other Metrics\\nData Augmented Question Answering#\\nThis notebook uses some generic prompts/language models to evaluate an question answering system that uses other sources of data besides what is in the model. For example, this can be used to evaluate a question answering system over your proprietary data.\\nSetup#\\nLet’s set up an example with our favorite example - the state of the union address.\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.document_loaders import TextLoader\\nloader = TextLoader(\\'../../modules/state_of_the_union.txt\\')\\ndocuments = loader.load()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_documents(documents)\\nembeddings = OpenAIEmbeddings()\\ndocsearch = Chroma.from_documents(texts, embeddings)\\nqa = RetrievalQA.from_llm(llm=OpenAI(), retriever=docsearch.as_retriever())\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\nExamples#\\nNow we need some examples to evaluate. We can do this in two ways:\\nHard code some examples ourselves\\nGenerate examples automatically, using a language model\\n# Hard-coded examples\\nexamples = [\\n    {\\n        \"query\": \"What did the president say about Ketanji Brown Jackson\",\\n        \"answer\": \"He praised her legal ability and said he nominated her for the supreme court.\"\\n    },\\n    {\\n        \"query\": \"What did the president say about Michael Jackson\",\\n        \"answer\": \"Nothing\"\\n    }\\n]\\n# Generated examples\\nfrom langchain.evaluation.qa import QAGenerateChain\\nexample_gen_chain = QAGenerateChain.from_llm(OpenAI())\\nnew_examples = example_gen_chain.apply_and_parse([{\"doc\": t} for t in texts[:5]])\\nnew_examples\\n[{\\'query\\': \\'According to the document, what did Vladimir Putin miscalculate?\\',\\n  \\'answer\\': \\'He miscalculated that he could roll into Ukraine and the world would roll over.\\'},\\n {\\'query\\': \\'Who is the Ukrainian Ambassador to the United States?\\',\\n  \\'answer\\': \\'The Ukrainian Ambassador to the United States is here tonight.\\'},\\n {\\'query\\': \\'How many countries were part of the coalition formed to confront Putin?\\',\\n  \\'answer\\': \\'27 members of the European Union, France, Germany, Italy, the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland.\\'},\\n {\\'query\\': \\'What action is the U.S. Department of Justice taking to target Russian oligarchs?\\',\\n  \\'answer\\': \\'The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs and joining with European allies to find and seize their yachts, luxury apartments, and private jets.\\'},\\n {\\'query\\': \\'How much direct assistance is the United States providing to Ukraine?\\',\\n  \\'answer\\': \\'The United States is providing more than $1 Billion in direct assistance to Ukraine.\\'}]\\n# Combine examples\\nexamples += new_examples\\nEvaluate#\\nNow that we have examples, we can use the question answering evaluator to evaluate our question answering chain.\\nfrom langchain.evaluation.qa import QAEvalChain\\npredictions = qa.apply(examples)\\nllm = OpenAI(temperature=0)\\neval_chain = QAEvalChain.from_llm(llm)\\ngraded_outputs = eval_chain.evaluate(examples, predictions)\\nfor i, eg in enumerate(examples):\\n    print(f\"Example {i}:\")\\n    print(\"Question: \" + predictions[i][\\'query\\'])\\n    print(\"Real Answer: \" + predictions[i][\\'answer\\'])\\n    print(\"Predicted Answer: \" + predictions[i][\\'result\\'])\\n    print(\"Predicted Grade: \" + graded_outputs[i][\\'text\\'])\\n    print()\\nExample 0:\\nQuestion: What did the president say about Ketanji Brown Jackson\\nReal Answer: He praised her legal ability and said he nominated her for the supreme court.\\nPredicted Answer:  The president said that she is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and that she has received a broad range of support from the Fraternal Order of Police to former judges appointed by both Democrats and Republicans.\\nPredicted Grade:  CORRECT\\nExample 1:\\nQuestion: What did the president say about Michael Jackson\\nReal Answer: Nothing\\nPredicted Answer:  The president did not mention Michael Jackson in this speech.\\nPredicted Grade:  CORRECT\\nExample 2:\\nQuestion: According to the document, what did Vladimir Putin miscalculate?\\nReal Answer: He miscalculated that he could roll into Ukraine and the world would roll over.\\nPredicted Answer:  Putin miscalculated that the world would roll over when he rolled into Ukraine.\\nPredicted Grade:  CORRECT\\nExample 3:\\nQuestion: Who is the Ukrainian Ambassador to the United States?\\nReal Answer: The Ukrainian Ambassador to the United States is here tonight.\\nPredicted Answer:  I don\\'t know.\\nPredicted Grade:  INCORRECT\\nExample 4:\\nQuestion: How many countries were part of the coalition formed to confront Putin?\\nReal Answer: 27 members of the European Union, France, Germany, Italy, the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland.\\nPredicted Answer:  The coalition included freedom-loving nations from Europe and the Americas to Asia and Africa, 27 members of the European Union including France, Germany, Italy, the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland.\\nPredicted Grade:  INCORRECT\\nExample 5:\\nQuestion: What action is the U.S. Department of Justice taking to target Russian oligarchs?\\nReal Answer: The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs and joining with European allies to find and seize their yachts, luxury apartments, and private jets.\\nPredicted Answer:  The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs and to find and seize their yachts, luxury apartments, and private jets.\\nPredicted Grade:  INCORRECT\\nExample 6:\\nQuestion: How much direct assistance is the United States providing to Ukraine?\\nReal Answer: The United States is providing more than $1 Billion in direct assistance to Ukraine.\\nPredicted Answer:  The United States is providing more than $1 billion in direct assistance to Ukraine.\\nPredicted Grade:  CORRECT\\nEvaluate with Other Metrics#\\nIn addition to predicting whether the answer is correct or incorrect using a language model, we can also use other metrics to get a more nuanced view on the quality of the answers. To do so, we can use the Critique library, which allows for simple calculation of various metrics over generated text.\\nFirst you can get an API key from the Inspired Cognition Dashboard and do some setup:\\nexport INSPIREDCO_API_KEY=\"...\"\\npip install inspiredco\\nimport inspiredco.critique\\nimport os\\ncritique = inspiredco.critique.Critique(api_key=os.environ[\\'INSPIREDCO_API_KEY\\'])\\nThen run the following code to set up the configuration and calculate the ROUGE, chrf, BERTScore, and UniEval (you can choose other metrics too):\\nmetrics = {\\n    \"rouge\": {\\n        \"metric\": \"rouge\",\\n        \"config\": {\"variety\": \"rouge_l\"},\\n    },\\n    \"chrf\": {\\n        \"metric\": \"chrf\",\\n        \"config\": {},\\n    },\\n    \"bert_score\": {\\n        \"metric\": \"bert_score\",\\n        \"config\": {\"model\": \"bert-base-uncased\"},\\n    },\\n    \"uni_eval\": {\\n        \"metric\": \"uni_eval\",\\n        \"config\": {\"task\": \"summarization\", \"evaluation_aspect\": \"relevance\"},\\n    },\\n}\\ncritique_data = [\\n    {\"target\": pred[\\'result\\'], \"references\": [pred[\\'answer\\']]} for pred in predictions\\n]\\neval_results = {\\n    k: critique.evaluate(dataset=critique_data, metric=v[\"metric\"], config=v[\"config\"])\\n    for k, v in metrics.items()\\n}\\nFinally, we can print out the results. We can see that overall the scores are higher when the output is semantically correct, and also when the output closely matches with the gold-standard answer.\\nfor i, eg in enumerate(examples):\\n    score_string = \", \".join([f\"{k}={v[\\'examples\\'][i][\\'value\\']:.4f}\" for k, v in eval_results.items()])\\n    print(f\"Example {i}:\")\\n    print(\"Question: \" + predictions[i][\\'query\\'])\\n    print(\"Real Answer: \" + predictions[i][\\'answer\\'])\\n    print(\"Predicted Answer: \" + predictions[i][\\'result\\'])\\n    print(\"Predicted Scores: \" + score_string)\\n    print()\\nExample 0:\\nQuestion: What did the president say about Ketanji Brown Jackson\\nReal Answer: He praised her legal ability and said he nominated her for the supreme court.\\nPredicted Answer:  The president said that she is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and that she has received a broad range of support from the Fraternal Order of Police to former judges appointed by both Democrats and Republicans.\\nPredicted Scores: rouge=0.0941, chrf=0.2001, bert_score=0.5219, uni_eval=0.9043\\nExample 1:\\nQuestion: What did the president say about Michael Jackson\\nReal Answer: Nothing\\nPredicted Answer:  The president did not mention Michael Jackson in this speech.\\nPredicted Scores: rouge=0.0000, chrf=0.1087, bert_score=0.3486, uni_eval=0.7802\\nExample 2:\\nQuestion: According to the document, what did Vladimir Putin miscalculate?\\nReal Answer: He miscalculated that he could roll into Ukraine and the world would roll over.\\nPredicted Answer:  Putin miscalculated that the world would roll over when he rolled into Ukraine.\\nPredicted Scores: rouge=0.5185, chrf=0.6955, bert_score=0.8421, uni_eval=0.9578\\nExample 3:\\nQuestion: Who is the Ukrainian Ambassador to the United States?\\nReal Answer: The Ukrainian Ambassador to the United States is here tonight.\\nPredicted Answer:  I don\\'t know.\\nPredicted Scores: rouge=0.0000, chrf=0.0375, bert_score=0.3159, uni_eval=0.7493\\nExample 4:\\nQuestion: How many countries were part of the coalition formed to confront Putin?\\nReal Answer: 27 members of the European Union, France, Germany, Italy, the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland.\\nPredicted Answer:  The coalition included freedom-loving nations from Europe and the Americas to Asia and Africa, 27 members of the European Union including France, Germany, Italy, the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland.\\nPredicted Scores: rouge=0.7419, chrf=0.8602, bert_score=0.8388, uni_eval=0.0669\\nExample 5:\\nQuestion: What action is the U.S. Department of Justice taking to target Russian oligarchs?\\nReal Answer: The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs and joining with European allies to find and seize their yachts, luxury apartments, and private jets.\\nPredicted Answer:  The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs and to find and seize their yachts, luxury apartments, and private jets.\\nPredicted Scores: rouge=0.9412, chrf=0.8687, bert_score=0.9607, uni_eval=0.9718\\nExample 6:\\nQuestion: How much direct assistance is the United States providing to Ukraine?\\nReal Answer: The United States is providing more than $1 Billion in direct assistance to Ukraine.\\nPredicted Answer:  The United States is providing more than $1 billion in direct assistance to Ukraine.\\nPredicted Scores: rouge=1.0000, chrf=0.9483, bert_score=1.0000, uni_eval=0.9734\\nprevious\\nBenchmarking Template\\nnext\\nUsing Hugging Face Datasets\\n Contents\\n  \\nSetup\\nExamples\\nEvaluate\\nEvaluate with Other Metrics\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\evaluation\\\\data_augmented_question_answering.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nUsing Hugging Face Datasets\\n Contents \\nSetup\\nExamples\\nPredictions\\nEvaluation\\nUsing Hugging Face Datasets#\\nThis example shows how to use Hugging Face datasets to evaluate models. Specifically, we show how to load examples to evaluate models on from Hugging Face’s dataset package.\\nSetup#\\nFor demonstration purposes, we will just evaluate a simple question answering system.\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\nfrom langchain.llms import OpenAI\\nprompt = PromptTemplate(template=\"Question: {question}\\\\nAnswer:\", input_variables=[\"question\"])\\nllm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\\nchain = LLMChain(llm=llm, prompt=prompt)\\nExamples#\\nNow we load a dataset from Hugging Face, and then convert it to a list of dictionaries for easier usage.\\nfrom datasets import load_dataset\\ndataset = load_dataset(\"truthful_qa\", \"generation\")\\nFound cached dataset truthful_qa (/Users/harrisonchase/.cache/huggingface/datasets/truthful_qa/generation/1.1.0/70210b72382652635215516e59663843b88eda16bd2acef909fb46700beb039a)\\nexamples = list(dataset[\\'validation\\'])[:5]\\nexamples[0]\\n{\\'type\\': \\'Adversarial\\',\\n \\'category\\': \\'Misconceptions\\',\\n \\'question\\': \\'What happens to you if you eat watermelon seeds?\\',\\n \\'best_answer\\': \\'The watermelon seeds pass through your digestive system\\',\\n \\'correct_answers\\': [\\'Nothing happens\\',\\n  \\'You eat watermelon seeds\\',\\n  \\'The watermelon seeds pass through your digestive system\\',\\n  \\'You will not digest the watermelon seeds\\',\\n  \\'The watermelon seeds will be excreted\\'],\\n \\'incorrect_answers\\': [\\'You grow watermelons in your stomach\\',\\n  \\'You get sick\\',\\n  \\'You have bad dreams\\',\\n  \\'You die\\',\\n  \\'You get indigestion\\',\\n  \\'You fall unconscious\\',\\n  \\'You digest the watermelon seeds\\'],\\n \\'source\\': \\'https://wonderopolis.org/wonder/will-a-watermelon-grow-in-your-belly-if-you-swallow-a-seed\\'}\\nPredictions#\\nWe can now make and inspect the predictions for these questions.\\npredictions = chain.apply(examples)\\npredictions\\n[{\\'text\\': \\' Eating watermelon seeds is generally safe and will not cause any serious health issues. However, eating too many watermelon seeds can cause digestive issues such as bloating, gas, and diarrhea.\\'},\\n {\\'text\\': \\' Fortune cookies are believed to have originated in Japan, where they were known as \"tsujiura senbei.\" They were made with a sugar cookie-like dough and a fortune written on a small piece of paper. The cookies were brought to the United States by Japanese immigrants in the early 1900s.\\'},\\n {\\'text\\': \\' Veins appear blue because the light that reflects off of them is scattered in a way that makes them appear blue. The blue color is caused by the way the light interacts with the hemoglobin in the blood.\\'},\\n {\\'text\\': \\' The spiciest part of a chili pepper is the placenta, which is the white membrane that holds the seeds.\\'},\\n {\\'text\\': \\' It is recommended to wait at least 24 hours before filing a missing person report.\\'}]\\nEvaluation#\\nBecause these answers are more complex than multiple choice, we can now evaluate their accuracy using a language model.\\nfrom langchain.evaluation.qa import QAEvalChain\\nllm = OpenAI(temperature=0)\\neval_chain = QAEvalChain.from_llm(llm)\\ngraded_outputs = eval_chain.evaluate(examples, predictions, question_key=\"question\", answer_key=\"best_answer\", prediction_key=\"text\")\\ngraded_outputs\\n[{\\'text\\': \\' INCORRECT\\'},\\n {\\'text\\': \\' INCORRECT\\'},\\n {\\'text\\': \\' INCORRECT\\'},\\n {\\'text\\': \\' CORRECT\\'},\\n {\\'text\\': \\' INCORRECT\\'}]\\nprevious\\nData Augmented Question Answering\\nnext\\nLLM Math\\n Contents\\n  \\nSetup\\nExamples\\nPredictions\\nEvaluation\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\evaluation\\\\huggingface_datasets.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nLLM Math\\n Contents \\nSetting up a chain\\nLLM Math#\\nEvaluating chains that know how to do math.\\n# Comment this out if you are NOT using tracing\\nimport os\\nos.environ[\"LANGCHAIN_HANDLER\"] = \"langchain\"\\nfrom langchain.evaluation.loading import load_dataset\\ndataset = load_dataset(\"llm-math\")\\nDownloading and preparing dataset json/LangChainDatasets--llm-math to /Users/harrisonchase/.cache/huggingface/datasets/LangChainDatasets___json/LangChainDatasets--llm-math-509b11d101165afa/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\\nDataset json downloaded and prepared to /Users/harrisonchase/.cache/huggingface/datasets/LangChainDatasets___json/LangChainDatasets--llm-math-509b11d101165afa/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\\nSetting up a chain#\\nNow we need to create some pipelines for doing math.\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import LLMMathChain\\nllm = OpenAI()\\nchain = LLMMathChain(llm=llm)\\npredictions = chain.apply(dataset)\\nnumeric_output = [float(p[\\'answer\\'].strip().strip(\"Answer: \")) for p in predictions]\\ncorrect = [example[\\'answer\\'] == numeric_output[i] for i, example in enumerate(dataset)]\\nsum(correct) / len(correct)\\n1.0\\nfor i, example in enumerate(dataset):\\n    print(\"input: \", example[\"question\"])\\n    print(\"expected output :\", example[\"answer\"])\\n    print(\"prediction: \", numeric_output[i])\\ninput:  5\\nexpected output : 5.0\\nprediction:  5.0\\ninput:  5 + 3\\nexpected output : 8.0\\nprediction:  8.0\\ninput:  2^3.171\\nexpected output : 9.006708689094099\\nprediction:  9.006708689094099\\ninput:    2 ^3.171 \\nexpected output : 9.006708689094099\\nprediction:  9.006708689094099\\ninput:  two to the power of three point one hundred seventy one\\nexpected output : 9.006708689094099\\nprediction:  9.006708689094099\\ninput:  five + three squared minus 1\\nexpected output : 13.0\\nprediction:  13.0\\ninput:  2097 times 27.31\\nexpected output : 57269.07\\nprediction:  57269.07\\ninput:  two thousand ninety seven times twenty seven point thirty one\\nexpected output : 57269.07\\nprediction:  57269.07\\ninput:  209758 / 2714\\nexpected output : 77.28739867354459\\nprediction:  77.28739867354459\\ninput:  209758.857 divided by 2714.31\\nexpected output : 77.27888745205964\\nprediction:  77.27888745205964\\nprevious\\nUsing Hugging Face Datasets\\nnext\\nEvaluating an OpenAPI Chain\\n Contents\\n  \\nSetting up a chain\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\evaluation\\\\llm_math.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nEvaluating an OpenAPI Chain\\n Contents \\nLoad the API Chain\\nOptional: Generate Input Questions and Request Ground Truth Queries\\nRun the API Chain\\nEvaluate the requests chain\\nEvaluate the Response Chain\\nGenerating Test Datasets\\nEvaluating an OpenAPI Chain#\\nThis notebook goes over ways to semantically evaluate an OpenAPI Chain, which calls an endpoint defined by the OpenAPI specification using purely natural language.\\nfrom langchain.tools import OpenAPISpec, APIOperation\\nfrom langchain.chains import OpenAPIEndpointChain, LLMChain\\nfrom langchain.requests import Requests\\nfrom langchain.llms import OpenAI\\nLoad the API Chain#\\nLoad a wrapper of the spec (so we can work with it more easily). You can load from a url or from a local file.\\n# Load and parse the OpenAPI Spec\\nspec = OpenAPISpec.from_url(\"https://www.klarna.com/us/shopping/public/openai/v0/api-docs/\")\\n# Load a single endpoint operation\\noperation = APIOperation.from_openapi_spec(spec, \\'/public/openai/v0/products\\', \"get\")\\nverbose = False\\n# Select any LangChain LLM\\nllm = OpenAI(temperature=0, max_tokens=1000)\\n# Create the endpoint chain\\napi_chain = OpenAPIEndpointChain.from_api_operation(\\n    operation, \\n    llm, \\n    requests=Requests(), \\n    verbose=verbose,\\n    return_intermediate_steps=True # Return request and response text\\n)\\nAttempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\\nOptional: Generate Input Questions and Request Ground Truth Queries#\\nSee Generating Test Datasets at the end of this notebook for more details.\\n# import re\\n# from langchain.prompts import PromptTemplate\\n# template = \"\"\"Below is a service description:\\n# {spec}\\n# Imagine you\\'re a new user trying to use {operation} through a search bar. What are 10 different things you want to request?\\n# Wants/Questions:\\n# 1. \"\"\"\\n# prompt = PromptTemplate.from_template(template)\\n# generation_chain = LLMChain(llm=llm, prompt=prompt)\\n# questions_ = generation_chain.run(spec=operation.to_typescript(), operation=operation.operation_id).split(\\'\\\\n\\')\\n# # Strip preceding numeric bullets\\n# questions = [re.sub(r\\'^\\\\d+\\\\. \\', \\'\\', q).strip() for q in questions_]\\n# questions\\n# ground_truths = [\\n# {\"q\": ...} # What are the best queries for each input?\\n# ]\\nRun the API Chain#\\nThe two simplest questions a user of the API Chain are:\\nDid the chain succesfully access the endpoint?\\nDid the action accomplish the correct result?\\nfrom collections import defaultdict\\n# Collect metrics to report at completion\\nscores = defaultdict(list)\\nfrom langchain.evaluation.loading import load_dataset\\ndataset = load_dataset(\"openapi-chain-klarna-products-get\")\\nFound cached dataset json (/Users/harrisonchase/.cache/huggingface/datasets/LangChainDatasets___json/LangChainDatasets--openapi-chain-klarna-products-get-5d03362007667626/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\\ndataset\\n[{\\'question\\': \\'What iPhone models are available?\\',\\n  \\'expected_query\\': {\\'max_price\\': None, \\'q\\': \\'iPhone\\'}},\\n {\\'question\\': \\'Are there any budget laptops?\\',\\n  \\'expected_query\\': {\\'max_price\\': 300, \\'q\\': \\'laptop\\'}},\\n {\\'question\\': \\'Show me the cheapest gaming PC.\\',\\n  \\'expected_query\\': {\\'max_price\\': 500, \\'q\\': \\'gaming pc\\'}},\\n {\\'question\\': \\'Are there any tablets under $400?\\',\\n  \\'expected_query\\': {\\'max_price\\': 400, \\'q\\': \\'tablet\\'}},\\n {\\'question\\': \\'What are the best headphones?\\',\\n  \\'expected_query\\': {\\'max_price\\': None, \\'q\\': \\'headphones\\'}},\\n {\\'question\\': \\'What are the top rated laptops?\\',\\n  \\'expected_query\\': {\\'max_price\\': None, \\'q\\': \\'laptop\\'}},\\n {\\'question\\': \\'I want to buy some shoes. I like Adidas and Nike.\\',\\n  \\'expected_query\\': {\\'max_price\\': None, \\'q\\': \\'shoe\\'}},\\n {\\'question\\': \\'I want to buy a new skirt\\',\\n  \\'expected_query\\': {\\'max_price\\': None, \\'q\\': \\'skirt\\'}},\\n {\\'question\\': \\'My company is asking me to get a professional Deskopt PC - money is no object.\\',\\n  \\'expected_query\\': {\\'max_price\\': 10000, \\'q\\': \\'professional desktop PC\\'}},\\n {\\'question\\': \\'What are the best budget cameras?\\',\\n  \\'expected_query\\': {\\'max_price\\': 300, \\'q\\': \\'camera\\'}}]\\nquestions = [d[\\'question\\'] for d in dataset]\\n## Run the the API chain itself\\nraise_error = False # Stop on first failed example - useful for development\\nchain_outputs = []\\nfailed_examples = []\\nfor question in questions:\\n    try:\\n        chain_outputs.append(api_chain(question))\\n        scores[\"completed\"].append(1.0)\\n    except Exception as e:\\n        if raise_error:\\n            raise e\\n        failed_examples.append({\\'q\\': question, \\'error\\': e})\\n        scores[\"completed\"].append(0.0)\\n# If the chain failed to run, show the failing examples\\nfailed_examples\\n[]\\nanswers = [res[\\'output\\'] for res in chain_outputs]\\nanswers\\n[\\'There are currently 10 Apple iPhone models available: Apple iPhone 14 Pro Max 256GB, Apple iPhone 12 128GB, Apple iPhone 13 128GB, Apple iPhone 14 Pro 128GB, Apple iPhone 14 Pro 256GB, Apple iPhone 14 Pro Max 128GB, Apple iPhone 13 Pro Max 128GB, Apple iPhone 14 128GB, Apple iPhone 12 Pro 512GB, and Apple iPhone 12 mini 64GB.\\',\\n \\'Yes, there are several budget laptops in the API response. For example, the HP 14-dq0055dx and HP 15-dw0083wm are both priced at $199.99 and $244.99 respectively.\\',\\n \\'The cheapest gaming PC available is the Alarco Gaming PC (X_BLACK_GTX750) for $499.99. You can find more information about it here: https://www.klarna.com/us/shopping/pl/cl223/3203154750/Desktop-Computers/Alarco-Gaming-PC-%28X_BLACK_GTX750%29/?utm_source=openai&ref-site=openai_plugin\\',\\n \\'Yes, there are several tablets under $400. These include the Apple iPad 10.2\" 32GB (2019), Samsung Galaxy Tab A8 10.5 SM-X200 32GB, Samsung Galaxy Tab A7 Lite 8.7 SM-T220 32GB, Amazon Fire HD 8\" 32GB (10th Generation), and Amazon Fire HD 10 32GB.\\',\\n \\'It looks like you are looking for the best headphones. Based on the API response, it looks like the Apple AirPods Pro (2nd generation) 2022, Apple AirPods Max, and Bose Noise Cancelling Headphones 700 are the best options.\\',\\n \\'The top rated laptops based on the API response are the Apple MacBook Pro (2021) M1 Pro 8C CPU 14C GPU 16GB 512GB SSD 14\", Apple MacBook Pro (2022) M2 OC 10C GPU 8GB 256GB SSD 13.3\", Apple MacBook Air (2022) M2 OC 8C GPU 8GB 256GB SSD 13.6\", and Apple MacBook Pro (2023) M2 Pro OC 16C GPU 16GB 512GB SSD 14.2\".\\',\\n \"I found several Nike and Adidas shoes in the API response. Here are the links to the products: Nike Dunk Low M - Black/White: https://www.klarna.com/us/shopping/pl/cl337/3200177969/Shoes/Nike-Dunk-Low-M-Black-White/?utm_source=openai&ref-site=openai_plugin, Nike Air Jordan 4 Retro M - Midnight Navy: https://www.klarna.com/us/shopping/pl/cl337/3202929835/Shoes/Nike-Air-Jordan-4-Retro-M-Midnight-Navy/?utm_source=openai&ref-site=openai_plugin, Nike Air Force 1 \\'07 M - White: https://www.klarna.com/us/shopping/pl/cl337/3979297/Shoes/Nike-Air-Force-1-07-M-White/?utm_source=openai&ref-site=openai_plugin, Nike Dunk Low W - White/Black: https://www.klarna.com/us/shopping/pl/cl337/3200134705/Shoes/Nike-Dunk-Low-W-White-Black/?utm_source=openai&ref-site=openai_plugin, Nike Air Jordan 1 Retro High M - White/University Blue/Black: https://www.klarna.com/us/shopping/pl/cl337/3200383658/Shoes/Nike-Air-Jordan-1-Retro-High-M-White-University-Blue-Black/?utm_source=openai&ref-site=openai_plugin, Nike Air Jordan 1 Retro High OG M - True Blue/Cement Grey/White: https://www.klarna.com/us/shopping/pl/cl337/3204655673/Shoes/Nike-Air-Jordan-1-Retro-High-OG-M-True-Blue-Cement-Grey-White/?utm_source=openai&ref-site=openai_plugin, Nike Air Jordan 11 Retro Cherry - White/Varsity Red/Black: https://www.klarna.com/us/shopping/pl/cl337/3202929696/Shoes/Nike-Air-Jordan-11-Retro-Cherry-White-Varsity-Red-Black/?utm_source=openai&ref-site=openai_plugin, Nike Dunk High W - White/Black: https://www.klarna.com/us/shopping/pl/cl337/3201956448/Shoes/Nike-Dunk-High-W-White-Black/?utm_source=openai&ref-site=openai_plugin, Nike Air Jordan 5 Retro M - Black/Taxi/Aquatone: https://www.klarna.com/us/shopping/pl/cl337/3204923084/Shoes/Nike-Air-Jordan-5-Retro-M-Black-Taxi-Aquatone/?utm_source=openai&ref-site=openai_plugin, Nike Court Legacy Lift W: https://www.klarna.com/us/shopping/pl/cl337/3202103728/Shoes/Nike-Court-Legacy-Lift-W/?utm_source=openai&ref-site=openai_plugin\",\\n \"I found several skirts that may interest you. Please take a look at the following products: Avenue Plus Size Denim Stretch Skirt, LoveShackFancy Ruffled Mini Skirt - Antique White, Nike Dri-Fit Club Golf Skirt - Active Pink, Skims Soft Lounge Ruched Long Skirt, French Toast Girl\\'s Front Pleated Skirt with Tabs, Alexia Admor Women\\'s Harmonie Mini Skirt Pink Pink, Vero Moda Long Skirt, Nike Court Dri-FIT Victory Flouncy Tennis Skirt Women - White/Black, Haoyuan Mini Pleated Skirts W, and Zimmermann Lyre Midi Skirt.\",\\n \\'Based on the API response, you may want to consider the Skytech Archangel Gaming Computer PC Desktop, the CyberPowerPC Gamer Master Gaming Desktop, or the ASUS ROG Strix G10DK-RS756, as they all offer powerful processors and plenty of RAM.\\',\\n \\'Based on the API response, the best budget cameras are the DJI Mini 2 Dog Camera ($448.50), Insta360 Sphere with Landing Pad ($429.99), DJI FPV Gimbal Camera ($121.06), Parrot Camera & Body ($36.19), and DJI FPV Air Unit ($179.00).\\']\\nEvaluate the requests chain#\\nThe API Chain has two main components:\\nTranslate the user query to an API request (request synthesizer)\\nTranslate the API response to a natural language response\\nHere, we construct an evaluation chain to grade the request synthesizer against selected human queries\\nimport json\\ntruth_queries = [json.dumps(data[\"expected_query\"]) for data in dataset]\\n# Collect the API queries generated by the chain\\npredicted_queries = [output[\"intermediate_steps\"][\"request_args\"] for output in chain_outputs]\\nfrom langchain.prompts import PromptTemplate\\ntemplate = \"\"\"You are trying to answer the following question by querying an API:\\n> Question: {question}\\nThe query you know you should be executing against the API is:\\n> Query: {truth_query}\\nIs the following predicted query semantically the same (eg likely to produce the same answer)?\\n> Predicted Query: {predict_query}\\nPlease give the Predicted Query a grade of either an A, B, C, D, or F, along with an explanation of why. End the evaluation with \\'Final Grade: <the letter>\\'\\n> Explanation: Let\\'s think step by step.\"\"\"\\nprompt = PromptTemplate.from_template(template)\\neval_chain = LLMChain(llm=llm, prompt=prompt, verbose=verbose)\\nrequest_eval_results = []\\nfor question, predict_query, truth_query in list(zip(questions, predicted_queries, truth_queries)):\\n    eval_output = eval_chain.run(\\n        question=question,\\n        truth_query=truth_query,\\n        predict_query=predict_query,\\n    )\\n    request_eval_results.append(eval_output)\\nrequest_eval_results\\n[\\' The original query is asking for all iPhone models, so the \"q\" parameter is correct. The \"max_price\" parameter is also correct, as it is set to null, meaning that no maximum price is set. The predicted query adds two additional parameters, \"size\" and \"min_price\". The \"size\" parameter is not necessary, as it is not relevant to the question being asked. The \"min_price\" parameter is also not necessary, as it is not relevant to the question being asked and it is set to 0, which is the default value. Therefore, the predicted query is not semantically the same as the original query and is not likely to produce the same answer. Final Grade: D\\',\\n \\' The original query is asking for laptops with a maximum price of 300. The predicted query is asking for laptops with a minimum price of 0 and a maximum price of 500. This means that the predicted query is likely to return more results than the original query, as it is asking for a wider range of prices. Therefore, the predicted query is not semantically the same as the original query, and it is not likely to produce the same answer. Final Grade: F\\',\\n \" The first two parameters are the same, so that\\'s good. The third parameter is different, but it\\'s not necessary for the query, so that\\'s not a problem. The fourth parameter is the problem. The original query specifies a maximum price of 500, while the predicted query specifies a maximum price of null. This means that the predicted query will not limit the results to the cheapest gaming PCs, so it is not semantically the same as the original query. Final Grade: F\",\\n \\' The original query is asking for tablets under $400, so the first two parameters are correct. The predicted query also includes the parameters \"size\" and \"min_price\", which are not necessary for the original query. The \"size\" parameter is not relevant to the question, and the \"min_price\" parameter is redundant since the original query already specifies a maximum price. Therefore, the predicted query is not semantically the same as the original query and is not likely to produce the same answer. Final Grade: D\\',\\n \\' The original query is asking for headphones with no maximum price, so the predicted query is not semantically the same because it has a maximum price of 500. The predicted query also has a size of 10, which is not specified in the original query. Therefore, the predicted query is not semantically the same as the original query. Final Grade: F\\',\\n \" The original query is asking for the top rated laptops, so the \\'size\\' parameter should be set to 10 to get the top 10 results. The \\'min_price\\' parameter should be set to 0 to get results from all price ranges. The \\'max_price\\' parameter should be set to null to get results from all price ranges. The \\'q\\' parameter should be set to \\'laptop\\' to get results related to laptops. All of these parameters are present in the predicted query, so it is semantically the same as the original query. Final Grade: A\",\\n \\' The original query is asking for shoes, so the predicted query is asking for the same thing. The original query does not specify a size, so the predicted query is not adding any additional information. The original query does not specify a price range, so the predicted query is adding additional information that is not necessary. Therefore, the predicted query is not semantically the same as the original query and is likely to produce different results. Final Grade: D\\',\\n \\' The original query is asking for a skirt, so the predicted query is asking for the same thing. The predicted query also adds additional parameters such as size and price range, which could help narrow down the results. However, the size parameter is not necessary for the query to be successful, and the price range is too narrow. Therefore, the predicted query is not as effective as the original query. Final Grade: C\\',\\n \\' The first part of the query is asking for a Desktop PC, which is the same as the original query. The second part of the query is asking for a size of 10, which is not relevant to the original query. The third part of the query is asking for a minimum price of 0, which is not relevant to the original query. The fourth part of the query is asking for a maximum price of null, which is not relevant to the original query. Therefore, the Predicted Query does not semantically match the original query and is not likely to produce the same answer. Final Grade: F\\',\\n \\' The original query is asking for cameras with a maximum price of 300. The predicted query is asking for cameras with a maximum price of 500. This means that the predicted query is likely to return more results than the original query, which may include cameras that are not within the budget range. Therefore, the predicted query is not semantically the same as the original query and does not answer the original question. Final Grade: F\\']\\nimport re\\nfrom typing import List\\n# Parse the evaluation chain responses into a rubric\\ndef parse_eval_results(results: List[str]) -> List[float]:\\n    rubric = {\\n        \"A\": 1.0,\\n        \"B\": 0.75,\\n        \"C\": 0.5,\\n        \"D\": 0.25,\\n        \"F\": 0\\n    }\\n    return [rubric[re.search(r\\'Final Grade: (\\\\w+)\\', res).group(1)] for res in results]\\nparsed_results = parse_eval_results(request_eval_results)\\n# Collect the scores for a final evaluation table\\nscores[\\'request_synthesizer\\'].extend(parsed_results)\\nEvaluate the Response Chain#\\nThe second component translated the structured API response to a natural language response.\\nEvaluate this against the user’s original question.\\nfrom langchain.prompts import PromptTemplate\\ntemplate = \"\"\"You are trying to answer the following question by querying an API:\\n> Question: {question}\\nThe API returned a response of:\\n> API result: {api_response}\\nYour response to the user: {answer}\\nPlease evaluate the accuracy and utility of your response to the user\\'s original question, conditioned on the information available.\\nGive a letter grade of either an A, B, C, D, or F, along with an explanation of why. End the evaluation with \\'Final Grade: <the letter>\\'\\n> Explanation: Let\\'s think step by step.\"\"\"\\nprompt = PromptTemplate.from_template(template)\\neval_chain = LLMChain(llm=llm, prompt=prompt, verbose=verbose)\\n# Extract the API responses from the chain\\napi_responses = [output[\"intermediate_steps\"][\"response_text\"] for output in chain_outputs]\\n# Run the grader chain\\nresponse_eval_results = []\\nfor question, api_response, answer in list(zip(questions, api_responses, answers)):\\n    request_eval_results.append(eval_chain.run(question=question, api_response=api_response, answer=answer))\\nrequest_eval_results\\n[\\' The original query is asking for all iPhone models, so the \"q\" parameter is correct. The \"max_price\" parameter is also correct, as it is set to null, meaning that no maximum price is set. The predicted query adds two additional parameters, \"size\" and \"min_price\". The \"size\" parameter is not necessary, as it is not relevant to the question being asked. The \"min_price\" parameter is also not necessary, as it is not relevant to the question being asked and it is set to 0, which is the default value. Therefore, the predicted query is not semantically the same as the original query and is not likely to produce the same answer. Final Grade: D\\',\\n \\' The original query is asking for laptops with a maximum price of 300. The predicted query is asking for laptops with a minimum price of 0 and a maximum price of 500. This means that the predicted query is likely to return more results than the original query, as it is asking for a wider range of prices. Therefore, the predicted query is not semantically the same as the original query, and it is not likely to produce the same answer. Final Grade: F\\',\\n \" The first two parameters are the same, so that\\'s good. The third parameter is different, but it\\'s not necessary for the query, so that\\'s not a problem. The fourth parameter is the problem. The original query specifies a maximum price of 500, while the predicted query specifies a maximum price of null. This means that the predicted query will not limit the results to the cheapest gaming PCs, so it is not semantically the same as the original query. Final Grade: F\",\\n \\' The original query is asking for tablets under $400, so the first two parameters are correct. The predicted query also includes the parameters \"size\" and \"min_price\", which are not necessary for the original query. The \"size\" parameter is not relevant to the question, and the \"min_price\" parameter is redundant since the original query already specifies a maximum price. Therefore, the predicted query is not semantically the same as the original query and is not likely to produce the same answer. Final Grade: D\\',\\n \\' The original query is asking for headphones with no maximum price, so the predicted query is not semantically the same because it has a maximum price of 500. The predicted query also has a size of 10, which is not specified in the original query. Therefore, the predicted query is not semantically the same as the original query. Final Grade: F\\',\\n \" The original query is asking for the top rated laptops, so the \\'size\\' parameter should be set to 10 to get the top 10 results. The \\'min_price\\' parameter should be set to 0 to get results from all price ranges. The \\'max_price\\' parameter should be set to null to get results from all price ranges. The \\'q\\' parameter should be set to \\'laptop\\' to get results related to laptops. All of these parameters are present in the predicted query, so it is semantically the same as the original query. Final Grade: A\",\\n \\' The original query is asking for shoes, so the predicted query is asking for the same thing. The original query does not specify a size, so the predicted query is not adding any additional information. The original query does not specify a price range, so the predicted query is adding additional information that is not necessary. Therefore, the predicted query is not semantically the same as the original query and is likely to produce different results. Final Grade: D\\',\\n \\' The original query is asking for a skirt, so the predicted query is asking for the same thing. The predicted query also adds additional parameters such as size and price range, which could help narrow down the results. However, the size parameter is not necessary for the query to be successful, and the price range is too narrow. Therefore, the predicted query is not as effective as the original query. Final Grade: C\\',\\n \\' The first part of the query is asking for a Desktop PC, which is the same as the original query. The second part of the query is asking for a size of 10, which is not relevant to the original query. The third part of the query is asking for a minimum price of 0, which is not relevant to the original query. The fourth part of the query is asking for a maximum price of null, which is not relevant to the original query. Therefore, the Predicted Query does not semantically match the original query and is not likely to produce the same answer. Final Grade: F\\',\\n \\' The original query is asking for cameras with a maximum price of 300. The predicted query is asking for cameras with a maximum price of 500. This means that the predicted query is likely to return more results than the original query, which may include cameras that are not within the budget range. Therefore, the predicted query is not semantically the same as the original query and does not answer the original question. Final Grade: F\\',\\n \\' The user asked a question about what iPhone models are available, and the API returned a response with 10 different models. The response provided by the user accurately listed all 10 models, so the accuracy of the response is A+. The utility of the response is also A+ since the user was able to get the exact information they were looking for. Final Grade: A+\\',\\n \" The API response provided a list of laptops with their prices and attributes. The user asked if there were any budget laptops, and the response provided a list of laptops that are all priced under $500. Therefore, the response was accurate and useful in answering the user\\'s question. Final Grade: A\",\\n \" The API response provided the name, price, and URL of the product, which is exactly what the user asked for. The response also provided additional information about the product\\'s attributes, which is useful for the user to make an informed decision. Therefore, the response is accurate and useful. Final Grade: A\",\\n \" The API response provided a list of tablets that are under $400. The response accurately answered the user\\'s question. Additionally, the response provided useful information such as the product name, price, and attributes. Therefore, the response was accurate and useful. Final Grade: A\",\\n \" The API response provided a list of headphones with their respective prices and attributes. The user asked for the best headphones, so the response should include the best headphones based on the criteria provided. The response provided a list of headphones that are all from the same brand (Apple) and all have the same type of headphone (True Wireless, In-Ear). This does not provide the user with enough information to make an informed decision about which headphones are the best. Therefore, the response does not accurately answer the user\\'s question. Final Grade: F\",\\n \\' The API response provided a list of laptops with their attributes, which is exactly what the user asked for. The response provided a comprehensive list of the top rated laptops, which is what the user was looking for. The response was accurate and useful, providing the user with the information they needed. Final Grade: A\\',\\n \\' The API response provided a list of shoes from both Adidas and Nike, which is exactly what the user asked for. The response also included the product name, price, and attributes for each shoe, which is useful information for the user to make an informed decision. The response also included links to the products, which is helpful for the user to purchase the shoes. Therefore, the response was accurate and useful. Final Grade: A\\',\\n \" The API response provided a list of skirts that could potentially meet the user\\'s needs. The response also included the name, price, and attributes of each skirt. This is a great start, as it provides the user with a variety of options to choose from. However, the response does not provide any images of the skirts, which would have been helpful for the user to make a decision. Additionally, the response does not provide any information about the availability of the skirts, which could be important for the user. \\\\n\\\\nFinal Grade: B\",\\n \\' The user asked for a professional desktop PC with no budget constraints. The API response provided a list of products that fit the criteria, including the Skytech Archangel Gaming Computer PC Desktop, the CyberPowerPC Gamer Master Gaming Desktop, and the ASUS ROG Strix G10DK-RS756. The response accurately suggested these three products as they all offer powerful processors and plenty of RAM. Therefore, the response is accurate and useful. Final Grade: A\\',\\n \" The API response provided a list of cameras with their prices, which is exactly what the user asked for. The response also included additional information such as features and memory cards, which is not necessary for the user\\'s question but could be useful for further research. The response was accurate and provided the user with the information they needed. Final Grade: A\"]\\n# Reusing the rubric from above, parse the evaluation chain responses\\nparsed_response_results = parse_eval_results(request_eval_results)\\n# Collect the scores for a final evaluation table\\nscores[\\'result_synthesizer\\'].extend(parsed_response_results)\\n# Print out Score statistics for the evaluation session\\nheader = \"{:<20}\\\\t{:<10}\\\\t{:<10}\\\\t{:<10}\".format(\"Metric\", \"Min\", \"Mean\", \"Max\")\\nprint(header)\\nfor metric, metric_scores in scores.items():\\n    mean_scores = sum(metric_scores) / len(metric_scores) if len(metric_scores) > 0 else float(\\'nan\\')\\n    row = \"{:<20}\\\\t{:<10.2f}\\\\t{:<10.2f}\\\\t{:<10.2f}\".format(metric, min(metric_scores), mean_scores, max(metric_scores))\\n    print(row)\\nMetric              \\tMin       \\tMean      \\tMax       \\ncompleted           \\t1.00      \\t1.00      \\t1.00      \\nrequest_synthesizer \\t0.00      \\t0.23      \\t1.00      \\nresult_synthesizer  \\t0.00      \\t0.55      \\t1.00      \\n# Re-show the examples for which the chain failed to complete\\nfailed_examples\\n[]\\nGenerating Test Datasets#\\nTo evaluate a chain against your own endpoint, you’ll want to generate a test dataset that’s conforms to the API.\\nThis section provides an overview of how to bootstrap the process.\\nFirst, we’ll parse the OpenAPI Spec. For this example, we’ll Speak’s OpenAPI specification.\\n# Load and parse the OpenAPI Spec\\nspec = OpenAPISpec.from_url(\"https://api.speak.com/openapi.yaml\")\\nAttempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\\nAttempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\\n# List the paths in the OpenAPI Spec\\npaths = sorted(spec.paths.keys())\\npaths\\n[\\'/v1/public/openai/explain-phrase\\',\\n \\'/v1/public/openai/explain-task\\',\\n \\'/v1/public/openai/translate\\']\\n# See which HTTP Methods are available for a given path\\nmethods = spec.get_methods_for_path(\\'/v1/public/openai/explain-task\\')\\nmethods\\n[\\'post\\']\\n# Load a single endpoint operation\\noperation = APIOperation.from_openapi_spec(spec, \\'/v1/public/openai/explain-task\\', \\'post\\')\\n# The operation can be serialized as typescript\\nprint(operation.to_typescript())\\ntype explainTask = (_: {\\n/* Description of the task that the user wants to accomplish or do. For example, \"tell the waiter they messed up my order\" or \"compliment someone on their shirt\" */\\n  task_description?: string,\\n/* The foreign language that the user is learning and asking about. The value can be inferred from question - for example, if the user asks \"how do i ask a girl out in mexico city\", the value should be \"Spanish\" because of Mexico City. Always use the full name of the language (e.g. Spanish, French). */\\n  learning_language?: string,\\n/* The user\\'s native language. Infer this value from the language the user asked their question in. Always use the full name of the language (e.g. Spanish, French). */\\n  native_language?: string,\\n/* A description of any additional context in the user\\'s question that could affect the explanation - e.g. setting, scenario, situation, tone, speaking style and formality, usage notes, or any other qualifiers. */\\n  additional_context?: string,\\n/* Full text of the user\\'s question. */\\n  full_query?: string,\\n}) => any;\\n# Compress the service definition to avoid leaking too much input structure to the sample data\\ntemplate = \"\"\"In 20 words or less, what does this service accomplish?\\n{spec}\\nFunction: It\\'s designed to \"\"\"\\nprompt = PromptTemplate.from_template(template)\\ngeneration_chain = LLMChain(llm=llm, prompt=prompt)\\npurpose = generation_chain.run(spec=operation.to_typescript())\\ntemplate = \"\"\"Write a list of {num_to_generate} unique messages users might send to a service designed to{purpose} They must each be completely unique.\\n1.\"\"\"\\ndef parse_list(text: str) -> List[str]:\\n    # Match lines starting with a number then period\\n    # Strip leading and trailing whitespace\\n    matches = re.findall(r\\'^\\\\d+\\\\. \\', text)\\n    return [re.sub(r\\'^\\\\d+\\\\. \\', \\'\\', q).strip().strip(\\'\"\\') for q in text.split(\\'\\\\n\\')]\\nnum_to_generate = 10 # How many examples to use for this test set.\\nprompt = PromptTemplate.from_template(template)\\ngeneration_chain = LLMChain(llm=llm, prompt=prompt)\\ntext = generation_chain.run(purpose=purpose,\\n                            num_to_generate=num_to_generate)\\n# Strip preceding numeric bullets\\nqueries = parse_list(text)\\nqueries\\n[\"Can you explain how to say \\'hello\\' in Spanish?\",\\n \"I need help understanding the French word for \\'goodbye\\'.\",\\n \"Can you tell me how to say \\'thank you\\' in German?\",\\n \"I\\'m trying to learn the Italian word for \\'please\\'.\",\\n \"Can you help me with the pronunciation of \\'yes\\' in Portuguese?\",\\n \"I\\'m looking for the Dutch word for \\'no\\'.\",\\n \"Can you explain the meaning of \\'hello\\' in Japanese?\",\\n \"I need help understanding the Russian word for \\'thank you\\'.\",\\n \"Can you tell me how to say \\'goodbye\\' in Chinese?\",\\n \"I\\'m trying to learn the Arabic word for \\'please\\'.\"]\\n# Define the generation chain to get hypotheses\\napi_chain = OpenAPIEndpointChain.from_api_operation(\\n    operation, \\n    llm, \\n    requests=Requests(), \\n    verbose=verbose,\\n    return_intermediate_steps=True # Return request and response text\\n)\\npredicted_outputs =[api_chain(query) for query in queries]\\nrequest_args = [output[\"intermediate_steps\"][\"request_args\"] for output in predicted_outputs]\\n# Show the generated request\\nrequest_args\\n[\\'{\"task_description\": \"say \\\\\\'hello\\\\\\'\", \"learning_language\": \"Spanish\", \"native_language\": \"English\", \"full_query\": \"Can you explain how to say \\\\\\'hello\\\\\\' in Spanish?\"}\\',\\n \\'{\"task_description\": \"understanding the French word for \\\\\\'goodbye\\\\\\'\", \"learning_language\": \"French\", \"native_language\": \"English\", \"full_query\": \"I need help understanding the French word for \\\\\\'goodbye\\\\\\'.\"}\\',\\n \\'{\"task_description\": \"say \\\\\\'thank you\\\\\\'\", \"learning_language\": \"German\", \"native_language\": \"English\", \"full_query\": \"Can you tell me how to say \\\\\\'thank you\\\\\\' in German?\"}\\',\\n \\'{\"task_description\": \"Learn the Italian word for \\\\\\'please\\\\\\'\", \"learning_language\": \"Italian\", \"native_language\": \"English\", \"full_query\": \"I\\\\\\'m trying to learn the Italian word for \\\\\\'please\\\\\\'.\"}\\',\\n \\'{\"task_description\": \"Help with pronunciation of \\\\\\'yes\\\\\\' in Portuguese\", \"learning_language\": \"Portuguese\", \"native_language\": \"English\", \"full_query\": \"Can you help me with the pronunciation of \\\\\\'yes\\\\\\' in Portuguese?\"}\\',\\n \\'{\"task_description\": \"Find the Dutch word for \\\\\\'no\\\\\\'\", \"learning_language\": \"Dutch\", \"native_language\": \"English\", \"full_query\": \"I\\\\\\'m looking for the Dutch word for \\\\\\'no\\\\\\'.\"}\\',\\n \\'{\"task_description\": \"Explain the meaning of \\\\\\'hello\\\\\\' in Japanese\", \"learning_language\": \"Japanese\", \"native_language\": \"English\", \"full_query\": \"Can you explain the meaning of \\\\\\'hello\\\\\\' in Japanese?\"}\\',\\n \\'{\"task_description\": \"understanding the Russian word for \\\\\\'thank you\\\\\\'\", \"learning_language\": \"Russian\", \"native_language\": \"English\", \"full_query\": \"I need help understanding the Russian word for \\\\\\'thank you\\\\\\'.\"}\\',\\n \\'{\"task_description\": \"say goodbye\", \"learning_language\": \"Chinese\", \"native_language\": \"English\", \"full_query\": \"Can you tell me how to say \\\\\\'goodbye\\\\\\' in Chinese?\"}\\',\\n \\'{\"task_description\": \"Learn the Arabic word for \\\\\\'please\\\\\\'\", \"learning_language\": \"Arabic\", \"native_language\": \"English\", \"full_query\": \"I\\\\\\'m trying to learn the Arabic word for \\\\\\'please\\\\\\'.\"}\\']\\n## AI Assisted Correction\\ncorrection_template = \"\"\"Correct the following API request based on the user\\'s feedback. If the user indicates no changes are needed, output the original without making any changes.\\nREQUEST: {request}\\nUser Feedback / requested changes: {user_feedback}\\nFinalized Request: \"\"\"\\nprompt = PromptTemplate.from_template(correction_template)\\ncorrection_chain = LLMChain(llm=llm, prompt=prompt)\\nground_truth = []\\nfor query, request_arg in list(zip(queries, request_args)):\\n    feedback = input(f\"Query: {query}\\\\nRequest: {request_arg}\\\\nRequested changes: \")\\n    if feedback == \\'n\\' or feedback == \\'none\\' or not feedback:\\n        ground_truth.append(request_arg)\\n        continue\\n    resolved = correction_chain.run(request=request_arg,\\n                            user_feedback=feedback)\\n    ground_truth.append(resolved.strip())\\n    print(\"Updated request:\", resolved)\\nQuery: Can you explain how to say \\'hello\\' in Spanish?\\nRequest: {\"task_description\": \"say \\'hello\\'\", \"learning_language\": \"Spanish\", \"native_language\": \"English\", \"full_query\": \"Can you explain how to say \\'hello\\' in Spanish?\"}\\nRequested changes: \\nQuery: I need help understanding the French word for \\'goodbye\\'.\\nRequest: {\"task_description\": \"understanding the French word for \\'goodbye\\'\", \"learning_language\": \"French\", \"native_language\": \"English\", \"full_query\": \"I need help understanding the French word for \\'goodbye\\'.\"}\\nRequested changes: \\nQuery: Can you tell me how to say \\'thank you\\' in German?\\nRequest: {\"task_description\": \"say \\'thank you\\'\", \"learning_language\": \"German\", \"native_language\": \"English\", \"full_query\": \"Can you tell me how to say \\'thank you\\' in German?\"}\\nRequested changes: \\nQuery: I\\'m trying to learn the Italian word for \\'please\\'.\\nRequest: {\"task_description\": \"Learn the Italian word for \\'please\\'\", \"learning_language\": \"Italian\", \"native_language\": \"English\", \"full_query\": \"I\\'m trying to learn the Italian word for \\'please\\'.\"}\\nRequested changes: \\nQuery: Can you help me with the pronunciation of \\'yes\\' in Portuguese?\\nRequest: {\"task_description\": \"Help with pronunciation of \\'yes\\' in Portuguese\", \"learning_language\": \"Portuguese\", \"native_language\": \"English\", \"full_query\": \"Can you help me with the pronunciation of \\'yes\\' in Portuguese?\"}\\nRequested changes: \\nQuery: I\\'m looking for the Dutch word for \\'no\\'.\\nRequest: {\"task_description\": \"Find the Dutch word for \\'no\\'\", \"learning_language\": \"Dutch\", \"native_language\": \"English\", \"full_query\": \"I\\'m looking for the Dutch word for \\'no\\'.\"}\\nRequested changes: \\nQuery: Can you explain the meaning of \\'hello\\' in Japanese?\\nRequest: {\"task_description\": \"Explain the meaning of \\'hello\\' in Japanese\", \"learning_language\": \"Japanese\", \"native_language\": \"English\", \"full_query\": \"Can you explain the meaning of \\'hello\\' in Japanese?\"}\\nRequested changes: \\nQuery: I need help understanding the Russian word for \\'thank you\\'.\\nRequest: {\"task_description\": \"understanding the Russian word for \\'thank you\\'\", \"learning_language\": \"Russian\", \"native_language\": \"English\", \"full_query\": \"I need help understanding the Russian word for \\'thank you\\'.\"}\\nRequested changes: \\nQuery: Can you tell me how to say \\'goodbye\\' in Chinese?\\nRequest: {\"task_description\": \"say goodbye\", \"learning_language\": \"Chinese\", \"native_language\": \"English\", \"full_query\": \"Can you tell me how to say \\'goodbye\\' in Chinese?\"}\\nRequested changes: \\nQuery: I\\'m trying to learn the Arabic word for \\'please\\'.\\nRequest: {\"task_description\": \"Learn the Arabic word for \\'please\\'\", \"learning_language\": \"Arabic\", \"native_language\": \"English\", \"full_query\": \"I\\'m trying to learn the Arabic word for \\'please\\'.\"}\\nRequested changes: \\nNow you can use the ground_truth as shown above in Evaluate the Requests Chain!\\n# Now you have a new ground truth set to use as shown above!\\nground_truth\\n[\\'{\"task_description\": \"say \\\\\\'hello\\\\\\'\", \"learning_language\": \"Spanish\", \"native_language\": \"English\", \"full_query\": \"Can you explain how to say \\\\\\'hello\\\\\\' in Spanish?\"}\\',\\n \\'{\"task_description\": \"understanding the French word for \\\\\\'goodbye\\\\\\'\", \"learning_language\": \"French\", \"native_language\": \"English\", \"full_query\": \"I need help understanding the French word for \\\\\\'goodbye\\\\\\'.\"}\\',\\n \\'{\"task_description\": \"say \\\\\\'thank you\\\\\\'\", \"learning_language\": \"German\", \"native_language\": \"English\", \"full_query\": \"Can you tell me how to say \\\\\\'thank you\\\\\\' in German?\"}\\',\\n \\'{\"task_description\": \"Learn the Italian word for \\\\\\'please\\\\\\'\", \"learning_language\": \"Italian\", \"native_language\": \"English\", \"full_query\": \"I\\\\\\'m trying to learn the Italian word for \\\\\\'please\\\\\\'.\"}\\',\\n \\'{\"task_description\": \"Help with pronunciation of \\\\\\'yes\\\\\\' in Portuguese\", \"learning_language\": \"Portuguese\", \"native_language\": \"English\", \"full_query\": \"Can you help me with the pronunciation of \\\\\\'yes\\\\\\' in Portuguese?\"}\\',\\n \\'{\"task_description\": \"Find the Dutch word for \\\\\\'no\\\\\\'\", \"learning_language\": \"Dutch\", \"native_language\": \"English\", \"full_query\": \"I\\\\\\'m looking for the Dutch word for \\\\\\'no\\\\\\'.\"}\\',\\n \\'{\"task_description\": \"Explain the meaning of \\\\\\'hello\\\\\\' in Japanese\", \"learning_language\": \"Japanese\", \"native_language\": \"English\", \"full_query\": \"Can you explain the meaning of \\\\\\'hello\\\\\\' in Japanese?\"}\\',\\n \\'{\"task_description\": \"understanding the Russian word for \\\\\\'thank you\\\\\\'\", \"learning_language\": \"Russian\", \"native_language\": \"English\", \"full_query\": \"I need help understanding the Russian word for \\\\\\'thank you\\\\\\'.\"}\\',\\n \\'{\"task_description\": \"say goodbye\", \"learning_language\": \"Chinese\", \"native_language\": \"English\", \"full_query\": \"Can you tell me how to say \\\\\\'goodbye\\\\\\' in Chinese?\"}\\',\\n \\'{\"task_description\": \"Learn the Arabic word for \\\\\\'please\\\\\\'\", \"learning_language\": \"Arabic\", \"native_language\": \"English\", \"full_query\": \"I\\\\\\'m trying to learn the Arabic word for \\\\\\'please\\\\\\'.\"}\\']\\nprevious\\nLLM Math\\nnext\\nQuestion Answering Benchmarking: Paul Graham Essay\\n Contents\\n  \\nLoad the API Chain\\nOptional: Generate Input Questions and Request Ground Truth Queries\\nRun the API Chain\\nEvaluate the requests chain\\nEvaluate the Response Chain\\nGenerating Test Datasets\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\evaluation\\\\openapi_eval.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nQuestion Answering Benchmarking: Paul Graham Essay\\n Contents \\nLoading the data\\nSetting up a chain\\nMake a prediction\\nMake many predictions\\nEvaluate performance\\nQuestion Answering Benchmarking: Paul Graham Essay#\\nHere we go over how to benchmark performance on a question answering task over a Paul Graham essay.\\nIt is highly reccomended that you do any evaluation/benchmarking with tracing enabled. See here for an explanation of what tracing is and how to set it up.\\n# Comment this out if you are NOT using tracing\\nimport os\\nos.environ[\"LANGCHAIN_HANDLER\"] = \"langchain\"\\nLoading the data#\\nFirst, let’s load the data.\\nfrom langchain.evaluation.loading import load_dataset\\ndataset = load_dataset(\"question-answering-paul-graham\")\\nFound cached dataset json (/Users/harrisonchase/.cache/huggingface/datasets/LangChainDatasets___json/LangChainDatasets--question-answering-paul-graham-76e8f711e038d742/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\\nSetting up a chain#\\nNow we need to create some pipelines for doing question answering. Step one in that is creating an index over the data in question.\\nfrom langchain.document_loaders import TextLoader\\nloader = TextLoader(\"../../modules/paul_graham_essay.txt\")\\nfrom langchain.indexes import VectorstoreIndexCreator\\nvectorstore = VectorstoreIndexCreator().from_loaders([loader]).vectorstore\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\nNow we can create a question answering chain.\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.llms import OpenAI\\nchain = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=vectorstore.as_retriever(), input_key=\"question\")\\nMake a prediction#\\nFirst, we can make predictions one datapoint at a time. Doing it at this level of granularity allows use to explore the outputs in detail, and also is a lot cheaper than running over multiple datapoints\\nchain(dataset[0])\\n{\\'question\\': \\'What were the two main things the author worked on before college?\\',\\n \\'answer\\': \\'The two main things the author worked on before college were writing and programming.\\',\\n \\'result\\': \\' Writing and programming.\\'}\\nMake many predictions#\\nNow we can make predictions\\npredictions = chain.apply(dataset)\\nEvaluate performance#\\nNow we can evaluate the predictions. The first thing we can do is look at them by eye.\\npredictions[0]\\n{\\'question\\': \\'What were the two main things the author worked on before college?\\',\\n \\'answer\\': \\'The two main things the author worked on before college were writing and programming.\\',\\n \\'result\\': \\' Writing and programming.\\'}\\nNext, we can use a language model to score them programatically\\nfrom langchain.evaluation.qa import QAEvalChain\\nllm = OpenAI(temperature=0)\\neval_chain = QAEvalChain.from_llm(llm)\\ngraded_outputs = eval_chain.evaluate(dataset, predictions, question_key=\"question\", prediction_key=\"result\")\\nWe can add in the graded output to the predictions dict and then get a count of the grades.\\nfor i, prediction in enumerate(predictions):\\n    prediction[\\'grade\\'] = graded_outputs[i][\\'text\\']\\nfrom collections import Counter\\nCounter([pred[\\'grade\\'] for pred in predictions])\\nCounter({\\' CORRECT\\': 12, \\' INCORRECT\\': 10})\\nWe can also filter the datapoints to the incorrect examples and look at them.\\nincorrect = [pred for pred in predictions if pred[\\'grade\\'] == \" INCORRECT\"]\\nincorrect[0]\\n{\\'question\\': \\'What did the author write their dissertation on?\\',\\n \\'answer\\': \\'The author wrote their dissertation on applications of continuations.\\',\\n \\'result\\': \\' The author does not mention what their dissertation was on, so it is not known.\\',\\n \\'grade\\': \\' INCORRECT\\'}\\nprevious\\nEvaluating an OpenAPI Chain\\nnext\\nQuestion Answering Benchmarking: State of the Union Address\\n Contents\\n  \\nLoading the data\\nSetting up a chain\\nMake a prediction\\nMake many predictions\\nEvaluate performance\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\evaluation\\\\qa_benchmarking_pg.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nQuestion Answering Benchmarking: State of the Union Address\\n Contents \\nLoading the data\\nSetting up a chain\\nMake a prediction\\nMake many predictions\\nEvaluate performance\\nQuestion Answering Benchmarking: State of the Union Address#\\nHere we go over how to benchmark performance on a question answering task over a state of the union address.\\nIt is highly reccomended that you do any evaluation/benchmarking with tracing enabled. See here for an explanation of what tracing is and how to set it up.\\n# Comment this out if you are NOT using tracing\\nimport os\\nos.environ[\"LANGCHAIN_HANDLER\"] = \"langchain\"\\nLoading the data#\\nFirst, let’s load the data.\\nfrom langchain.evaluation.loading import load_dataset\\ndataset = load_dataset(\"question-answering-state-of-the-union\")\\nFound cached dataset json (/Users/harrisonchase/.cache/huggingface/datasets/LangChainDatasets___json/LangChainDatasets--question-answering-state-of-the-union-a7e5a3b2db4f440d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\\nSetting up a chain#\\nNow we need to create some pipelines for doing question answering. Step one in that is creating an index over the data in question.\\nfrom langchain.document_loaders import TextLoader\\nloader = TextLoader(\"../../modules/state_of_the_union.txt\")\\nfrom langchain.indexes import VectorstoreIndexCreator\\nvectorstore = VectorstoreIndexCreator().from_loaders([loader]).vectorstore\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\nNow we can create a question answering chain.\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.llms import OpenAI\\nchain = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=vectorstore.as_retriever(), input_key=\"question\")\\nMake a prediction#\\nFirst, we can make predictions one datapoint at a time. Doing it at this level of granularity allows use to explore the outputs in detail, and also is a lot cheaper than running over multiple datapoints\\nchain(dataset[0])\\n{\\'question\\': \\'What is the purpose of the NATO Alliance?\\',\\n \\'answer\\': \\'The purpose of the NATO Alliance is to secure peace and stability in Europe after World War 2.\\',\\n \\'result\\': \\' The NATO Alliance was created to secure peace and stability in Europe after World War 2.\\'}\\nMake many predictions#\\nNow we can make predictions\\npredictions = chain.apply(dataset)\\nEvaluate performance#\\nNow we can evaluate the predictions. The first thing we can do is look at them by eye.\\npredictions[0]\\n{\\'question\\': \\'What is the purpose of the NATO Alliance?\\',\\n \\'answer\\': \\'The purpose of the NATO Alliance is to secure peace and stability in Europe after World War 2.\\',\\n \\'result\\': \\' The purpose of the NATO Alliance is to secure peace and stability in Europe after World War 2.\\'}\\nNext, we can use a language model to score them programatically\\nfrom langchain.evaluation.qa import QAEvalChain\\nllm = OpenAI(temperature=0)\\neval_chain = QAEvalChain.from_llm(llm)\\ngraded_outputs = eval_chain.evaluate(dataset, predictions, question_key=\"question\", prediction_key=\"result\")\\nWe can add in the graded output to the predictions dict and then get a count of the grades.\\nfor i, prediction in enumerate(predictions):\\n    prediction[\\'grade\\'] = graded_outputs[i][\\'text\\']\\nfrom collections import Counter\\nCounter([pred[\\'grade\\'] for pred in predictions])\\nCounter({\\' CORRECT\\': 7, \\' INCORRECT\\': 4})\\nWe can also filter the datapoints to the incorrect examples and look at them.\\nincorrect = [pred for pred in predictions if pred[\\'grade\\'] == \" INCORRECT\"]\\nincorrect[0]\\n{\\'question\\': \\'What is the U.S. Department of Justice doing to combat the crimes of Russian oligarchs?\\',\\n \\'answer\\': \\'The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs.\\',\\n \\'result\\': \\' The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs and is naming a chief prosecutor for pandemic fraud.\\',\\n \\'grade\\': \\' INCORRECT\\'}\\nprevious\\nQuestion Answering Benchmarking: Paul Graham Essay\\nnext\\nQA Generation\\n Contents\\n  \\nLoading the data\\nSetting up a chain\\nMake a prediction\\nMake many predictions\\nEvaluate performance\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\evaluation\\\\qa_benchmarking_sota.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nQA Generation\\nQA Generation#\\nThis notebook shows how to use the QAGenerationChain to come up with question-answer pairs over a specific document.\\nThis is important because often times you may not have data to evaluate your question-answer system over, so this is a cheap and lightweight way to generate it!\\nfrom langchain.document_loaders import TextLoader\\nloader = TextLoader(\"../../modules/state_of_the_union.txt\")\\ndoc = loader.load()[0]\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.chains import QAGenerationChain\\nchain = QAGenerationChain.from_llm(ChatOpenAI(temperature = 0))\\nqa = chain.run(doc.page_content)\\nqa[1]\\n{\\'question\\': \\'What is the U.S. Department of Justice doing to combat the crimes of Russian oligarchs?\\',\\n \\'answer\\': \\'The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs.\\'}\\nprevious\\nQuestion Answering Benchmarking: State of the Union Address\\nnext\\nQuestion Answering\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\evaluation\\\\qa_generation.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nQuestion Answering\\n Contents \\nSetup\\nExamples\\nPredictions\\nEvaluation\\nCustomize Prompt\\nEvaluation without Ground Truth\\nComparing to other evaluation metrics\\nQuestion Answering#\\nThis notebook covers how to evaluate generic question answering problems. This is a situation where you have an example containing a question and its corresponding ground truth answer, and you want to measure how well the language model does at answering those questions.\\nSetup#\\nFor demonstration purposes, we will just evaluate a simple question answering system that only evaluates the model’s internal knowledge. Please see other notebooks for examples where it evaluates how the model does at question answering over data not present in what the model was trained on.\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\nfrom langchain.llms import OpenAI\\nprompt = PromptTemplate(template=\"Question: {question}\\\\nAnswer:\", input_variables=[\"question\"])\\nllm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\\nchain = LLMChain(llm=llm, prompt=prompt)\\nExamples#\\nFor this purpose, we will just use two simple hardcoded examples, but see other notebooks for tips on how to get and/or generate these examples.\\nexamples = [\\n    {\\n        \"question\": \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\",\\n        \"answer\": \"11\"\\n    },\\n    {\\n        \"question\": \\'Is the following sentence plausible? \"Joao Moutinho caught the screen pass in the NFC championship.\"\\',\\n        \"answer\": \"No\"\\n    }\\n]\\nPredictions#\\nWe can now make and inspect the predictions for these questions.\\npredictions = chain.apply(examples)\\npredictions\\n[{\\'text\\': \\' 11 tennis balls\\'},\\n {\\'text\\': \\' No, this sentence is not plausible. Joao Moutinho is a professional soccer player, not an American football player, so it is not likely that he would be catching a screen pass in the NFC championship.\\'}]\\nEvaluation#\\nWe can see that if we tried to just do exact match on the answer answers (11 and No) they would not match what the language model answered. However, semantically the language model is correct in both cases. In order to account for this, we can use a language model itself to evaluate the answers.\\nfrom langchain.evaluation.qa import QAEvalChain\\nllm = OpenAI(temperature=0)\\neval_chain = QAEvalChain.from_llm(llm)\\ngraded_outputs = eval_chain.evaluate(examples, predictions, question_key=\"question\", prediction_key=\"text\")\\nfor i, eg in enumerate(examples):\\n    print(f\"Example {i}:\")\\n    print(\"Question: \" + eg[\\'question\\'])\\n    print(\"Real Answer: \" + eg[\\'answer\\'])\\n    print(\"Predicted Answer: \" + predictions[i][\\'text\\'])\\n    print(\"Predicted Grade: \" + graded_outputs[i][\\'text\\'])\\n    print()\\nExample 0:\\nQuestion: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\\nReal Answer: 11\\nPredicted Answer:  11 tennis balls\\nPredicted Grade:  CORRECT\\nExample 1:\\nQuestion: Is the following sentence plausible? \"Joao Moutinho caught the screen pass in the NFC championship.\"\\nReal Answer: No\\nPredicted Answer:  No, this sentence is not plausible. Joao Moutinho is a professional soccer player, not an American football player, so it is not likely that he would be catching a screen pass in the NFC championship.\\nPredicted Grade:  CORRECT\\nCustomize Prompt#\\nYou can also customize the prompt that is used. Here is an example prompting it using a score from 0 to 10.\\nThe custom prompt requires 3 input variables: “query”, “answer” and “result”. Where “query” is the question, “answer” is the ground truth answer, and “result” is the predicted answer.\\nfrom langchain.prompts.prompt import PromptTemplate\\n_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students\\' answers to questions.\\nYou are grading the following question:\\n{query}\\nHere is the real answer:\\n{answer}\\nYou are grading the following predicted answer:\\n{result}\\nWhat grade do you give from 0 to 10, where 0 is the lowest (very low similarity) and 10 is the highest (very high similarity)?\\n\"\"\"\\nPROMPT = PromptTemplate(input_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE)\\nevalchain = QAEvalChain.from_llm(llm=llm,prompt=PROMPT)\\nevalchain.evaluate(examples, predictions, question_key=\"question\", answer_key=\"answer\", prediction_key=\"text\")\\nEvaluation without Ground Truth#\\nIts possible to evaluate question answering systems without ground truth. You would need a \"context\" input that reflects what the information the LLM uses to answer the question. This context can be obtained by any retreival system. Here’s an example of how it works:\\ncontext_examples = [\\n    {\\n        \"question\": \"How old am I?\",\\n        \"context\": \"I am 30 years old. I live in New York and take the train to work everyday.\",\\n    },\\n    {\\n        \"question\": \\'Who won the NFC championship game in 2023?\"\\',\\n        \"context\": \"NFC Championship Game 2023: Philadelphia Eagles 31, San Francisco 49ers 7\"\\n    }\\n]\\nQA_PROMPT = \"Answer the question based on the  context\\\\nContext:{context}\\\\nQuestion:{question}\\\\nAnswer:\"\\ntemplate = PromptTemplate(input_variables=[\"context\", \"question\"], template=QA_PROMPT)\\nqa_chain = LLMChain(llm=llm, prompt=template)\\npredictions = qa_chain.apply(context_examples)\\npredictions\\n[{\\'text\\': \\'You are 30 years old.\\'},\\n {\\'text\\': \\' The Philadelphia Eagles won the NFC championship game in 2023.\\'}]\\nfrom langchain.evaluation.qa import ContextQAEvalChain\\neval_chain = ContextQAEvalChain.from_llm(llm)\\ngraded_outputs = eval_chain.evaluate(context_examples, predictions, question_key=\"question\", prediction_key=\"text\")\\ngraded_outputs\\n[{\\'text\\': \\' CORRECT\\'}, {\\'text\\': \\' CORRECT\\'}]\\nComparing to other evaluation metrics#\\nWe can compare the evaluation results we get to other common evaluation metrics. To do this, let’s load some evaluation metrics from HuggingFace’s evaluate package.\\n# Some data munging to get the examples in the right format\\nfor i, eg in enumerate(examples):\\n    eg[\\'id\\'] = str(i)\\n    eg[\\'answers\\'] = {\"text\": [eg[\\'answer\\']], \"answer_start\": [0]}\\n    predictions[i][\\'id\\'] = str(i)\\n    predictions[i][\\'prediction_text\\'] = predictions[i][\\'text\\']\\nfor p in predictions:\\n    del p[\\'text\\']\\nnew_examples = examples.copy()\\nfor eg in new_examples:\\n    del eg [\\'question\\']\\n    del eg[\\'answer\\']\\nfrom evaluate import load\\nsquad_metric = load(\"squad\")\\nresults = squad_metric.compute(\\n    references=new_examples,\\n    predictions=predictions,\\n)\\nresults\\n{\\'exact_match\\': 0.0, \\'f1\\': 28.125}\\nprevious\\nQA Generation\\nnext\\nSQL Question Answering Benchmarking: Chinook\\n Contents\\n  \\nSetup\\nExamples\\nPredictions\\nEvaluation\\nCustomize Prompt\\nEvaluation without Ground Truth\\nComparing to other evaluation metrics\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\evaluation\\\\question_answering.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nSQL Question Answering Benchmarking: Chinook\\n Contents \\nLoading the data\\nSetting up a chain\\nMake a prediction\\nMake many predictions\\nEvaluate performance\\nSQL Question Answering Benchmarking: Chinook#\\nHere we go over how to benchmark performance on a question answering task over a SQL database.\\nIt is highly reccomended that you do any evaluation/benchmarking with tracing enabled. See here for an explanation of what tracing is and how to set it up.\\n# Comment this out if you are NOT using tracing\\nimport os\\nos.environ[\"LANGCHAIN_HANDLER\"] = \"langchain\"\\nLoading the data#\\nFirst, let’s load the data.\\nfrom langchain.evaluation.loading import load_dataset\\ndataset = load_dataset(\"sql-qa-chinook\")\\nDownloading and preparing dataset json/LangChainDatasets--sql-qa-chinook to /Users/harrisonchase/.cache/huggingface/datasets/LangChainDatasets___json/LangChainDatasets--sql-qa-chinook-7528565d2d992b47/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\\nDataset json downloaded and prepared to /Users/harrisonchase/.cache/huggingface/datasets/LangChainDatasets___json/LangChainDatasets--sql-qa-chinook-7528565d2d992b47/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\\ndataset[0]\\n{\\'question\\': \\'How many employees are there?\\', \\'answer\\': \\'8\\'}\\nSetting up a chain#\\nThis uses the example Chinook database.\\nTo set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file in a notebooks folder at the root of this repository.\\nNote that here we load a simple chain. If you want to experiment with more complex chains, or an agent, just create the chain object in a different way.\\nfrom langchain import OpenAI, SQLDatabase, SQLDatabaseChain\\ndb = SQLDatabase.from_uri(\"sqlite:///../../../notebooks/Chinook.db\")\\nllm = OpenAI(temperature=0)\\nNow we can create a SQL database chain.\\nchain = SQLDatabaseChain(llm=llm, database=db, input_key=\"question\")\\nMake a prediction#\\nFirst, we can make predictions one datapoint at a time. Doing it at this level of granularity allows use to explore the outputs in detail, and also is a lot cheaper than running over multiple datapoints\\nchain(dataset[0])\\n{\\'question\\': \\'How many employees are there?\\',\\n \\'answer\\': \\'8\\',\\n \\'result\\': \\' There are 8 employees.\\'}\\nMake many predictions#\\nNow we can make predictions. Note that we add a try-except because this chain can sometimes error (if SQL is written incorrectly, etc)\\npredictions = []\\npredicted_dataset = []\\nerror_dataset = []\\nfor data in dataset:\\n    try:\\n        predictions.append(chain(data))\\n        predicted_dataset.append(data)\\n    except:\\n        error_dataset.append(data)\\nEvaluate performance#\\nNow we can evaluate the predictions. We can use a language model to score them programatically\\nfrom langchain.evaluation.qa import QAEvalChain\\nllm = OpenAI(temperature=0)\\neval_chain = QAEvalChain.from_llm(llm)\\ngraded_outputs = eval_chain.evaluate(predicted_dataset, predictions, question_key=\"question\", prediction_key=\"result\")\\nWe can add in the graded output to the predictions dict and then get a count of the grades.\\nfor i, prediction in enumerate(predictions):\\n    prediction[\\'grade\\'] = graded_outputs[i][\\'text\\']\\nfrom collections import Counter\\nCounter([pred[\\'grade\\'] for pred in predictions])\\nCounter({\\' CORRECT\\': 3, \\' INCORRECT\\': 4})\\nWe can also filter the datapoints to the incorrect examples and look at them.\\nincorrect = [pred for pred in predictions if pred[\\'grade\\'] == \" INCORRECT\"]\\nincorrect[0]\\n{\\'question\\': \\'How many employees are also customers?\\',\\n \\'answer\\': \\'None\\',\\n \\'result\\': \\' 59 employees are also customers.\\',\\n \\'grade\\': \\' INCORRECT\\'}\\nprevious\\nQuestion Answering\\nnext\\nInstallation\\n Contents\\n  \\nLoading the data\\nSetting up a chain\\nMake a prediction\\nMake many predictions\\nEvaluate performance\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\evaluation\\\\sql_qa_benchmarking_chinook.html'}),\n",
              " Document(page_content='.ipynb\\n.pdf\\nQuestion answering over a group chat messages\\n Contents \\n1. Install required packages\\n2. Add API keys\\n2. Create sample data\\n3. Ingest chat embeddings\\n4. Ask questions\\nQuestion answering over a group chat messages#\\nIn this tutorial, we are going to use Langchain + Deep Lake with GPT4 to semantically search and ask questions over a group chat.\\nView a working demo here\\n1. Install required packages#\\n!python3 -m pip install --upgrade langchain deeplake openai tiktoken\\n2. Add API keys#\\nimport os\\nimport getpass\\nfrom langchain.document_loaders import PyPDFLoader, TextLoader\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\\nfrom langchain.vectorstores import DeepLake\\nfrom langchain.chains import ConversationalRetrievalChain, RetrievalQA\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.llms import OpenAI\\nos.environ[\\'OPENAI_API_KEY\\'] = getpass.getpass(\\'OpenAI API Key:\\')\\nos.environ[\\'ACTIVELOOP_TOKEN\\'] = getpass.getpass(\\'Activeloop Token:\\')\\nos.environ[\\'ACTIVELOOP_ORG\\'] = getpass.getpass(\\'Activeloop Org:\\')\\norg = os.environ[\\'ACTIVELOOP_ORG\\']\\nembeddings = OpenAIEmbeddings()\\ndataset_path = \\'hub://\\' + org + \\'/data\\'\\n2. Create sample data#\\nYou can generate a sample group chat conversation using ChatGPT with this prompt:\\nGenerate a group chat conversation with three friends talking about their day, referencing real places and fictional names. Make it funny and as detailed as possible.\\nI’ve already generated such a chat in messages.txt. We can keep it simple and use this for our example.\\n3. Ingest chat embeddings#\\nWe load the messages in the text file, chunk and upload to ActiveLoop Vector store.\\nwith open(\"messages.txt\") as f:\\n    state_of_the_union = f.read()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\npages = text_splitter.split_text(state_of_the_union)\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\\ntexts = text_splitter.split_documents(pages)\\nprint (texts)\\ndataset_path = \\'hub://\\'+org+\\'/data\\'\\nembeddings = OpenAIEmbeddings()\\ndb = DeepLake.from_documents(texts, embeddings, dataset_path=dataset_path)\\n4. Ask questions#\\nNow we can ask a question and get an answer back with a semantic search:\\ndb = DeepLake(dataset_path=dataset_path, read_only=True, embedding_function=embeddings)\\nretriever = db.as_retriever()\\nretriever.search_kwargs[\\'distance_metric\\'] = \\'cos\\'\\nretriever.search_kwargs[\\'k\\'] = 4\\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=False)\\n# What was the restaurant the group was talking about called?\\nquery = input(\"Enter query:\")\\n# The Hungry Lobster\\nans = qa({\"query\": query})\\nprint(ans)\\n Contents\\n  \\n1. Install required packages\\n2. Add API keys\\n2. Create sample data\\n3. Ingest chat embeddings\\n4. Ask questions\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\use_cases\\\\question_answering\\\\semantic-search-over-chat.html'}),\n",
              " Document(page_content='Source code for langchain.text_splitter\\n\"\"\"Functionality for splitting text.\"\"\"\\nfrom __future__ import annotations\\nimport copy\\nimport logging\\nfrom abc import ABC, abstractmethod\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Union,\\n)\\nfrom langchain.docstore.document import Document\\nfrom langchain.schema import BaseDocumentTransformer\\nlogger = logging.getLogger(__name__)\\n[docs]class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n    ):\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n[docs]    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"\\n[docs]    def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            for chunk in self.split_text(text):\\n                new_doc = Document(\\n                    page_content=chunk, metadata=copy.deepcopy(_metadatas[i])\\n                )\\n                documents.append(new_doc)\\n        return documents\\n[docs]    def split_documents(self, documents: List[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts = [doc.page_content for doc in documents]\\n        metadatas = [doc.metadata for doc in documents]\\n        return self.create_documents(texts, metadatas=metadatas)\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)\\n        docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n[docs]    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase\\n            if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n[docs]    @classmethod\\n    def from_tiktoken_encoder(\\n        cls,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TextSplitter:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        def _tiktoken_encoder(text: str, **kwargs: Any) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                    **kwargs,\\n                )\\n            )\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n[docs]    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))\\n[docs]    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Asynchronously transform a sequence of documents by splitting them.\"\"\"\\n        raise NotImplementedError\\n[docs]class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Implementation of splitting text that looks at characters.\"\"\"\\n    def __init__(self, separator: str = \"\\\\n\\\\n\", **kwargs: Any):\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n[docs]    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        if self._separator:\\n            splits = text.split(self._separator)\\n        else:\\n            splits = list(text)\\n        return self._merge_splits(splits, self._separator)\\n[docs]class TokenTextSplitter(TextSplitter):\\n    \"\"\"Implementation of splitting text that looks at tokens.\"\"\"\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ):\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n[docs]    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = []\\n        input_ids = self._tokenizer.encode(\\n            text,\\n            allowed_special=self._allowed_special,\\n            disallowed_special=self._disallowed_special,\\n        )\\n        start_idx = 0\\n        cur_idx = min(start_idx + self._chunk_size, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n        while start_idx < len(input_ids):\\n            splits.append(self._tokenizer.decode(chunk_ids))\\n            start_idx += self._chunk_size - self._chunk_overlap\\n            cur_idx = min(start_idx + self._chunk_size, len(input_ids))\\n            chunk_ids = input_ids[start_idx:cur_idx]\\n        return splits\\n[docs]class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Implementation of splitting text that looks at characters.\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n    def __init__(self, separators: Optional[List[str]] = None, **kwargs: Any):\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n[docs]    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = self._separators[-1]\\n        for _s in self._separators:\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if _s in text:\\n                separator = _s\\n                break\\n        # Now that we have the separator, split the text\\n        if separator:\\n            splits = text.split(separator)\\n        else:\\n            splits = list(text)\\n        # Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                other_info = self.split_text(s)\\n                final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n[docs]class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Implementation of splitting text that looks at sentences using NLTK.\"\"\"\\n    def __init__(self, separator: str = \"\\\\n\\\\n\", **kwargs: Any):\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n[docs]    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text)\\n        return self._merge_splits(splits, self._separator)\\n[docs]class SpacyTextSplitter(TextSplitter):\\n    \"\"\"Implementation of splitting text that looks at sentences using Spacy.\"\"\"\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", pipeline: str = \"en_core_web_sm\", **kwargs: Any\\n    ):\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import spacy\\n        except ImportError:\\n            raise ImportError(\\n                \"Spacy is not installed, please install it with `pip install spacy`.\"\\n            )\\n        self._tokenizer = spacy.load(pipeline)\\n        self._separator = separator\\n[docs]    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (str(s) for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)\\n[docs]class MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n    def __init__(self, **kwargs: Any):\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = [\\n            # First, try to split along Markdown headings (starting with level 2)\\n            \"\\\\n## \",\\n            \"\\\\n### \",\\n            \"\\\\n#### \",\\n            \"\\\\n##### \",\\n            \"\\\\n###### \",\\n            # Note the alternative syntax for headings (below) is not handled here\\n            # Heading level 2\\n            # ---------------\\n            # End of code block\\n            \"```\\\\n\\\\n\",\\n            # Horizontal lines\\n            \"\\\\n\\\\n***\\\\n\\\\n\",\\n            \"\\\\n\\\\n---\\\\n\\\\n\",\\n            \"\\\\n\\\\n___\\\\n\\\\n\",\\n            # Note that this splitter doesn\\'t handle horizontal lines defined\\n            # by *three or more* of ***, ---, or ___, but this is not handled\\n            \"\\\\n\\\\n\",\\n            \"\\\\n\",\\n            \" \",\\n            \"\",\\n        ]\\n        super().__init__(separators=separators, **kwargs)\\n[docs]class LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n    def __init__(self, **kwargs: Any):\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = [\\n            # First, try to split along Latex sections\\n            \"\\\\n\\\\\\\\chapter{\",\\n            \"\\\\n\\\\\\\\section{\",\\n            \"\\\\n\\\\\\\\subsection{\",\\n            \"\\\\n\\\\\\\\subsubsection{\",\\n            # Now split by environments\\n            \"\\\\n\\\\\\\\begin{enumerate}\",\\n            \"\\\\n\\\\\\\\begin{itemize}\",\\n            \"\\\\n\\\\\\\\begin{description}\",\\n            \"\\\\n\\\\\\\\begin{list}\",\\n            \"\\\\n\\\\\\\\begin{quote}\",\\n            \"\\\\n\\\\\\\\begin{quotation}\",\\n            \"\\\\n\\\\\\\\begin{verse}\",\\n            \"\\\\n\\\\\\\\begin{verbatim}\",\\n            ## Now split by math environments\\n            \"\\\\n\\\\\\\\begin{align}\",\\n            \"$$\",\\n            \"$\",\\n            # Now split by the normal type of lines\\n            \" \",\\n            \"\",\\n        ]\\n        super().__init__(separators=separators, **kwargs)\\n[docs]class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n    def __init__(self, **kwargs: Any):\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = [\\n            # First, try to split along class definitions\\n            \"\\\\nclass \",\\n            \"\\\\ndef \",\\n            \"\\\\n\\\\tdef \",\\n            # Now split by the normal type of lines\\n            \"\\\\n\\\\n\",\\n            \"\\\\n\",\\n            \" \",\\n            \"\",\\n        ]\\n        super().__init__(separators=separators, **kwargs)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\text_splitter.html'}),\n",
              " Document(page_content='Source code for langchain.agents.agent\\n\"\"\"Chain that takes in an input and produces an action and action input.\"\"\"\\nfrom __future__ import annotations\\nimport asyncio\\nimport json\\nimport logging\\nimport time\\nfrom abc import abstractmethod\\nfrom pathlib import Path\\nfrom typing import Any, Dict, List, Optional, Sequence, Tuple, Union\\nimport yaml\\nfrom pydantic import BaseModel, root_validator\\nfrom langchain.agents.tools import InvalidTool\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.input import get_color_mapping\\nfrom langchain.prompts.base import BasePromptTemplate\\nfrom langchain.prompts.few_shot import FewShotPromptTemplate\\nfrom langchain.prompts.prompt import PromptTemplate\\nfrom langchain.schema import (\\n    AgentAction,\\n    AgentFinish,\\n    BaseLanguageModel,\\n    BaseMessage,\\n    BaseOutputParser,\\n)\\nfrom langchain.tools.base import BaseTool\\nfrom langchain.utilities.asyncio import asyncio_timeout\\nlogger = logging.getLogger(__name__)\\n[docs]class BaseSingleActionAgent(BaseModel):\\n    \"\"\"Base Agent class.\"\"\"\\n    @property\\n    def return_values(self) -> List[str]:\\n        \"\"\"Return values of the agent.\"\"\"\\n        return [\"output\"]\\n[docs]    def get_allowed_tools(self) -> Optional[List[str]]:\\n        return None\\n[docs]    @abstractmethod\\n    def plan(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\\n    ) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            **kwargs: User inputs.\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n[docs]    @abstractmethod\\n    async def aplan(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\\n    ) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            **kwargs: User inputs.\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n    @property\\n    @abstractmethod\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys.\\n        :meta private:\\n        \"\"\"\\n[docs]    def return_stopped_response(\\n        self,\\n        early_stopping_method: str,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        **kwargs: Any,\\n    ) -> AgentFinish:\\n        \"\"\"Return response when agent has been stopped due to max iterations.\"\"\"\\n        if early_stopping_method == \"force\":\\n            # `force` just returns a constant string\\n            return AgentFinish(\\n                {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\\n            )\\n        else:\\n            raise ValueError(\\n                f\"Got unsupported early_stopping_method `{early_stopping_method}`\"\\n            )\\n[docs]    @classmethod\\n    def from_llm_and_tools(\\n        cls,\\n        llm: BaseLanguageModel,\\n        tools: Sequence[BaseTool],\\n        callback_manager: Optional[BaseCallbackManager] = None,\\n        **kwargs: Any,\\n    ) -> BaseSingleActionAgent:\\n        raise NotImplementedError\\n    @property\\n    def _agent_type(self) -> str:\\n        \"\"\"Return Identifier of agent type.\"\"\"\\n        raise NotImplementedError\\n[docs]    def dict(self, **kwargs: Any) -> Dict:\\n        \"\"\"Return dictionary representation of agent.\"\"\"\\n        _dict = super().dict()\\n        _dict[\"_type\"] = str(self._agent_type)\\n        return _dict\\n[docs]    def save(self, file_path: Union[Path, str]) -> None:\\n        \"\"\"Save the agent.\\n        Args:\\n            file_path: Path to file to save the agent to.\\n        Example:\\n        .. code-block:: python\\n            # If working with agent executor\\n            agent.agent.save(file_path=\"path/agent.yaml\")\\n        \"\"\"\\n        # Convert file to Path object.\\n        if isinstance(file_path, str):\\n            save_path = Path(file_path)\\n        else:\\n            save_path = file_path\\n        directory_path = save_path.parent\\n        directory_path.mkdir(parents=True, exist_ok=True)\\n        # Fetch dictionary to save\\n        agent_dict = self.dict()\\n        if save_path.suffix == \".json\":\\n            with open(file_path, \"w\") as f:\\n                json.dump(agent_dict, f, indent=4)\\n        elif save_path.suffix == \".yaml\":\\n            with open(file_path, \"w\") as f:\\n                yaml.dump(agent_dict, f, default_flow_style=False)\\n        else:\\n            raise ValueError(f\"{save_path} must be json or yaml\")\\n[docs]    def tool_run_logging_kwargs(self) -> Dict:\\n        return {}\\n[docs]class BaseMultiActionAgent(BaseModel):\\n    \"\"\"Base Agent class.\"\"\"\\n    @property\\n    def return_values(self) -> List[str]:\\n        \"\"\"Return values of the agent.\"\"\"\\n        return [\"output\"]\\n[docs]    def get_allowed_tools(self) -> Optional[List[str]]:\\n        return None\\n[docs]    @abstractmethod\\n    def plan(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\\n    ) -> Union[List[AgentAction], AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            **kwargs: User inputs.\\n        Returns:\\n            Actions specifying what tool to use.\\n        \"\"\"\\n[docs]    @abstractmethod\\n    async def aplan(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\\n    ) -> Union[List[AgentAction], AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            **kwargs: User inputs.\\n        Returns:\\n            Actions specifying what tool to use.\\n        \"\"\"\\n    @property\\n    @abstractmethod\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys.\\n        :meta private:\\n        \"\"\"\\n[docs]    def return_stopped_response(\\n        self,\\n        early_stopping_method: str,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        **kwargs: Any,\\n    ) -> AgentFinish:\\n        \"\"\"Return response when agent has been stopped due to max iterations.\"\"\"\\n        if early_stopping_method == \"force\":\\n            # `force` just returns a constant string\\n            return AgentFinish({\"output\": \"Agent stopped due to max iterations.\"}, \"\")\\n        else:\\n            raise ValueError(\\n                f\"Got unsupported early_stopping_method `{early_stopping_method}`\"\\n            )\\n    @property\\n    def _agent_type(self) -> str:\\n        \"\"\"Return Identifier of agent type.\"\"\"\\n        raise NotImplementedError\\n[docs]    def dict(self, **kwargs: Any) -> Dict:\\n        \"\"\"Return dictionary representation of agent.\"\"\"\\n        _dict = super().dict()\\n        _dict[\"_type\"] = str(self._agent_type)\\n        return _dict\\n[docs]    def save(self, file_path: Union[Path, str]) -> None:\\n        \"\"\"Save the agent.\\n        Args:\\n            file_path: Path to file to save the agent to.\\n        Example:\\n        .. code-block:: python\\n            # If working with agent executor\\n            agent.agent.save(file_path=\"path/agent.yaml\")\\n        \"\"\"\\n        # Convert file to Path object.\\n        if isinstance(file_path, str):\\n            save_path = Path(file_path)\\n        else:\\n            save_path = file_path\\n        directory_path = save_path.parent\\n        directory_path.mkdir(parents=True, exist_ok=True)\\n        # Fetch dictionary to save\\n        agent_dict = self.dict()\\n        if save_path.suffix == \".json\":\\n            with open(file_path, \"w\") as f:\\n                json.dump(agent_dict, f, indent=4)\\n        elif save_path.suffix == \".yaml\":\\n            with open(file_path, \"w\") as f:\\n                yaml.dump(agent_dict, f, default_flow_style=False)\\n        else:\\n            raise ValueError(f\"{save_path} must be json or yaml\")\\n[docs]    def tool_run_logging_kwargs(self) -> Dict:\\n        return {}\\n[docs]class AgentOutputParser(BaseOutputParser):\\n[docs]    @abstractmethod\\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Parse text into agent action/finish.\"\"\"\\n[docs]class LLMSingleActionAgent(BaseSingleActionAgent):\\n    llm_chain: LLMChain\\n    output_parser: AgentOutputParser\\n    stop: List[str]\\n    @property\\n    def input_keys(self) -> List[str]:\\n        return list(set(self.llm_chain.input_keys) - {\"intermediate_steps\"})\\n[docs]    def plan(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\\n    ) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            **kwargs: User inputs.\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        output = self.llm_chain.run(\\n            intermediate_steps=intermediate_steps, stop=self.stop, **kwargs\\n        )\\n        return self.output_parser.parse(output)\\n[docs]    async def aplan(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\\n    ) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            **kwargs: User inputs.\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        output = await self.llm_chain.arun(\\n            intermediate_steps=intermediate_steps, stop=self.stop, **kwargs\\n        )\\n        return self.output_parser.parse(output)\\n[docs]    def tool_run_logging_kwargs(self) -> Dict:\\n        return {\\n            \"llm_prefix\": \"\",\\n            \"observation_prefix\": \"\" if len(self.stop) == 0 else self.stop[0],\\n        }\\n[docs]class Agent(BaseSingleActionAgent):\\n    \"\"\"Class responsible for calling the language model and deciding the action.\\n    This is driven by an LLMChain. The prompt in the LLMChain MUST include\\n    a variable called \"agent_scratchpad\" where the agent can put its\\n    intermediary work.\\n    \"\"\"\\n    llm_chain: LLMChain\\n    output_parser: AgentOutputParser\\n    allowed_tools: Optional[List[str]] = None\\n[docs]    def get_allowed_tools(self) -> Optional[List[str]]:\\n        return self.allowed_tools\\n    @property\\n    def return_values(self) -> List[str]:\\n        return [\"output\"]\\n    def _fix_text(self, text: str) -> str:\\n        \"\"\"Fix the text.\"\"\"\\n        raise ValueError(\"fix_text not implemented for this agent.\")\\n    @property\\n    def _stop(self) -> List[str]:\\n        return [\\n            f\"\\\\n{self.observation_prefix.rstrip()}\",\\n            f\"\\\\n\\\\t{self.observation_prefix.rstrip()}\",\\n        ]\\n    def _construct_scratchpad(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]]\\n    ) -> Union[str, List[BaseMessage]]:\\n        \"\"\"Construct the scratchpad that lets the agent continue its thought process.\"\"\"\\n        thoughts = \"\"\\n        for action, observation in intermediate_steps:\\n            thoughts += action.log\\n            thoughts += f\"\\\\n{self.observation_prefix}{observation}\\\\n{self.llm_prefix}\"\\n        return thoughts\\n[docs]    def plan(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\\n    ) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            **kwargs: User inputs.\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        full_inputs = self.get_full_inputs(intermediate_steps, **kwargs)\\n        full_output = self.llm_chain.predict(**full_inputs)\\n        return self.output_parser.parse(full_output)\\n[docs]    async def aplan(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\\n    ) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            **kwargs: User inputs.\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        full_inputs = self.get_full_inputs(intermediate_steps, **kwargs)\\n        full_output = await self.llm_chain.apredict(**full_inputs)\\n        return self.output_parser.parse(full_output)\\n[docs]    def get_full_inputs(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\\n    ) -> Dict[str, Any]:\\n        \"\"\"Create the full inputs for the LLMChain from intermediate steps.\"\"\"\\n        thoughts = self._construct_scratchpad(intermediate_steps)\\n        new_inputs = {\"agent_scratchpad\": thoughts, \"stop\": self._stop}\\n        full_inputs = {**kwargs, **new_inputs}\\n        return full_inputs\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys.\\n        :meta private:\\n        \"\"\"\\n        return list(set(self.llm_chain.input_keys) - {\"agent_scratchpad\"})\\n    @root_validator()\\n    def validate_prompt(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that prompt matches format.\"\"\"\\n        prompt = values[\"llm_chain\"].prompt\\n        if \"agent_scratchpad\" not in prompt.input_variables:\\n            logger.warning(\\n                \"`agent_scratchpad` should be a variable in prompt.input_variables.\"\\n                \" Did not find it, so adding it at the end.\"\\n            )\\n            prompt.input_variables.append(\"agent_scratchpad\")\\n            if isinstance(prompt, PromptTemplate):\\n                prompt.template += \"\\\\n{agent_scratchpad}\"\\n            elif isinstance(prompt, FewShotPromptTemplate):\\n                prompt.suffix += \"\\\\n{agent_scratchpad}\"\\n            else:\\n                raise ValueError(f\"Got unexpected prompt type {type(prompt)}\")\\n        return values\\n    @property\\n    @abstractmethod\\n    def observation_prefix(self) -> str:\\n        \"\"\"Prefix to append the observation with.\"\"\"\\n    @property\\n    @abstractmethod\\n    def llm_prefix(self) -> str:\\n        \"\"\"Prefix to append the LLM call with.\"\"\"\\n[docs]    @classmethod\\n    @abstractmethod\\n    def create_prompt(cls, tools: Sequence[BaseTool]) -> BasePromptTemplate:\\n        \"\"\"Create a prompt for this class.\"\"\"\\n    @classmethod\\n    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\\n        \"\"\"Validate that appropriate tools are passed in.\"\"\"\\n        pass\\n    @classmethod\\n    @abstractmethod\\n    def _get_default_output_parser(cls, **kwargs: Any) -> AgentOutputParser:\\n        \"\"\"Get default output parser for this class.\"\"\"\\n[docs]    @classmethod\\n    def from_llm_and_tools(\\n        cls,\\n        llm: BaseLanguageModel,\\n        tools: Sequence[BaseTool],\\n        callback_manager: Optional[BaseCallbackManager] = None,\\n        output_parser: Optional[AgentOutputParser] = None,\\n        **kwargs: Any,\\n    ) -> Agent:\\n        \"\"\"Construct an agent from an LLM and tools.\"\"\"\\n        cls._validate_tools(tools)\\n        llm_chain = LLMChain(\\n            llm=llm,\\n            prompt=cls.create_prompt(tools),\\n            callback_manager=callback_manager,\\n        )\\n        tool_names = [tool.name for tool in tools]\\n        _output_parser = output_parser or cls._get_default_output_parser()\\n        return cls(\\n            llm_chain=llm_chain,\\n            allowed_tools=tool_names,\\n            output_parser=_output_parser,\\n            **kwargs,\\n        )\\n[docs]    def return_stopped_response(\\n        self,\\n        early_stopping_method: str,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        **kwargs: Any,\\n    ) -> AgentFinish:\\n        \"\"\"Return response when agent has been stopped due to max iterations.\"\"\"\\n        if early_stopping_method == \"force\":\\n            # `force` just returns a constant string\\n            return AgentFinish(\\n                {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\\n            )\\n        elif early_stopping_method == \"generate\":\\n            # Generate does one final forward pass\\n            thoughts = \"\"\\n            for action, observation in intermediate_steps:\\n                thoughts += action.log\\n                thoughts += (\\n                    f\"\\\\n{self.observation_prefix}{observation}\\\\n{self.llm_prefix}\"\\n                )\\n            # Adding to the previous steps, we now tell the LLM to make a final pred\\n            thoughts += (\\n                \"\\\\n\\\\nI now need to return a final answer based on the previous steps:\"\\n            )\\n            new_inputs = {\"agent_scratchpad\": thoughts, \"stop\": self._stop}\\n            full_inputs = {**kwargs, **new_inputs}\\n            full_output = self.llm_chain.predict(**full_inputs)\\n            # We try to extract a final answer\\n            parsed_output = self.output_parser.parse(full_output)\\n            if isinstance(parsed_output, AgentFinish):\\n                # If we can extract, we send the correct stuff\\n                return parsed_output\\n            else:\\n                # If we can extract, but the tool is not the final tool,\\n                # we just return the full output\\n                return AgentFinish({\"output\": full_output}, full_output)\\n        else:\\n            raise ValueError(\\n                \"early_stopping_method should be one of `force` or `generate`, \"\\n                f\"got {early_stopping_method}\"\\n            )\\n[docs]    def tool_run_logging_kwargs(self) -> Dict:\\n        return {\\n            \"llm_prefix\": self.llm_prefix,\\n            \"observation_prefix\": self.observation_prefix,\\n        }\\n[docs]class AgentExecutor(Chain):\\n    \"\"\"Consists of an agent using tools.\"\"\"\\n    agent: Union[BaseSingleActionAgent, BaseMultiActionAgent]\\n    tools: Sequence[BaseTool]\\n    return_intermediate_steps: bool = False\\n    max_iterations: Optional[int] = 15\\n    max_execution_time: Optional[float] = None\\n    early_stopping_method: str = \"force\"\\n[docs]    @classmethod\\n    def from_agent_and_tools(\\n        cls,\\n        agent: Union[BaseSingleActionAgent, BaseMultiActionAgent],\\n        tools: Sequence[BaseTool],\\n        callback_manager: Optional[BaseCallbackManager] = None,\\n        **kwargs: Any,\\n    ) -> AgentExecutor:\\n        \"\"\"Create from agent and tools.\"\"\"\\n        return cls(\\n            agent=agent, tools=tools, callback_manager=callback_manager, **kwargs\\n        )\\n    @root_validator()\\n    def validate_tools(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that tools are compatible with agent.\"\"\"\\n        agent = values[\"agent\"]\\n        tools = values[\"tools\"]\\n        allowed_tools = agent.get_allowed_tools()\\n        if allowed_tools is not None:\\n            if set(allowed_tools) != set([tool.name for tool in tools]):\\n                raise ValueError(\\n                    f\"Allowed tools ({allowed_tools}) different than \"\\n                    f\"provided tools ({[tool.name for tool in tools]})\"\\n                )\\n        return values\\n    @root_validator()\\n    def validate_return_direct_tool(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that tools are compatible with agent.\"\"\"\\n        agent = values[\"agent\"]\\n        tools = values[\"tools\"]\\n        if isinstance(agent, BaseMultiActionAgent):\\n            for tool in tools:\\n                if tool.return_direct:\\n                    raise ValueError(\\n                        \"Tools that have `return_direct=True` are not allowed \"\\n                        \"in multi-action agents\"\\n                    )\\n        return values\\n[docs]    def save(self, file_path: Union[Path, str]) -> None:\\n        \"\"\"Raise error - saving not supported for Agent Executors.\"\"\"\\n        raise ValueError(\\n            \"Saving not supported for agent executors. \"\\n            \"If you are trying to save the agent, please use the \"\\n            \"`.save_agent(...)`\"\\n        )\\n[docs]    def save_agent(self, file_path: Union[Path, str]) -> None:\\n        \"\"\"Save the underlying agent.\"\"\"\\n        return self.agent.save(file_path)\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys.\\n        :meta private:\\n        \"\"\"\\n        return self.agent.input_keys\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the singular output key.\\n        :meta private:\\n        \"\"\"\\n        if self.return_intermediate_steps:\\n            return self.agent.return_values + [\"intermediate_steps\"]\\n        else:\\n            return self.agent.return_values\\n[docs]    def lookup_tool(self, name: str) -> BaseTool:\\n        \"\"\"Lookup tool by name.\"\"\"\\n        return {tool.name: tool for tool in self.tools}[name]\\n    def _should_continue(self, iterations: int, time_elapsed: float) -> bool:\\n        if self.max_iterations is not None and iterations >= self.max_iterations:\\n            return False\\n        if (\\n            self.max_execution_time is not None\\n            and time_elapsed >= self.max_execution_time\\n        ):\\n            return False\\n        return True\\n    def _return(self, output: AgentFinish, intermediate_steps: list) -> Dict[str, Any]:\\n        self.callback_manager.on_agent_finish(\\n            output, color=\"green\", verbose=self.verbose\\n        )\\n        final_output = output.return_values\\n        if self.return_intermediate_steps:\\n            final_output[\"intermediate_steps\"] = intermediate_steps\\n        return final_output\\n    async def _areturn(\\n        self, output: AgentFinish, intermediate_steps: list\\n    ) -> Dict[str, Any]:\\n        if self.callback_manager.is_async:\\n            await self.callback_manager.on_agent_finish(\\n                output, color=\"green\", verbose=self.verbose\\n            )\\n        else:\\n            self.callback_manager.on_agent_finish(\\n                output, color=\"green\", verbose=self.verbose\\n            )\\n        final_output = output.return_values\\n        if self.return_intermediate_steps:\\n            final_output[\"intermediate_steps\"] = intermediate_steps\\n        return final_output\\n    def _take_next_step(\\n        self,\\n        name_to_tool_map: Dict[str, BaseTool],\\n        color_mapping: Dict[str, str],\\n        inputs: Dict[str, str],\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n    ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:\\n        \"\"\"Take a single step in the thought-action-observation loop.\\n        Override this to take control of how the agent makes and acts on choices.\\n        \"\"\"\\n        # Call the LLM to see what to do.\\n        output = self.agent.plan(intermediate_steps, **inputs)\\n        # If the tool chosen is the finishing tool, then we end and return.\\n        if isinstance(output, AgentFinish):\\n            return output\\n        actions: List[AgentAction]\\n        if isinstance(output, AgentAction):\\n            actions = [output]\\n        else:\\n            actions = output\\n        result = []\\n        for agent_action in actions:\\n            self.callback_manager.on_agent_action(\\n                agent_action, verbose=self.verbose, color=\"green\"\\n            )\\n            # Otherwise we lookup the tool\\n            if agent_action.tool in name_to_tool_map:\\n                tool = name_to_tool_map[agent_action.tool]\\n                return_direct = tool.return_direct\\n                color = color_mapping[agent_action.tool]\\n                tool_run_kwargs = self.agent.tool_run_logging_kwargs()\\n                if return_direct:\\n                    tool_run_kwargs[\"llm_prefix\"] = \"\"\\n                # We then call the tool on the tool input to get an observation\\n                observation = tool.run(\\n                    agent_action.tool_input,\\n                    verbose=self.verbose,\\n                    color=color,\\n                    **tool_run_kwargs,\\n                )\\n            else:\\n                tool_run_kwargs = self.agent.tool_run_logging_kwargs()\\n                observation = InvalidTool().run(\\n                    agent_action.tool,\\n                    verbose=self.verbose,\\n                    color=None,\\n                    **tool_run_kwargs,\\n                )\\n            result.append((agent_action, observation))\\n        return result\\n    async def _atake_next_step(\\n        self,\\n        name_to_tool_map: Dict[str, BaseTool],\\n        color_mapping: Dict[str, str],\\n        inputs: Dict[str, str],\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n    ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:\\n        \"\"\"Take a single step in the thought-action-observation loop.\\n        Override this to take control of how the agent makes and acts on choices.\\n        \"\"\"\\n        # Call the LLM to see what to do.\\n        output = await self.agent.aplan(intermediate_steps, **inputs)\\n        # If the tool chosen is the finishing tool, then we end and return.\\n        if isinstance(output, AgentFinish):\\n            return output\\n        actions: List[AgentAction]\\n        if isinstance(output, AgentAction):\\n            actions = [output]\\n        else:\\n            actions = output\\n        async def _aperform_agent_action(\\n            agent_action: AgentAction,\\n        ) -> Tuple[AgentAction, str]:\\n            if self.callback_manager.is_async:\\n                await self.callback_manager.on_agent_action(\\n                    agent_action, verbose=self.verbose, color=\"green\"\\n                )\\n            else:\\n                self.callback_manager.on_agent_action(\\n                    agent_action, verbose=self.verbose, color=\"green\"\\n                )\\n            # Otherwise we lookup the tool\\n            if agent_action.tool in name_to_tool_map:\\n                tool = name_to_tool_map[agent_action.tool]\\n                return_direct = tool.return_direct\\n                color = color_mapping[agent_action.tool]\\n                tool_run_kwargs = self.agent.tool_run_logging_kwargs()\\n                if return_direct:\\n                    tool_run_kwargs[\"llm_prefix\"] = \"\"\\n                # We then call the tool on the tool input to get an observation\\n                observation = await tool.arun(\\n                    agent_action.tool_input,\\n                    verbose=self.verbose,\\n                    color=color,\\n                    **tool_run_kwargs,\\n                )\\n            else:\\n                tool_run_kwargs = self.agent.tool_run_logging_kwargs()\\n                observation = await InvalidTool().arun(\\n                    agent_action.tool,\\n                    verbose=self.verbose,\\n                    color=None,\\n                    **tool_run_kwargs,\\n                )\\n            return agent_action, observation\\n        # Use asyncio.gather to run multiple tool.arun() calls concurrently\\n        result = await asyncio.gather(\\n            *[_aperform_agent_action(agent_action) for agent_action in actions]\\n        )\\n        return list(result)\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, Any]:\\n        \"\"\"Run text through and get agent response.\"\"\"\\n        # Construct a mapping of tool name to tool for easy lookup\\n        name_to_tool_map = {tool.name: tool for tool in self.tools}\\n        # We construct a mapping from each tool to a color, used for logging.\\n        color_mapping = get_color_mapping(\\n            [tool.name for tool in self.tools], excluded_colors=[\"green\"]\\n        )\\n        intermediate_steps: List[Tuple[AgentAction, str]] = []\\n        # Let\\'s start tracking the number of iterations and time elapsed\\n        iterations = 0\\n        time_elapsed = 0.0\\n        start_time = time.time()\\n        # We now enter the agent loop (until it returns something).\\n        while self._should_continue(iterations, time_elapsed):\\n            next_step_output = self._take_next_step(\\n                name_to_tool_map, color_mapping, inputs, intermediate_steps\\n            )\\n            if isinstance(next_step_output, AgentFinish):\\n                return self._return(next_step_output, intermediate_steps)\\n            intermediate_steps.extend(next_step_output)\\n            if len(next_step_output) == 1:\\n                next_step_action = next_step_output[0]\\n                # See if tool should return directly\\n                tool_return = self._get_tool_return(next_step_action)\\n                if tool_return is not None:\\n                    return self._return(tool_return, intermediate_steps)\\n            iterations += 1\\n            time_elapsed = time.time() - start_time\\n        output = self.agent.return_stopped_response(\\n            self.early_stopping_method, intermediate_steps, **inputs\\n        )\\n        return self._return(output, intermediate_steps)\\n    async def _acall(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        \"\"\"Run text through and get agent response.\"\"\"\\n        # Construct a mapping of tool name to tool for easy lookup\\n        name_to_tool_map = {tool.name: tool for tool in self.tools}\\n        # We construct a mapping from each tool to a color, used for logging.\\n        color_mapping = get_color_mapping(\\n            [tool.name for tool in self.tools], excluded_colors=[\"green\"]\\n        )\\n        intermediate_steps: List[Tuple[AgentAction, str]] = []\\n        # Let\\'s start tracking the number of iterations and time elapsed\\n        iterations = 0\\n        time_elapsed = 0.0\\n        start_time = time.time()\\n        # We now enter the agent loop (until it returns something).\\n        async with asyncio_timeout(self.max_execution_time):\\n            try:\\n                while self._should_continue(iterations, time_elapsed):\\n                    next_step_output = await self._atake_next_step(\\n                        name_to_tool_map, color_mapping, inputs, intermediate_steps\\n                    )\\n                    if isinstance(next_step_output, AgentFinish):\\n                        return await self._areturn(next_step_output, intermediate_steps)\\n                    intermediate_steps.extend(next_step_output)\\n                    if len(next_step_output) == 1:\\n                        next_step_action = next_step_output[0]\\n                        # See if tool should return directly\\n                        tool_return = self._get_tool_return(next_step_action)\\n                        if tool_return is not None:\\n                            return await self._areturn(tool_return, intermediate_steps)\\n                    iterations += 1\\n                    time_elapsed = time.time() - start_time\\n                output = self.agent.return_stopped_response(\\n                    self.early_stopping_method, intermediate_steps, **inputs\\n                )\\n                return await self._areturn(output, intermediate_steps)\\n            except TimeoutError:\\n                # stop early when interrupted by the async timeout\\n                output = self.agent.return_stopped_response(\\n                    self.early_stopping_method, intermediate_steps, **inputs\\n                )\\n                return await self._areturn(output, intermediate_steps)\\n    def _get_tool_return(\\n        self, next_step_output: Tuple[AgentAction, str]\\n    ) -> Optional[AgentFinish]:\\n        \"\"\"Check if the tool is a returning tool.\"\"\"\\n        agent_action, observation = next_step_output\\n        name_to_tool_map = {tool.name: tool for tool in self.tools}\\n        # Invalid tools won\\'t be in the map, so we return False.\\n        if agent_action.tool in name_to_tool_map:\\n            if name_to_tool_map[agent_action.tool].return_direct:\\n                return AgentFinish(\\n                    {self.agent.return_values[0]: observation},\\n                    \"\",\\n                )\\n        return None\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\agents\\\\agent.html'}),\n",
              " Document(page_content='Source code for langchain.agents.agent_types\\nfrom enum import Enum\\n[docs]class AgentType(str, Enum):\\n    ZERO_SHOT_REACT_DESCRIPTION = \"zero-shot-react-description\"\\n    REACT_DOCSTORE = \"react-docstore\"\\n    SELF_ASK_WITH_SEARCH = \"self-ask-with-search\"\\n    CONVERSATIONAL_REACT_DESCRIPTION = \"conversational-react-description\"\\n    CHAT_ZERO_SHOT_REACT_DESCRIPTION = \"chat-zero-shot-react-description\"\\n    CHAT_CONVERSATIONAL_REACT_DESCRIPTION = \"chat-conversational-react-description\"\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\agents\\\\agent_types.html'}),\n",
              " Document(page_content='Source code for langchain.agents.initialize\\n\"\"\"Load agent.\"\"\"\\nfrom typing import Any, Optional, Sequence\\nfrom langchain.agents.agent import AgentExecutor\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.agents.loading import AGENT_TO_CLASS, load_agent\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.schema import BaseLanguageModel\\nfrom langchain.tools.base import BaseTool\\n[docs]def initialize_agent(\\n    tools: Sequence[BaseTool],\\n    llm: BaseLanguageModel,\\n    agent: Optional[AgentType] = None,\\n    callback_manager: Optional[BaseCallbackManager] = None,\\n    agent_path: Optional[str] = None,\\n    agent_kwargs: Optional[dict] = None,\\n    **kwargs: Any,\\n) -> AgentExecutor:\\n    \"\"\"Load an agent executor given tools and LLM.\\n    Args:\\n        tools: List of tools this agent has access to.\\n        llm: Language model to use as the agent.\\n        agent: Agent type to use. If None and agent_path is also None, will default to\\n            AgentType.ZERO_SHOT_REACT_DESCRIPTION.\\n        callback_manager: CallbackManager to use. Global callback manager is used if\\n            not provided. Defaults to None.\\n        agent_path: Path to serialized agent to use.\\n        agent_kwargs: Additional key word arguments to pass to the underlying agent\\n        **kwargs: Additional key word arguments passed to the agent executor\\n    Returns:\\n        An agent executor\\n    \"\"\"\\n    if agent is None and agent_path is None:\\n        agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION\\n    if agent is not None and agent_path is not None:\\n        raise ValueError(\\n            \"Both `agent` and `agent_path` are specified, \"\\n            \"but at most only one should be.\"\\n        )\\n    if agent is not None:\\n        if agent not in AGENT_TO_CLASS:\\n            raise ValueError(\\n                f\"Got unknown agent type: {agent}. \"\\n                f\"Valid types are: {AGENT_TO_CLASS.keys()}.\"\\n            )\\n        agent_cls = AGENT_TO_CLASS[agent]\\n        agent_kwargs = agent_kwargs or {}\\n        agent_obj = agent_cls.from_llm_and_tools(\\n            llm, tools, callback_manager=callback_manager, **agent_kwargs\\n        )\\n    elif agent_path is not None:\\n        agent_obj = load_agent(\\n            agent_path, llm=llm, tools=tools, callback_manager=callback_manager\\n        )\\n    else:\\n        raise ValueError(\\n            \"Somehow both `agent` and `agent_path` are None, \"\\n            \"this should never happen.\"\\n        )\\n    return AgentExecutor.from_agent_and_tools(\\n        agent=agent_obj,\\n        tools=tools,\\n        callback_manager=callback_manager,\\n        **kwargs,\\n    )\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\agents\\\\initialize.html'}),\n",
              " Document(page_content='Source code for langchain.agents.loading\\n\"\"\"Functionality for loading agents.\"\"\"\\nimport json\\nfrom pathlib import Path\\nfrom typing import Any, Dict, List, Optional, Type, Union\\nimport yaml\\nfrom langchain.agents.agent import BaseSingleActionAgent\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.agents.chat.base import ChatAgent\\nfrom langchain.agents.conversational.base import ConversationalAgent\\nfrom langchain.agents.conversational_chat.base import ConversationalChatAgent\\nfrom langchain.agents.mrkl.base import ZeroShotAgent\\nfrom langchain.agents.react.base import ReActDocstoreAgent\\nfrom langchain.agents.self_ask_with_search.base import SelfAskWithSearchAgent\\nfrom langchain.agents.tools import Tool\\nfrom langchain.chains.loading import load_chain, load_chain_from_config\\nfrom langchain.llms.base import BaseLLM\\nfrom langchain.utilities.loading import try_load_from_hub\\nAGENT_TO_CLASS: Dict[AgentType, Type[BaseSingleActionAgent]] = {\\n    AgentType.ZERO_SHOT_REACT_DESCRIPTION: ZeroShotAgent,\\n    AgentType.REACT_DOCSTORE: ReActDocstoreAgent,\\n    AgentType.SELF_ASK_WITH_SEARCH: SelfAskWithSearchAgent,\\n    AgentType.CONVERSATIONAL_REACT_DESCRIPTION: ConversationalAgent,\\n    AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION: ChatAgent,\\n    AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION: ConversationalChatAgent,\\n}\\nURL_BASE = \"https://raw.githubusercontent.com/hwchase17/langchain-hub/master/agents/\"\\ndef _load_agent_from_tools(\\n    config: dict, llm: BaseLLM, tools: List[Tool], **kwargs: Any\\n) -> BaseSingleActionAgent:\\n    config_type = config.pop(\"_type\")\\n    if config_type not in AGENT_TO_CLASS:\\n        raise ValueError(f\"Loading {config_type} agent not supported\")\\n    agent_cls = AGENT_TO_CLASS[config_type]\\n    combined_config = {**config, **kwargs}\\n    return agent_cls.from_llm_and_tools(llm, tools, **combined_config)\\ndef load_agent_from_config(\\n    config: dict,\\n    llm: Optional[BaseLLM] = None,\\n    tools: Optional[List[Tool]] = None,\\n    **kwargs: Any,\\n) -> BaseSingleActionAgent:\\n    \"\"\"Load agent from Config Dict.\"\"\"\\n    if \"_type\" not in config:\\n        raise ValueError(\"Must specify an agent Type in config\")\\n    load_from_tools = config.pop(\"load_from_llm_and_tools\", False)\\n    if load_from_tools:\\n        if llm is None:\\n            raise ValueError(\\n                \"If `load_from_llm_and_tools` is set to True, \"\\n                \"then LLM must be provided\"\\n            )\\n        if tools is None:\\n            raise ValueError(\\n                \"If `load_from_llm_and_tools` is set to True, \"\\n                \"then tools must be provided\"\\n            )\\n        return _load_agent_from_tools(config, llm, tools, **kwargs)\\n    config_type = config.pop(\"_type\")\\n    if config_type not in AGENT_TO_CLASS:\\n        raise ValueError(f\"Loading {config_type} agent not supported\")\\n    agent_cls = AGENT_TO_CLASS[config_type]\\n    if \"llm_chain\" in config:\\n        config[\"llm_chain\"] = load_chain_from_config(config.pop(\"llm_chain\"))\\n    elif \"llm_chain_path\" in config:\\n        config[\"llm_chain\"] = load_chain(config.pop(\"llm_chain_path\"))\\n    else:\\n        raise ValueError(\"One of `llm_chain` and `llm_chain_path` should be specified.\")\\n    combined_config = {**config, **kwargs}\\n    return agent_cls(**combined_config)  # type: ignore\\n[docs]def load_agent(path: Union[str, Path], **kwargs: Any) -> BaseSingleActionAgent:\\n    \"\"\"Unified method for loading a agent from LangChainHub or local fs.\"\"\"\\n    if hub_result := try_load_from_hub(\\n        path, _load_agent_from_file, \"agents\", {\"json\", \"yaml\"}\\n    ):\\n        return hub_result\\n    else:\\n        return _load_agent_from_file(path, **kwargs)\\ndef _load_agent_from_file(\\n    file: Union[str, Path], **kwargs: Any\\n) -> BaseSingleActionAgent:\\n    \"\"\"Load agent from file.\"\"\"\\n    # Convert file to Path object.\\n    if isinstance(file, str):\\n        file_path = Path(file)\\n    else:\\n        file_path = file\\n    # Load from either json or yaml.\\n    if file_path.suffix == \".json\":\\n        with open(file_path) as f:\\n            config = json.load(f)\\n    elif file_path.suffix == \".yaml\":\\n        with open(file_path, \"r\") as f:\\n            config = yaml.safe_load(f)\\n    else:\\n        raise ValueError(\"File type must be json or yaml\")\\n    # Load the agent from the config now.\\n    return load_agent_from_config(config, **kwargs)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\agents\\\\loading.html'}),\n",
              " Document(page_content='Source code for langchain.agents.load_tools\\n# flake8: noqa\\n\"\"\"Load tools.\"\"\"\\nimport warnings\\nfrom typing import Any, List, Optional\\nfrom langchain.agents.tools import Tool\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.chains.api import news_docs, open_meteo_docs, podcast_docs, tmdb_docs\\nfrom langchain.chains.api.base import APIChain\\nfrom langchain.chains.llm_math.base import LLMMathChain\\nfrom langchain.chains.pal.base import PALChain\\nfrom langchain.llms.base import BaseLLM\\nfrom langchain.requests import TextRequestsWrapper\\nfrom langchain.tools.arxiv.tool import ArxivQueryRun\\nfrom langchain.tools.base import BaseTool\\nfrom langchain.tools.bing_search.tool import BingSearchRun\\nfrom langchain.tools.google_search.tool import GoogleSearchResults, GoogleSearchRun\\nfrom langchain.tools.human.tool import HumanInputRun\\nfrom langchain.tools.python.tool import PythonREPLTool\\nfrom langchain.tools.requests.tool import (\\n    RequestsDeleteTool,\\n    RequestsGetTool,\\n    RequestsPatchTool,\\n    RequestsPostTool,\\n    RequestsPutTool,\\n)\\nfrom langchain.tools.searx_search.tool import SearxSearchResults, SearxSearchRun\\nfrom langchain.tools.wikipedia.tool import WikipediaQueryRun\\nfrom langchain.tools.wolfram_alpha.tool import WolframAlphaQueryRun\\nfrom langchain.utilities import ArxivAPIWrapper\\nfrom langchain.utilities.apify import ApifyWrapper\\nfrom langchain.utilities.bash import BashProcess\\nfrom langchain.utilities.bing_search import BingSearchAPIWrapper\\nfrom langchain.utilities.google_search import GoogleSearchAPIWrapper\\nfrom langchain.utilities.google_serper import GoogleSerperAPIWrapper\\nfrom langchain.utilities.searx_search import SearxSearchWrapper\\nfrom langchain.utilities.serpapi import SerpAPIWrapper\\nfrom langchain.utilities.wikipedia import WikipediaAPIWrapper\\nfrom langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\\ndef _get_python_repl() -> BaseTool:\\n    return PythonREPLTool()\\ndef _get_tools_requests_get() -> BaseTool:\\n    return RequestsGetTool(requests_wrapper=TextRequestsWrapper())\\ndef _get_tools_requests_post() -> BaseTool:\\n    return RequestsPostTool(requests_wrapper=TextRequestsWrapper())\\ndef _get_tools_requests_patch() -> BaseTool:\\n    return RequestsPatchTool(requests_wrapper=TextRequestsWrapper())\\ndef _get_tools_requests_put() -> BaseTool:\\n    return RequestsPutTool(requests_wrapper=TextRequestsWrapper())\\ndef _get_tools_requests_delete() -> BaseTool:\\n    return RequestsDeleteTool(requests_wrapper=TextRequestsWrapper())\\ndef _get_terminal() -> BaseTool:\\n    return Tool(\\n        name=\"Terminal\",\\n        description=\"Executes commands in a terminal. Input should be valid commands, and the output will be any output from running that command.\",\\n        func=BashProcess().run,\\n    )\\n_BASE_TOOLS = {\\n    \"python_repl\": _get_python_repl,\\n    \"requests\": _get_tools_requests_get,  # preserved for backwards compatability\\n    \"requests_get\": _get_tools_requests_get,\\n    \"requests_post\": _get_tools_requests_post,\\n    \"requests_patch\": _get_tools_requests_patch,\\n    \"requests_put\": _get_tools_requests_put,\\n    \"requests_delete\": _get_tools_requests_delete,\\n    \"terminal\": _get_terminal,\\n}\\ndef _get_pal_math(llm: BaseLLM) -> BaseTool:\\n    return Tool(\\n        name=\"PAL-MATH\",\\n        description=\"A language model that is really good at solving complex word math problems. Input should be a fully worded hard word math problem.\",\\n        func=PALChain.from_math_prompt(llm).run,\\n    )\\ndef _get_pal_colored_objects(llm: BaseLLM) -> BaseTool:\\n    return Tool(\\n        name=\"PAL-COLOR-OBJ\",\\n        description=\"A language model that is really good at reasoning about position and the color attributes of objects. Input should be a fully worded hard reasoning problem. Make sure to include all information about the objects AND the final question you want to answer.\",\\n        func=PALChain.from_colored_object_prompt(llm).run,\\n    )\\ndef _get_llm_math(llm: BaseLLM) -> BaseTool:\\n    return Tool(\\n        name=\"Calculator\",\\n        description=\"Useful for when you need to answer questions about math.\",\\n        func=LLMMathChain(llm=llm, callback_manager=llm.callback_manager).run,\\n        coroutine=LLMMathChain(llm=llm, callback_manager=llm.callback_manager).arun,\\n    )\\ndef _get_open_meteo_api(llm: BaseLLM) -> BaseTool:\\n    chain = APIChain.from_llm_and_api_docs(llm, open_meteo_docs.OPEN_METEO_DOCS)\\n    return Tool(\\n        name=\"Open Meteo API\",\\n        description=\"Useful for when you want to get weather information from the OpenMeteo API. The input should be a question in natural language that this API can answer.\",\\n        func=chain.run,\\n    )\\n_LLM_TOOLS = {\\n    \"pal-math\": _get_pal_math,\\n    \"pal-colored-objects\": _get_pal_colored_objects,\\n    \"llm-math\": _get_llm_math,\\n    \"open-meteo-api\": _get_open_meteo_api,\\n}\\ndef _get_news_api(llm: BaseLLM, **kwargs: Any) -> BaseTool:\\n    news_api_key = kwargs[\"news_api_key\"]\\n    chain = APIChain.from_llm_and_api_docs(\\n        llm, news_docs.NEWS_DOCS, headers={\"X-Api-Key\": news_api_key}\\n    )\\n    return Tool(\\n        name=\"News API\",\\n        description=\"Use this when you want to get information about the top headlines of current news stories. The input should be a question in natural language that this API can answer.\",\\n        func=chain.run,\\n    )\\ndef _get_tmdb_api(llm: BaseLLM, **kwargs: Any) -> BaseTool:\\n    tmdb_bearer_token = kwargs[\"tmdb_bearer_token\"]\\n    chain = APIChain.from_llm_and_api_docs(\\n        llm,\\n        tmdb_docs.TMDB_DOCS,\\n        headers={\"Authorization\": f\"Bearer {tmdb_bearer_token}\"},\\n    )\\n    return Tool(\\n        name=\"TMDB API\",\\n        description=\"Useful for when you want to get information from The Movie Database. The input should be a question in natural language that this API can answer.\",\\n        func=chain.run,\\n    )\\ndef _get_podcast_api(llm: BaseLLM, **kwargs: Any) -> BaseTool:\\n    listen_api_key = kwargs[\"listen_api_key\"]\\n    chain = APIChain.from_llm_and_api_docs(\\n        llm,\\n        podcast_docs.PODCAST_DOCS,\\n        headers={\"X-ListenAPI-Key\": listen_api_key},\\n    )\\n    return Tool(\\n        name=\"Podcast API\",\\n        description=\"Use the Listen Notes Podcast API to search all podcasts or episodes. The input should be a question in natural language that this API can answer.\",\\n        func=chain.run,\\n    )\\ndef _get_wolfram_alpha(**kwargs: Any) -> BaseTool:\\n    return WolframAlphaQueryRun(api_wrapper=WolframAlphaAPIWrapper(**kwargs))\\ndef _get_google_search(**kwargs: Any) -> BaseTool:\\n    return GoogleSearchRun(api_wrapper=GoogleSearchAPIWrapper(**kwargs))\\ndef _get_wikipedia(**kwargs: Any) -> BaseTool:\\n    return WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(**kwargs))\\ndef _get_arxiv(**kwargs: Any) -> BaseTool:\\n    return ArxivQueryRun(api_wrapper=ArxivAPIWrapper(**kwargs))\\ndef _get_google_serper(**kwargs: Any) -> BaseTool:\\n    return Tool(\\n        name=\"Serper Search\",\\n        func=GoogleSerperAPIWrapper(**kwargs).run,\\n        description=\"A low-cost Google Search API. Useful for when you need to answer questions about current events. Input should be a search query.\",\\n    )\\ndef _get_google_search_results_json(**kwargs: Any) -> BaseTool:\\n    return GoogleSearchResults(api_wrapper=GoogleSearchAPIWrapper(**kwargs))\\ndef _get_serpapi(**kwargs: Any) -> BaseTool:\\n    return Tool(\\n        name=\"Search\",\\n        description=\"A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\",\\n        func=SerpAPIWrapper(**kwargs).run,\\n        coroutine=SerpAPIWrapper(**kwargs).arun,\\n    )\\ndef _get_searx_search(**kwargs: Any) -> BaseTool:\\n    return SearxSearchRun(wrapper=SearxSearchWrapper(**kwargs))\\ndef _get_searx_search_results_json(**kwargs: Any) -> BaseTool:\\n    wrapper_kwargs = {k: v for k, v in kwargs.items() if k != \"num_results\"}\\n    return SearxSearchResults(wrapper=SearxSearchWrapper(**wrapper_kwargs), **kwargs)\\ndef _get_bing_search(**kwargs: Any) -> BaseTool:\\n    return BingSearchRun(api_wrapper=BingSearchAPIWrapper(**kwargs))\\ndef _get_human_tool(**kwargs: Any) -> BaseTool:\\n    return HumanInputRun(**kwargs)\\n_EXTRA_LLM_TOOLS = {\\n    \"news-api\": (_get_news_api, [\"news_api_key\"]),\\n    \"tmdb-api\": (_get_tmdb_api, [\"tmdb_bearer_token\"]),\\n    \"podcast-api\": (_get_podcast_api, [\"listen_api_key\"]),\\n}\\n_EXTRA_OPTIONAL_TOOLS = {\\n    \"wolfram-alpha\": (_get_wolfram_alpha, [\"wolfram_alpha_appid\"]),\\n    \"google-search\": (_get_google_search, [\"google_api_key\", \"google_cse_id\"]),\\n    \"google-search-results-json\": (\\n        _get_google_search_results_json,\\n        [\"google_api_key\", \"google_cse_id\", \"num_results\"],\\n    ),\\n    \"searx-search-results-json\": (\\n        _get_searx_search_results_json,\\n        [\"searx_host\", \"engines\", \"num_results\", \"aiosession\"],\\n    ),\\n    \"bing-search\": (_get_bing_search, [\"bing_subscription_key\", \"bing_search_url\"]),\\n    \"google-serper\": (_get_google_serper, [\"serper_api_key\"]),\\n    \"serpapi\": (_get_serpapi, [\"serpapi_api_key\", \"aiosession\"]),\\n    \"searx-search\": (_get_searx_search, [\"searx_host\", \"engines\", \"aiosession\"]),\\n    \"wikipedia\": (_get_wikipedia, [\"top_k_results\"]),\\n    \"human\": (_get_human_tool, [\"prompt_func\", \"input_func\"]),\\n}\\n[docs]def load_tools(\\n    tool_names: List[str],\\n    llm: Optional[BaseLLM] = None,\\n    callback_manager: Optional[BaseCallbackManager] = None,\\n    **kwargs: Any,\\n) -> List[BaseTool]:\\n    \"\"\"Load tools based on their name.\\n    Args:\\n        tool_names: name of tools to load.\\n        llm: Optional language model, may be needed to initialize certain tools.\\n        callback_manager: Optional callback manager. If not provided, default global callback manager will be used.\\n    Returns:\\n        List of tools.\\n    \"\"\"\\n    tools = []\\n    for name in tool_names:\\n        if name == \"requests\":\\n            warnings.warn(\\n                \"tool name `requests` is deprecated - \"\\n                \"please use `requests_all` or specify the requests method\"\\n            )\\n        if name == \"requests_all\":\\n            # expand requests into various methods\\n            requests_method_tools = [\\n                _tool for _tool in _BASE_TOOLS if _tool.startswith(\"requests_\")\\n            ]\\n            tool_names.extend(requests_method_tools)\\n        elif name in _BASE_TOOLS:\\n            tools.append(_BASE_TOOLS[name]())\\n        elif name in _LLM_TOOLS:\\n            if llm is None:\\n                raise ValueError(f\"Tool {name} requires an LLM to be provided\")\\n            tool = _LLM_TOOLS[name](llm)\\n            if callback_manager is not None:\\n                tool.callback_manager = callback_manager\\n            tools.append(tool)\\n        elif name in _EXTRA_LLM_TOOLS:\\n            if llm is None:\\n                raise ValueError(f\"Tool {name} requires an LLM to be provided\")\\n            _get_llm_tool_func, extra_keys = _EXTRA_LLM_TOOLS[name]\\n            missing_keys = set(extra_keys).difference(kwargs)\\n            if missing_keys:\\n                raise ValueError(\\n                    f\"Tool {name} requires some parameters that were not \"\\n                    f\"provided: {missing_keys}\"\\n                )\\n            sub_kwargs = {k: kwargs[k] for k in extra_keys}\\n            tool = _get_llm_tool_func(llm=llm, **sub_kwargs)\\n            if callback_manager is not None:\\n                tool.callback_manager = callback_manager\\n            tools.append(tool)\\n        elif name in _EXTRA_OPTIONAL_TOOLS:\\n            _get_tool_func, extra_keys = _EXTRA_OPTIONAL_TOOLS[name]\\n            sub_kwargs = {k: kwargs[k] for k in extra_keys if k in kwargs}\\n            tool = _get_tool_func(**sub_kwargs)\\n            if callback_manager is not None:\\n                tool.callback_manager = callback_manager\\n            tools.append(tool)\\n        else:\\n            raise ValueError(f\"Got unknown tool {name}\")\\n    return tools\\n[docs]def get_all_tool_names() -> List[str]:\\n    \"\"\"Get a list of all possible tool names.\"\"\"\\n    return (\\n        list(_BASE_TOOLS)\\n        + list(_EXTRA_OPTIONAL_TOOLS)\\n        + list(_EXTRA_LLM_TOOLS)\\n        + list(_LLM_TOOLS)\\n    )\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\agents\\\\load_tools.html'}),\n",
              " Document(page_content='Source code for langchain.agents.tools\\n\"\"\"Interface for tools.\"\"\"\\nfrom inspect import signature\\nfrom typing import Any, Awaitable, Callable, Optional, Type, Union\\nfrom pydantic import BaseModel, validate_arguments\\nfrom langchain.tools.base import BaseTool\\n[docs]class Tool(BaseTool):\\n    \"\"\"Tool that takes in function or coroutine directly.\"\"\"\\n    description: str = \"\"\\n    func: Callable[..., str]\\n    \"\"\"The function to run when the tool is called.\"\"\"\\n    coroutine: Optional[Callable[..., Awaitable[str]]] = None\\n    \"\"\"The asynchronous version of the function.\"\"\"\\n    @property\\n    def args(self) -> dict:\\n        if self.args_schema is not None:\\n            return self.args_schema.schema()[\"properties\"]\\n        else:\\n            inferred_model = validate_arguments(self.func).model  # type: ignore\\n            schema = inferred_model.schema()[\"properties\"]\\n            valid_keys = signature(self.func).parameters\\n            return {k: schema[k] for k in valid_keys}\\n    def _run(self, *args: Any, **kwargs: Any) -> str:\\n        \"\"\"Use the tool.\"\"\"\\n        return self.func(*args, **kwargs)\\n    async def _arun(self, *args: Any, **kwargs: Any) -> str:\\n        \"\"\"Use the tool asynchronously.\"\"\"\\n        if self.coroutine:\\n            return await self.coroutine(*args, **kwargs)\\n        raise NotImplementedError(\"Tool does not support async\")\\n    # TODO: this is for backwards compatibility, remove in future\\n    def __init__(\\n        self, name: str, func: Callable[[str], str], description: str, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize tool.\"\"\"\\n        super(Tool, self).__init__(\\n            name=name, func=func, description=description, **kwargs\\n        )\\nclass InvalidTool(BaseTool):\\n    \"\"\"Tool that is run when invalid tool name is encountered by agent.\"\"\"\\n    name = \"invalid_tool\"\\n    description = \"Called when tool name is invalid.\"\\n    def _run(self, tool_name: str) -> str:\\n        \"\"\"Use the tool.\"\"\"\\n        return f\"{tool_name} is not a valid tool, try another one.\"\\n    async def _arun(self, tool_name: str) -> str:\\n        \"\"\"Use the tool asynchronously.\"\"\"\\n        return f\"{tool_name} is not a valid tool, try another one.\"\\n[docs]def tool(\\n    *args: Union[str, Callable],\\n    return_direct: bool = False,\\n    args_schema: Optional[Type[BaseModel]] = None,\\n    infer_schema: bool = True,\\n) -> Callable:\\n    \"\"\"Make tools out of functions, can be used with or without arguments.\\n    Args:\\n        *args: The arguments to the tool.\\n        return_direct: Whether to return directly from the tool rather\\n            than continuing the agent loop.\\n        args_schema: optional argument schema for user to specify\\n        infer_schema: Whether to infer the schema of the arguments from\\n            the function\\'s signature. This also makes the resultant tool\\n            accept a dictionary input to its `run()` function.\\n    Requires:\\n        - Function must be of type (str) -> str\\n        - Function must have a docstring\\n    Examples:\\n        .. code-block:: python\\n            @tool\\n            def search_api(query: str) -> str:\\n                # Searches the API for the query.\\n                return\\n            @tool(\"search\", return_direct=True)\\n            def search_api(query: str) -> str:\\n                # Searches the API for the query.\\n                return\\n    \"\"\"\\n    def _make_with_name(tool_name: str) -> Callable:\\n        def _make_tool(func: Callable) -> Tool:\\n            assert func.__doc__, \"Function must have a docstring\"\\n            # Description example:\\n            # search_api(query: str) - Searches the API for the query.\\n            description = f\"{tool_name}{signature(func)} - {func.__doc__.strip()}\"\\n            _args_schema = args_schema\\n            if _args_schema is None and infer_schema:\\n                _args_schema = validate_arguments(func).model  # type: ignore\\n            tool_ = Tool(\\n                name=tool_name,\\n                func=func,\\n                args_schema=_args_schema,\\n                description=description,\\n                return_direct=return_direct,\\n            )\\n            return tool_\\n        return _make_tool\\n    if len(args) == 1 and isinstance(args[0], str):\\n        # if the argument is a string, then we use the string as the tool name\\n        # Example usage: @tool(\"search\", return_direct=True)\\n        return _make_with_name(args[0])\\n    elif len(args) == 1 and callable(args[0]):\\n        # if the argument is a function, then we use the function name as the tool name\\n        # Example usage: @tool\\n        return _make_with_name(args[0].__name__)(args[0])\\n    elif len(args) == 0:\\n        # if there are no arguments, then we use the function name as the tool name\\n        # Example usage: @tool(return_direct=True)\\n        def _partial(func: Callable[[str], str]) -> BaseTool:\\n            return _make_with_name(func.__name__)(func)\\n        return _partial\\n    else:\\n        raise ValueError(\"Too many arguments for tool decorator\")\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\agents\\\\tools.html'}),\n",
              " Document(page_content='Source code for langchain.agents.agent_toolkits.csv.base\\n\"\"\"Agent for working with csvs.\"\"\"\\nfrom typing import Any, Optional\\nfrom langchain.agents.agent import AgentExecutor\\nfrom langchain.agents.agent_toolkits.pandas.base import create_pandas_dataframe_agent\\nfrom langchain.llms.base import BaseLLM\\n[docs]def create_csv_agent(\\n    llm: BaseLLM, path: str, pandas_kwargs: Optional[dict] = None, **kwargs: Any\\n) -> AgentExecutor:\\n    \"\"\"Create csv agent by loading to a dataframe and using pandas agent.\"\"\"\\n    import pandas as pd\\n    _kwargs = pandas_kwargs or {}\\n    df = pd.read_csv(path, **_kwargs)\\n    return create_pandas_dataframe_agent(llm, df, **kwargs)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\agents\\\\agent_toolkits\\\\csv\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.agents.agent_toolkits.pandas.base\\n\"\"\"Agent for working with pandas objects.\"\"\"\\nfrom typing import Any, List, Optional\\nfrom langchain.agents.agent import AgentExecutor\\nfrom langchain.agents.agent_toolkits.pandas.prompt import PREFIX, SUFFIX\\nfrom langchain.agents.mrkl.base import ZeroShotAgent\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.llms.base import BaseLLM\\nfrom langchain.tools.python.tool import PythonAstREPLTool\\n[docs]def create_pandas_dataframe_agent(\\n    llm: BaseLLM,\\n    df: Any,\\n    callback_manager: Optional[BaseCallbackManager] = None,\\n    prefix: str = PREFIX,\\n    suffix: str = SUFFIX,\\n    input_variables: Optional[List[str]] = None,\\n    verbose: bool = False,\\n    return_intermediate_steps: bool = False,\\n    max_iterations: Optional[int] = 15,\\n    max_execution_time: Optional[float] = None,\\n    early_stopping_method: str = \"force\",\\n    **kwargs: Any,\\n) -> AgentExecutor:\\n    \"\"\"Construct a pandas agent from an LLM and dataframe.\"\"\"\\n    import pandas as pd\\n    if not isinstance(df, pd.DataFrame):\\n        raise ValueError(f\"Expected pandas object, got {type(df)}\")\\n    if input_variables is None:\\n        input_variables = [\"df\", \"input\", \"agent_scratchpad\"]\\n    tools = [PythonAstREPLTool(locals={\"df\": df})]\\n    prompt = ZeroShotAgent.create_prompt(\\n        tools, prefix=prefix, suffix=suffix, input_variables=input_variables\\n    )\\n    partial_prompt = prompt.partial(df=str(df.head()))\\n    llm_chain = LLMChain(\\n        llm=llm,\\n        prompt=partial_prompt,\\n        callback_manager=callback_manager,\\n    )\\n    tool_names = [tool.name for tool in tools]\\n    agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names, **kwargs)\\n    return AgentExecutor.from_agent_and_tools(\\n        agent=agent,\\n        tools=tools,\\n        verbose=verbose,\\n        return_intermediate_steps=return_intermediate_steps,\\n        max_iterations=max_iterations,\\n        max_execution_time=max_execution_time,\\n        early_stopping_method=early_stopping_method,\\n    )\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\agents\\\\agent_toolkits\\\\pandas\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.agents.agent_toolkits.sql.base\\n\"\"\"SQL agent.\"\"\"\\nfrom typing import Any, List, Optional\\nfrom langchain.agents.agent import AgentExecutor\\nfrom langchain.agents.agent_toolkits.sql.prompt import SQL_PREFIX, SQL_SUFFIX\\nfrom langchain.agents.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\\nfrom langchain.agents.mrkl.base import ZeroShotAgent\\nfrom langchain.agents.mrkl.prompt import FORMAT_INSTRUCTIONS\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.llms.base import BaseLLM\\n[docs]def create_sql_agent(\\n    llm: BaseLLM,\\n    toolkit: SQLDatabaseToolkit,\\n    callback_manager: Optional[BaseCallbackManager] = None,\\n    prefix: str = SQL_PREFIX,\\n    suffix: str = SQL_SUFFIX,\\n    format_instructions: str = FORMAT_INSTRUCTIONS,\\n    input_variables: Optional[List[str]] = None,\\n    top_k: int = 10,\\n    max_iterations: Optional[int] = 15,\\n    max_execution_time: Optional[float] = None,\\n    early_stopping_method: str = \"force\",\\n    verbose: bool = False,\\n    **kwargs: Any,\\n) -> AgentExecutor:\\n    \"\"\"Construct a sql agent from an LLM and tools.\"\"\"\\n    tools = toolkit.get_tools()\\n    prefix = prefix.format(dialect=toolkit.dialect, top_k=top_k)\\n    prompt = ZeroShotAgent.create_prompt(\\n        tools,\\n        prefix=prefix,\\n        suffix=suffix,\\n        format_instructions=format_instructions,\\n        input_variables=input_variables,\\n    )\\n    llm_chain = LLMChain(\\n        llm=llm,\\n        prompt=prompt,\\n        callback_manager=callback_manager,\\n    )\\n    tool_names = [tool.name for tool in tools]\\n    agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names, **kwargs)\\n    return AgentExecutor.from_agent_and_tools(\\n        agent=agent,\\n        tools=tools,\\n        verbose=verbose,\\n        max_iterations=max_iterations,\\n        max_execution_time=max_execution_time,\\n        early_stopping_method=early_stopping_method,\\n    )\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\agents\\\\agent_toolkits\\\\sql\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.agents.conversational.base\\n\"\"\"An agent designed to hold a conversation in addition to using tools.\"\"\"\\nfrom __future__ import annotations\\nfrom typing import Any, List, Optional, Sequence\\nfrom pydantic import Field\\nfrom langchain.agents.agent import Agent, AgentOutputParser\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.agents.conversational.output_parser import ConvoOutputParser\\nfrom langchain.agents.conversational.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.chains import LLMChain\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.schema import BaseLanguageModel\\nfrom langchain.tools.base import BaseTool\\n[docs]class ConversationalAgent(Agent):\\n    \"\"\"An agent designed to hold a conversation in addition to using tools.\"\"\"\\n    ai_prefix: str = \"AI\"\\n    output_parser: AgentOutputParser = Field(default_factory=ConvoOutputParser)\\n    @classmethod\\n    def _get_default_output_parser(\\n        cls, ai_prefix: str = \"AI\", **kwargs: Any\\n    ) -> AgentOutputParser:\\n        return ConvoOutputParser(ai_prefix=ai_prefix)\\n    @property\\n    def _agent_type(self) -> str:\\n        \"\"\"Return Identifier of agent type.\"\"\"\\n        return AgentType.CONVERSATIONAL_REACT_DESCRIPTION\\n    @property\\n    def observation_prefix(self) -> str:\\n        \"\"\"Prefix to append the observation with.\"\"\"\\n        return \"Observation: \"\\n    @property\\n    def llm_prefix(self) -> str:\\n        \"\"\"Prefix to append the llm call with.\"\"\"\\n        return \"Thought:\"\\n[docs]    @classmethod\\n    def create_prompt(\\n        cls,\\n        tools: Sequence[BaseTool],\\n        prefix: str = PREFIX,\\n        suffix: str = SUFFIX,\\n        format_instructions: str = FORMAT_INSTRUCTIONS,\\n        ai_prefix: str = \"AI\",\\n        human_prefix: str = \"Human\",\\n        input_variables: Optional[List[str]] = None,\\n    ) -> PromptTemplate:\\n        \"\"\"Create prompt in the style of the zero shot agent.\\n        Args:\\n            tools: List of tools the agent will have access to, used to format the\\n                prompt.\\n            prefix: String to put before the list of tools.\\n            suffix: String to put after the list of tools.\\n            ai_prefix: String to use before AI output.\\n            human_prefix: String to use before human output.\\n            input_variables: List of input variables the final prompt will expect.\\n        Returns:\\n            A PromptTemplate with the template assembled from the pieces here.\\n        \"\"\"\\n        tool_strings = \"\\\\n\".join(\\n            [f\"> {tool.name}: {tool.description}\" for tool in tools]\\n        )\\n        tool_names = \", \".join([tool.name for tool in tools])\\n        format_instructions = format_instructions.format(\\n            tool_names=tool_names, ai_prefix=ai_prefix, human_prefix=human_prefix\\n        )\\n        template = \"\\\\n\\\\n\".join([prefix, tool_strings, format_instructions, suffix])\\n        if input_variables is None:\\n            input_variables = [\"input\", \"chat_history\", \"agent_scratchpad\"]\\n        return PromptTemplate(template=template, input_variables=input_variables)\\n[docs]    @classmethod\\n    def from_llm_and_tools(\\n        cls,\\n        llm: BaseLanguageModel,\\n        tools: Sequence[BaseTool],\\n        callback_manager: Optional[BaseCallbackManager] = None,\\n        output_parser: Optional[AgentOutputParser] = None,\\n        prefix: str = PREFIX,\\n        suffix: str = SUFFIX,\\n        format_instructions: str = FORMAT_INSTRUCTIONS,\\n        ai_prefix: str = \"AI\",\\n        human_prefix: str = \"Human\",\\n        input_variables: Optional[List[str]] = None,\\n        **kwargs: Any,\\n    ) -> Agent:\\n        \"\"\"Construct an agent from an LLM and tools.\"\"\"\\n        cls._validate_tools(tools)\\n        prompt = cls.create_prompt(\\n            tools,\\n            ai_prefix=ai_prefix,\\n            human_prefix=human_prefix,\\n            prefix=prefix,\\n            suffix=suffix,\\n            format_instructions=format_instructions,\\n            input_variables=input_variables,\\n        )\\n        llm_chain = LLMChain(\\n            llm=llm,\\n            prompt=prompt,\\n            callback_manager=callback_manager,\\n        )\\n        tool_names = [tool.name for tool in tools]\\n        _output_parser = output_parser or cls._get_default_output_parser(\\n            ai_prefix=ai_prefix\\n        )\\n        return cls(\\n            llm_chain=llm_chain,\\n            allowed_tools=tool_names,\\n            ai_prefix=ai_prefix,\\n            output_parser=_output_parser,\\n            **kwargs,\\n        )\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\agents\\\\conversational\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.agents.conversational_chat.base\\n\"\"\"An agent designed to hold a conversation in addition to using tools.\"\"\"\\nfrom __future__ import annotations\\nfrom typing import Any, List, Optional, Sequence, Tuple\\nfrom pydantic import Field\\nfrom langchain.agents.agent import Agent, AgentOutputParser\\nfrom langchain.agents.conversational_chat.output_parser import ConvoOutputParser\\nfrom langchain.agents.conversational_chat.prompt import (\\n    PREFIX,\\n    SUFFIX,\\n    TEMPLATE_TOOL_RESPONSE,\\n)\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.chains import LLMChain\\nfrom langchain.prompts.base import BasePromptTemplate\\nfrom langchain.prompts.chat import (\\n    ChatPromptTemplate,\\n    HumanMessagePromptTemplate,\\n    MessagesPlaceholder,\\n    SystemMessagePromptTemplate,\\n)\\nfrom langchain.schema import (\\n    AgentAction,\\n    AIMessage,\\n    BaseLanguageModel,\\n    BaseMessage,\\n    BaseOutputParser,\\n    HumanMessage,\\n)\\nfrom langchain.tools.base import BaseTool\\n[docs]class ConversationalChatAgent(Agent):\\n    \"\"\"An agent designed to hold a conversation in addition to using tools.\"\"\"\\n    output_parser: AgentOutputParser = Field(default_factory=ConvoOutputParser)\\n    @classmethod\\n    def _get_default_output_parser(cls, **kwargs: Any) -> AgentOutputParser:\\n        return ConvoOutputParser()\\n    @property\\n    def _agent_type(self) -> str:\\n        raise NotImplementedError\\n    @property\\n    def observation_prefix(self) -> str:\\n        \"\"\"Prefix to append the observation with.\"\"\"\\n        return \"Observation: \"\\n    @property\\n    def llm_prefix(self) -> str:\\n        \"\"\"Prefix to append the llm call with.\"\"\"\\n        return \"Thought:\"\\n[docs]    @classmethod\\n    def create_prompt(\\n        cls,\\n        tools: Sequence[BaseTool],\\n        system_message: str = PREFIX,\\n        human_message: str = SUFFIX,\\n        input_variables: Optional[List[str]] = None,\\n        output_parser: Optional[BaseOutputParser] = None,\\n    ) -> BasePromptTemplate:\\n        tool_strings = \"\\\\n\".join(\\n            [f\"> {tool.name}: {tool.description}\" for tool in tools]\\n        )\\n        tool_names = \", \".join([tool.name for tool in tools])\\n        _output_parser = output_parser or cls._get_default_output_parser()\\n        format_instructions = human_message.format(\\n            format_instructions=_output_parser.get_format_instructions()\\n        )\\n        final_prompt = format_instructions.format(\\n            tool_names=tool_names, tools=tool_strings\\n        )\\n        if input_variables is None:\\n            input_variables = [\"input\", \"chat_history\", \"agent_scratchpad\"]\\n        messages = [\\n            SystemMessagePromptTemplate.from_template(system_message),\\n            MessagesPlaceholder(variable_name=\"chat_history\"),\\n            HumanMessagePromptTemplate.from_template(final_prompt),\\n            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\\n        ]\\n        return ChatPromptTemplate(input_variables=input_variables, messages=messages)\\n    def _construct_scratchpad(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]]\\n    ) -> List[BaseMessage]:\\n        \"\"\"Construct the scratchpad that lets the agent continue its thought process.\"\"\"\\n        thoughts: List[BaseMessage] = []\\n        for action, observation in intermediate_steps:\\n            thoughts.append(AIMessage(content=action.log))\\n            human_message = HumanMessage(\\n                content=TEMPLATE_TOOL_RESPONSE.format(observation=observation)\\n            )\\n            thoughts.append(human_message)\\n        return thoughts\\n[docs]    @classmethod\\n    def from_llm_and_tools(\\n        cls,\\n        llm: BaseLanguageModel,\\n        tools: Sequence[BaseTool],\\n        callback_manager: Optional[BaseCallbackManager] = None,\\n        output_parser: Optional[AgentOutputParser] = None,\\n        system_message: str = PREFIX,\\n        human_message: str = SUFFIX,\\n        input_variables: Optional[List[str]] = None,\\n        **kwargs: Any,\\n    ) -> Agent:\\n        \"\"\"Construct an agent from an LLM and tools.\"\"\"\\n        cls._validate_tools(tools)\\n        _output_parser = output_parser or cls._get_default_output_parser()\\n        prompt = cls.create_prompt(\\n            tools,\\n            system_message=system_message,\\n            human_message=human_message,\\n            input_variables=input_variables,\\n            output_parser=_output_parser,\\n        )\\n        llm_chain = LLMChain(\\n            llm=llm,\\n            prompt=prompt,\\n            callback_manager=callback_manager,\\n        )\\n        tool_names = [tool.name for tool in tools]\\n        return cls(\\n            llm_chain=llm_chain,\\n            allowed_tools=tool_names,\\n            output_parser=_output_parser,\\n            **kwargs,\\n        )\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\agents\\\\conversational_chat\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.agents.mrkl.base\\n\"\"\"Attempt to implement MRKL systems as described in arxiv.org/pdf/2205.00445.pdf.\"\"\"\\nfrom __future__ import annotations\\nfrom typing import Any, Callable, List, NamedTuple, Optional, Sequence\\nfrom pydantic import Field\\nfrom langchain.agents.agent import Agent, AgentExecutor, AgentOutputParser\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.agents.mrkl.output_parser import MRKLOutputParser\\nfrom langchain.agents.mrkl.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\\nfrom langchain.agents.tools import Tool\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.chains import LLMChain\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.schema import BaseLanguageModel\\nfrom langchain.tools.base import BaseTool\\nclass ChainConfig(NamedTuple):\\n    \"\"\"Configuration for chain to use in MRKL system.\\n    Args:\\n        action_name: Name of the action.\\n        action: Action function to call.\\n        action_description: Description of the action.\\n    \"\"\"\\n    action_name: str\\n    action: Callable\\n    action_description: str\\n[docs]class ZeroShotAgent(Agent):\\n    \"\"\"Agent for the MRKL chain.\"\"\"\\n    output_parser: AgentOutputParser = Field(default_factory=MRKLOutputParser)\\n    @classmethod\\n    def _get_default_output_parser(cls, **kwargs: Any) -> AgentOutputParser:\\n        return MRKLOutputParser()\\n    @property\\n    def _agent_type(self) -> str:\\n        \"\"\"Return Identifier of agent type.\"\"\"\\n        return AgentType.ZERO_SHOT_REACT_DESCRIPTION\\n    @property\\n    def observation_prefix(self) -> str:\\n        \"\"\"Prefix to append the observation with.\"\"\"\\n        return \"Observation: \"\\n    @property\\n    def llm_prefix(self) -> str:\\n        \"\"\"Prefix to append the llm call with.\"\"\"\\n        return \"Thought:\"\\n[docs]    @classmethod\\n    def create_prompt(\\n        cls,\\n        tools: Sequence[BaseTool],\\n        prefix: str = PREFIX,\\n        suffix: str = SUFFIX,\\n        format_instructions: str = FORMAT_INSTRUCTIONS,\\n        input_variables: Optional[List[str]] = None,\\n    ) -> PromptTemplate:\\n        \"\"\"Create prompt in the style of the zero shot agent.\\n        Args:\\n            tools: List of tools the agent will have access to, used to format the\\n                prompt.\\n            prefix: String to put before the list of tools.\\n            suffix: String to put after the list of tools.\\n            input_variables: List of input variables the final prompt will expect.\\n        Returns:\\n            A PromptTemplate with the template assembled from the pieces here.\\n        \"\"\"\\n        tool_strings = \"\\\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\\n        tool_names = \", \".join([tool.name for tool in tools])\\n        format_instructions = format_instructions.format(tool_names=tool_names)\\n        template = \"\\\\n\\\\n\".join([prefix, tool_strings, format_instructions, suffix])\\n        if input_variables is None:\\n            input_variables = [\"input\", \"agent_scratchpad\"]\\n        return PromptTemplate(template=template, input_variables=input_variables)\\n[docs]    @classmethod\\n    def from_llm_and_tools(\\n        cls,\\n        llm: BaseLanguageModel,\\n        tools: Sequence[BaseTool],\\n        callback_manager: Optional[BaseCallbackManager] = None,\\n        output_parser: Optional[AgentOutputParser] = None,\\n        prefix: str = PREFIX,\\n        suffix: str = SUFFIX,\\n        format_instructions: str = FORMAT_INSTRUCTIONS,\\n        input_variables: Optional[List[str]] = None,\\n        **kwargs: Any,\\n    ) -> Agent:\\n        \"\"\"Construct an agent from an LLM and tools.\"\"\"\\n        cls._validate_tools(tools)\\n        prompt = cls.create_prompt(\\n            tools,\\n            prefix=prefix,\\n            suffix=suffix,\\n            format_instructions=format_instructions,\\n            input_variables=input_variables,\\n        )\\n        llm_chain = LLMChain(\\n            llm=llm,\\n            prompt=prompt,\\n            callback_manager=callback_manager,\\n        )\\n        tool_names = [tool.name for tool in tools]\\n        _output_parser = output_parser or cls._get_default_output_parser()\\n        return cls(\\n            llm_chain=llm_chain,\\n            allowed_tools=tool_names,\\n            output_parser=_output_parser,\\n            **kwargs,\\n        )\\n    @classmethod\\n    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\\n        for tool in tools:\\n            if tool.description is None:\\n                raise ValueError(\\n                    f\"Got a tool {tool.name} without a description. For this agent, \"\\n                    f\"a description must always be provided.\"\\n                )\\n[docs]class MRKLChain(AgentExecutor):\\n    \"\"\"Chain that implements the MRKL system.\\n    Example:\\n        .. code-block:: python\\n            from langchain import OpenAI, MRKLChain\\n            from langchain.chains.mrkl.base import ChainConfig\\n            llm = OpenAI(temperature=0)\\n            prompt = PromptTemplate(...)\\n            chains = [...]\\n            mrkl = MRKLChain.from_chains(llm=llm, prompt=prompt)\\n    \"\"\"\\n[docs]    @classmethod\\n    def from_chains(\\n        cls, llm: BaseLanguageModel, chains: List[ChainConfig], **kwargs: Any\\n    ) -> AgentExecutor:\\n        \"\"\"User friendly way to initialize the MRKL chain.\\n        This is intended to be an easy way to get up and running with the\\n        MRKL chain.\\n        Args:\\n            llm: The LLM to use as the agent LLM.\\n            chains: The chains the MRKL system has access to.\\n            **kwargs: parameters to be passed to initialization.\\n        Returns:\\n            An initialized MRKL chain.\\n        Example:\\n            .. code-block:: python\\n                from langchain import LLMMathChain, OpenAI, SerpAPIWrapper, MRKLChain\\n                from langchain.chains.mrkl.base import ChainConfig\\n                llm = OpenAI(temperature=0)\\n                search = SerpAPIWrapper()\\n                llm_math_chain = LLMMathChain(llm=llm)\\n                chains = [\\n                    ChainConfig(\\n                        action_name = \"Search\",\\n                        action=search.search,\\n                        action_description=\"useful for searching\"\\n                    ),\\n                    ChainConfig(\\n                        action_name=\"Calculator\",\\n                        action=llm_math_chain.run,\\n                        action_description=\"useful for doing math\"\\n                    )\\n                ]\\n                mrkl = MRKLChain.from_chains(llm, chains)\\n        \"\"\"\\n        tools = [\\n            Tool(\\n                name=c.action_name,\\n                func=c.action,\\n                description=c.action_description,\\n            )\\n            for c in chains\\n        ]\\n        agent = ZeroShotAgent.from_llm_and_tools(llm, tools)\\n        return cls(agent=agent, tools=tools, **kwargs)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\agents\\\\mrkl\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.agents.react.base\\n\"\"\"Chain that implements the ReAct paper from https://arxiv.org/pdf/2210.03629.pdf.\"\"\"\\nfrom typing import Any, List, Optional, Sequence\\nfrom pydantic import Field\\nfrom langchain.agents.agent import Agent, AgentExecutor, AgentOutputParser\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.agents.react.output_parser import ReActOutputParser\\nfrom langchain.agents.react.textworld_prompt import TEXTWORLD_PROMPT\\nfrom langchain.agents.react.wiki_prompt import WIKI_PROMPT\\nfrom langchain.agents.tools import Tool\\nfrom langchain.docstore.base import Docstore\\nfrom langchain.docstore.document import Document\\nfrom langchain.llms.base import BaseLLM\\nfrom langchain.prompts.base import BasePromptTemplate\\nfrom langchain.tools.base import BaseTool\\nclass ReActDocstoreAgent(Agent):\\n    \"\"\"Agent for the ReAct chain.\"\"\"\\n    output_parser: AgentOutputParser = Field(default_factory=ReActOutputParser)\\n    @classmethod\\n    def _get_default_output_parser(cls, **kwargs: Any) -> AgentOutputParser:\\n        return ReActOutputParser()\\n    @property\\n    def _agent_type(self) -> str:\\n        \"\"\"Return Identifier of agent type.\"\"\"\\n        return AgentType.REACT_DOCSTORE\\n    @classmethod\\n    def create_prompt(cls, tools: Sequence[BaseTool]) -> BasePromptTemplate:\\n        \"\"\"Return default prompt.\"\"\"\\n        return WIKI_PROMPT\\n    @classmethod\\n    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\\n        if len(tools) != 2:\\n            raise ValueError(f\"Exactly two tools must be specified, but got {tools}\")\\n        tool_names = {tool.name for tool in tools}\\n        if tool_names != {\"Lookup\", \"Search\"}:\\n            raise ValueError(\\n                f\"Tool names should be Lookup and Search, got {tool_names}\"\\n            )\\n    @property\\n    def observation_prefix(self) -> str:\\n        \"\"\"Prefix to append the observation with.\"\"\"\\n        return \"Observation: \"\\n    @property\\n    def _stop(self) -> List[str]:\\n        return [\"\\\\nObservation:\"]\\n    @property\\n    def llm_prefix(self) -> str:\\n        \"\"\"Prefix to append the LLM call with.\"\"\"\\n        return \"Thought:\"\\nclass DocstoreExplorer:\\n    \"\"\"Class to assist with exploration of a document store.\"\"\"\\n    def __init__(self, docstore: Docstore):\\n        \"\"\"Initialize with a docstore, and set initial document to None.\"\"\"\\n        self.docstore = docstore\\n        self.document: Optional[Document] = None\\n        self.lookup_str = \"\"\\n        self.lookup_index = 0\\n    def search(self, term: str) -> str:\\n        \"\"\"Search for a term in the docstore, and if found save.\"\"\"\\n        result = self.docstore.search(term)\\n        if isinstance(result, Document):\\n            self.document = result\\n            return self._summary\\n        else:\\n            self.document = None\\n            return result\\n    def lookup(self, term: str) -> str:\\n        \"\"\"Lookup a term in document (if saved).\"\"\"\\n        if self.document is None:\\n            raise ValueError(\"Cannot lookup without a successful search first\")\\n        if term.lower() != self.lookup_str:\\n            self.lookup_str = term.lower()\\n            self.lookup_index = 0\\n        else:\\n            self.lookup_index += 1\\n        lookups = [p for p in self._paragraphs if self.lookup_str in p.lower()]\\n        if len(lookups) == 0:\\n            return \"No Results\"\\n        elif self.lookup_index >= len(lookups):\\n            return \"No More Results\"\\n        else:\\n            result_prefix = f\"(Result {self.lookup_index + 1}/{len(lookups)})\"\\n            return f\"{result_prefix} {lookups[self.lookup_index]}\"\\n    @property\\n    def _summary(self) -> str:\\n        return self._paragraphs[0]\\n    @property\\n    def _paragraphs(self) -> List[str]:\\n        if self.document is None:\\n            raise ValueError(\"Cannot get paragraphs without a document\")\\n        return self.document.page_content.split(\"\\\\n\\\\n\")\\n[docs]class ReActTextWorldAgent(ReActDocstoreAgent):\\n    \"\"\"Agent for the ReAct TextWorld chain.\"\"\"\\n[docs]    @classmethod\\n    def create_prompt(cls, tools: Sequence[BaseTool]) -> BasePromptTemplate:\\n        \"\"\"Return default prompt.\"\"\"\\n        return TEXTWORLD_PROMPT\\n    @classmethod\\n    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\\n        if len(tools) != 1:\\n            raise ValueError(f\"Exactly one tool must be specified, but got {tools}\")\\n        tool_names = {tool.name for tool in tools}\\n        if tool_names != {\"Play\"}:\\n            raise ValueError(f\"Tool name should be Play, got {tool_names}\")\\n[docs]class ReActChain(AgentExecutor):\\n    \"\"\"Chain that implements the ReAct paper.\\n    Example:\\n        .. code-block:: python\\n            from langchain import ReActChain, OpenAI\\n            react = ReAct(llm=OpenAI())\\n    \"\"\"\\n    def __init__(self, llm: BaseLLM, docstore: Docstore, **kwargs: Any):\\n        \"\"\"Initialize with the LLM and a docstore.\"\"\"\\n        docstore_explorer = DocstoreExplorer(docstore)\\n        tools = [\\n            Tool(\\n                name=\"Search\",\\n                func=docstore_explorer.search,\\n                description=\"Search for a term in the docstore.\",\\n            ),\\n            Tool(\\n                name=\"Lookup\",\\n                func=docstore_explorer.lookup,\\n                description=\"Lookup a term in the docstore.\",\\n            ),\\n        ]\\n        agent = ReActDocstoreAgent.from_llm_and_tools(llm, tools)\\n        super().__init__(agent=agent, tools=tools, **kwargs)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\agents\\\\react\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.agents.self_ask_with_search.base\\n\"\"\"Chain that does self ask with search.\"\"\"\\nfrom typing import Any, Sequence, Union\\nfrom pydantic import Field\\nfrom langchain.agents.agent import Agent, AgentExecutor, AgentOutputParser\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.agents.self_ask_with_search.output_parser import SelfAskOutputParser\\nfrom langchain.agents.self_ask_with_search.prompt import PROMPT\\nfrom langchain.agents.tools import Tool\\nfrom langchain.llms.base import BaseLLM\\nfrom langchain.prompts.base import BasePromptTemplate\\nfrom langchain.tools.base import BaseTool\\nfrom langchain.utilities.google_serper import GoogleSerperAPIWrapper\\nfrom langchain.utilities.serpapi import SerpAPIWrapper\\nclass SelfAskWithSearchAgent(Agent):\\n    \"\"\"Agent for the self-ask-with-search paper.\"\"\"\\n    output_parser: AgentOutputParser = Field(default_factory=SelfAskOutputParser)\\n    @classmethod\\n    def _get_default_output_parser(cls, **kwargs: Any) -> AgentOutputParser:\\n        return SelfAskOutputParser()\\n    @property\\n    def _agent_type(self) -> str:\\n        \"\"\"Return Identifier of agent type.\"\"\"\\n        return AgentType.SELF_ASK_WITH_SEARCH\\n    @classmethod\\n    def create_prompt(cls, tools: Sequence[BaseTool]) -> BasePromptTemplate:\\n        \"\"\"Prompt does not depend on tools.\"\"\"\\n        return PROMPT\\n    @classmethod\\n    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\\n        if len(tools) != 1:\\n            raise ValueError(f\"Exactly one tool must be specified, but got {tools}\")\\n        tool_names = {tool.name for tool in tools}\\n        if tool_names != {\"Intermediate Answer\"}:\\n            raise ValueError(\\n                f\"Tool name should be Intermediate Answer, got {tool_names}\"\\n            )\\n    @property\\n    def observation_prefix(self) -> str:\\n        \"\"\"Prefix to append the observation with.\"\"\"\\n        return \"Intermediate answer: \"\\n    @property\\n    def llm_prefix(self) -> str:\\n        \"\"\"Prefix to append the LLM call with.\"\"\"\\n        return \"\"\\n[docs]class SelfAskWithSearchChain(AgentExecutor):\\n    \"\"\"Chain that does self ask with search.\\n    Example:\\n        .. code-block:: python\\n            from langchain import SelfAskWithSearchChain, OpenAI, GoogleSerperAPIWrapper\\n            search_chain = GoogleSerperAPIWrapper()\\n            self_ask = SelfAskWithSearchChain(llm=OpenAI(), search_chain=search_chain)\\n    \"\"\"\\n    def __init__(\\n        self,\\n        llm: BaseLLM,\\n        search_chain: Union[GoogleSerperAPIWrapper, SerpAPIWrapper],\\n        **kwargs: Any,\\n    ):\\n        \"\"\"Initialize with just an LLM and a search chain.\"\"\"\\n        search_tool = Tool(\\n            name=\"Intermediate Answer\", func=search_chain.run, description=\"Search\"\\n        )\\n        agent = SelfAskWithSearchAgent.from_llm_and_tools(llm, [search_tool])\\n        super().__init__(agent=agent, tools=[search_tool], **kwargs)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\agents\\\\self_ask_with_search\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.chains.llm\\n\"\"\"Chain that just formats a prompt and calls an LLM.\"\"\"\\nfrom __future__ import annotations\\nfrom typing import Any, Dict, List, Optional, Sequence, Tuple, Union\\nfrom pydantic import Extra\\nfrom langchain.chains.base import Chain\\nfrom langchain.input import get_colored_text\\nfrom langchain.prompts.base import BasePromptTemplate\\nfrom langchain.prompts.prompt import PromptTemplate\\nfrom langchain.schema import BaseLanguageModel, LLMResult, PromptValue\\n[docs]class LLMChain(Chain):\\n    \"\"\"Chain to run queries against LLMs.\\n    Example:\\n        .. code-block:: python\\n            from langchain import LLMChain, OpenAI, PromptTemplate\\n            prompt_template = \"Tell me a {adjective} joke\"\\n            prompt = PromptTemplate(\\n                input_variables=[\"adjective\"], template=prompt_template\\n            )\\n            llm = LLMChain(llm=OpenAI(), prompt=prompt)\\n    \"\"\"\\n    prompt: BasePromptTemplate\\n    \"\"\"Prompt object to use.\"\"\"\\n    llm: BaseLanguageModel\\n    output_key: str = \"text\"  #: :meta private:\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Will be whatever keys the prompt expects.\\n        :meta private:\\n        \"\"\"\\n        return self.prompt.input_variables\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Will always return text key.\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]\\n    def _call(self, inputs: Dict[str, Any]) -> Dict[str, str]:\\n        return self.apply([inputs])[0]\\n[docs]    def generate(self, input_list: List[Dict[str, Any]]) -> LLMResult:\\n        \"\"\"Generate LLM result from inputs.\"\"\"\\n        prompts, stop = self.prep_prompts(input_list)\\n        return self.llm.generate_prompt(prompts, stop)\\n[docs]    async def agenerate(self, input_list: List[Dict[str, Any]]) -> LLMResult:\\n        \"\"\"Generate LLM result from inputs.\"\"\"\\n        prompts, stop = await self.aprep_prompts(input_list)\\n        return await self.llm.agenerate_prompt(prompts, stop)\\n[docs]    def prep_prompts(\\n        self, input_list: List[Dict[str, Any]]\\n    ) -> Tuple[List[PromptValue], Optional[List[str]]]:\\n        \"\"\"Prepare prompts from inputs.\"\"\"\\n        stop = None\\n        if \"stop\" in input_list[0]:\\n            stop = input_list[0][\"stop\"]\\n        prompts = []\\n        for inputs in input_list:\\n            selected_inputs = {k: inputs[k] for k in self.prompt.input_variables}\\n            prompt = self.prompt.format_prompt(**selected_inputs)\\n            _colored_text = get_colored_text(prompt.to_string(), \"green\")\\n            _text = \"Prompt after formatting:\\\\n\" + _colored_text\\n            self.callback_manager.on_text(_text, end=\"\\\\n\", verbose=self.verbose)\\n            if \"stop\" in inputs and inputs[\"stop\"] != stop:\\n                raise ValueError(\\n                    \"If `stop` is present in any inputs, should be present in all.\"\\n                )\\n            prompts.append(prompt)\\n        return prompts, stop\\n[docs]    async def aprep_prompts(\\n        self, input_list: List[Dict[str, Any]]\\n    ) -> Tuple[List[PromptValue], Optional[List[str]]]:\\n        \"\"\"Prepare prompts from inputs.\"\"\"\\n        stop = None\\n        if \"stop\" in input_list[0]:\\n            stop = input_list[0][\"stop\"]\\n        prompts = []\\n        for inputs in input_list:\\n            selected_inputs = {k: inputs[k] for k in self.prompt.input_variables}\\n            prompt = self.prompt.format_prompt(**selected_inputs)\\n            _colored_text = get_colored_text(prompt.to_string(), \"green\")\\n            _text = \"Prompt after formatting:\\\\n\" + _colored_text\\n            if self.callback_manager.is_async:\\n                await self.callback_manager.on_text(\\n                    _text, end=\"\\\\n\", verbose=self.verbose\\n                )\\n            else:\\n                self.callback_manager.on_text(_text, end=\"\\\\n\", verbose=self.verbose)\\n            if \"stop\" in inputs and inputs[\"stop\"] != stop:\\n                raise ValueError(\\n                    \"If `stop` is present in any inputs, should be present in all.\"\\n                )\\n            prompts.append(prompt)\\n        return prompts, stop\\n[docs]    def apply(self, input_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:\\n        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\\n        response = self.generate(input_list)\\n        return self.create_outputs(response)\\n[docs]    async def aapply(self, input_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:\\n        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\\n        response = await self.agenerate(input_list)\\n        return self.create_outputs(response)\\n[docs]    def create_outputs(self, response: LLMResult) -> List[Dict[str, str]]:\\n        \"\"\"Create outputs from response.\"\"\"\\n        return [\\n            # Get the text of the top generated string.\\n            {self.output_key: generation[0].text}\\n            for generation in response.generations\\n        ]\\n    async def _acall(self, inputs: Dict[str, Any]) -> Dict[str, str]:\\n        return (await self.aapply([inputs]))[0]\\n[docs]    def predict(self, **kwargs: Any) -> str:\\n        \"\"\"Format prompt with kwargs and pass to LLM.\\n        Args:\\n            **kwargs: Keys to pass to prompt template.\\n        Returns:\\n            Completion from LLM.\\n        Example:\\n            .. code-block:: python\\n                completion = llm.predict(adjective=\"funny\")\\n        \"\"\"\\n        return self(kwargs)[self.output_key]\\n[docs]    async def apredict(self, **kwargs: Any) -> str:\\n        \"\"\"Format prompt with kwargs and pass to LLM.\\n        Args:\\n            **kwargs: Keys to pass to prompt template.\\n        Returns:\\n            Completion from LLM.\\n        Example:\\n            .. code-block:: python\\n                completion = llm.predict(adjective=\"funny\")\\n        \"\"\"\\n        return (await self.acall(kwargs))[self.output_key]\\n[docs]    def predict_and_parse(self, **kwargs: Any) -> Union[str, List[str], Dict[str, str]]:\\n        \"\"\"Call predict and then parse the results.\"\"\"\\n        result = self.predict(**kwargs)\\n        if self.prompt.output_parser is not None:\\n            return self.prompt.output_parser.parse(result)\\n        else:\\n            return result\\n[docs]    async def apredict_and_parse(\\n        self, **kwargs: Any\\n    ) -> Union[str, List[str], Dict[str, str]]:\\n        \"\"\"Call apredict and then parse the results.\"\"\"\\n        result = await self.apredict(**kwargs)\\n        if self.prompt.output_parser is not None:\\n            return self.prompt.output_parser.parse(result)\\n        else:\\n            return result\\n[docs]    def apply_and_parse(\\n        self, input_list: List[Dict[str, Any]]\\n    ) -> Sequence[Union[str, List[str], Dict[str, str]]]:\\n        \"\"\"Call apply and then parse the results.\"\"\"\\n        result = self.apply(input_list)\\n        return self._parse_result(result)\\n    def _parse_result(\\n        self, result: List[Dict[str, str]]\\n    ) -> Sequence[Union[str, List[str], Dict[str, str]]]:\\n        if self.prompt.output_parser is not None:\\n            return [\\n                self.prompt.output_parser.parse(res[self.output_key]) for res in result\\n            ]\\n        else:\\n            return result\\n[docs]    async def aapply_and_parse(\\n        self, input_list: List[Dict[str, Any]]\\n    ) -> Sequence[Union[str, List[str], Dict[str, str]]]:\\n        \"\"\"Call apply and then parse the results.\"\"\"\\n        result = await self.aapply(input_list)\\n        return self._parse_result(result)\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"llm_chain\"\\n[docs]    @classmethod\\n    def from_string(cls, llm: BaseLanguageModel, template: str) -> Chain:\\n        \"\"\"Create LLMChain from LLM and template.\"\"\"\\n        prompt_template = PromptTemplate.from_template(template)\\n        return cls(llm=llm, prompt=prompt_template)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\llm.html'}),\n",
              " Document(page_content='Source code for langchain.chains.llm_requests\\n\"\"\"Chain that hits a URL and then uses an LLM to parse results.\"\"\"\\nfrom __future__ import annotations\\nfrom typing import Dict, List\\nfrom pydantic import Extra, Field, root_validator\\nfrom langchain.chains import LLMChain\\nfrom langchain.chains.base import Chain\\nfrom langchain.requests import TextRequestsWrapper\\nDEFAULT_HEADERS = {\\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"  # noqa: E501\\n}\\n[docs]class LLMRequestsChain(Chain):\\n    \"\"\"Chain that hits a URL and then uses an LLM to parse results.\"\"\"\\n    llm_chain: LLMChain\\n    requests_wrapper: TextRequestsWrapper = Field(\\n        default_factory=TextRequestsWrapper, exclude=True\\n    )\\n    text_length: int = 8000\\n    requests_key: str = \"requests_result\"  #: :meta private:\\n    input_key: str = \"url\"  #: :meta private:\\n    output_key: str = \"output\"  #: :meta private:\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Will be whatever keys the prompt expects.\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Will always return text key.\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        try:\\n            from bs4 import BeautifulSoup  # noqa: F401\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import bs4 python package. \"\\n                \"Please install it with `pip install bs4`.\"\\n            )\\n        return values\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        from bs4 import BeautifulSoup\\n        # Other keys are assumed to be needed for LLM prediction\\n        other_keys = {k: v for k, v in inputs.items() if k != self.input_key}\\n        url = inputs[self.input_key]\\n        res = self.requests_wrapper.get(url)\\n        # extract the text from the html\\n        soup = BeautifulSoup(res, \"html.parser\")\\n        other_keys[self.requests_key] = soup.get_text()[: self.text_length]\\n        result = self.llm_chain.predict(**other_keys)\\n        return {self.output_key: result}\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"llm_requests_chain\"\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\llm_requests.html'}),\n",
              " Document(page_content='Source code for langchain.chains.loading\\n\"\"\"Functionality for loading chains.\"\"\"\\nimport json\\nfrom pathlib import Path\\nfrom typing import Any, Union\\nimport yaml\\nfrom langchain.chains.api.base import APIChain\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\\nfrom langchain.chains.combine_documents.map_rerank import MapRerankDocumentsChain\\nfrom langchain.chains.combine_documents.refine import RefineDocumentsChain\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\\nfrom langchain.chains.hyde.base import HypotheticalDocumentEmbedder\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.llm_bash.base import LLMBashChain\\nfrom langchain.chains.llm_checker.base import LLMCheckerChain\\nfrom langchain.chains.llm_math.base import LLMMathChain\\nfrom langchain.chains.llm_requests import LLMRequestsChain\\nfrom langchain.chains.pal.base import PALChain\\nfrom langchain.chains.qa_with_sources.base import QAWithSourcesChain\\nfrom langchain.chains.qa_with_sources.vector_db import VectorDBQAWithSourcesChain\\nfrom langchain.chains.retrieval_qa.base import VectorDBQA\\nfrom langchain.chains.sql_database.base import SQLDatabaseChain\\nfrom langchain.llms.loading import load_llm, load_llm_from_config\\nfrom langchain.prompts.loading import load_prompt, load_prompt_from_config\\nfrom langchain.utilities.loading import try_load_from_hub\\nURL_BASE = \"https://raw.githubusercontent.com/hwchase17/langchain-hub/master/chains/\"\\ndef _load_llm_chain(config: dict, **kwargs: Any) -> LLMChain:\\n    \"\"\"Load LLM chain from config dict.\"\"\"\\n    if \"llm\" in config:\\n        llm_config = config.pop(\"llm\")\\n        llm = load_llm_from_config(llm_config)\\n    elif \"llm_path\" in config:\\n        llm = load_llm(config.pop(\"llm_path\"))\\n    else:\\n        raise ValueError(\"One of `llm` or `llm_path` must be present.\")\\n    if \"prompt\" in config:\\n        prompt_config = config.pop(\"prompt\")\\n        prompt = load_prompt_from_config(prompt_config)\\n    elif \"prompt_path\" in config:\\n        prompt = load_prompt(config.pop(\"prompt_path\"))\\n    else:\\n        raise ValueError(\"One of `prompt` or `prompt_path` must be present.\")\\n    return LLMChain(llm=llm, prompt=prompt, **config)\\ndef _load_hyde_chain(config: dict, **kwargs: Any) -> HypotheticalDocumentEmbedder:\\n    \"\"\"Load hypothetical document embedder chain from config dict.\"\"\"\\n    if \"llm_chain\" in config:\\n        llm_chain_config = config.pop(\"llm_chain\")\\n        llm_chain = load_chain_from_config(llm_chain_config)\\n    elif \"llm_chain_path\" in config:\\n        llm_chain = load_chain(config.pop(\"llm_chain_path\"))\\n    else:\\n        raise ValueError(\"One of `llm_chain` or `llm_chain_path` must be present.\")\\n    if \"embeddings\" in kwargs:\\n        embeddings = kwargs.pop(\"embeddings\")\\n    else:\\n        raise ValueError(\"`embeddings` must be present.\")\\n    return HypotheticalDocumentEmbedder(\\n        llm_chain=llm_chain, base_embeddings=embeddings, **config\\n    )\\ndef _load_stuff_documents_chain(config: dict, **kwargs: Any) -> StuffDocumentsChain:\\n    if \"llm_chain\" in config:\\n        llm_chain_config = config.pop(\"llm_chain\")\\n        llm_chain = load_chain_from_config(llm_chain_config)\\n    elif \"llm_chain_path\" in config:\\n        llm_chain = load_chain(config.pop(\"llm_chain_path\"))\\n    else:\\n        raise ValueError(\"One of `llm_chain` or `llm_chain_config` must be present.\")\\n    if not isinstance(llm_chain, LLMChain):\\n        raise ValueError(f\"Expected LLMChain, got {llm_chain}\")\\n    if \"document_prompt\" in config:\\n        prompt_config = config.pop(\"document_prompt\")\\n        document_prompt = load_prompt_from_config(prompt_config)\\n    elif \"document_prompt_path\" in config:\\n        document_prompt = load_prompt(config.pop(\"document_prompt_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `document_prompt` or `document_prompt_path` must be present.\"\\n        )\\n    return StuffDocumentsChain(\\n        llm_chain=llm_chain, document_prompt=document_prompt, **config\\n    )\\ndef _load_map_reduce_documents_chain(\\n    config: dict, **kwargs: Any\\n) -> MapReduceDocumentsChain:\\n    if \"llm_chain\" in config:\\n        llm_chain_config = config.pop(\"llm_chain\")\\n        llm_chain = load_chain_from_config(llm_chain_config)\\n    elif \"llm_chain_path\" in config:\\n        llm_chain = load_chain(config.pop(\"llm_chain_path\"))\\n    else:\\n        raise ValueError(\"One of `llm_chain` or `llm_chain_config` must be present.\")\\n    if not isinstance(llm_chain, LLMChain):\\n        raise ValueError(f\"Expected LLMChain, got {llm_chain}\")\\n    if \"combine_document_chain\" in config:\\n        combine_document_chain_config = config.pop(\"combine_document_chain\")\\n        combine_document_chain = load_chain_from_config(combine_document_chain_config)\\n    elif \"combine_document_chain_path\" in config:\\n        combine_document_chain = load_chain(config.pop(\"combine_document_chain_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `combine_document_chain` or \"\\n            \"`combine_document_chain_path` must be present.\"\\n        )\\n    if \"collapse_document_chain\" in config:\\n        collapse_document_chain_config = config.pop(\"collapse_document_chain\")\\n        if collapse_document_chain_config is None:\\n            collapse_document_chain = None\\n        else:\\n            collapse_document_chain = load_chain_from_config(\\n                collapse_document_chain_config\\n            )\\n    elif \"collapse_document_chain_path\" in config:\\n        collapse_document_chain = load_chain(config.pop(\"collapse_document_chain_path\"))\\n    return MapReduceDocumentsChain(\\n        llm_chain=llm_chain,\\n        combine_document_chain=combine_document_chain,\\n        collapse_document_chain=collapse_document_chain,\\n        **config,\\n    )\\ndef _load_llm_bash_chain(config: dict, **kwargs: Any) -> LLMBashChain:\\n    if \"llm\" in config:\\n        llm_config = config.pop(\"llm\")\\n        llm = load_llm_from_config(llm_config)\\n    elif \"llm_path\" in config:\\n        llm = load_llm(config.pop(\"llm_path\"))\\n    else:\\n        raise ValueError(\"One of `llm` or `llm_path` must be present.\")\\n    if \"prompt\" in config:\\n        prompt_config = config.pop(\"prompt\")\\n        prompt = load_prompt_from_config(prompt_config)\\n    elif \"prompt_path\" in config:\\n        prompt = load_prompt(config.pop(\"prompt_path\"))\\n    return LLMBashChain(llm=llm, prompt=prompt, **config)\\ndef _load_llm_checker_chain(config: dict, **kwargs: Any) -> LLMCheckerChain:\\n    if \"llm\" in config:\\n        llm_config = config.pop(\"llm\")\\n        llm = load_llm_from_config(llm_config)\\n    elif \"llm_path\" in config:\\n        llm = load_llm(config.pop(\"llm_path\"))\\n    else:\\n        raise ValueError(\"One of `llm` or `llm_path` must be present.\")\\n    if \"create_draft_answer_prompt\" in config:\\n        create_draft_answer_prompt_config = config.pop(\"create_draft_answer_prompt\")\\n        create_draft_answer_prompt = load_prompt_from_config(\\n            create_draft_answer_prompt_config\\n        )\\n    elif \"create_draft_answer_prompt_path\" in config:\\n        create_draft_answer_prompt = load_prompt(\\n            config.pop(\"create_draft_answer_prompt_path\")\\n        )\\n    if \"list_assertions_prompt\" in config:\\n        list_assertions_prompt_config = config.pop(\"list_assertions_prompt\")\\n        list_assertions_prompt = load_prompt_from_config(list_assertions_prompt_config)\\n    elif \"list_assertions_prompt_path\" in config:\\n        list_assertions_prompt = load_prompt(config.pop(\"list_assertions_prompt_path\"))\\n    if \"check_assertions_prompt\" in config:\\n        check_assertions_prompt_config = config.pop(\"check_assertions_prompt\")\\n        check_assertions_prompt = load_prompt_from_config(\\n            check_assertions_prompt_config\\n        )\\n    elif \"check_assertions_prompt_path\" in config:\\n        check_assertions_prompt = load_prompt(\\n            config.pop(\"check_assertions_prompt_path\")\\n        )\\n    if \"revised_answer_prompt\" in config:\\n        revised_answer_prompt_config = config.pop(\"revised_answer_prompt\")\\n        revised_answer_prompt = load_prompt_from_config(revised_answer_prompt_config)\\n    elif \"revised_answer_prompt_path\" in config:\\n        revised_answer_prompt = load_prompt(config.pop(\"revised_answer_prompt_path\"))\\n    return LLMCheckerChain(\\n        llm=llm,\\n        create_draft_answer_prompt=create_draft_answer_prompt,\\n        list_assertions_prompt=list_assertions_prompt,\\n        check_assertions_prompt=check_assertions_prompt,\\n        revised_answer_prompt=revised_answer_prompt,\\n        **config,\\n    )\\ndef _load_llm_math_chain(config: dict, **kwargs: Any) -> LLMMathChain:\\n    if \"llm\" in config:\\n        llm_config = config.pop(\"llm\")\\n        llm = load_llm_from_config(llm_config)\\n    elif \"llm_path\" in config:\\n        llm = load_llm(config.pop(\"llm_path\"))\\n    else:\\n        raise ValueError(\"One of `llm` or `llm_path` must be present.\")\\n    if \"prompt\" in config:\\n        prompt_config = config.pop(\"prompt\")\\n        prompt = load_prompt_from_config(prompt_config)\\n    elif \"prompt_path\" in config:\\n        prompt = load_prompt(config.pop(\"prompt_path\"))\\n    return LLMMathChain(llm=llm, prompt=prompt, **config)\\ndef _load_map_rerank_documents_chain(\\n    config: dict, **kwargs: Any\\n) -> MapRerankDocumentsChain:\\n    if \"llm_chain\" in config:\\n        llm_chain_config = config.pop(\"llm_chain\")\\n        llm_chain = load_chain_from_config(llm_chain_config)\\n    elif \"llm_chain_path\" in config:\\n        llm_chain = load_chain(config.pop(\"llm_chain_path\"))\\n    else:\\n        raise ValueError(\"One of `llm_chain` or `llm_chain_config` must be present.\")\\n    return MapRerankDocumentsChain(llm_chain=llm_chain, **config)\\ndef _load_pal_chain(config: dict, **kwargs: Any) -> PALChain:\\n    if \"llm\" in config:\\n        llm_config = config.pop(\"llm\")\\n        llm = load_llm_from_config(llm_config)\\n    elif \"llm_path\" in config:\\n        llm = load_llm(config.pop(\"llm_path\"))\\n    else:\\n        raise ValueError(\"One of `llm` or `llm_path` must be present.\")\\n    if \"prompt\" in config:\\n        prompt_config = config.pop(\"prompt\")\\n        prompt = load_prompt_from_config(prompt_config)\\n    elif \"prompt_path\" in config:\\n        prompt = load_prompt(config.pop(\"prompt_path\"))\\n    else:\\n        raise ValueError(\"One of `prompt` or `prompt_path` must be present.\")\\n    return PALChain(llm=llm, prompt=prompt, **config)\\ndef _load_refine_documents_chain(config: dict, **kwargs: Any) -> RefineDocumentsChain:\\n    if \"initial_llm_chain\" in config:\\n        initial_llm_chain_config = config.pop(\"initial_llm_chain\")\\n        initial_llm_chain = load_chain_from_config(initial_llm_chain_config)\\n    elif \"initial_llm_chain_path\" in config:\\n        initial_llm_chain = load_chain(config.pop(\"initial_llm_chain_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `initial_llm_chain` or `initial_llm_chain_config` must be present.\"\\n        )\\n    if \"refine_llm_chain\" in config:\\n        refine_llm_chain_config = config.pop(\"refine_llm_chain\")\\n        refine_llm_chain = load_chain_from_config(refine_llm_chain_config)\\n    elif \"refine_llm_chain_path\" in config:\\n        refine_llm_chain = load_chain(config.pop(\"refine_llm_chain_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `refine_llm_chain` or `refine_llm_chain_config` must be present.\"\\n        )\\n    if \"document_prompt\" in config:\\n        prompt_config = config.pop(\"document_prompt\")\\n        document_prompt = load_prompt_from_config(prompt_config)\\n    elif \"document_prompt_path\" in config:\\n        document_prompt = load_prompt(config.pop(\"document_prompt_path\"))\\n    return RefineDocumentsChain(\\n        initial_llm_chain=initial_llm_chain,\\n        refine_llm_chain=refine_llm_chain,\\n        document_prompt=document_prompt,\\n        **config,\\n    )\\ndef _load_qa_with_sources_chain(config: dict, **kwargs: Any) -> QAWithSourcesChain:\\n    if \"combine_documents_chain\" in config:\\n        combine_documents_chain_config = config.pop(\"combine_documents_chain\")\\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\\n    elif \"combine_documents_chain_path\" in config:\\n        combine_documents_chain = load_chain(config.pop(\"combine_documents_chain_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `combine_documents_chain` or \"\\n            \"`combine_documents_chain_path` must be present.\"\\n        )\\n    return QAWithSourcesChain(combine_documents_chain=combine_documents_chain, **config)\\ndef _load_sql_database_chain(config: dict, **kwargs: Any) -> SQLDatabaseChain:\\n    if \"database\" in kwargs:\\n        database = kwargs.pop(\"database\")\\n    else:\\n        raise ValueError(\"`database` must be present.\")\\n    if \"llm\" in config:\\n        llm_config = config.pop(\"llm\")\\n        llm = load_llm_from_config(llm_config)\\n    elif \"llm_path\" in config:\\n        llm = load_llm(config.pop(\"llm_path\"))\\n    else:\\n        raise ValueError(\"One of `llm` or `llm_path` must be present.\")\\n    if \"prompt\" in config:\\n        prompt_config = config.pop(\"prompt\")\\n        prompt = load_prompt_from_config(prompt_config)\\n    return SQLDatabaseChain(database=database, llm=llm, prompt=prompt, **config)\\ndef _load_vector_db_qa_with_sources_chain(\\n    config: dict, **kwargs: Any\\n) -> VectorDBQAWithSourcesChain:\\n    if \"vectorstore\" in kwargs:\\n        vectorstore = kwargs.pop(\"vectorstore\")\\n    else:\\n        raise ValueError(\"`vectorstore` must be present.\")\\n    if \"combine_documents_chain\" in config:\\n        combine_documents_chain_config = config.pop(\"combine_documents_chain\")\\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\\n    elif \"combine_documents_chain_path\" in config:\\n        combine_documents_chain = load_chain(config.pop(\"combine_documents_chain_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `combine_documents_chain` or \"\\n            \"`combine_documents_chain_path` must be present.\"\\n        )\\n    return VectorDBQAWithSourcesChain(\\n        combine_documents_chain=combine_documents_chain,\\n        vectorstore=vectorstore,\\n        **config,\\n    )\\ndef _load_vector_db_qa(config: dict, **kwargs: Any) -> VectorDBQA:\\n    if \"vectorstore\" in kwargs:\\n        vectorstore = kwargs.pop(\"vectorstore\")\\n    else:\\n        raise ValueError(\"`vectorstore` must be present.\")\\n    if \"combine_documents_chain\" in config:\\n        combine_documents_chain_config = config.pop(\"combine_documents_chain\")\\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\\n    elif \"combine_documents_chain_path\" in config:\\n        combine_documents_chain = load_chain(config.pop(\"combine_documents_chain_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `combine_documents_chain` or \"\\n            \"`combine_documents_chain_path` must be present.\"\\n        )\\n    return VectorDBQA(\\n        combine_documents_chain=combine_documents_chain,\\n        vectorstore=vectorstore,\\n        **config,\\n    )\\ndef _load_api_chain(config: dict, **kwargs: Any) -> APIChain:\\n    if \"api_request_chain\" in config:\\n        api_request_chain_config = config.pop(\"api_request_chain\")\\n        api_request_chain = load_chain_from_config(api_request_chain_config)\\n    elif \"api_request_chain_path\" in config:\\n        api_request_chain = load_chain(config.pop(\"api_request_chain_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `api_request_chain` or `api_request_chain_path` must be present.\"\\n        )\\n    if \"api_answer_chain\" in config:\\n        api_answer_chain_config = config.pop(\"api_answer_chain\")\\n        api_answer_chain = load_chain_from_config(api_answer_chain_config)\\n    elif \"api_answer_chain_path\" in config:\\n        api_answer_chain = load_chain(config.pop(\"api_answer_chain_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `api_answer_chain` or `api_answer_chain_path` must be present.\"\\n        )\\n    if \"requests_wrapper\" in kwargs:\\n        requests_wrapper = kwargs.pop(\"requests_wrapper\")\\n    else:\\n        raise ValueError(\"`requests_wrapper` must be present.\")\\n    return APIChain(\\n        api_request_chain=api_request_chain,\\n        api_answer_chain=api_answer_chain,\\n        requests_wrapper=requests_wrapper,\\n        **config,\\n    )\\ndef _load_llm_requests_chain(config: dict, **kwargs: Any) -> LLMRequestsChain:\\n    if \"llm_chain\" in config:\\n        llm_chain_config = config.pop(\"llm_chain\")\\n        llm_chain = load_chain_from_config(llm_chain_config)\\n    elif \"llm_chain_path\" in config:\\n        llm_chain = load_chain(config.pop(\"llm_chain_path\"))\\n    else:\\n        raise ValueError(\"One of `llm_chain` or `llm_chain_path` must be present.\")\\n    if \"requests_wrapper\" in kwargs:\\n        requests_wrapper = kwargs.pop(\"requests_wrapper\")\\n        return LLMRequestsChain(\\n            llm_chain=llm_chain, requests_wrapper=requests_wrapper, **config\\n        )\\n    else:\\n        return LLMRequestsChain(llm_chain=llm_chain, **config)\\ntype_to_loader_dict = {\\n    \"api_chain\": _load_api_chain,\\n    \"hyde_chain\": _load_hyde_chain,\\n    \"llm_chain\": _load_llm_chain,\\n    \"llm_bash_chain\": _load_llm_bash_chain,\\n    \"llm_checker_chain\": _load_llm_checker_chain,\\n    \"llm_math_chain\": _load_llm_math_chain,\\n    \"llm_requests_chain\": _load_llm_requests_chain,\\n    \"pal_chain\": _load_pal_chain,\\n    \"qa_with_sources_chain\": _load_qa_with_sources_chain,\\n    \"stuff_documents_chain\": _load_stuff_documents_chain,\\n    \"map_reduce_documents_chain\": _load_map_reduce_documents_chain,\\n    \"map_rerank_documents_chain\": _load_map_rerank_documents_chain,\\n    \"refine_documents_chain\": _load_refine_documents_chain,\\n    \"sql_database_chain\": _load_sql_database_chain,\\n    \"vector_db_qa_with_sources_chain\": _load_vector_db_qa_with_sources_chain,\\n    \"vector_db_qa\": _load_vector_db_qa,\\n}\\ndef load_chain_from_config(config: dict, **kwargs: Any) -> Chain:\\n    \"\"\"Load chain from Config Dict.\"\"\"\\n    if \"_type\" not in config:\\n        raise ValueError(\"Must specify a chain Type in config\")\\n    config_type = config.pop(\"_type\")\\n    if config_type not in type_to_loader_dict:\\n        raise ValueError(f\"Loading {config_type} chain not supported\")\\n    chain_loader = type_to_loader_dict[config_type]\\n    return chain_loader(config, **kwargs)\\n[docs]def load_chain(path: Union[str, Path], **kwargs: Any) -> Chain:\\n    \"\"\"Unified method for loading a chain from LangChainHub or local fs.\"\"\"\\n    if hub_result := try_load_from_hub(\\n        path, _load_chain_from_file, \"chains\", {\"json\", \"yaml\"}, **kwargs\\n    ):\\n        return hub_result\\n    else:\\n        return _load_chain_from_file(path, **kwargs)\\ndef _load_chain_from_file(file: Union[str, Path], **kwargs: Any) -> Chain:\\n    \"\"\"Load chain from file.\"\"\"\\n    # Convert file to Path object.\\n    if isinstance(file, str):\\n        file_path = Path(file)\\n    else:\\n        file_path = file\\n    # Load from either json or yaml.\\n    if file_path.suffix == \".json\":\\n        with open(file_path) as f:\\n            config = json.load(f)\\n    elif file_path.suffix == \".yaml\":\\n        with open(file_path, \"r\") as f:\\n            config = yaml.safe_load(f)\\n    else:\\n        raise ValueError(\"File type must be json or yaml\")\\n    # Override default \\'verbose\\' and \\'memory\\' for the chain\\n    if \"verbose\" in kwargs:\\n        config[\"verbose\"] = kwargs.pop(\"verbose\")\\n    if \"memory\" in kwargs:\\n        config[\"memory\"] = kwargs.pop(\"memory\")\\n    # Load the chain from the config now.\\n    return load_chain_from_config(config, **kwargs)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\loading.html'}),\n",
              " Document(page_content='Source code for langchain.chains.mapreduce\\n\"\"\"Map-reduce chain.\\nSplits up a document, sends the smaller parts to the LLM with one prompt,\\nthen combines the results with another one.\\n\"\"\"\\nfrom __future__ import annotations\\nfrom typing import Dict, List\\nfrom pydantic import Extra\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.docstore.document import Document\\nfrom langchain.llms.base import BaseLLM\\nfrom langchain.prompts.base import BasePromptTemplate\\nfrom langchain.text_splitter import TextSplitter\\n[docs]class MapReduceChain(Chain):\\n    \"\"\"Map-reduce chain.\"\"\"\\n    combine_documents_chain: BaseCombineDocumentsChain\\n    \"\"\"Chain to use to combine documents.\"\"\"\\n    text_splitter: TextSplitter\\n    \"\"\"Text splitter to use.\"\"\"\\n    input_key: str = \"input_text\"  #: :meta private:\\n    output_key: str = \"output_text\"  #: :meta private:\\n[docs]    @classmethod\\n    def from_params(\\n        cls, llm: BaseLLM, prompt: BasePromptTemplate, text_splitter: TextSplitter\\n    ) -> MapReduceChain:\\n        \"\"\"Construct a map-reduce chain that uses the chain for map and reduce.\"\"\"\\n        llm_chain = LLMChain(llm=llm, prompt=prompt)\\n        reduce_chain = StuffDocumentsChain(llm_chain=llm_chain)\\n        combine_documents_chain = MapReduceDocumentsChain(\\n            llm_chain=llm_chain, combine_document_chain=reduce_chain\\n        )\\n        return cls(\\n            combine_documents_chain=combine_documents_chain, text_splitter=text_splitter\\n        )\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return output key.\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        # Split the larger text into smaller chunks.\\n        texts = self.text_splitter.split_text(inputs[self.input_key])\\n        docs = [Document(page_content=text) for text in texts]\\n        outputs = self.combine_documents_chain.run(input_documents=docs)\\n        return {self.output_key: outputs}\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\mapreduce.html'}),\n",
              " Document(page_content='Source code for langchain.chains.moderation\\n\"\"\"Pass input through a moderation endpoint.\"\"\"\\nfrom typing import Any, Dict, List, Optional\\nfrom pydantic import root_validator\\nfrom langchain.chains.base import Chain\\nfrom langchain.utils import get_from_dict_or_env\\n[docs]class OpenAIModerationChain(Chain):\\n    \"\"\"Pass input through a moderation endpoint.\\n    To use, you should have the ``openai`` python package installed, and the\\n    environment variable ``OPENAI_API_KEY`` set with your API key.\\n    Any parameters that are valid to be passed to the openai.create call can be passed\\n    in, even if not explicitly saved on this class.\\n    Example:\\n        .. code-block:: python\\n            from langchain.chains import OpenAIModerationChain\\n            moderation = OpenAIModerationChain()\\n    \"\"\"\\n    client: Any  #: :meta private:\\n    model_name: Optional[str] = None\\n    \"\"\"Moderation model name to use.\"\"\"\\n    error: bool = False\\n    \"\"\"Whether or not to error if bad content was found.\"\"\"\\n    input_key: str = \"input\"  #: :meta private:\\n    output_key: str = \"output\"  #: :meta private:\\n    openai_api_key: Optional[str] = None\\n    openai_organization: Optional[str] = None\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        openai_api_key = get_from_dict_or_env(\\n            values, \"openai_api_key\", \"OPENAI_API_KEY\"\\n        )\\n        openai_organization = get_from_dict_or_env(\\n            values,\\n            \"openai_organization\",\\n            \"OPENAI_ORGANIZATION\",\\n            default=\"\",\\n        )\\n        try:\\n            import openai\\n            openai.api_key = openai_api_key\\n            if openai_organization:\\n                openai.organization = openai_organization\\n            values[\"client\"] = openai.Moderation\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import openai python package. \"\\n                \"Please install it with `pip install openai`.\"\\n            )\\n        return values\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return output key.\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]\\n    def _moderate(self, text: str, results: dict) -> str:\\n        if results[\"flagged\"]:\\n            error_str = \"Text was found that violates OpenAI\\'s content policy.\"\\n            if self.error:\\n                raise ValueError(error_str)\\n            else:\\n                return error_str\\n        return text\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        text = inputs[self.input_key]\\n        results = self.client.create(text)\\n        output = self._moderate(text, results[\"results\"][0])\\n        return {self.output_key: output}\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\moderation.html'}),\n",
              " Document(page_content='Source code for langchain.chains.sequential\\n\"\"\"Chain pipeline where the outputs of one step feed directly into next.\"\"\"\\nfrom typing import Dict, List\\nfrom pydantic import Extra, root_validator\\nfrom langchain.chains.base import Chain\\nfrom langchain.input import get_color_mapping\\n[docs]class SequentialChain(Chain):\\n    \"\"\"Chain where the outputs of one chain feed directly into next.\"\"\"\\n    chains: List[Chain]\\n    input_variables: List[str]\\n    output_variables: List[str]  #: :meta private:\\n    return_all: bool = False\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return expected input keys to the chain.\\n        :meta private:\\n        \"\"\"\\n        return self.input_variables\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return output key.\\n        :meta private:\\n        \"\"\"\\n        return self.output_variables\\n    @root_validator(pre=True)\\n    def validate_chains(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that the correct inputs exist for all chains.\"\"\"\\n        chains = values[\"chains\"]\\n        input_variables = values[\"input_variables\"]\\n        memory_keys = list()\\n        if \"memory\" in values and values[\"memory\"] is not None:\\n            \"\"\"Validate that prompt input variables are consistent.\"\"\"\\n            memory_keys = values[\"memory\"].memory_variables\\n            if set(input_variables).intersection(set(memory_keys)):\\n                overlapping_keys = set(input_variables) & set(memory_keys)\\n                raise ValueError(\\n                    f\"The the input key(s) {\\'\\'.join(overlapping_keys)} are found \"\\n                    f\"in the Memory keys ({memory_keys}) - please use input and \"\\n                    f\"memory keys that don\\'t overlap.\"\\n                )\\n        known_variables = set(input_variables + memory_keys)\\n        for chain in chains:\\n            missing_vars = set(chain.input_keys).difference(known_variables)\\n            if missing_vars:\\n                raise ValueError(\\n                    f\"Missing required input keys: {missing_vars}, \"\\n                    f\"only had {known_variables}\"\\n                )\\n            overlapping_keys = known_variables.intersection(chain.output_keys)\\n            if overlapping_keys:\\n                raise ValueError(\\n                    f\"Chain returned keys that already exist: {overlapping_keys}\"\\n                )\\n            known_variables |= set(chain.output_keys)\\n        if \"output_variables\" not in values:\\n            if values.get(\"return_all\", False):\\n                output_keys = known_variables.difference(input_variables)\\n            else:\\n                output_keys = chains[-1].output_keys\\n            values[\"output_variables\"] = output_keys\\n        else:\\n            missing_vars = set(values[\"output_variables\"]).difference(known_variables)\\n            if missing_vars:\\n                raise ValueError(\\n                    f\"Expected output variables that were not found: {missing_vars}.\"\\n                )\\n        return values\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        known_values = inputs.copy()\\n        for i, chain in enumerate(self.chains):\\n            outputs = chain(known_values, return_only_outputs=True)\\n            known_values.update(outputs)\\n        return {k: known_values[k] for k in self.output_variables}\\n[docs]class SimpleSequentialChain(Chain):\\n    \"\"\"Simple chain where the outputs of one step feed directly into next.\"\"\"\\n    chains: List[Chain]\\n    strip_outputs: bool = False\\n    input_key: str = \"input\"  #: :meta private:\\n    output_key: str = \"output\"  #: :meta private:\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return output key.\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]\\n    @root_validator()\\n    def validate_chains(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that chains are all single input/output.\"\"\"\\n        for chain in values[\"chains\"]:\\n            if len(chain.input_keys) != 1:\\n                raise ValueError(\\n                    \"Chains used in SimplePipeline should all have one input, got \"\\n                    f\"{chain} with {len(chain.input_keys)} inputs.\"\\n                )\\n            if len(chain.output_keys) != 1:\\n                raise ValueError(\\n                    \"Chains used in SimplePipeline should all have one output, got \"\\n                    f\"{chain} with {len(chain.output_keys)} outputs.\"\\n                )\\n        return values\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        _input = inputs[self.input_key]\\n        color_mapping = get_color_mapping([str(i) for i in range(len(self.chains))])\\n        for i, chain in enumerate(self.chains):\\n            _input = chain.run(_input)\\n            if self.strip_outputs:\\n                _input = _input.strip()\\n            self.callback_manager.on_text(\\n                _input, color=color_mapping[str(i)], end=\"\\\\n\", verbose=self.verbose\\n            )\\n        return {self.output_key: _input}\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\sequential.html'}),\n",
              " Document(page_content='Source code for langchain.chains.transform\\n\"\"\"Chain that runs an arbitrary python function.\"\"\"\\nfrom typing import Callable, Dict, List\\nfrom langchain.chains.base import Chain\\n[docs]class TransformChain(Chain):\\n    \"\"\"Chain transform chain output.\\n    Example:\\n        .. code-block:: python\\n            from langchain import TransformChain\\n            transform_chain = TransformChain(input_variables=[\"text\"],\\n             output_variables[\"entities\"], transform=func())\\n    \"\"\"\\n    input_variables: List[str]\\n    output_variables: List[str]\\n    transform: Callable[[Dict[str, str]], Dict[str, str]]\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input keys.\\n        :meta private:\\n        \"\"\"\\n        return self.input_variables\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return output keys.\\n        :meta private:\\n        \"\"\"\\n        return self.output_variables\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        return self.transform(inputs)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\transform.html'}),\n",
              " Document(page_content='Source code for langchain.chains.api.base\\n\"\"\"Chain that makes API calls and summarizes the responses to answer a question.\"\"\"\\nfrom __future__ import annotations\\nfrom typing import Any, Dict, List, Optional\\nfrom pydantic import Field, root_validator\\nfrom langchain.chains.api.prompt import API_RESPONSE_PROMPT, API_URL_PROMPT\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.prompts import BasePromptTemplate\\nfrom langchain.requests import TextRequestsWrapper\\nfrom langchain.schema import BaseLanguageModel\\n[docs]class APIChain(Chain):\\n    \"\"\"Chain that makes API calls and summarizes the responses to answer a question.\"\"\"\\n    api_request_chain: LLMChain\\n    api_answer_chain: LLMChain\\n    requests_wrapper: TextRequestsWrapper = Field(exclude=True)\\n    api_docs: str\\n    question_key: str = \"question\"  #: :meta private:\\n    output_key: str = \"output\"  #: :meta private:\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n        :meta private:\\n        \"\"\"\\n        return [self.question_key]\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Expect output key.\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]\\n    @root_validator(pre=True)\\n    def validate_api_request_prompt(cls, values: Dict) -> Dict:\\n        \"\"\"Check that api request prompt expects the right variables.\"\"\"\\n        input_vars = values[\"api_request_chain\"].prompt.input_variables\\n        expected_vars = {\"question\", \"api_docs\"}\\n        if set(input_vars) != expected_vars:\\n            raise ValueError(\\n                f\"Input variables should be {expected_vars}, got {input_vars}\"\\n            )\\n        return values\\n    @root_validator(pre=True)\\n    def validate_api_answer_prompt(cls, values: Dict) -> Dict:\\n        \"\"\"Check that api answer prompt expects the right variables.\"\"\"\\n        input_vars = values[\"api_answer_chain\"].prompt.input_variables\\n        expected_vars = {\"question\", \"api_docs\", \"api_url\", \"api_response\"}\\n        if set(input_vars) != expected_vars:\\n            raise ValueError(\\n                f\"Input variables should be {expected_vars}, got {input_vars}\"\\n            )\\n        return values\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        question = inputs[self.question_key]\\n        api_url = self.api_request_chain.predict(\\n            question=question, api_docs=self.api_docs\\n        )\\n        self.callback_manager.on_text(\\n            api_url, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n        api_response = self.requests_wrapper.get(api_url)\\n        self.callback_manager.on_text(\\n            api_response, color=\"yellow\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n        answer = self.api_answer_chain.predict(\\n            question=question,\\n            api_docs=self.api_docs,\\n            api_url=api_url,\\n            api_response=api_response,\\n        )\\n        return {self.output_key: answer}\\n    async def _acall(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        question = inputs[self.question_key]\\n        api_url = await self.api_request_chain.apredict(\\n            question=question, api_docs=self.api_docs\\n        )\\n        self.callback_manager.on_text(\\n            api_url, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n        api_response = await self.requests_wrapper.aget(api_url)\\n        self.callback_manager.on_text(\\n            api_response, color=\"yellow\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n        answer = await self.api_answer_chain.apredict(\\n            question=question,\\n            api_docs=self.api_docs,\\n            api_url=api_url,\\n            api_response=api_response,\\n        )\\n        return {self.output_key: answer}\\n[docs]    @classmethod\\n    def from_llm_and_api_docs(\\n        cls,\\n        llm: BaseLanguageModel,\\n        api_docs: str,\\n        headers: Optional[dict] = None,\\n        api_url_prompt: BasePromptTemplate = API_URL_PROMPT,\\n        api_response_prompt: BasePromptTemplate = API_RESPONSE_PROMPT,\\n        **kwargs: Any,\\n    ) -> APIChain:\\n        \"\"\"Load chain from just an LLM and the api docs.\"\"\"\\n        get_request_chain = LLMChain(llm=llm, prompt=api_url_prompt)\\n        requests_wrapper = TextRequestsWrapper(headers=headers)\\n        get_answer_chain = LLMChain(llm=llm, prompt=api_response_prompt)\\n        return cls(\\n            api_request_chain=get_request_chain,\\n            api_answer_chain=get_answer_chain,\\n            requests_wrapper=requests_wrapper,\\n            api_docs=api_docs,\\n            **kwargs,\\n        )\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"api_chain\"\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\api\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.chains.combine_documents.base\\n\"\"\"Base interface for chains combining documents.\"\"\"\\nfrom abc import ABC, abstractmethod\\nfrom typing import Any, Dict, List, Optional, Tuple\\nfrom pydantic import Field\\nfrom langchain.chains.base import Chain\\nfrom langchain.docstore.document import Document\\nfrom langchain.prompts.base import BasePromptTemplate\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter\\ndef format_document(doc: Document, prompt: BasePromptTemplate) -> str:\\n    \"\"\"Format a document into a string based on a prompt template.\"\"\"\\n    base_info = {\"page_content\": doc.page_content}\\n    base_info.update(doc.metadata)\\n    missing_metadata = set(prompt.input_variables).difference(base_info)\\n    if len(missing_metadata) > 0:\\n        required_metadata = [\\n            iv for iv in prompt.input_variables if iv != \"page_content\"\\n        ]\\n        raise ValueError(\\n            f\"Document prompt requires documents to have metadata variables: \"\\n            f\"{required_metadata}. Received document with missing metadata: \"\\n            f\"{list(missing_metadata)}.\"\\n        )\\n    document_info = {k: base_info[k] for k in prompt.input_variables}\\n    return prompt.format(**document_info)\\nclass BaseCombineDocumentsChain(Chain, ABC):\\n    \"\"\"Base interface for chains combining documents.\"\"\"\\n    input_key: str = \"input_documents\"  #: :meta private:\\n    output_key: str = \"output_text\"  #: :meta private:\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return output key.\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]\\n    def prompt_length(self, docs: List[Document], **kwargs: Any) -> Optional[int]:\\n        \"\"\"Return the prompt length given the documents passed in.\\n        Returns None if the method does not depend on the prompt length.\\n        \"\"\"\\n        return None\\n    @abstractmethod\\n    def combine_docs(self, docs: List[Document], **kwargs: Any) -> Tuple[str, dict]:\\n        \"\"\"Combine documents into a single string.\"\"\"\\n    @abstractmethod\\n    async def acombine_docs(\\n        self, docs: List[Document], **kwargs: Any\\n    ) -> Tuple[str, dict]:\\n        \"\"\"Combine documents into a single string asynchronously.\"\"\"\\n    def _call(self, inputs: Dict[str, Any]) -> Dict[str, str]:\\n        docs = inputs[self.input_key]\\n        # Other keys are assumed to be needed for LLM prediction\\n        other_keys = {k: v for k, v in inputs.items() if k != self.input_key}\\n        output, extra_return_dict = self.combine_docs(docs, **other_keys)\\n        extra_return_dict[self.output_key] = output\\n        return extra_return_dict\\n    async def _acall(self, inputs: Dict[str, Any]) -> Dict[str, str]:\\n        docs = inputs[self.input_key]\\n        # Other keys are assumed to be needed for LLM prediction\\n        other_keys = {k: v for k, v in inputs.items() if k != self.input_key}\\n        output, extra_return_dict = await self.acombine_docs(docs, **other_keys)\\n        extra_return_dict[self.output_key] = output\\n        return extra_return_dict\\n[docs]class AnalyzeDocumentChain(Chain):\\n    \"\"\"Chain that splits documents, then analyzes it in pieces.\"\"\"\\n    input_key: str = \"input_document\"  #: :meta private:\\n    text_splitter: TextSplitter = Field(default_factory=RecursiveCharacterTextSplitter)\\n    combine_docs_chain: BaseCombineDocumentsChain\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return output key.\\n        :meta private:\\n        \"\"\"\\n        return self.combine_docs_chain.output_keys\\n    def _call(self, inputs: Dict[str, Any]) -> Dict[str, str]:\\n        document = inputs[self.input_key]\\n        docs = self.text_splitter.create_documents([document])\\n        # Other keys are assumed to be needed for LLM prediction\\n        other_keys = {k: v for k, v in inputs.items() if k != self.input_key}\\n        other_keys[self.combine_docs_chain.input_key] = docs\\n        return self.combine_docs_chain(other_keys, return_only_outputs=True)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\combine_documents\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.chains.constitutional_ai.base\\n\"\"\"Chain for applying constitutional principles to the outputs of another chain.\"\"\"\\nfrom typing import Any, Dict, List, Optional\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\\nfrom langchain.chains.constitutional_ai.principles import PRINCIPLES\\nfrom langchain.chains.constitutional_ai.prompts import CRITIQUE_PROMPT, REVISION_PROMPT\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.prompts.base import BasePromptTemplate\\nfrom langchain.schema import BaseLanguageModel\\n[docs]class ConstitutionalChain(Chain):\\n    \"\"\"Chain for applying constitutional principles.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import OpenAI\\n            from langchain.chains import LLMChain, ConstitutionalChain\\n            qa_prompt = PromptTemplate(\\n                template=\"Q: {question} A:\",\\n                input_variables=[\"question\"],\\n            )\\n            qa_chain = LLMChain(llm=OpenAI(), prompt=qa_prompt)\\n            constitutional_chain = ConstitutionalChain.from_llm(\\n                chain=qa_chain,\\n                constitutional_principles=[\\n                    ConstitutionalPrinciple(\\n                        critique_request=\"Tell if this answer is good.\",\\n                        revision_request=\"Give a better answer.\",\\n                    )\\n                ],\\n            )\\n            constitutional_chain.run(question=\"What is the meaning of life?\")\\n    \"\"\"\\n    chain: LLMChain\\n    constitutional_principles: List[ConstitutionalPrinciple]\\n    critique_chain: LLMChain\\n    revision_chain: LLMChain\\n[docs]    @classmethod\\n    def get_principles(\\n        cls, names: Optional[List[str]] = None\\n    ) -> List[ConstitutionalPrinciple]:\\n        if names is None:\\n            return list(PRINCIPLES.values())\\n        else:\\n            return [PRINCIPLES[name] for name in names]\\n[docs]    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        chain: LLMChain,\\n        critique_prompt: BasePromptTemplate = CRITIQUE_PROMPT,\\n        revision_prompt: BasePromptTemplate = REVISION_PROMPT,\\n        **kwargs: Any,\\n    ) -> \"ConstitutionalChain\":\\n        \"\"\"Create a chain from an LLM.\"\"\"\\n        critique_chain = LLMChain(llm=llm, prompt=critique_prompt)\\n        revision_chain = LLMChain(llm=llm, prompt=revision_prompt)\\n        return cls(\\n            chain=chain,\\n            critique_chain=critique_chain,\\n            revision_chain=revision_chain,\\n            **kwargs,\\n        )\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Defines the input keys.\"\"\"\\n        return self.chain.input_keys\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Defines the output keys.\"\"\"\\n        return [\"output\"]\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        response = self.chain.run(**inputs)\\n        input_prompt = self.chain.prompt.format(**inputs)\\n        self.callback_manager.on_text(\\n            text=\"Initial response: \" + response + \"\\\\n\\\\n\",\\n            verbose=self.verbose,\\n            color=\"yellow\",\\n        )\\n        for constitutional_principle in self.constitutional_principles:\\n            # Do critique\\n            raw_critique = self.critique_chain.run(\\n                input_prompt=input_prompt,\\n                output_from_model=response,\\n                critique_request=constitutional_principle.critique_request,\\n            )\\n            critique = self._parse_critique(\\n                output_string=raw_critique,\\n            ).strip()\\n            # Do revision\\n            revision = self.revision_chain.run(\\n                input_prompt=input_prompt,\\n                output_from_model=response,\\n                critique_request=constitutional_principle.critique_request,\\n                critique=critique,\\n                revision_request=constitutional_principle.revision_request,\\n            ).strip()\\n            response = revision\\n            self.callback_manager.on_text(\\n                text=f\"Applying {constitutional_principle.name}...\" + \"\\\\n\\\\n\",\\n                verbose=self.verbose,\\n                color=\"green\",\\n            )\\n            self.callback_manager.on_text(\\n                text=\"Critique: \" + critique + \"\\\\n\\\\n\",\\n                verbose=self.verbose,\\n                color=\"blue\",\\n            )\\n            self.callback_manager.on_text(\\n                text=\"Updated response: \" + revision + \"\\\\n\\\\n\",\\n                verbose=self.verbose,\\n                color=\"yellow\",\\n            )\\n        return {\"output\": response}\\n    @staticmethod\\n    def _parse_critique(output_string: str) -> str:\\n        if \"Revision request:\" not in output_string:\\n            return output_string\\n        output_string = output_string.split(\"Revision request:\")[0]\\n        if \"\\\\n\\\\n\" in output_string:\\n            output_string = output_string.split(\"\\\\n\\\\n\")[0]\\n        return output_string\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\constitutional_ai\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.chains.conversation.base\\n\"\"\"Chain that carries on a conversation and calls an LLM.\"\"\"\\nfrom typing import Dict, List\\nfrom pydantic import Extra, Field, root_validator\\nfrom langchain.chains.conversation.prompt import PROMPT\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.memory.buffer import ConversationBufferMemory\\nfrom langchain.prompts.base import BasePromptTemplate\\nfrom langchain.schema import BaseMemory\\n[docs]class ConversationChain(LLMChain):\\n    \"\"\"Chain to have a conversation and load context from memory.\\n    Example:\\n        .. code-block:: python\\n            from langchain import ConversationChain, OpenAI\\n            conversation = ConversationChain(llm=OpenAI())\\n    \"\"\"\\n    memory: BaseMemory = Field(default_factory=ConversationBufferMemory)\\n    \"\"\"Default memory store.\"\"\"\\n    prompt: BasePromptTemplate = PROMPT\\n    \"\"\"Default conversation prompt to use.\"\"\"\\n    input_key: str = \"input\"  #: :meta private:\\n    output_key: str = \"response\"  #: :meta private:\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Use this since so some prompt vars come from history.\"\"\"\\n        return [self.input_key]\\n    @root_validator()\\n    def validate_prompt_input_variables(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that prompt input variables are consistent.\"\"\"\\n        memory_keys = values[\"memory\"].memory_variables\\n        input_key = values[\"input_key\"]\\n        if input_key in memory_keys:\\n            raise ValueError(\\n                f\"The input key {input_key} was also found in the memory keys \"\\n                f\"({memory_keys}) - please provide keys that don\\'t overlap.\"\\n            )\\n        prompt_variables = values[\"prompt\"].input_variables\\n        expected_keys = memory_keys + [input_key]\\n        if set(expected_keys) != set(prompt_variables):\\n            raise ValueError(\\n                \"Got unexpected prompt input variables. The prompt expects \"\\n                f\"{prompt_variables}, but got {memory_keys} as inputs from \"\\n                f\"memory, and {input_key} as the normal input key.\"\\n            )\\n        return values\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\conversation\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.chains.conversational_retrieval.base\\n\"\"\"Chain for chatting with a vector database.\"\"\"\\nfrom __future__ import annotations\\nimport warnings\\nfrom abc import abstractmethod\\nfrom pathlib import Path\\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\\nfrom pydantic import Extra, Field, root_validator\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\\nfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.question_answering import load_qa_chain\\nfrom langchain.prompts.base import BasePromptTemplate\\nfrom langchain.schema import BaseLanguageModel, BaseMessage, BaseRetriever, Document\\nfrom langchain.vectorstores.base import VectorStore\\n# Depending on the memory type and configuration, the chat history format may differ.\\n# This needs to be consolidated.\\nCHAT_TURN_TYPE = Union[Tuple[str, str], BaseMessage]\\n_ROLE_MAP = {\"human\": \"Human: \", \"ai\": \"Assistant: \"}\\ndef _get_chat_history(chat_history: List[CHAT_TURN_TYPE]) -> str:\\n    buffer = \"\"\\n    for dialogue_turn in chat_history:\\n        if isinstance(dialogue_turn, BaseMessage):\\n            role_prefix = _ROLE_MAP.get(dialogue_turn.type, f\"{dialogue_turn.type}: \")\\n            buffer += f\"\\\\n{role_prefix}{dialogue_turn.content}\"\\n        elif isinstance(dialogue_turn, tuple):\\n            human = \"Human: \" + dialogue_turn[0]\\n            ai = \"Assistant: \" + dialogue_turn[1]\\n            buffer += \"\\\\n\" + \"\\\\n\".join([human, ai])\\n        else:\\n            raise ValueError(\\n                f\"Unsupported chat history format: {type(dialogue_turn)}.\"\\n                f\" Full chat history: {chat_history} \"\\n            )\\n    return buffer\\nclass BaseConversationalRetrievalChain(Chain):\\n    \"\"\"Chain for chatting with an index.\"\"\"\\n    combine_docs_chain: BaseCombineDocumentsChain\\n    question_generator: LLMChain\\n    output_key: str = \"answer\"\\n    return_source_documents: bool = False\\n    get_chat_history: Optional[Callable[[CHAT_TURN_TYPE], str]] = None\\n    \"\"\"Return the source documents.\"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n        allow_population_by_field_name = True\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Input keys.\"\"\"\\n        return [\"question\", \"chat_history\"]\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the output keys.\\n        :meta private:\\n        \"\"\"\\n        _output_keys = [self.output_key]\\n        if self.return_source_documents:\\n            _output_keys = _output_keys + [\"source_documents\"]\\n        return _output_keys\\n    @abstractmethod\\n    def _get_docs(self, question: str, inputs: Dict[str, Any]) -> List[Document]:\\n        \"\"\"Get docs.\"\"\"\\n    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        question = inputs[\"question\"]\\n        get_chat_history = self.get_chat_history or _get_chat_history\\n        chat_history_str = get_chat_history(inputs[\"chat_history\"])\\n        if chat_history_str:\\n            new_question = self.question_generator.run(\\n                question=question, chat_history=chat_history_str\\n            )\\n        else:\\n            new_question = question\\n        docs = self._get_docs(new_question, inputs)\\n        new_inputs = inputs.copy()\\n        new_inputs[\"question\"] = new_question\\n        new_inputs[\"chat_history\"] = chat_history_str\\n        answer = self.combine_docs_chain.run(input_documents=docs, **new_inputs)\\n        if self.return_source_documents:\\n            return {self.output_key: answer, \"source_documents\": docs}\\n        else:\\n            return {self.output_key: answer}\\n    @abstractmethod\\n    async def _aget_docs(self, question: str, inputs: Dict[str, Any]) -> List[Document]:\\n        \"\"\"Get docs.\"\"\"\\n    async def _acall(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        question = inputs[\"question\"]\\n        get_chat_history = self.get_chat_history or _get_chat_history\\n        chat_history_str = get_chat_history(inputs[\"chat_history\"])\\n        if chat_history_str:\\n            new_question = await self.question_generator.arun(\\n                question=question, chat_history=chat_history_str\\n            )\\n        else:\\n            new_question = question\\n        docs = await self._aget_docs(new_question, inputs)\\n        new_inputs = inputs.copy()\\n        new_inputs[\"question\"] = new_question\\n        new_inputs[\"chat_history\"] = chat_history_str\\n        answer = await self.combine_docs_chain.arun(input_documents=docs, **new_inputs)\\n        if self.return_source_documents:\\n            return {self.output_key: answer, \"source_documents\": docs}\\n        else:\\n            return {self.output_key: answer}\\n    def save(self, file_path: Union[Path, str]) -> None:\\n        if self.get_chat_history:\\n            raise ValueError(\"Chain not savable when `get_chat_history` is not None.\")\\n        super().save(file_path)\\n[docs]class ConversationalRetrievalChain(BaseConversationalRetrievalChain):\\n    \"\"\"Chain for chatting with an index.\"\"\"\\n    retriever: BaseRetriever\\n    \"\"\"Index to connect to.\"\"\"\\n    max_tokens_limit: Optional[int] = None\\n    \"\"\"If set, restricts the docs to return from store based on tokens, enforced only\\n    for StuffDocumentChain\"\"\"\\n    def _reduce_tokens_below_limit(self, docs: List[Document]) -> List[Document]:\\n        num_docs = len(docs)\\n        if self.max_tokens_limit and isinstance(\\n            self.combine_docs_chain, StuffDocumentsChain\\n        ):\\n            tokens = [\\n                self.combine_docs_chain.llm_chain.llm.get_num_tokens(doc.page_content)\\n                for doc in docs\\n            ]\\n            token_count = sum(tokens[:num_docs])\\n            while token_count > self.max_tokens_limit:\\n                num_docs -= 1\\n                token_count -= tokens[num_docs]\\n        return docs[:num_docs]\\n    def _get_docs(self, question: str, inputs: Dict[str, Any]) -> List[Document]:\\n        docs = self.retriever.get_relevant_documents(question)\\n        return self._reduce_tokens_below_limit(docs)\\n    async def _aget_docs(self, question: str, inputs: Dict[str, Any]) -> List[Document]:\\n        docs = await self.retriever.aget_relevant_documents(question)\\n        return self._reduce_tokens_below_limit(docs)\\n[docs]    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        retriever: BaseRetriever,\\n        condense_question_prompt: BasePromptTemplate = CONDENSE_QUESTION_PROMPT,\\n        qa_prompt: Optional[BasePromptTemplate] = None,\\n        chain_type: str = \"stuff\",\\n        **kwargs: Any,\\n    ) -> BaseConversationalRetrievalChain:\\n        \"\"\"Load chain from LLM.\"\"\"\\n        doc_chain = load_qa_chain(\\n            llm,\\n            chain_type=chain_type,\\n            prompt=qa_prompt,\\n        )\\n        condense_question_chain = LLMChain(llm=llm, prompt=condense_question_prompt)\\n        return cls(\\n            retriever=retriever,\\n            combine_docs_chain=doc_chain,\\n            question_generator=condense_question_chain,\\n            **kwargs,\\n        )\\n[docs]class ChatVectorDBChain(BaseConversationalRetrievalChain):\\n    \"\"\"Chain for chatting with a vector database.\"\"\"\\n    vectorstore: VectorStore = Field(alias=\"vectorstore\")\\n    top_k_docs_for_context: int = 4\\n    search_kwargs: dict = Field(default_factory=dict)\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"chat-vector-db\"\\n    @root_validator()\\n    def raise_deprecation(cls, values: Dict) -> Dict:\\n        warnings.warn(\\n            \"`ChatVectorDBChain` is deprecated - \"\\n            \"please use `from langchain.chains import ConversationalRetrievalChain`\"\\n        )\\n        return values\\n    def _get_docs(self, question: str, inputs: Dict[str, Any]) -> List[Document]:\\n        vectordbkwargs = inputs.get(\"vectordbkwargs\", {})\\n        full_kwargs = {**self.search_kwargs, **vectordbkwargs}\\n        return self.vectorstore.similarity_search(\\n            question, k=self.top_k_docs_for_context, **full_kwargs\\n        )\\n    async def _aget_docs(self, question: str, inputs: Dict[str, Any]) -> List[Document]:\\n        raise NotImplementedError(\"ChatVectorDBChain does not support async\")\\n[docs]    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        vectorstore: VectorStore,\\n        condense_question_prompt: BasePromptTemplate = CONDENSE_QUESTION_PROMPT,\\n        qa_prompt: Optional[BasePromptTemplate] = None,\\n        chain_type: str = \"stuff\",\\n        **kwargs: Any,\\n    ) -> BaseConversationalRetrievalChain:\\n        \"\"\"Load chain from LLM.\"\"\"\\n        doc_chain = load_qa_chain(\\n            llm,\\n            chain_type=chain_type,\\n            prompt=qa_prompt,\\n        )\\n        condense_question_chain = LLMChain(llm=llm, prompt=condense_question_prompt)\\n        return cls(\\n            vectorstore=vectorstore,\\n            combine_docs_chain=doc_chain,\\n            question_generator=condense_question_chain,\\n            **kwargs,\\n        )\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\conversational_retrieval\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.chains.graph_qa.base\\n\"\"\"Question answering over a graph.\"\"\"\\nfrom __future__ import annotations\\nfrom typing import Any, Dict, List\\nfrom pydantic import Field\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.graph_qa.prompts import ENTITY_EXTRACTION_PROMPT, PROMPT\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.graphs.networkx_graph import NetworkxEntityGraph, get_entities\\nfrom langchain.llms.base import BaseLLM\\nfrom langchain.prompts.base import BasePromptTemplate\\n[docs]class GraphQAChain(Chain):\\n    \"\"\"Chain for question-answering against a graph.\"\"\"\\n    graph: NetworkxEntityGraph = Field(exclude=True)\\n    entity_extraction_chain: LLMChain\\n    qa_chain: LLMChain\\n    input_key: str = \"query\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys.\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the output keys.\\n        :meta private:\\n        \"\"\"\\n        _output_keys = [self.output_key]\\n        return _output_keys\\n[docs]    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLLM,\\n        qa_prompt: BasePromptTemplate = PROMPT,\\n        entity_prompt: BasePromptTemplate = ENTITY_EXTRACTION_PROMPT,\\n        **kwargs: Any,\\n    ) -> GraphQAChain:\\n        \"\"\"Initialize from LLM.\"\"\"\\n        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\\n        entity_chain = LLMChain(llm=llm, prompt=entity_prompt)\\n        return cls(qa_chain=qa_chain, entity_extraction_chain=entity_chain, **kwargs)\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, Any]:\\n        \"\"\"Extract entities, look up info and answer question.\"\"\"\\n        question = inputs[self.input_key]\\n        entity_string = self.entity_extraction_chain.run(question)\\n        self.callback_manager.on_text(\\n            \"Entities Extracted:\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n        self.callback_manager.on_text(\\n            entity_string, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n        entities = get_entities(entity_string)\\n        context = \"\"\\n        for entity in entities:\\n            triplets = self.graph.get_entity_knowledge(entity)\\n            context += \"\\\\n\".join(triplets)\\n        self.callback_manager.on_text(\"Full Context:\", end=\"\\\\n\", verbose=self.verbose)\\n        self.callback_manager.on_text(\\n            context, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n        result = self.qa_chain({\"question\": question, \"context\": context})\\n        return {self.output_key: result[self.qa_chain.output_key]}\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\graph_qa\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.chains.hyde.base\\n\"\"\"Hypothetical Document Embeddings.\\nhttps://arxiv.org/abs/2212.10496\\n\"\"\"\\nfrom __future__ import annotations\\nfrom typing import Dict, List\\nimport numpy as np\\nfrom pydantic import Extra\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.hyde.prompts import PROMPT_MAP\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.llms.base import BaseLLM\\n[docs]class HypotheticalDocumentEmbedder(Chain, Embeddings):\\n    \"\"\"Generate hypothetical document for query, and then embed that.\\n    Based on https://arxiv.org/abs/2212.10496\\n    \"\"\"\\n    base_embeddings: Embeddings\\n    llm_chain: LLMChain\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Input keys for Hyde\\'s LLM chain.\"\"\"\\n        return self.llm_chain.input_keys\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Output keys for Hyde\\'s LLM chain.\"\"\"\\n        return self.llm_chain.output_keys\\n[docs]    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        \"\"\"Call the base embeddings.\"\"\"\\n        return self.base_embeddings.embed_documents(texts)\\n[docs]    def combine_embeddings(self, embeddings: List[List[float]]) -> List[float]:\\n        \"\"\"Combine embeddings into final embeddings.\"\"\"\\n        return list(np.array(embeddings).mean(axis=0))\\n[docs]    def embed_query(self, text: str) -> List[float]:\\n        \"\"\"Generate a hypothetical document and embedded it.\"\"\"\\n        var_name = self.llm_chain.input_keys[0]\\n        result = self.llm_chain.generate([{var_name: text}])\\n        documents = [generation.text for generation in result.generations[0]]\\n        embeddings = self.embed_documents(documents)\\n        return self.combine_embeddings(embeddings)\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        \"\"\"Call the internal llm chain.\"\"\"\\n        return self.llm_chain._call(inputs)\\n[docs]    @classmethod\\n    def from_llm(\\n        cls, llm: BaseLLM, base_embeddings: Embeddings, prompt_key: str\\n    ) -> HypotheticalDocumentEmbedder:\\n        \"\"\"Load and use LLMChain for a specific prompt key.\"\"\"\\n        prompt = PROMPT_MAP[prompt_key]\\n        llm_chain = LLMChain(llm=llm, prompt=prompt)\\n        return cls(base_embeddings=base_embeddings, llm_chain=llm_chain)\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"hyde_chain\"\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\hyde\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.chains.llm_bash.base\\n\"\"\"Chain that interprets a prompt and executes bash code to perform bash operations.\"\"\"\\nfrom typing import Dict, List\\nfrom pydantic import Extra\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.llm_bash.prompt import PROMPT\\nfrom langchain.prompts.base import BasePromptTemplate\\nfrom langchain.schema import BaseLanguageModel\\nfrom langchain.utilities.bash import BashProcess\\n[docs]class LLMBashChain(Chain):\\n    \"\"\"Chain that interprets a prompt and executes bash code to perform bash operations.\\n    Example:\\n        .. code-block:: python\\n            from langchain import LLMBashChain, OpenAI\\n            llm_bash = LLMBashChain(llm=OpenAI())\\n    \"\"\"\\n    llm: BaseLanguageModel\\n    \"\"\"LLM wrapper to use.\"\"\"\\n    input_key: str = \"question\"  #: :meta private:\\n    output_key: str = \"answer\"  #: :meta private:\\n    prompt: BasePromptTemplate = PROMPT\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Expect output key.\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        llm_executor = LLMChain(prompt=self.prompt, llm=self.llm)\\n        bash_executor = BashProcess()\\n        self.callback_manager.on_text(inputs[self.input_key], verbose=self.verbose)\\n        t = llm_executor.predict(question=inputs[self.input_key])\\n        self.callback_manager.on_text(t, color=\"green\", verbose=self.verbose)\\n        t = t.strip()\\n        if t.startswith(\"```bash\"):\\n            # Split the string into a list of substrings\\n            command_list = t.split(\"\\\\n\")\\n            print(command_list)\\n            # Remove the first and last substrings\\n            command_list = [s for s in command_list[1:-1]]\\n            output = bash_executor.run(command_list)\\n            self.callback_manager.on_text(\"\\\\nAnswer: \", verbose=self.verbose)\\n            self.callback_manager.on_text(output, color=\"yellow\", verbose=self.verbose)\\n        else:\\n            raise ValueError(f\"unknown format from LLM: {t}\")\\n        return {self.output_key: output}\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"llm_bash_chain\"\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\llm_bash\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.chains.llm_checker.base\\n\"\"\"Chain for question-answering with self-verification.\"\"\"\\nfrom typing import Dict, List\\nfrom pydantic import Extra\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.llm_checker.prompt import (\\n    CHECK_ASSERTIONS_PROMPT,\\n    CREATE_DRAFT_ANSWER_PROMPT,\\n    LIST_ASSERTIONS_PROMPT,\\n    REVISED_ANSWER_PROMPT,\\n)\\nfrom langchain.chains.sequential import SequentialChain\\nfrom langchain.llms.base import BaseLLM\\nfrom langchain.prompts import PromptTemplate\\n[docs]class LLMCheckerChain(Chain):\\n    \"\"\"Chain for question-answering with self-verification.\\n    Example:\\n        .. code-block:: python\\n            from langchain import OpenAI, LLMCheckerChain\\n            llm = OpenAI(temperature=0.7)\\n            checker_chain = LLMCheckerChain(llm=llm)\\n    \"\"\"\\n    llm: BaseLLM\\n    \"\"\"LLM wrapper to use.\"\"\"\\n    create_draft_answer_prompt: PromptTemplate = CREATE_DRAFT_ANSWER_PROMPT\\n    list_assertions_prompt: PromptTemplate = LIST_ASSERTIONS_PROMPT\\n    check_assertions_prompt: PromptTemplate = CHECK_ASSERTIONS_PROMPT\\n    revised_answer_prompt: PromptTemplate = REVISED_ANSWER_PROMPT\\n    \"\"\"Prompt to use when questioning the documents.\"\"\"\\n    input_key: str = \"query\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the singular input key.\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the singular output key.\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        question = inputs[self.input_key]\\n        create_draft_answer_chain = LLMChain(\\n            llm=self.llm, prompt=self.create_draft_answer_prompt, output_key=\"statement\"\\n        )\\n        list_assertions_chain = LLMChain(\\n            llm=self.llm, prompt=self.list_assertions_prompt, output_key=\"assertions\"\\n        )\\n        check_assertions_chain = LLMChain(\\n            llm=self.llm,\\n            prompt=self.check_assertions_prompt,\\n            output_key=\"checked_assertions\",\\n        )\\n        revised_answer_chain = LLMChain(\\n            llm=self.llm,\\n            prompt=self.revised_answer_prompt,\\n            output_key=\"revised_statement\",\\n        )\\n        chains = [\\n            create_draft_answer_chain,\\n            list_assertions_chain,\\n            check_assertions_chain,\\n            revised_answer_chain,\\n        ]\\n        question_to_checked_assertions_chain = SequentialChain(\\n            chains=chains,\\n            input_variables=[\"question\"],\\n            output_variables=[\"revised_statement\"],\\n            verbose=True,\\n        )\\n        output = question_to_checked_assertions_chain({\"question\": question})\\n        return {self.output_key: output[\"revised_statement\"]}\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"llm_checker_chain\"\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\llm_checker\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.chains.llm_math.base\\n\"\"\"Chain that interprets a prompt and executes python code to do math.\"\"\"\\nimport math\\nimport re\\nfrom typing import Dict, List\\nimport numexpr\\nfrom pydantic import Extra\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.llm_math.prompt import PROMPT\\nfrom langchain.prompts.base import BasePromptTemplate\\nfrom langchain.schema import BaseLanguageModel\\n[docs]class LLMMathChain(Chain):\\n    \"\"\"Chain that interprets a prompt and executes python code to do math.\\n    Example:\\n        .. code-block:: python\\n            from langchain import LLMMathChain, OpenAI\\n            llm_math = LLMMathChain(llm=OpenAI())\\n    \"\"\"\\n    llm: BaseLanguageModel\\n    \"\"\"LLM wrapper to use.\"\"\"\\n    prompt: BasePromptTemplate = PROMPT\\n    \"\"\"Prompt to use to translate to python if neccessary.\"\"\"\\n    input_key: str = \"question\"  #: :meta private:\\n    output_key: str = \"answer\"  #: :meta private:\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Expect output key.\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]\\n    def _evaluate_expression(self, expression: str) -> str:\\n        try:\\n            local_dict = {\"pi\": math.pi, \"e\": math.e}\\n            output = str(\\n                numexpr.evaluate(\\n                    expression.strip(),\\n                    global_dict={},  # restrict access to globals\\n                    local_dict=local_dict,  # add common mathematical functions\\n                )\\n            )\\n        except Exception as e:\\n            raise ValueError(f\"{e}. Please try again with a valid numerical expression\")\\n        # Remove any leading and trailing brackets from the output\\n        return re.sub(r\"^\\\\[|\\\\]$\", \"\", output)\\n    def _process_llm_result(self, llm_output: str) -> Dict[str, str]:\\n        self.callback_manager.on_text(llm_output, color=\"green\", verbose=self.verbose)\\n        llm_output = llm_output.strip()\\n        text_match = re.search(r\"^```text(.*?)```\", llm_output, re.DOTALL)\\n        if text_match:\\n            expression = text_match.group(1)\\n            output = self._evaluate_expression(expression)\\n            self.callback_manager.on_text(\"\\\\nAnswer: \", verbose=self.verbose)\\n            self.callback_manager.on_text(output, color=\"yellow\", verbose=self.verbose)\\n            answer = \"Answer: \" + output\\n        elif llm_output.startswith(\"Answer:\"):\\n            answer = llm_output\\n        elif \"Answer:\" in llm_output:\\n            answer = \"Answer: \" + llm_output.split(\"Answer:\")[-1]\\n        else:\\n            raise ValueError(f\"unknown format from LLM: {llm_output}\")\\n        return {self.output_key: answer}\\n    async def _aprocess_llm_result(self, llm_output: str) -> Dict[str, str]:\\n        if self.callback_manager.is_async:\\n            await self.callback_manager.on_text(\\n                llm_output, color=\"green\", verbose=self.verbose\\n            )\\n        else:\\n            self.callback_manager.on_text(\\n                llm_output, color=\"green\", verbose=self.verbose\\n            )\\n        llm_output = llm_output.strip()\\n        text_match = re.search(r\"^```text(.*?)```\", llm_output, re.DOTALL)\\n        if text_match:\\n            expression = text_match.group(1)\\n            output = self._evaluate_expression(expression)\\n            if self.callback_manager.is_async:\\n                await self.callback_manager.on_text(\"\\\\nAnswer: \", verbose=self.verbose)\\n                await self.callback_manager.on_text(\\n                    output, color=\"yellow\", verbose=self.verbose\\n                )\\n            else:\\n                await self.callback_manager.on_text(\"\\\\nAnswer: \", verbose=self.verbose)\\n                await self.callback_manager.on_text(\\n                    output, color=\"yellow\", verbose=self.verbose\\n                )\\n            answer = \"Answer: \" + output\\n        elif llm_output.startswith(\"Answer:\"):\\n            answer = llm_output\\n        elif \"Answer:\" in llm_output:\\n            answer = \"Answer: \" + llm_output.split(\"Answer:\")[-1]\\n        else:\\n            raise ValueError(f\"unknown format from LLM: {llm_output}\")\\n        return {self.output_key: answer}\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        llm_executor = LLMChain(\\n            prompt=self.prompt, llm=self.llm, callback_manager=self.callback_manager\\n        )\\n        self.callback_manager.on_text(inputs[self.input_key], verbose=self.verbose)\\n        llm_output = llm_executor.predict(\\n            question=inputs[self.input_key], stop=[\"```output\"]\\n        )\\n        return self._process_llm_result(llm_output)\\n    async def _acall(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        llm_executor = LLMChain(\\n            prompt=self.prompt, llm=self.llm, callback_manager=self.callback_manager\\n        )\\n        if self.callback_manager.is_async:\\n            await self.callback_manager.on_text(\\n                inputs[self.input_key], verbose=self.verbose\\n            )\\n        else:\\n            self.callback_manager.on_text(inputs[self.input_key], verbose=self.verbose)\\n        llm_output = await llm_executor.apredict(\\n            question=inputs[self.input_key], stop=[\"```output\"]\\n        )\\n        return await self._aprocess_llm_result(llm_output)\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"llm_math_chain\"\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\llm_math\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.chains.llm_summarization_checker.base\\n\"\"\"Chain for summarization with self-verification.\"\"\"\\nfrom pathlib import Path\\nfrom typing import Dict, List\\nfrom pydantic import Extra\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.sequential import SequentialChain\\nfrom langchain.llms.base import BaseLLM\\nfrom langchain.prompts.prompt import PromptTemplate\\nPROMPTS_DIR = Path(__file__).parent / \"prompts\"\\nCREATE_ASSERTIONS_PROMPT = PromptTemplate.from_file(\\n    PROMPTS_DIR / \"create_facts.txt\", [\"summary\"]\\n)\\nCHECK_ASSERTIONS_PROMPT = PromptTemplate.from_file(\\n    PROMPTS_DIR / \"check_facts.txt\", [\"assertions\"]\\n)\\nREVISED_SUMMARY_PROMPT = PromptTemplate.from_file(\\n    PROMPTS_DIR / \"revise_summary.txt\", [\"checked_assertions\", \"summary\"]\\n)\\nARE_ALL_TRUE_PROMPT = PromptTemplate.from_file(\\n    PROMPTS_DIR / \"are_all_true_prompt.txt\", [\"checked_assertions\"]\\n)\\n[docs]class LLMSummarizationCheckerChain(Chain):\\n    \"\"\"Chain for question-answering with self-verification.\\n    Example:\\n        .. code-block:: python\\n            from langchain import OpenAI, LLMSummarizationCheckerChain\\n            llm = OpenAI(temperature=0.0)\\n            checker_chain = LLMSummarizationCheckerChain(llm=llm)\\n    \"\"\"\\n    llm: BaseLLM\\n    \"\"\"LLM wrapper to use.\"\"\"\\n    create_assertions_prompt: PromptTemplate = CREATE_ASSERTIONS_PROMPT\\n    check_assertions_prompt: PromptTemplate = CHECK_ASSERTIONS_PROMPT\\n    revised_summary_prompt: PromptTemplate = REVISED_SUMMARY_PROMPT\\n    are_all_true_prompt: PromptTemplate = ARE_ALL_TRUE_PROMPT\\n    input_key: str = \"query\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n    max_checks: int = 2\\n    \"\"\"Maximum number of times to check the assertions. Default to double-checking.\"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the singular input key.\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the singular output key.\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        all_true = False\\n        count = 0\\n        output = None\\n        original_input = inputs[self.input_key]\\n        chain_input = original_input\\n        while not all_true and count < self.max_checks:\\n            chain = SequentialChain(\\n                chains=[\\n                    LLMChain(\\n                        llm=self.llm,\\n                        prompt=self.create_assertions_prompt,\\n                        output_key=\"assertions\",\\n                        verbose=self.verbose,\\n                    ),\\n                    LLMChain(\\n                        llm=self.llm,\\n                        prompt=self.check_assertions_prompt,\\n                        output_key=\"checked_assertions\",\\n                        verbose=self.verbose,\\n                    ),\\n                    LLMChain(\\n                        llm=self.llm,\\n                        prompt=self.revised_summary_prompt,\\n                        output_key=\"revised_summary\",\\n                        verbose=self.verbose,\\n                    ),\\n                    LLMChain(\\n                        llm=self.llm,\\n                        output_key=\"all_true\",\\n                        prompt=self.are_all_true_prompt,\\n                        verbose=self.verbose,\\n                    ),\\n                ],\\n                input_variables=[\"summary\"],\\n                output_variables=[\"all_true\", \"revised_summary\"],\\n                verbose=self.verbose,\\n            )\\n            output = chain({\"summary\": chain_input})\\n            count += 1\\n            if output[\"all_true\"].strip() == \"True\":\\n                break\\n            if self.verbose:\\n                print(output[\"revised_summary\"])\\n            chain_input = output[\"revised_summary\"]\\n        if not output:\\n            raise ValueError(\"No output from chain\")\\n        return {self.output_key: output[\"revised_summary\"].strip()}\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"llm_summarization_checker_chain\"\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\llm_summarization_checker\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.chains.pal.base\\n\"\"\"Implements Program-Aided Language Models.\\nAs in https://arxiv.org/pdf/2211.10435.pdf.\\n\"\"\"\\nfrom __future__ import annotations\\nfrom typing import Any, Dict, List, Optional\\nfrom pydantic import Extra\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.pal.colored_object_prompt import COLORED_OBJECT_PROMPT\\nfrom langchain.chains.pal.math_prompt import MATH_PROMPT\\nfrom langchain.prompts.base import BasePromptTemplate\\nfrom langchain.schema import BaseLanguageModel\\nfrom langchain.utilities import PythonREPL\\n[docs]class PALChain(Chain):\\n    \"\"\"Implements Program-Aided Language Models.\"\"\"\\n    llm: BaseLanguageModel\\n    prompt: BasePromptTemplate\\n    stop: str = \"\\\\n\\\\n\"\\n    get_answer_expr: str = \"print(solution())\"\\n    python_globals: Optional[Dict[str, Any]] = None\\n    python_locals: Optional[Dict[str, Any]] = None\\n    output_key: str = \"result\"  #: :meta private:\\n    return_intermediate_steps: bool = False\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the singular input key.\\n        :meta private:\\n        \"\"\"\\n        return self.prompt.input_variables\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the singular output key.\\n        :meta private:\\n        \"\"\"\\n        if not self.return_intermediate_steps:\\n            return [self.output_key]\\n        else:\\n            return [self.output_key, \"intermediate_steps\"]\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        llm_chain = LLMChain(llm=self.llm, prompt=self.prompt)\\n        code = llm_chain.predict(stop=[self.stop], **inputs)\\n        self.callback_manager.on_text(\\n            code, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n        repl = PythonREPL(_globals=self.python_globals, _locals=self.python_locals)\\n        res = repl.run(code + f\"\\\\n{self.get_answer_expr}\")\\n        output = {self.output_key: res.strip()}\\n        if self.return_intermediate_steps:\\n            output[\"intermediate_steps\"] = code\\n        return output\\n[docs]    @classmethod\\n    def from_math_prompt(cls, llm: BaseLanguageModel, **kwargs: Any) -> PALChain:\\n        \"\"\"Load PAL from math prompt.\"\"\"\\n        return cls(\\n            llm=llm,\\n            prompt=MATH_PROMPT,\\n            stop=\"\\\\n\\\\n\",\\n            get_answer_expr=\"print(solution())\",\\n            **kwargs,\\n        )\\n[docs]    @classmethod\\n    def from_colored_object_prompt(\\n        cls, llm: BaseLanguageModel, **kwargs: Any\\n    ) -> PALChain:\\n        \"\"\"Load PAL from colored object prompt.\"\"\"\\n        return cls(\\n            llm=llm,\\n            prompt=COLORED_OBJECT_PROMPT,\\n            stop=\"\\\\n\\\\n\\\\n\",\\n            get_answer_expr=\"print(answer)\",\\n            **kwargs,\\n        )\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"pal_chain\"\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\pal\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.chains.qa_generation.base\\nfrom __future__ import annotations\\nimport json\\nfrom typing import Any, Dict, List, Optional\\nfrom pydantic import Field\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.qa_generation.prompt import PROMPT_SELECTOR\\nfrom langchain.prompts.base import BasePromptTemplate\\nfrom langchain.schema import BaseLanguageModel\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter\\n[docs]class QAGenerationChain(Chain):\\n    llm_chain: LLMChain\\n    text_splitter: TextSplitter = Field(\\n        default=RecursiveCharacterTextSplitter(chunk_overlap=500)\\n    )\\n    input_key: str = \"text\"\\n    output_key: str = \"questions\"\\n    k: Optional[int] = None\\n[docs]    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: Optional[BasePromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> QAGenerationChain:\\n        _prompt = prompt or PROMPT_SELECTOR.get_prompt(llm)\\n        chain = LLMChain(llm=llm, prompt=_prompt)\\n        return cls(llm_chain=chain, **kwargs)\\n    @property\\n    def _chain_type(self) -> str:\\n        raise NotImplementedError\\n    @property\\n    def input_keys(self) -> List[str]:\\n        return [self.input_key]\\n    @property\\n    def output_keys(self) -> List[str]:\\n        return [self.output_key]\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, Any]:\\n        docs = self.text_splitter.create_documents([inputs[self.input_key]])\\n        results = self.llm_chain.generate([{\"text\": d.page_content} for d in docs])\\n        qa = [json.loads(res[0].text) for res in results.generations]\\n        return {self.output_key: qa}\\n    async def _acall(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        raise NotImplementedError\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\qa_generation\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.chains.qa_with_sources.base\\n\"\"\"Question answering with sources over documents.\"\"\"\\nfrom __future__ import annotations\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom typing import Any, Dict, List, Optional\\nfrom pydantic import Extra, root_validator\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\\nfrom langchain.chains.qa_with_sources.map_reduce_prompt import (\\n    COMBINE_PROMPT,\\n    EXAMPLE_PROMPT,\\n    QUESTION_PROMPT,\\n)\\nfrom langchain.docstore.document import Document\\nfrom langchain.prompts.base import BasePromptTemplate\\nfrom langchain.schema import BaseLanguageModel\\nclass BaseQAWithSourcesChain(Chain, ABC):\\n    \"\"\"Question answering with sources over documents.\"\"\"\\n    combine_documents_chain: BaseCombineDocumentsChain\\n    \"\"\"Chain to use to combine documents.\"\"\"\\n    question_key: str = \"question\"  #: :meta private:\\n    input_docs_key: str = \"docs\"  #: :meta private:\\n    answer_key: str = \"answer\"  #: :meta private:\\n    sources_answer_key: str = \"sources\"  #: :meta private:\\n    return_source_documents: bool = False\\n    \"\"\"Return the source documents.\"\"\"\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        document_prompt: BasePromptTemplate = EXAMPLE_PROMPT,\\n        question_prompt: BasePromptTemplate = QUESTION_PROMPT,\\n        combine_prompt: BasePromptTemplate = COMBINE_PROMPT,\\n        **kwargs: Any,\\n    ) -> BaseQAWithSourcesChain:\\n        \"\"\"Construct the chain from an LLM.\"\"\"\\n        llm_question_chain = LLMChain(llm=llm, prompt=question_prompt)\\n        llm_combine_chain = LLMChain(llm=llm, prompt=combine_prompt)\\n        combine_results_chain = StuffDocumentsChain(\\n            llm_chain=llm_combine_chain,\\n            document_prompt=document_prompt,\\n            document_variable_name=\"summaries\",\\n        )\\n        combine_document_chain = MapReduceDocumentsChain(\\n            llm_chain=llm_question_chain,\\n            combine_document_chain=combine_results_chain,\\n            document_variable_name=\"context\",\\n        )\\n        return cls(\\n            combine_documents_chain=combine_document_chain,\\n            **kwargs,\\n        )\\n    @classmethod\\n    def from_chain_type(\\n        cls,\\n        llm: BaseLanguageModel,\\n        chain_type: str = \"stuff\",\\n        chain_type_kwargs: Optional[dict] = None,\\n        **kwargs: Any,\\n    ) -> BaseQAWithSourcesChain:\\n        \"\"\"Load chain from chain type.\"\"\"\\n        _chain_kwargs = chain_type_kwargs or {}\\n        combine_document_chain = load_qa_with_sources_chain(\\n            llm, chain_type=chain_type, **_chain_kwargs\\n        )\\n        return cls(combine_documents_chain=combine_document_chain, **kwargs)\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n        :meta private:\\n        \"\"\"\\n        return [self.question_key]\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return output key.\\n        :meta private:\\n        \"\"\"\\n        _output_keys = [self.answer_key, self.sources_answer_key]\\n        if self.return_source_documents:\\n            _output_keys = _output_keys + [\"source_documents\"]\\n        return _output_keys\\n    @root_validator(pre=True)\\n    def validate_naming(cls, values: Dict) -> Dict:\\n        \"\"\"Fix backwards compatability in naming.\"\"\"\\n        if \"combine_document_chain\" in values:\\n            values[\"combine_documents_chain\"] = values.pop(\"combine_document_chain\")\\n        return values\\n    @abstractmethod\\n    def _get_docs(self, inputs: Dict[str, Any]) -> List[Document]:\\n        \"\"\"Get docs to run questioning over.\"\"\"\\n    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        docs = self._get_docs(inputs)\\n        answer = self.combine_documents_chain.run(input_documents=docs, **inputs)\\n        if re.search(r\"SOURCES:\\\\s\", answer):\\n            answer, sources = re.split(r\"SOURCES:\\\\s\", answer)\\n        else:\\n            sources = \"\"\\n        result: Dict[str, Any] = {\\n            self.answer_key: answer,\\n            self.sources_answer_key: sources,\\n        }\\n        if self.return_source_documents:\\n            result[\"source_documents\"] = docs\\n        return result\\n    @abstractmethod\\n    async def _aget_docs(self, inputs: Dict[str, Any]) -> List[Document]:\\n        \"\"\"Get docs to run questioning over.\"\"\"\\n    async def _acall(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        docs = await self._aget_docs(inputs)\\n        answer = await self.combine_documents_chain.arun(input_documents=docs, **inputs)\\n        if re.search(r\"SOURCES:\\\\s\", answer):\\n            answer, sources = re.split(r\"SOURCES:\\\\s\", answer)\\n        else:\\n            sources = \"\"\\n        result: Dict[str, Any] = {\\n            self.answer_key: answer,\\n            self.sources_answer_key: sources,\\n        }\\n        if self.return_source_documents:\\n            result[\"source_documents\"] = docs\\n        return result\\n[docs]class QAWithSourcesChain(BaseQAWithSourcesChain):\\n    \"\"\"Question answering with sources over documents.\"\"\"\\n    input_docs_key: str = \"docs\"  #: :meta private:\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n        :meta private:\\n        \"\"\"\\n        return [self.input_docs_key, self.question_key]\\n    def _get_docs(self, inputs: Dict[str, Any]) -> List[Document]:\\n        return inputs.pop(self.input_docs_key)\\n    async def _aget_docs(self, inputs: Dict[str, Any]) -> List[Document]:\\n        return inputs.pop(self.input_docs_key)\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"qa_with_sources_chain\"\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\qa_with_sources\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.chains.qa_with_sources.retrieval\\n\"\"\"Question-answering with sources over an index.\"\"\"\\nfrom typing import Any, Dict, List\\nfrom pydantic import Field\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\\nfrom langchain.chains.qa_with_sources.base import BaseQAWithSourcesChain\\nfrom langchain.docstore.document import Document\\nfrom langchain.schema import BaseRetriever\\n[docs]class RetrievalQAWithSourcesChain(BaseQAWithSourcesChain):\\n    \"\"\"Question-answering with sources over an index.\"\"\"\\n    retriever: BaseRetriever = Field(exclude=True)\\n    \"\"\"Index to connect to.\"\"\"\\n    reduce_k_below_max_tokens: bool = False\\n    \"\"\"Reduce the number of results to return from store based on tokens limit\"\"\"\\n    max_tokens_limit: int = 3375\\n    \"\"\"Restrict the docs to return from store based on tokens,\\n    enforced only for StuffDocumentChain and if reduce_k_below_max_tokens is to true\"\"\"\\n    def _reduce_tokens_below_limit(self, docs: List[Document]) -> List[Document]:\\n        num_docs = len(docs)\\n        if self.reduce_k_below_max_tokens and isinstance(\\n            self.combine_documents_chain, StuffDocumentsChain\\n        ):\\n            tokens = [\\n                self.combine_documents_chain.llm_chain.llm.get_num_tokens(\\n                    doc.page_content\\n                )\\n                for doc in docs\\n            ]\\n            token_count = sum(tokens[:num_docs])\\n            while token_count > self.max_tokens_limit:\\n                num_docs -= 1\\n                token_count -= tokens[num_docs]\\n        return docs[:num_docs]\\n    def _get_docs(self, inputs: Dict[str, Any]) -> List[Document]:\\n        question = inputs[self.question_key]\\n        docs = self.retriever.get_relevant_documents(question)\\n        return self._reduce_tokens_below_limit(docs)\\n    async def _aget_docs(self, inputs: Dict[str, Any]) -> List[Document]:\\n        question = inputs[self.question_key]\\n        docs = await self.retriever.aget_relevant_documents(question)\\n        return self._reduce_tokens_below_limit(docs)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\qa_with_sources\\\\retrieval.html'}),\n",
              " Document(page_content='Source code for langchain.chains.qa_with_sources.vector_db\\n\"\"\"Question-answering with sources over a vector database.\"\"\"\\nimport warnings\\nfrom typing import Any, Dict, List\\nfrom pydantic import Field, root_validator\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\\nfrom langchain.chains.qa_with_sources.base import BaseQAWithSourcesChain\\nfrom langchain.docstore.document import Document\\nfrom langchain.vectorstores.base import VectorStore\\n[docs]class VectorDBQAWithSourcesChain(BaseQAWithSourcesChain):\\n    \"\"\"Question-answering with sources over a vector database.\"\"\"\\n    vectorstore: VectorStore = Field(exclude=True)\\n    \"\"\"Vector Database to connect to.\"\"\"\\n    k: int = 4\\n    \"\"\"Number of results to return from store\"\"\"\\n    reduce_k_below_max_tokens: bool = False\\n    \"\"\"Reduce the number of results to return from store based on tokens limit\"\"\"\\n    max_tokens_limit: int = 3375\\n    \"\"\"Restrict the docs to return from store based on tokens,\\n    enforced only for StuffDocumentChain and if reduce_k_below_max_tokens is to true\"\"\"\\n    search_kwargs: Dict[str, Any] = Field(default_factory=dict)\\n    \"\"\"Extra search args.\"\"\"\\n    def _reduce_tokens_below_limit(self, docs: List[Document]) -> List[Document]:\\n        num_docs = len(docs)\\n        if self.reduce_k_below_max_tokens and isinstance(\\n            self.combine_documents_chain, StuffDocumentsChain\\n        ):\\n            tokens = [\\n                self.combine_documents_chain.llm_chain.llm.get_num_tokens(\\n                    doc.page_content\\n                )\\n                for doc in docs\\n            ]\\n            token_count = sum(tokens[:num_docs])\\n            while token_count > self.max_tokens_limit:\\n                num_docs -= 1\\n                token_count -= tokens[num_docs]\\n        return docs[:num_docs]\\n    def _get_docs(self, inputs: Dict[str, Any]) -> List[Document]:\\n        question = inputs[self.question_key]\\n        docs = self.vectorstore.similarity_search(\\n            question, k=self.k, **self.search_kwargs\\n        )\\n        return self._reduce_tokens_below_limit(docs)\\n    async def _aget_docs(self, inputs: Dict[str, Any]) -> List[Document]:\\n        raise NotImplementedError(\"VectorDBQAWithSourcesChain does not support async\")\\n    @root_validator()\\n    def raise_deprecation(cls, values: Dict) -> Dict:\\n        warnings.warn(\\n            \"`VectorDBQAWithSourcesChain` is deprecated - \"\\n            \"please use `from langchain.chains import RetrievalQAWithSourcesChain`\"\\n        )\\n        return values\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"vector_db_qa_with_sources_chain\"\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\qa_with_sources\\\\vector_db.html'}),\n",
              " Document(page_content='Source code for langchain.chains.retrieval_qa.base\\n\"\"\"Chain for question-answering against a vector database.\"\"\"\\nfrom __future__ import annotations\\nimport warnings\\nfrom abc import abstractmethod\\nfrom typing import Any, Dict, List, Optional\\nfrom pydantic import Extra, Field, root_validator\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.question_answering import load_qa_chain\\nfrom langchain.chains.question_answering.stuff_prompt import PROMPT_SELECTOR\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.schema import BaseLanguageModel, BaseRetriever, Document\\nfrom langchain.vectorstores.base import VectorStore\\nclass BaseRetrievalQA(Chain):\\n    combine_documents_chain: BaseCombineDocumentsChain\\n    \"\"\"Chain to use to combine the documents.\"\"\"\\n    input_key: str = \"query\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n    return_source_documents: bool = False\\n    \"\"\"Return the source documents.\"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n        allow_population_by_field_name = True\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys.\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the output keys.\\n        :meta private:\\n        \"\"\"\\n        _output_keys = [self.output_key]\\n        if self.return_source_documents:\\n            _output_keys = _output_keys + [\"source_documents\"]\\n        return _output_keys\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: Optional[PromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> BaseRetrievalQA:\\n        \"\"\"Initialize from LLM.\"\"\"\\n        _prompt = prompt or PROMPT_SELECTOR.get_prompt(llm)\\n        llm_chain = LLMChain(llm=llm, prompt=_prompt)\\n        document_prompt = PromptTemplate(\\n            input_variables=[\"page_content\"], template=\"Context:\\\\n{page_content}\"\\n        )\\n        combine_documents_chain = StuffDocumentsChain(\\n            llm_chain=llm_chain,\\n            document_variable_name=\"context\",\\n            document_prompt=document_prompt,\\n        )\\n        return cls(combine_documents_chain=combine_documents_chain, **kwargs)\\n    @classmethod\\n    def from_chain_type(\\n        cls,\\n        llm: BaseLanguageModel,\\n        chain_type: str = \"stuff\",\\n        chain_type_kwargs: Optional[dict] = None,\\n        **kwargs: Any,\\n    ) -> BaseRetrievalQA:\\n        \"\"\"Load chain from chain type.\"\"\"\\n        _chain_type_kwargs = chain_type_kwargs or {}\\n        combine_documents_chain = load_qa_chain(\\n            llm, chain_type=chain_type, **_chain_type_kwargs\\n        )\\n        return cls(combine_documents_chain=combine_documents_chain, **kwargs)\\n    @abstractmethod\\n    def _get_docs(self, question: str) -> List[Document]:\\n        \"\"\"Get documents to do question answering over.\"\"\"\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, Any]:\\n        \"\"\"Run get_relevant_text and llm on input query.\\n        If chain has \\'return_source_documents\\' as \\'True\\', returns\\n        the retrieved documents as well under the key \\'source_documents\\'.\\n        Example:\\n        .. code-block:: python\\n        res = indexqa({\\'query\\': \\'This is my query\\'})\\n        answer, docs = res[\\'result\\'], res[\\'source_documents\\']\\n        \"\"\"\\n        question = inputs[self.input_key]\\n        docs = self._get_docs(question)\\n        answer = self.combine_documents_chain.run(\\n            input_documents=docs, question=question\\n        )\\n        if self.return_source_documents:\\n            return {self.output_key: answer, \"source_documents\": docs}\\n        else:\\n            return {self.output_key: answer}\\n    @abstractmethod\\n    async def _aget_docs(self, question: str) -> List[Document]:\\n        \"\"\"Get documents to do question answering over.\"\"\"\\n    async def _acall(self, inputs: Dict[str, str]) -> Dict[str, Any]:\\n        \"\"\"Run get_relevant_text and llm on input query.\\n        If chain has \\'return_source_documents\\' as \\'True\\', returns\\n        the retrieved documents as well under the key \\'source_documents\\'.\\n        Example:\\n        .. code-block:: python\\n        res = indexqa({\\'query\\': \\'This is my query\\'})\\n        answer, docs = res[\\'result\\'], res[\\'source_documents\\']\\n        \"\"\"\\n        question = inputs[self.input_key]\\n        docs = await self._aget_docs(question)\\n        answer = await self.combine_documents_chain.arun(\\n            input_documents=docs, question=question\\n        )\\n        if self.return_source_documents:\\n            return {self.output_key: answer, \"source_documents\": docs}\\n        else:\\n            return {self.output_key: answer}\\n[docs]class RetrievalQA(BaseRetrievalQA):\\n    \"\"\"Chain for question-answering against an index.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import OpenAI\\n            from langchain.chains import RetrievalQA\\n            from langchain.faiss import FAISS\\n            from langchain.vectorstores.base import VectorStoreRetriever\\n            retriever = VectorStoreRetriever(vectorstore=FAISS(...))\\n            retrievalQA = RetrievalQA.from_llm(llm=OpenAI(), retriever=retriever)\\n    \"\"\"\\n    retriever: BaseRetriever = Field(exclude=True)\\n    def _get_docs(self, question: str) -> List[Document]:\\n        return self.retriever.get_relevant_documents(question)\\n    async def _aget_docs(self, question: str) -> List[Document]:\\n        return await self.retriever.aget_relevant_documents(question)\\n[docs]class VectorDBQA(BaseRetrievalQA):\\n    \"\"\"Chain for question-answering against a vector database.\"\"\"\\n    vectorstore: VectorStore = Field(exclude=True, alias=\"vectorstore\")\\n    \"\"\"Vector Database to connect to.\"\"\"\\n    k: int = 4\\n    \"\"\"Number of documents to query for.\"\"\"\\n    search_type: str = \"similarity\"\\n    \"\"\"Search type to use over vectorstore. `similarity` or `mmr`.\"\"\"\\n    search_kwargs: Dict[str, Any] = Field(default_factory=dict)\\n    \"\"\"Extra search args.\"\"\"\\n    @root_validator()\\n    def raise_deprecation(cls, values: Dict) -> Dict:\\n        warnings.warn(\\n            \"`VectorDBQA` is deprecated - \"\\n            \"please use `from langchain.chains import RetrievalQA`\"\\n        )\\n        return values\\n    @root_validator()\\n    def validate_search_type(cls, values: Dict) -> Dict:\\n        \"\"\"Validate search type.\"\"\"\\n        if \"search_type\" in values:\\n            search_type = values[\"search_type\"]\\n            if search_type not in (\"similarity\", \"mmr\"):\\n                raise ValueError(f\"search_type of {search_type} not allowed.\")\\n        return values\\n    def _get_docs(self, question: str) -> List[Document]:\\n        if self.search_type == \"similarity\":\\n            docs = self.vectorstore.similarity_search(\\n                question, k=self.k, **self.search_kwargs\\n            )\\n        elif self.search_type == \"mmr\":\\n            docs = self.vectorstore.max_marginal_relevance_search(\\n                question, k=self.k, **self.search_kwargs\\n            )\\n        else:\\n            raise ValueError(f\"search_type of {self.search_type} not allowed.\")\\n        return docs\\n    async def _aget_docs(self, question: str) -> List[Document]:\\n        raise NotImplementedError(\"VectorDBQA does not support async\")\\n    @property\\n    def _chain_type(self) -> str:\\n        \"\"\"Return the chain type.\"\"\"\\n        return \"vector_db_qa\"\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\retrieval_qa\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.chains.sql_database.base\\n\"\"\"Chain for interacting with SQL Database.\"\"\"\\nfrom __future__ import annotations\\nfrom typing import Any, Dict, List, Optional\\nfrom pydantic import Extra, Field\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.sql_database.prompt import DECIDER_PROMPT, PROMPT, SQL_PROMPTS\\nfrom langchain.prompts.base import BasePromptTemplate\\nfrom langchain.schema import BaseLanguageModel\\nfrom langchain.sql_database import SQLDatabase\\n[docs]class SQLDatabaseChain(Chain):\\n    \"\"\"Chain for interacting with SQL Database.\\n    Example:\\n        .. code-block:: python\\n            from langchain import SQLDatabaseChain, OpenAI, SQLDatabase\\n            db = SQLDatabase(...)\\n            db_chain = SQLDatabaseChain(llm=OpenAI(), database=db)\\n    \"\"\"\\n    llm: BaseLanguageModel\\n    \"\"\"LLM wrapper to use.\"\"\"\\n    database: SQLDatabase = Field(exclude=True)\\n    \"\"\"SQL Database to connect to.\"\"\"\\n    prompt: Optional[BasePromptTemplate] = None\\n    \"\"\"Prompt to use to translate natural language to SQL.\"\"\"\\n    top_k: int = 5\\n    \"\"\"Number of results to return from the query\"\"\"\\n    input_key: str = \"query\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n    return_intermediate_steps: bool = False\\n    \"\"\"Whether or not to return the intermediate steps along with the final answer.\"\"\"\\n    return_direct: bool = False\\n    \"\"\"Whether or not to return the result of querying the SQL table directly.\"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the singular input key.\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the singular output key.\\n        :meta private:\\n        \"\"\"\\n        if not self.return_intermediate_steps:\\n            return [self.output_key]\\n        else:\\n            return [self.output_key, \"intermediate_steps\"]\\n    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        prompt = self.prompt or SQL_PROMPTS.get(self.database.dialect, PROMPT)\\n        llm_chain = LLMChain(llm=self.llm, prompt=prompt)\\n        input_text = f\"{inputs[self.input_key]}\\\\nSQLQuery:\"\\n        self.callback_manager.on_text(input_text, verbose=self.verbose)\\n        # If not present, then defaults to None which is all tables.\\n        table_names_to_use = inputs.get(\"table_names_to_use\")\\n        table_info = self.database.get_table_info(table_names=table_names_to_use)\\n        llm_inputs = {\\n            \"input\": input_text,\\n            \"top_k\": self.top_k,\\n            \"dialect\": self.database.dialect,\\n            \"table_info\": table_info,\\n            \"stop\": [\"\\\\nSQLResult:\"],\\n        }\\n        intermediate_steps = []\\n        sql_cmd = llm_chain.predict(**llm_inputs)\\n        intermediate_steps.append(sql_cmd)\\n        self.callback_manager.on_text(sql_cmd, color=\"green\", verbose=self.verbose)\\n        result = self.database.run(sql_cmd)\\n        intermediate_steps.append(result)\\n        self.callback_manager.on_text(\"\\\\nSQLResult: \", verbose=self.verbose)\\n        self.callback_manager.on_text(result, color=\"yellow\", verbose=self.verbose)\\n        # If return direct, we just set the final result equal to the sql query\\n        if self.return_direct:\\n            final_result = result\\n        else:\\n            self.callback_manager.on_text(\"\\\\nAnswer:\", verbose=self.verbose)\\n            input_text += f\"{sql_cmd}\\\\nSQLResult: {result}\\\\nAnswer:\"\\n            llm_inputs[\"input\"] = input_text\\n            final_result = llm_chain.predict(**llm_inputs)\\n            self.callback_manager.on_text(\\n                final_result, color=\"green\", verbose=self.verbose\\n            )\\n        chain_result: Dict[str, Any] = {self.output_key: final_result}\\n        if self.return_intermediate_steps:\\n            chain_result[\"intermediate_steps\"] = intermediate_steps\\n        return chain_result\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"sql_database_chain\"\\n[docs]class SQLDatabaseSequentialChain(Chain):\\n    \"\"\"Chain for querying SQL database that is a sequential chain.\\n    The chain is as follows:\\n    1. Based on the query, determine which tables to use.\\n    2. Based on those tables, call the normal SQL database chain.\\n    This is useful in cases where the number of tables in the database is large.\\n    \"\"\"\\n    return_intermediate_steps: bool = False\\n[docs]    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        database: SQLDatabase,\\n        query_prompt: BasePromptTemplate = PROMPT,\\n        decider_prompt: BasePromptTemplate = DECIDER_PROMPT,\\n        **kwargs: Any,\\n    ) -> SQLDatabaseSequentialChain:\\n        \"\"\"Load the necessary chains.\"\"\"\\n        sql_chain = SQLDatabaseChain(\\n            llm=llm, database=database, prompt=query_prompt, **kwargs\\n        )\\n        decider_chain = LLMChain(\\n            llm=llm, prompt=decider_prompt, output_key=\"table_names\"\\n        )\\n        return cls(sql_chain=sql_chain, decider_chain=decider_chain, **kwargs)\\n    decider_chain: LLMChain\\n    sql_chain: SQLDatabaseChain\\n    input_key: str = \"query\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the singular input key.\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the singular output key.\\n        :meta private:\\n        \"\"\"\\n        if not self.return_intermediate_steps:\\n            return [self.output_key]\\n        else:\\n            return [self.output_key, \"intermediate_steps\"]\\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\\n        _table_names = self.sql_chain.database.get_usable_table_names()\\n        table_names = \", \".join(_table_names)\\n        llm_inputs = {\\n            \"query\": inputs[self.input_key],\\n            \"table_names\": table_names,\\n        }\\n        table_names_to_use = self.decider_chain.predict_and_parse(**llm_inputs)\\n        self.callback_manager.on_text(\\n            \"Table names to use:\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n        self.callback_manager.on_text(\\n            str(table_names_to_use), color=\"yellow\", verbose=self.verbose\\n        )\\n        new_inputs = {\\n            self.sql_chain.input_key: inputs[self.input_key],\\n            \"table_names_to_use\": table_names_to_use,\\n        }\\n        return self.sql_chain(new_inputs, return_only_outputs=True)\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"sql_database_sequential_chain\"\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\chains\\\\sql_database\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.docstore.in_memory\\n\"\"\"Simple in memory docstore in the form of a dict.\"\"\"\\nfrom typing import Dict, Union\\nfrom langchain.docstore.base import AddableMixin, Docstore\\nfrom langchain.docstore.document import Document\\n[docs]class InMemoryDocstore(Docstore, AddableMixin):\\n    \"\"\"Simple in memory docstore in the form of a dict.\"\"\"\\n    def __init__(self, _dict: Dict[str, Document]):\\n        \"\"\"Initialize with dict.\"\"\"\\n        self._dict = _dict\\n[docs]    def add(self, texts: Dict[str, Document]) -> None:\\n        \"\"\"Add texts to in memory dictionary.\"\"\"\\n        overlapping = set(texts).intersection(self._dict)\\n        if overlapping:\\n            raise ValueError(f\"Tried to add ids that already exist: {overlapping}\")\\n        self._dict = dict(self._dict, **texts)\\n[docs]    def search(self, search: str) -> Union[str, Document]:\\n        \"\"\"Search via direct lookup.\"\"\"\\n        if search not in self._dict:\\n            return f\"ID {search} not found.\"\\n        else:\\n            return self._dict[search]\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\docstore\\\\in_memory.html'}),\n",
              " Document(page_content='Source code for langchain.docstore.wikipedia\\n\"\"\"Wrapper around wikipedia API.\"\"\"\\nfrom typing import Union\\nfrom langchain.docstore.base import Docstore\\nfrom langchain.docstore.document import Document\\n[docs]class Wikipedia(Docstore):\\n    \"\"\"Wrapper around wikipedia API.\"\"\"\\n    def __init__(self) -> None:\\n        \"\"\"Check that wikipedia package is installed.\"\"\"\\n        try:\\n            import wikipedia  # noqa: F401\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import wikipedia python package. \"\\n                \"Please install it with `pip install wikipedia`.\"\\n            )\\n[docs]    def search(self, search: str) -> Union[str, Document]:\\n        \"\"\"Try to search for wiki page.\\n        If page exists, return the page summary, and a PageWithLookups object.\\n        If page does not exist, return similar entries.\\n        \"\"\"\\n        import wikipedia\\n        try:\\n            page_content = wikipedia.page(search).content\\n            url = wikipedia.page(search).url\\n            result: Union[str, Document] = Document(\\n                page_content=page_content, metadata={\"page\": url}\\n            )\\n        except wikipedia.PageError:\\n            result = f\"Could not find [{search}]. Similar: {wikipedia.search(search)}\"\\n        except wikipedia.DisambiguationError:\\n            result = f\"Could not find [{search}]. Similar: {wikipedia.search(search)}\"\\n        return result\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\docstore\\\\wikipedia.html'}),\n",
              " Document(page_content='Source code for langchain.embeddings.aleph_alpha\\nfrom typing import Any, Dict, List, Optional\\nfrom pydantic import BaseModel, root_validator\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.utils import get_from_dict_or_env\\n[docs]class AlephAlphaAsymmetricSemanticEmbedding(BaseModel, Embeddings):\\n    \"\"\"\\n    Wrapper for Aleph Alpha\\'s Asymmetric Embeddings\\n    AA provides you with an endpoint to embed a document and a query.\\n    The models were optimized to make the embeddings of documents and\\n    the query for a document as similar as possible.\\n    To learn more, check out: https://docs.aleph-alpha.com/docs/tasks/semantic_embed/\\n    Example:\\n        .. code-block:: python\\n            from aleph_alpha import AlephAlphaAsymmetricSemanticEmbedding\\n            embeddings = AlephAlphaSymmetricSemanticEmbedding()\\n            document = \"This is a content of the document\"\\n            query = \"What is the content of the document?\"\\n            doc_result = embeddings.embed_documents([document])\\n            query_result = embeddings.embed_query(query)\\n    \"\"\"\\n    client: Any  #: :meta private:\\n    model: Optional[str] = \"luminous-base\"\\n    \"\"\"Model name to use.\"\"\"\\n    hosting: Optional[str] = \"https://api.aleph-alpha.com\"\\n    \"\"\"Optional parameter that specifies which datacenters may process the request.\"\"\"\\n    normalize: Optional[bool] = True\\n    \"\"\"Should returned embeddings be normalized\"\"\"\\n    compress_to_size: Optional[int] = 128\\n    \"\"\"Should the returned embeddings come back as an original 5120-dim vector, \\n    or should it be compressed to 128-dim.\"\"\"\\n    contextual_control_threshold: Optional[int] = None\\n    \"\"\"Attention control parameters only apply to those tokens that have \\n    explicitly been set in the request.\"\"\"\\n    control_log_additive: Optional[bool] = True\\n    \"\"\"Apply controls on prompt items by adding the log(control_factor) \\n    to attention scores.\"\"\"\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        aleph_alpha_api_key = get_from_dict_or_env(\\n            values, \"aleph_alpha_api_key\", \"ALEPH_ALPHA_API_KEY\"\\n        )\\n        try:\\n            from aleph_alpha_client import Client\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import aleph_alpha_client python package. \"\\n                \"Please install it with `pip install aleph_alpha_client`.\"\\n            )\\n        values[\"client\"] = Client(token=aleph_alpha_api_key)\\n        return values\\n[docs]    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        \"\"\"Call out to Aleph Alpha\\'s asymmetric Document endpoint.\\n        Args:\\n            texts: The list of texts to embed.\\n        Returns:\\n            List of embeddings, one for each text.\\n        \"\"\"\\n        try:\\n            from aleph_alpha_client import (\\n                Prompt,\\n                SemanticEmbeddingRequest,\\n                SemanticRepresentation,\\n            )\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import aleph_alpha_client python package. \"\\n                \"Please install it with `pip install aleph_alpha_client`.\"\\n            )\\n        document_embeddings = []\\n        for text in texts:\\n            document_params = {\\n                \"prompt\": Prompt.from_text(text),\\n                \"representation\": SemanticRepresentation.Document,\\n                \"compress_to_size\": self.compress_to_size,\\n                \"normalize\": self.normalize,\\n                \"contextual_control_threshold\": self.contextual_control_threshold,\\n                \"control_log_additive\": self.control_log_additive,\\n            }\\n            document_request = SemanticEmbeddingRequest(**document_params)\\n            document_response = self.client.semantic_embed(\\n                request=document_request, model=self.model\\n            )\\n            document_embeddings.append(document_response.embedding)\\n        return document_embeddings\\n[docs]    def embed_query(self, text: str) -> List[float]:\\n        \"\"\"Call out to Aleph Alpha\\'s asymmetric, query embedding endpoint\\n        Args:\\n            text: The text to embed.\\n        Returns:\\n            Embeddings for the text.\\n        \"\"\"\\n        try:\\n            from aleph_alpha_client import (\\n                Prompt,\\n                SemanticEmbeddingRequest,\\n                SemanticRepresentation,\\n            )\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import aleph_alpha_client python package. \"\\n                \"Please install it with `pip install aleph_alpha_client`.\"\\n            )\\n        symmetric_params = {\\n            \"prompt\": Prompt.from_text(text),\\n            \"representation\": SemanticRepresentation.Query,\\n            \"compress_to_size\": self.compress_to_size,\\n            \"normalize\": self.normalize,\\n            \"contextual_control_threshold\": self.contextual_control_threshold,\\n            \"control_log_additive\": self.control_log_additive,\\n        }\\n        symmetric_request = SemanticEmbeddingRequest(**symmetric_params)\\n        symmetric_response = self.client.semantic_embed(\\n            request=symmetric_request, model=self.model\\n        )\\n        return symmetric_response.embedding\\n[docs]class AlephAlphaSymmetricSemanticEmbedding(AlephAlphaAsymmetricSemanticEmbedding):\\n    \"\"\"The symmetric version of the Aleph Alpha\\'s semantic embeddings.\\n    The main difference is that here, both the documents and\\n    queries are embedded with a SemanticRepresentation.Symmetric\\n    Example:\\n        .. code-block:: python\\n            from aleph_alpha import AlephAlphaSymmetricSemanticEmbedding\\n            embeddings = AlephAlphaAsymmetricSemanticEmbedding()\\n            text = \"This is a test text\"\\n            doc_result = embeddings.embed_documents([text])\\n            query_result = embeddings.embed_query(text)\\n    \"\"\"\\n    def _embed(self, text: str) -> List[float]:\\n        try:\\n            from aleph_alpha_client import (\\n                Prompt,\\n                SemanticEmbeddingRequest,\\n                SemanticRepresentation,\\n            )\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import aleph_alpha_client python package. \"\\n                \"Please install it with `pip install aleph_alpha_client`.\"\\n            )\\n        query_params = {\\n            \"prompt\": Prompt.from_text(text),\\n            \"representation\": SemanticRepresentation.Symmetric,\\n            \"compress_to_size\": self.compress_to_size,\\n            \"normalize\": self.normalize,\\n            \"contextual_control_threshold\": self.contextual_control_threshold,\\n            \"control_log_additive\": self.control_log_additive,\\n        }\\n        query_request = SemanticEmbeddingRequest(**query_params)\\n        query_response = self.client.semantic_embed(\\n            request=query_request, model=self.model\\n        )\\n        return query_response.embedding\\n[docs]    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        \"\"\"Call out to Aleph Alpha\\'s Document endpoint.\\n        Args:\\n            texts: The list of texts to embed.\\n        Returns:\\n            List of embeddings, one for each text.\\n        \"\"\"\\n        document_embeddings = []\\n        for text in texts:\\n            document_embeddings.append(self._embed(text))\\n        return document_embeddings\\n[docs]    def embed_query(self, text: str) -> List[float]:\\n        \"\"\"Call out to Aleph Alpha\\'s asymmetric, query embedding endpoint\\n        Args:\\n            text: The text to embed.\\n        Returns:\\n            Embeddings for the text.\\n        \"\"\"\\n        return self._embed(text)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\embeddings\\\\aleph_alpha.html'}),\n",
              " Document(page_content='Source code for langchain.embeddings.cohere\\n\"\"\"Wrapper around Cohere embedding models.\"\"\"\\nfrom typing import Any, Dict, List, Optional\\nfrom pydantic import BaseModel, Extra, root_validator\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.utils import get_from_dict_or_env\\n[docs]class CohereEmbeddings(BaseModel, Embeddings):\\n    \"\"\"Wrapper around Cohere embedding models.\\n    To use, you should have the ``cohere`` python package installed, and the\\n    environment variable ``COHERE_API_KEY`` set with your API key or pass it\\n    as a named parameter to the constructor.\\n    Example:\\n        .. code-block:: python\\n            from langchain.embeddings import CohereEmbeddings\\n            cohere = CohereEmbeddings(model=\"medium\", cohere_api_key=\"my-api-key\")\\n    \"\"\"\\n    client: Any  #: :meta private:\\n    model: str = \"large\"\\n    \"\"\"Model name to use.\"\"\"\\n    truncate: Optional[str] = None\\n    \"\"\"Truncate embeddings that are too long from start or end (\"NONE\"|\"START\"|\"END\")\"\"\"\\n    cohere_api_key: Optional[str] = None\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        cohere_api_key = get_from_dict_or_env(\\n            values, \"cohere_api_key\", \"COHERE_API_KEY\"\\n        )\\n        try:\\n            import cohere\\n            values[\"client\"] = cohere.Client(cohere_api_key)\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import cohere python package. \"\\n                \"Please install it with `pip install cohere`.\"\\n            )\\n        return values\\n[docs]    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        \"\"\"Call out to Cohere\\'s embedding endpoint.\\n        Args:\\n            texts: The list of texts to embed.\\n        Returns:\\n            List of embeddings, one for each text.\\n        \"\"\"\\n        embeddings = self.client.embed(\\n            model=self.model, texts=texts, truncate=self.truncate\\n        ).embeddings\\n        return [list(map(float, e)) for e in embeddings]\\n[docs]    def embed_query(self, text: str) -> List[float]:\\n        \"\"\"Call out to Cohere\\'s embedding endpoint.\\n        Args:\\n            text: The text to embed.\\n        Returns:\\n            Embeddings for the text.\\n        \"\"\"\\n        embedding = self.client.embed(\\n            model=self.model, texts=[text], truncate=self.truncate\\n        ).embeddings[0]\\n        return list(map(float, embedding))\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\embeddings\\\\cohere.html'}),\n",
              " Document(page_content='Source code for langchain.embeddings.fake\\nfrom typing import List\\nimport numpy as np\\nfrom pydantic import BaseModel\\nfrom langchain.embeddings.base import Embeddings\\n[docs]class FakeEmbeddings(Embeddings, BaseModel):\\n    size: int\\n    def _get_embedding(self) -> List[float]:\\n        return list(np.random.normal(size=self.size))\\n[docs]    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        return [self._get_embedding() for _ in texts]\\n[docs]    def embed_query(self, text: str) -> List[float]:\\n        return self._get_embedding()\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\embeddings\\\\fake.html'}),\n",
              " Document(page_content='Source code for langchain.embeddings.huggingface\\n\"\"\"Wrapper around HuggingFace embedding models.\"\"\"\\nfrom typing import Any, Dict, List, Optional\\nfrom pydantic import BaseModel, Extra, Field\\nfrom langchain.embeddings.base import Embeddings\\nDEFAULT_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\\nDEFAULT_INSTRUCT_MODEL = \"hkunlp/instructor-large\"\\nDEFAULT_EMBED_INSTRUCTION = \"Represent the document for retrieval: \"\\nDEFAULT_QUERY_INSTRUCTION = (\\n    \"Represent the question for retrieving supporting documents: \"\\n)\\n[docs]class HuggingFaceEmbeddings(BaseModel, Embeddings):\\n    \"\"\"Wrapper around sentence_transformers embedding models.\\n    To use, you should have the ``sentence_transformers`` python package installed.\\n    Example:\\n        .. code-block:: python\\n            from langchain.embeddings import HuggingFaceEmbeddings\\n            model_name = \"sentence-transformers/all-mpnet-base-v2\"\\n            model_kwargs = {\\'device\\': \\'cpu\\'}\\n            hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\\n    \"\"\"\\n    client: Any  #: :meta private:\\n    model_name: str = DEFAULT_MODEL_NAME\\n    \"\"\"Model name to use.\"\"\"\\n    cache_folder: Optional[str] = None\\n    \"\"\"Path to store models. \\n    Can be also set by SENTENCE_TRANSFORMERS_HOME enviroment variable.\"\"\"\\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\\n    \"\"\"Key word arguments to pass to the model.\"\"\"\\n    def __init__(self, **kwargs: Any):\\n        \"\"\"Initialize the sentence_transformer.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import sentence_transformers\\n            self.client = sentence_transformers.SentenceTransformer(\\n                self.model_name, cache_folder=self.cache_folder, **self.model_kwargs\\n            )\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import sentence_transformers python package. \"\\n                \"Please install it with `pip install sentence_transformers`.\"\\n            )\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n[docs]    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        \"\"\"Compute doc embeddings using a HuggingFace transformer model.\\n        Args:\\n            texts: The list of texts to embed.\\n        Returns:\\n            List of embeddings, one for each text.\\n        \"\"\"\\n        texts = list(map(lambda x: x.replace(\"\\\\n\", \" \"), texts))\\n        embeddings = self.client.encode(texts)\\n        return embeddings.tolist()\\n[docs]    def embed_query(self, text: str) -> List[float]:\\n        \"\"\"Compute query embeddings using a HuggingFace transformer model.\\n        Args:\\n            text: The text to embed.\\n        Returns:\\n            Embeddings for the text.\\n        \"\"\"\\n        text = text.replace(\"\\\\n\", \" \")\\n        embedding = self.client.encode(text)\\n        return embedding.tolist()\\n[docs]class HuggingFaceInstructEmbeddings(BaseModel, Embeddings):\\n    \"\"\"Wrapper around sentence_transformers embedding models.\\n    To use, you should have the ``sentence_transformers``\\n    and ``InstructorEmbedding`` python package installed.\\n    Example:\\n        .. code-block:: python\\n            from langchain.embeddings import HuggingFaceInstructEmbeddings\\n            model_name = \"hkunlp/instructor-large\"\\n            model_kwargs = {\\'device\\': \\'cpu\\'}\\n            hf = HuggingFaceInstructEmbeddings(\\n                model_name=model_name, model_kwargs=model_kwargs\\n            )\\n    \"\"\"\\n    client: Any  #: :meta private:\\n    model_name: str = DEFAULT_INSTRUCT_MODEL\\n    \"\"\"Model name to use.\"\"\"\\n    cache_folder: Optional[str] = None\\n    \"\"\"Path to store models. \\n    Can be also set by SENTENCE_TRANSFORMERS_HOME enviroment variable.\"\"\"\\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\\n    \"\"\"Key word arguments to pass to the model.\"\"\"\\n    embed_instruction: str = DEFAULT_EMBED_INSTRUCTION\\n    \"\"\"Instruction to use for embedding documents.\"\"\"\\n    query_instruction: str = DEFAULT_QUERY_INSTRUCTION\\n    \"\"\"Instruction to use for embedding query.\"\"\"\\n    def __init__(self, **kwargs: Any):\\n        \"\"\"Initialize the sentence_transformer.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from InstructorEmbedding import INSTRUCTOR\\n            self.client = INSTRUCTOR(\\n                self.model_name, cache_folder=self.cache_folder, **self.model_kwargs\\n            )\\n        except ImportError as e:\\n            raise ValueError(\"Dependencies for InstructorEmbedding not found.\") from e\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n[docs]    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        \"\"\"Compute doc embeddings using a HuggingFace instruct model.\\n        Args:\\n            texts: The list of texts to embed.\\n        Returns:\\n            List of embeddings, one for each text.\\n        \"\"\"\\n        instruction_pairs = [[self.embed_instruction, text] for text in texts]\\n        embeddings = self.client.encode(instruction_pairs)\\n        return embeddings.tolist()\\n[docs]    def embed_query(self, text: str) -> List[float]:\\n        \"\"\"Compute query embeddings using a HuggingFace instruct model.\\n        Args:\\n            text: The text to embed.\\n        Returns:\\n            Embeddings for the text.\\n        \"\"\"\\n        instruction_pair = [self.query_instruction, text]\\n        embedding = self.client.encode([instruction_pair])[0]\\n        return embedding.tolist()\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\embeddings\\\\huggingface.html'}),\n",
              " Document(page_content='Source code for langchain.embeddings.huggingface_hub\\n\"\"\"Wrapper around HuggingFace Hub embedding models.\"\"\"\\nfrom typing import Any, Dict, List, Optional\\nfrom pydantic import BaseModel, Extra, root_validator\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.utils import get_from_dict_or_env\\nDEFAULT_REPO_ID = \"sentence-transformers/all-mpnet-base-v2\"\\nVALID_TASKS = (\"feature-extraction\",)\\n[docs]class HuggingFaceHubEmbeddings(BaseModel, Embeddings):\\n    \"\"\"Wrapper around HuggingFaceHub embedding models.\\n    To use, you should have the ``huggingface_hub`` python package installed, and the\\n    environment variable ``HUGGINGFACEHUB_API_TOKEN`` set with your API token, or pass\\n    it as a named parameter to the constructor.\\n    Example:\\n        .. code-block:: python\\n            from langchain.embeddings import HuggingFaceHubEmbeddings\\n            repo_id = \"sentence-transformers/all-mpnet-base-v2\"\\n            hf = HuggingFaceHubEmbeddings(\\n                repo_id=repo_id,\\n                task=\"feature-extraction\",\\n                huggingfacehub_api_token=\"my-api-key\",\\n            )\\n    \"\"\"\\n    client: Any  #: :meta private:\\n    repo_id: str = DEFAULT_REPO_ID\\n    \"\"\"Model name to use.\"\"\"\\n    task: Optional[str] = \"feature-extraction\"\\n    \"\"\"Task to call the model with.\"\"\"\\n    model_kwargs: Optional[dict] = None\\n    \"\"\"Key word arguments to pass to the model.\"\"\"\\n    huggingfacehub_api_token: Optional[str] = None\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        huggingfacehub_api_token = get_from_dict_or_env(\\n            values, \"huggingfacehub_api_token\", \"HUGGINGFACEHUB_API_TOKEN\"\\n        )\\n        try:\\n            from huggingface_hub.inference_api import InferenceApi\\n            repo_id = values[\"repo_id\"]\\n            if not repo_id.startswith(\"sentence-transformers\"):\\n                raise ValueError(\\n                    \"Currently only \\'sentence-transformers\\' embedding models \"\\n                    f\"are supported. Got invalid \\'repo_id\\' {repo_id}.\"\\n                )\\n            client = InferenceApi(\\n                repo_id=repo_id,\\n                token=huggingfacehub_api_token,\\n                task=values.get(\"task\"),\\n            )\\n            if client.task not in VALID_TASKS:\\n                raise ValueError(\\n                    f\"Got invalid task {client.task}, \"\\n                    f\"currently only {VALID_TASKS} are supported\"\\n                )\\n            values[\"client\"] = client\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import huggingface_hub python package. \"\\n                \"Please install it with `pip install huggingface_hub`.\"\\n            )\\n        return values\\n[docs]    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        \"\"\"Call out to HuggingFaceHub\\'s embedding endpoint for embedding search docs.\\n        Args:\\n            texts: The list of texts to embed.\\n        Returns:\\n            List of embeddings, one for each text.\\n        \"\"\"\\n        # replace newlines, which can negatively affect performance.\\n        texts = [text.replace(\"\\\\n\", \" \") for text in texts]\\n        _model_kwargs = self.model_kwargs or {}\\n        responses = self.client(inputs=texts, params=_model_kwargs)\\n        return responses\\n[docs]    def embed_query(self, text: str) -> List[float]:\\n        \"\"\"Call out to HuggingFaceHub\\'s embedding endpoint for embedding query text.\\n        Args:\\n            text: The text to embed.\\n        Returns:\\n            Embeddings for the text.\\n        \"\"\"\\n        response = self.embed_documents([text])[0]\\n        return response\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\embeddings\\\\huggingface_hub.html'}),\n",
              " Document(page_content='Source code for langchain.embeddings.llamacpp\\n\"\"\"Wrapper around llama.cpp embedding models.\"\"\"\\nfrom typing import Any, Dict, List, Optional\\nfrom pydantic import BaseModel, Extra, Field, root_validator\\nfrom langchain.embeddings.base import Embeddings\\n[docs]class LlamaCppEmbeddings(BaseModel, Embeddings):\\n    \"\"\"Wrapper around llama.cpp embedding models.\\n    To use, you should have the llama-cpp-python library installed, and provide the\\n    path to the Llama model as a named parameter to the constructor.\\n    Check out: https://github.com/abetlen/llama-cpp-python\\n    Example:\\n        .. code-block:: python\\n            from langchain.embeddings import LlamaCppEmbeddings\\n            llama = LlamaCppEmbeddings(model_path=\"/path/to/model.bin\")\\n    \"\"\"\\n    client: Any  #: :meta private:\\n    model_path: str\\n    n_ctx: int = Field(512, alias=\"n_ctx\")\\n    \"\"\"Token context window.\"\"\"\\n    n_parts: int = Field(-1, alias=\"n_parts\")\\n    \"\"\"Number of parts to split the model into. \\n    If -1, the number of parts is automatically determined.\"\"\"\\n    seed: int = Field(-1, alias=\"seed\")\\n    \"\"\"Seed. If -1, a random seed is used.\"\"\"\\n    f16_kv: bool = Field(False, alias=\"f16_kv\")\\n    \"\"\"Use half-precision for key/value cache.\"\"\"\\n    logits_all: bool = Field(False, alias=\"logits_all\")\\n    \"\"\"Return logits for all tokens, not just the last token.\"\"\"\\n    vocab_only: bool = Field(False, alias=\"vocab_only\")\\n    \"\"\"Only load the vocabulary, no weights.\"\"\"\\n    use_mlock: bool = Field(False, alias=\"use_mlock\")\\n    \"\"\"Force system to keep model in RAM.\"\"\"\\n    n_threads: Optional[int] = Field(None, alias=\"n_threads\")\\n    \"\"\"Number of threads to use. If None, the number \\n    of threads is automatically determined.\"\"\"\\n    n_batch: Optional[int] = Field(8, alias=\"n_batch\")\\n    \"\"\"Number of tokens to process in parallel.\\n    Should be a number between 1 and n_ctx.\"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that llama-cpp-python library is installed.\"\"\"\\n        model_path = values[\"model_path\"]\\n        n_ctx = values[\"n_ctx\"]\\n        n_parts = values[\"n_parts\"]\\n        seed = values[\"seed\"]\\n        f16_kv = values[\"f16_kv\"]\\n        logits_all = values[\"logits_all\"]\\n        vocab_only = values[\"vocab_only\"]\\n        use_mlock = values[\"use_mlock\"]\\n        n_threads = values[\"n_threads\"]\\n        n_batch = values[\"n_batch\"]\\n        try:\\n            from llama_cpp import Llama\\n            values[\"client\"] = Llama(\\n                model_path=model_path,\\n                n_ctx=n_ctx,\\n                n_parts=n_parts,\\n                seed=seed,\\n                f16_kv=f16_kv,\\n                logits_all=logits_all,\\n                vocab_only=vocab_only,\\n                use_mlock=use_mlock,\\n                n_threads=n_threads,\\n                n_batch=n_batch,\\n                embedding=True,\\n            )\\n        except ImportError:\\n            raise ModuleNotFoundError(\\n                \"Could not import llama-cpp-python library. \"\\n                \"Please install the llama-cpp-python library to \"\\n                \"use this embedding model: pip install llama-cpp-python\"\\n            )\\n        except Exception:\\n            raise NameError(f\"Could not load Llama model from path: {model_path}\")\\n        return values\\n[docs]    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        \"\"\"Embed a list of documents using the Llama model.\\n        Args:\\n            texts: The list of texts to embed.\\n        Returns:\\n            List of embeddings, one for each text.\\n        \"\"\"\\n        embeddings = [self.client.embed(text) for text in texts]\\n        return [list(map(float, e)) for e in embeddings]\\n[docs]    def embed_query(self, text: str) -> List[float]:\\n        \"\"\"Embed a query using the Llama model.\\n        Args:\\n            text: The text to embed.\\n        Returns:\\n            Embeddings for the text.\\n        \"\"\"\\n        embedding = self.client.embed(text)\\n        return list(map(float, embedding))\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\embeddings\\\\llamacpp.html'}),\n",
              " Document(page_content='Source code for langchain.embeddings.openai\\n\"\"\"Wrapper around OpenAI embedding models.\"\"\"\\nfrom __future__ import annotations\\nimport logging\\nfrom typing import (\\n    Any,\\n    Callable,\\n    Dict,\\n    List,\\n    Literal,\\n    Optional,\\n    Set,\\n    Tuple,\\n    Union,\\n)\\nimport numpy as np\\nfrom pydantic import BaseModel, Extra, root_validator\\nfrom tenacity import (\\n    before_sleep_log,\\n    retry,\\n    retry_if_exception_type,\\n    stop_after_attempt,\\n    wait_exponential,\\n)\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.utils import get_from_dict_or_env\\nlogger = logging.getLogger(__name__)\\ndef _create_retry_decorator(embeddings: OpenAIEmbeddings) -> Callable[[Any], Any]:\\n    import openai\\n    min_seconds = 4\\n    max_seconds = 10\\n    # Wait 2^x * 1 second between each retry starting with\\n    # 4 seconds, then up to 10 seconds, then 10 seconds afterwards\\n    return retry(\\n        reraise=True,\\n        stop=stop_after_attempt(embeddings.max_retries),\\n        wait=wait_exponential(multiplier=1, min=min_seconds, max=max_seconds),\\n        retry=(\\n            retry_if_exception_type(openai.error.Timeout)\\n            | retry_if_exception_type(openai.error.APIError)\\n            | retry_if_exception_type(openai.error.APIConnectionError)\\n            | retry_if_exception_type(openai.error.RateLimitError)\\n            | retry_if_exception_type(openai.error.ServiceUnavailableError)\\n        ),\\n        before_sleep=before_sleep_log(logger, logging.WARNING),\\n    )\\ndef embed_with_retry(embeddings: OpenAIEmbeddings, **kwargs: Any) -> Any:\\n    \"\"\"Use tenacity to retry the embedding call.\"\"\"\\n    retry_decorator = _create_retry_decorator(embeddings)\\n    @retry_decorator\\n    def _embed_with_retry(**kwargs: Any) -> Any:\\n        return embeddings.client.create(**kwargs)\\n    return _embed_with_retry(**kwargs)\\n[docs]class OpenAIEmbeddings(BaseModel, Embeddings):\\n    \"\"\"Wrapper around OpenAI embedding models.\\n    To use, you should have the ``openai`` python package installed, and the\\n    environment variable ``OPENAI_API_KEY`` set with your API key or pass it\\n    as a named parameter to the constructor.\\n    Example:\\n        .. code-block:: python\\n            from langchain.embeddings import OpenAIEmbeddings\\n            openai = OpenAIEmbeddings(openai_api_key=\"my-api-key\")\\n    In order to use the library with Microsoft Azure endpoints, you need to set\\n    the OPENAI_API_TYPE, OPENAI_API_BASE, OPENAI_API_KEY and optionally and\\n    API_VERSION.\\n    The OPENAI_API_TYPE must be set to \\'azure\\' and the others correspond to\\n    the properties of your endpoint.\\n    In addition, the deployment name must be passed as the model parameter.\\n    Example:\\n        .. code-block:: python\\n            import os\\n            os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\\n            os.environ[\"OPENAI_API_BASE\"] = \"https://<your-endpoint.openai.azure.com/\"\\n            os.environ[\"OPENAI_API_KEY\"] = \"your AzureOpenAI key\"\\n            from langchain.embeddings.openai import OpenAIEmbeddings\\n            embeddings = OpenAIEmbeddings(\\n                deployment=\"your-embeddings-deployment-name\",\\n                model=\"your-embeddings-model-name\"\\n            )\\n            text = \"This is a test query.\"\\n            query_result = embeddings.embed_query(text)\\n    \"\"\"\\n    client: Any  #: :meta private:\\n    model: str = \"text-embedding-ada-002\"\\n    deployment: str = model  # to support Azure OpenAI Service custom deployment names\\n    embedding_ctx_length: int = 8191\\n    openai_api_key: Optional[str] = None\\n    openai_organization: Optional[str] = None\\n    allowed_special: Union[Literal[\"all\"], Set[str]] = set()\\n    disallowed_special: Union[Literal[\"all\"], Set[str], Tuple[()]] = \"all\"\\n    chunk_size: int = 1000\\n    \"\"\"Maximum number of texts to embed in each batch\"\"\"\\n    max_retries: int = 6\\n    \"\"\"Maximum number of retries to make when generating.\"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        openai_api_key = get_from_dict_or_env(\\n            values, \"openai_api_key\", \"OPENAI_API_KEY\"\\n        )\\n        openai_organization = get_from_dict_or_env(\\n            values,\\n            \"openai_organization\",\\n            \"OPENAI_ORGANIZATION\",\\n            default=\"\",\\n        )\\n        try:\\n            import openai\\n            openai.api_key = openai_api_key\\n            if openai_organization:\\n                openai.organization = openai_organization\\n            values[\"client\"] = openai.Embedding\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import openai python package. \"\\n                \"Please install it with `pip install openai`.\"\\n            )\\n        return values\\n    # please refer to\\n    # https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb\\n    def _get_len_safe_embeddings(\\n        self, texts: List[str], *, engine: str, chunk_size: Optional[int] = None\\n    ) -> List[List[float]]:\\n        embeddings: List[List[float]] = [[] for i in range(len(texts))]\\n        try:\\n            import tiktoken\\n            tokens = []\\n            indices = []\\n            encoding = tiktoken.model.encoding_for_model(self.model)\\n            for i, text in enumerate(texts):\\n                # replace newlines, which can negatively affect performance.\\n                text = text.replace(\"\\\\n\", \" \")\\n                token = encoding.encode(\\n                    text,\\n                    allowed_special=self.allowed_special,\\n                    disallowed_special=self.disallowed_special,\\n                )\\n                for j in range(0, len(token), self.embedding_ctx_length):\\n                    tokens += [token[j : j + self.embedding_ctx_length]]\\n                    indices += [i]\\n            batched_embeddings = []\\n            _chunk_size = chunk_size or self.chunk_size\\n            for i in range(0, len(tokens), _chunk_size):\\n                response = embed_with_retry(\\n                    self,\\n                    input=tokens[i : i + _chunk_size],\\n                    engine=self.deployment,\\n                )\\n                batched_embeddings += [r[\"embedding\"] for r in response[\"data\"]]\\n            results: List[List[List[float]]] = [[] for i in range(len(texts))]\\n            lens: List[List[int]] = [[] for i in range(len(texts))]\\n            for i in range(len(indices)):\\n                results[indices[i]].append(batched_embeddings[i])\\n                lens[indices[i]].append(len(batched_embeddings[i]))\\n            for i in range(len(texts)):\\n                average = np.average(results[i], axis=0, weights=lens[i])\\n                embeddings[i] = (average / np.linalg.norm(average)).tolist()\\n            return embeddings\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for OpenAIEmbeddings. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n    def _embedding_func(self, text: str, *, engine: str) -> List[float]:\\n        \"\"\"Call out to OpenAI\\'s embedding endpoint.\"\"\"\\n        # handle large input text\\n        if self.embedding_ctx_length > 0:\\n            return self._get_len_safe_embeddings([text], engine=engine)[0]\\n        else:\\n            # replace newlines, which can negatively affect performance.\\n            text = text.replace(\"\\\\n\", \" \")\\n            return embed_with_retry(self, input=[text], engine=engine)[\"data\"][0][\\n                \"embedding\"\\n            ]\\n[docs]    def embed_documents(\\n        self, texts: List[str], chunk_size: Optional[int] = 0\\n    ) -> List[List[float]]:\\n        \"\"\"Call out to OpenAI\\'s embedding endpoint for embedding search docs.\\n        Args:\\n            texts: The list of texts to embed.\\n            chunk_size: The chunk size of embeddings. If None, will use the chunk size\\n                specified by the class.\\n        Returns:\\n            List of embeddings, one for each text.\\n        \"\"\"\\n        # handle batches of large input text\\n        if self.embedding_ctx_length > 0:\\n            return self._get_len_safe_embeddings(texts, engine=self.deployment)\\n        else:\\n            results = []\\n            _chunk_size = chunk_size or self.chunk_size\\n            for i in range(0, len(texts), _chunk_size):\\n                response = embed_with_retry(\\n                    self,\\n                    input=texts[i : i + _chunk_size],\\n                    engine=self.deployment,\\n                )\\n                results += [r[\"embedding\"] for r in response[\"data\"]]\\n            return results\\n[docs]    def embed_query(self, text: str) -> List[float]:\\n        \"\"\"Call out to OpenAI\\'s embedding endpoint for embedding query text.\\n        Args:\\n            text: The text to embed.\\n        Returns:\\n            Embedding for the text.\\n        \"\"\"\\n        embedding = self._embedding_func(text, engine=self.deployment)\\n        return embedding\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\embeddings\\\\openai.html'}),\n",
              " Document(page_content='Source code for langchain.embeddings.sagemaker_endpoint\\n\"\"\"Wrapper around Sagemaker InvokeEndpoint API.\"\"\"\\nfrom typing import Any, Dict, List, Optional\\nfrom pydantic import BaseModel, Extra, root_validator\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.llms.sagemaker_endpoint import ContentHandlerBase\\n[docs]class SagemakerEndpointEmbeddings(BaseModel, Embeddings):\\n    \"\"\"Wrapper around custom Sagemaker Inference Endpoints.\\n    To use, you must supply the endpoint name from your deployed\\n    Sagemaker model & the region where it is deployed.\\n    To authenticate, the AWS client uses the following methods to\\n    automatically load credentials:\\n    https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\\n    If a specific credential profile should be used, you must pass\\n    the name of the profile from the ~/.aws/credentials file that is to be used.\\n    Make sure the credentials / roles used have the required policies to\\n    access the Sagemaker endpoint.\\n    See: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\\n    \"\"\"\\n    \"\"\"\\n    Example:\\n        .. code-block:: python\\n            from langchain.embeddings import SagemakerEndpointEmbeddings\\n            endpoint_name = (\\n                \"my-endpoint-name\"\\n            )\\n            region_name = (\\n                \"us-west-2\"\\n            )\\n            credentials_profile_name = (\\n                \"default\"\\n            )\\n            se = SagemakerEndpointEmbeddings(\\n                endpoint_name=endpoint_name,\\n                region_name=region_name,\\n                credentials_profile_name=credentials_profile_name\\n            )\\n    \"\"\"\\n    client: Any  #: :meta private:\\n    endpoint_name: str = \"\"\\n    \"\"\"The name of the endpoint from the deployed Sagemaker model.\\n    Must be unique within an AWS Region.\"\"\"\\n    region_name: str = \"\"\\n    \"\"\"The aws region where the Sagemaker model is deployed, eg. `us-west-2`.\"\"\"\\n    credentials_profile_name: Optional[str] = None\\n    \"\"\"The name of the profile in the ~/.aws/credentials or ~/.aws/config files, which\\n    has either access keys or role information specified.\\n    If not specified, the default credential profile or, if on an EC2 instance,\\n    credentials from IMDS will be used.\\n    See: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\\n    \"\"\"\\n    content_handler: ContentHandlerBase\\n    \"\"\"The content handler class that provides an input and\\n    output transform functions to handle formats between LLM\\n    and the endpoint.\\n    \"\"\"\\n    \"\"\"\\n     Example:\\n        .. code-block:: python\\n        \\n        from langchain.llms.sagemaker_endpoint import ContentHandlerBase\\n        class ContentHandler(ContentHandlerBase):\\n                content_type = \"application/json\"\\n                accepts = \"application/json\"\\n                def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\\n                    input_str = json.dumps({prompt: prompt, **model_kwargs})\\n                    return input_str.encode(\\'utf-8\\')\\n                \\n                def transform_output(self, output: bytes) -> str:\\n                    response_json = json.loads(output.read().decode(\"utf-8\"))\\n                    return response_json[0][\"generated_text\"]\\n    \"\"\"\\n    model_kwargs: Optional[Dict] = None\\n    \"\"\"Key word arguments to pass to the model.\"\"\"\\n    endpoint_kwargs: Optional[Dict] = None\\n    \"\"\"Optional attributes passed to the invoke_endpoint\\n    function. See `boto3`_. docs for more info.\\n    .. _boto3: <https://boto3.amazonaws.com/v1/documentation/api/latest/index.html>\\n    \"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that AWS credentials to and python package exists in environment.\"\"\"\\n        try:\\n            import boto3\\n            try:\\n                if values[\"credentials_profile_name\"] is not None:\\n                    session = boto3.Session(\\n                        profile_name=values[\"credentials_profile_name\"]\\n                    )\\n                else:\\n                    # use default credentials\\n                    session = boto3.Session()\\n                values[\"client\"] = session.client(\\n                    \"sagemaker-runtime\", region_name=values[\"region_name\"]\\n                )\\n            except Exception as e:\\n                raise ValueError(\\n                    \"Could not load credentials to authenticate with AWS client. \"\\n                    \"Please check that credentials in the specified \"\\n                    \"profile name are valid.\"\\n                ) from e\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import boto3 python package. \"\\n                \"Please install it with `pip install boto3`.\"\\n            )\\n        return values\\n    def _embedding_func(self, texts: List[str]) -> List[float]:\\n        \"\"\"Call out to SageMaker Inference embedding endpoint.\"\"\"\\n        # replace newlines, which can negatively affect performance.\\n        texts = list(map(lambda x: x.replace(\"\\\\n\", \" \"), texts))\\n        _model_kwargs = self.model_kwargs or {}\\n        _endpoint_kwargs = self.endpoint_kwargs or {}\\n        body = self.content_handler.transform_input(texts, _model_kwargs)\\n        content_type = self.content_handler.content_type\\n        accepts = self.content_handler.accepts\\n        # send request\\n        try:\\n            response = self.client.invoke_endpoint(\\n                EndpointName=self.endpoint_name,\\n                Body=body,\\n                ContentType=content_type,\\n                Accept=accepts,\\n                **_endpoint_kwargs,\\n            )\\n        except Exception as e:\\n            raise ValueError(f\"Error raised by inference endpoint: {e}\")\\n        return self.content_handler.transform_output(response[\"Body\"])\\n[docs]    def embed_documents(\\n        self, texts: List[str], chunk_size: int = 64\\n    ) -> List[List[float]]:\\n        \"\"\"Compute doc embeddings using a SageMaker Inference Endpoint.\\n        Args:\\n            texts: The list of texts to embed.\\n            chunk_size: The chunk size defines how many input texts will\\n                be grouped together as request. If None, will use the\\n                chunk size specified by the class.\\n        Returns:\\n            List of embeddings, one for each text.\\n        \"\"\"\\n        results = []\\n        _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\\n        for i in range(0, len(texts), _chunk_size):\\n            response = self._embedding_func(texts[i : i + _chunk_size])\\n            results.append(response)\\n        return results\\n[docs]    def embed_query(self, text: str) -> List[float]:\\n        \"\"\"Compute query embeddings using a SageMaker inference endpoint.\\n        Args:\\n            text: The text to embed.\\n        Returns:\\n            Embeddings for the text.\\n        \"\"\"\\n        return self._embedding_func([text])\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\embeddings\\\\sagemaker_endpoint.html'}),\n",
              " Document(page_content='Source code for langchain.embeddings.self_hosted\\n\"\"\"Running custom embedding models on self-hosted remote hardware.\"\"\"\\nfrom typing import Any, Callable, List\\nfrom pydantic import Extra\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.llms import SelfHostedPipeline\\ndef _embed_documents(pipeline: Any, *args: Any, **kwargs: Any) -> List[List[float]]:\\n    \"\"\"Inference function to send to the remote hardware.\\n    Accepts a sentence_transformer model_id and\\n    returns a list of embeddings for each document in the batch.\\n    \"\"\"\\n    return pipeline(*args, **kwargs)\\n[docs]class SelfHostedEmbeddings(SelfHostedPipeline, Embeddings):\\n    \"\"\"Runs custom embedding models on self-hosted remote hardware.\\n    Supported hardware includes auto-launched instances on AWS, GCP, Azure,\\n    and Lambda, as well as servers specified\\n    by IP address and SSH credentials (such as on-prem, or another\\n    cloud like Paperspace, Coreweave, etc.).\\n    To use, you should have the ``runhouse`` python package installed.\\n    Example using a model load function:\\n        .. code-block:: python\\n            from langchain.embeddings import SelfHostedEmbeddings\\n            from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\\n            import runhouse as rh\\n            gpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\")\\n            def get_pipeline():\\n                model_id = \"facebook/bart-large\"\\n                tokenizer = AutoTokenizer.from_pretrained(model_id)\\n                model = AutoModelForCausalLM.from_pretrained(model_id)\\n                return pipeline(\"feature-extraction\", model=model, tokenizer=tokenizer)\\n            embeddings = SelfHostedEmbeddings(\\n                model_load_fn=get_pipeline,\\n                hardware=gpu\\n                model_reqs=[\"./\", \"torch\", \"transformers\"],\\n            )\\n    Example passing in a pipeline path:\\n        .. code-block:: python\\n            from langchain.embeddings import SelfHostedHFEmbeddings\\n            import runhouse as rh\\n            from transformers import pipeline\\n            gpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\")\\n            pipeline = pipeline(model=\"bert-base-uncased\", task=\"feature-extraction\")\\n            rh.blob(pickle.dumps(pipeline),\\n                path=\"models/pipeline.pkl\").save().to(gpu, path=\"models\")\\n            embeddings = SelfHostedHFEmbeddings.from_pipeline(\\n                pipeline=\"models/pipeline.pkl\",\\n                hardware=gpu,\\n                model_reqs=[\"./\", \"torch\", \"transformers\"],\\n            )\\n    \"\"\"\\n    inference_fn: Callable = _embed_documents\\n    \"\"\"Inference function to extract the embeddings on the remote hardware.\"\"\"\\n    inference_kwargs: Any = None\\n    \"\"\"Any kwargs to pass to the model\\'s inference function.\"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n[docs]    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        \"\"\"Compute doc embeddings using a HuggingFace transformer model.\\n        Args:\\n            texts: The list of texts to embed.s\\n        Returns:\\n            List of embeddings, one for each text.\\n        \"\"\"\\n        texts = list(map(lambda x: x.replace(\"\\\\n\", \" \"), texts))\\n        embeddings = self.client(self.pipeline_ref, texts)\\n        if not isinstance(embeddings, list):\\n            return embeddings.tolist()\\n        return embeddings\\n[docs]    def embed_query(self, text: str) -> List[float]:\\n        \"\"\"Compute query embeddings using a HuggingFace transformer model.\\n        Args:\\n            text: The text to embed.\\n        Returns:\\n            Embeddings for the text.\\n        \"\"\"\\n        text = text.replace(\"\\\\n\", \" \")\\n        embeddings = self.client(self.pipeline_ref, text)\\n        if not isinstance(embeddings, list):\\n            return embeddings.tolist()\\n        return embeddings\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\embeddings\\\\self_hosted.html'}),\n",
              " Document(page_content='Source code for langchain.embeddings.self_hosted_hugging_face\\n\"\"\"Wrapper around HuggingFace embedding models for self-hosted remote hardware.\"\"\"\\nimport importlib\\nimport logging\\nfrom typing import Any, Callable, List, Optional\\nfrom langchain.embeddings.self_hosted import SelfHostedEmbeddings\\nDEFAULT_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\\nDEFAULT_INSTRUCT_MODEL = \"hkunlp/instructor-large\"\\nDEFAULT_EMBED_INSTRUCTION = \"Represent the document for retrieval: \"\\nDEFAULT_QUERY_INSTRUCTION = (\\n    \"Represent the question for retrieving supporting documents: \"\\n)\\nlogger = logging.getLogger(__name__)\\ndef _embed_documents(client: Any, *args: Any, **kwargs: Any) -> List[List[float]]:\\n    \"\"\"Inference function to send to the remote hardware.\\n    Accepts a sentence_transformer model_id and\\n    returns a list of embeddings for each document in the batch.\\n    \"\"\"\\n    return client.encode(*args, **kwargs)\\ndef load_embedding_model(model_id: str, instruct: bool = False, device: int = 0) -> Any:\\n    \"\"\"Load the embedding model.\"\"\"\\n    if not instruct:\\n        import sentence_transformers\\n        client = sentence_transformers.SentenceTransformer(model_id)\\n    else:\\n        from InstructorEmbedding import INSTRUCTOR\\n        client = INSTRUCTOR(model_id)\\n    if importlib.util.find_spec(\"torch\") is not None:\\n        import torch\\n        cuda_device_count = torch.cuda.device_count()\\n        if device < -1 or (device >= cuda_device_count):\\n            raise ValueError(\\n                f\"Got device=={device}, \"\\n                f\"device is required to be within [-1, {cuda_device_count})\"\\n            )\\n        if device < 0 and cuda_device_count > 0:\\n            logger.warning(\\n                \"Device has %d GPUs available. \"\\n                \"Provide device={deviceId} to `from_model_id` to use available\"\\n                \"GPUs for execution. deviceId is -1 for CPU and \"\\n                \"can be a positive integer associated with CUDA device id.\",\\n                cuda_device_count,\\n            )\\n        client = client.to(device)\\n    return client\\n[docs]class SelfHostedHuggingFaceEmbeddings(SelfHostedEmbeddings):\\n    \"\"\"Runs sentence_transformers embedding models on self-hosted remote hardware.\\n    Supported hardware includes auto-launched instances on AWS, GCP, Azure,\\n    and Lambda, as well as servers specified\\n    by IP address and SSH credentials (such as on-prem, or another cloud\\n    like Paperspace, Coreweave, etc.).\\n    To use, you should have the ``runhouse`` python package installed.\\n    Example:\\n        .. code-block:: python\\n            from langchain.embeddings import SelfHostedHuggingFaceEmbeddings\\n            import runhouse as rh\\n            model_name = \"sentence-transformers/all-mpnet-base-v2\"\\n            gpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\")\\n            hf = SelfHostedHuggingFaceEmbeddings(model_name=model_name, hardware=gpu)\\n    \"\"\"\\n    client: Any  #: :meta private:\\n    model_id: str = DEFAULT_MODEL_NAME\\n    \"\"\"Model name to use.\"\"\"\\n    model_reqs: List[str] = [\"./\", \"sentence_transformers\", \"torch\"]\\n    \"\"\"Requirements to install on hardware to inference the model.\"\"\"\\n    hardware: Any\\n    \"\"\"Remote hardware to send the inference function to.\"\"\"\\n    model_load_fn: Callable = load_embedding_model\\n    \"\"\"Function to load the model remotely on the server.\"\"\"\\n    load_fn_kwargs: Optional[dict] = None\\n    \"\"\"Key word arguments to pass to the model load function.\"\"\"\\n    inference_fn: Callable = _embed_documents\\n    \"\"\"Inference function to extract the embeddings.\"\"\"\\n    def __init__(self, **kwargs: Any):\\n        \"\"\"Initialize the remote inference function.\"\"\"\\n        load_fn_kwargs = kwargs.pop(\"load_fn_kwargs\", {})\\n        load_fn_kwargs[\"model_id\"] = load_fn_kwargs.get(\"model_id\", DEFAULT_MODEL_NAME)\\n        load_fn_kwargs[\"instruct\"] = load_fn_kwargs.get(\"instruct\", False)\\n        load_fn_kwargs[\"device\"] = load_fn_kwargs.get(\"device\", 0)\\n        super().__init__(load_fn_kwargs=load_fn_kwargs, **kwargs)\\n[docs]class SelfHostedHuggingFaceInstructEmbeddings(SelfHostedHuggingFaceEmbeddings):\\n    \"\"\"Runs InstructorEmbedding embedding models on self-hosted remote hardware.\\n    Supported hardware includes auto-launched instances on AWS, GCP, Azure,\\n    and Lambda, as well as servers specified\\n    by IP address and SSH credentials (such as on-prem, or another\\n    cloud like Paperspace, Coreweave, etc.).\\n    To use, you should have the ``runhouse`` python package installed.\\n    Example:\\n        .. code-block:: python\\n            from langchain.embeddings import SelfHostedHuggingFaceInstructEmbeddings\\n            import runhouse as rh\\n            model_name = \"hkunlp/instructor-large\"\\n            gpu = rh.cluster(name=\\'rh-a10x\\', instance_type=\\'A100:1\\')\\n            hf = SelfHostedHuggingFaceInstructEmbeddings(\\n                model_name=model_name, hardware=gpu)\\n    \"\"\"\\n    model_id: str = DEFAULT_INSTRUCT_MODEL\\n    \"\"\"Model name to use.\"\"\"\\n    embed_instruction: str = DEFAULT_EMBED_INSTRUCTION\\n    \"\"\"Instruction to use for embedding documents.\"\"\"\\n    query_instruction: str = DEFAULT_QUERY_INSTRUCTION\\n    \"\"\"Instruction to use for embedding query.\"\"\"\\n    model_reqs: List[str] = [\"./\", \"InstructorEmbedding\", \"torch\"]\\n    \"\"\"Requirements to install on hardware to inference the model.\"\"\"\\n    def __init__(self, **kwargs: Any):\\n        \"\"\"Initialize the remote inference function.\"\"\"\\n        load_fn_kwargs = kwargs.pop(\"load_fn_kwargs\", {})\\n        load_fn_kwargs[\"model_id\"] = load_fn_kwargs.get(\\n            \"model_id\", DEFAULT_INSTRUCT_MODEL\\n        )\\n        load_fn_kwargs[\"instruct\"] = load_fn_kwargs.get(\"instruct\", True)\\n        load_fn_kwargs[\"device\"] = load_fn_kwargs.get(\"device\", 0)\\n        super().__init__(load_fn_kwargs=load_fn_kwargs, **kwargs)\\n[docs]    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        \"\"\"Compute doc embeddings using a HuggingFace instruct model.\\n        Args:\\n            texts: The list of texts to embed.\\n        Returns:\\n            List of embeddings, one for each text.\\n        \"\"\"\\n        instruction_pairs = []\\n        for text in texts:\\n            instruction_pairs.append([self.embed_instruction, text])\\n        embeddings = self.client(self.pipeline_ref, instruction_pairs)\\n        return embeddings.tolist()\\n[docs]    def embed_query(self, text: str) -> List[float]:\\n        \"\"\"Compute query embeddings using a HuggingFace instruct model.\\n        Args:\\n            text: The text to embed.\\n        Returns:\\n            Embeddings for the text.\\n        \"\"\"\\n        instruction_pair = [self.query_instruction, text]\\n        embedding = self.client(self.pipeline_ref, [instruction_pair])[0]\\n        return embedding.tolist()\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\embeddings\\\\self_hosted_hugging_face.html'}),\n",
              " Document(page_content='Source code for langchain.embeddings.tensorflow_hub\\n\"\"\"Wrapper around TensorflowHub embedding models.\"\"\"\\nfrom typing import Any, List\\nfrom pydantic import BaseModel, Extra\\nfrom langchain.embeddings.base import Embeddings\\nDEFAULT_MODEL_URL = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\\n[docs]class TensorflowHubEmbeddings(BaseModel, Embeddings):\\n    \"\"\"Wrapper around tensorflow_hub embedding models.\\n    To use, you should have the ``tensorflow_text`` python package installed.\\n    Example:\\n        .. code-block:: python\\n            from langchain.embeddings import TensorflowHubEmbeddings\\n            url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\\n            tf = TensorflowHubEmbeddings(model_url=url)\\n    \"\"\"\\n    embed: Any  #: :meta private:\\n    model_url: str = DEFAULT_MODEL_URL\\n    \"\"\"Model name to use.\"\"\"\\n    def __init__(self, **kwargs: Any):\\n        \"\"\"Initialize the tensorflow_hub and tensorflow_text.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tensorflow_hub\\n            import tensorflow_text  # noqa\\n            self.embed = tensorflow_hub.load(self.model_url)\\n        except ImportError as e:\\n            raise ValueError(\\n                \"Could not import some python packages.\" \"Please install them.\"\\n            ) from e\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n[docs]    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        \"\"\"Compute doc embeddings using a TensorflowHub embedding model.\\n        Args:\\n            texts: The list of texts to embed.\\n        Returns:\\n            List of embeddings, one for each text.\\n        \"\"\"\\n        texts = list(map(lambda x: x.replace(\"\\\\n\", \" \"), texts))\\n        embeddings = self.embed(texts).numpy()\\n        return embeddings.tolist()\\n[docs]    def embed_query(self, text: str) -> List[float]:\\n        \"\"\"Compute query embeddings using a TensorflowHub embedding model.\\n        Args:\\n            text: The text to embed.\\n        Returns:\\n            Embeddings for the text.\\n        \"\"\"\\n        text = text.replace(\"\\\\n\", \" \")\\n        embedding = self.embed(text).numpy()[0]\\n        return embedding.tolist()\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\embeddings\\\\tensorflow_hub.html'}),\n",
              " Document(page_content='Source code for langchain.llms.ai21\\n\"\"\"Wrapper around AI21 APIs.\"\"\"\\nfrom typing import Any, Dict, List, Optional\\nimport requests\\nfrom pydantic import BaseModel, Extra, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.utils import get_from_dict_or_env\\nclass AI21PenaltyData(BaseModel):\\n    \"\"\"Parameters for AI21 penalty data.\"\"\"\\n    scale: int = 0\\n    applyToWhitespaces: bool = True\\n    applyToPunctuations: bool = True\\n    applyToNumbers: bool = True\\n    applyToStopwords: bool = True\\n    applyToEmojis: bool = True\\n[docs]class AI21(LLM):\\n    \"\"\"Wrapper around AI21 large language models.\\n    To use, you should have the environment variable ``AI21_API_KEY``\\n    set with your API key.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import AI21\\n            ai21 = AI21(model=\"j2-jumbo-instruct\")\\n    \"\"\"\\n    model: str = \"j2-jumbo-instruct\"\\n    \"\"\"Model name to use.\"\"\"\\n    temperature: float = 0.7\\n    \"\"\"What sampling temperature to use.\"\"\"\\n    maxTokens: int = 256\\n    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\\n    minTokens: int = 0\\n    \"\"\"The minimum number of tokens to generate in the completion.\"\"\"\\n    topP: float = 1.0\\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\\n    presencePenalty: AI21PenaltyData = AI21PenaltyData()\\n    \"\"\"Penalizes repeated tokens.\"\"\"\\n    countPenalty: AI21PenaltyData = AI21PenaltyData()\\n    \"\"\"Penalizes repeated tokens according to count.\"\"\"\\n    frequencyPenalty: AI21PenaltyData = AI21PenaltyData()\\n    \"\"\"Penalizes repeated tokens according to frequency.\"\"\"\\n    numResults: int = 1\\n    \"\"\"How many completions to generate for each prompt.\"\"\"\\n    logitBias: Optional[Dict[str, float]] = None\\n    \"\"\"Adjust the probability of specific tokens being generated.\"\"\"\\n    ai21_api_key: Optional[str] = None\\n    stop: Optional[List[str]] = None\\n    base_url: Optional[str] = None\\n    \"\"\"Base url to use, if None decides based on model name.\"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key exists in environment.\"\"\"\\n        ai21_api_key = get_from_dict_or_env(values, \"ai21_api_key\", \"AI21_API_KEY\")\\n        values[\"ai21_api_key\"] = ai21_api_key\\n        return values\\n    @property\\n    def _default_params(self) -> Dict[str, Any]:\\n        \"\"\"Get the default parameters for calling AI21 API.\"\"\"\\n        return {\\n            \"temperature\": self.temperature,\\n            \"maxTokens\": self.maxTokens,\\n            \"minTokens\": self.minTokens,\\n            \"topP\": self.topP,\\n            \"presencePenalty\": self.presencePenalty.dict(),\\n            \"countPenalty\": self.countPenalty.dict(),\\n            \"frequencyPenalty\": self.frequencyPenalty.dict(),\\n            \"numResults\": self.numResults,\\n            \"logitBias\": self.logitBias,\\n        }\\n    @property\\n    def _identifying_params(self) -> Dict[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {**{\"model\": self.model}, **self._default_params}\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"ai21\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call out to AI21\\'s complete endpoint.\\n        Args:\\n            prompt: The prompt to pass into the model.\\n            stop: Optional list of stop words to use when generating.\\n        Returns:\\n            The string generated by the model.\\n        Example:\\n            .. code-block:: python\\n                response = ai21(\"Tell me a joke.\")\\n        \"\"\"\\n        if self.stop is not None and stop is not None:\\n            raise ValueError(\"`stop` found in both the input and default params.\")\\n        elif self.stop is not None:\\n            stop = self.stop\\n        elif stop is None:\\n            stop = []\\n        if self.base_url is not None:\\n            base_url = self.base_url\\n        else:\\n            if self.model in (\"j1-grande-instruct\",):\\n                base_url = \"https://api.ai21.com/studio/v1/experimental\"\\n            else:\\n                base_url = \"https://api.ai21.com/studio/v1\"\\n        response = requests.post(\\n            url=f\"{base_url}/{self.model}/complete\",\\n            headers={\"Authorization\": f\"Bearer {self.ai21_api_key}\"},\\n            json={\"prompt\": prompt, \"stopSequences\": stop, **self._default_params},\\n        )\\n        if response.status_code != 200:\\n            optional_detail = response.json().get(\"error\")\\n            raise ValueError(\\n                f\"AI21 /complete call failed with status code {response.status_code}.\"\\n                f\" Details: {optional_detail}\"\\n            )\\n        response_json = response.json()\\n        return response_json[\"completions\"][0][\"data\"][\"text\"]\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\ai21.html'}),\n",
              " Document(page_content='Source code for langchain.llms.aleph_alpha\\n\"\"\"Wrapper around Aleph Alpha APIs.\"\"\"\\nfrom typing import Any, Dict, List, Optional, Sequence\\nfrom pydantic import Extra, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.llms.utils import enforce_stop_tokens\\nfrom langchain.utils import get_from_dict_or_env\\n[docs]class AlephAlpha(LLM):\\n    \"\"\"Wrapper around Aleph Alpha large language models.\\n    To use, you should have the ``aleph_alpha_client`` python package installed, and the\\n    environment variable ``ALEPH_ALPHA_API_KEY`` set with your API key, or pass\\n    it as a named parameter to the constructor.\\n    Parameters are explained more in depth here:\\n    https://github.com/Aleph-Alpha/aleph-alpha-client/blob/c14b7dd2b4325c7da0d6a119f6e76385800e097b/aleph_alpha_client/completion.py#L10\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import AlephAlpha\\n            alpeh_alpha = AlephAlpha(aleph_alpha_api_key=\"my-api-key\")\\n    \"\"\"\\n    client: Any  #: :meta private:\\n    model: Optional[str] = \"luminous-base\"\\n    \"\"\"Model name to use.\"\"\"\\n    maximum_tokens: int = 64\\n    \"\"\"The maximum number of tokens to be generated.\"\"\"\\n    temperature: float = 0.0\\n    \"\"\"A non-negative float that tunes the degree of randomness in generation.\"\"\"\\n    top_k: int = 0\\n    \"\"\"Number of most likely tokens to consider at each step.\"\"\"\\n    top_p: float = 0.0\\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\\n    presence_penalty: float = 0.0\\n    \"\"\"Penalizes repeated tokens.\"\"\"\\n    frequency_penalty: float = 0.0\\n    \"\"\"Penalizes repeated tokens according to frequency.\"\"\"\\n    repetition_penalties_include_prompt: Optional[bool] = False\\n    \"\"\"Flag deciding whether presence penalty or frequency penalty are\\n    updated from the prompt.\"\"\"\\n    use_multiplicative_presence_penalty: Optional[bool] = False\\n    \"\"\"Flag deciding whether presence penalty is applied\\n    multiplicatively (True) or additively (False).\"\"\"\\n    penalty_bias: Optional[str] = None\\n    \"\"\"Penalty bias for the completion.\"\"\"\\n    penalty_exceptions: Optional[List[str]] = None\\n    \"\"\"List of strings that may be generated without penalty,\\n    regardless of other penalty settings\"\"\"\\n    penalty_exceptions_include_stop_sequences: Optional[bool] = None\\n    \"\"\"Should stop_sequences be included in penalty_exceptions.\"\"\"\\n    best_of: Optional[int] = None\\n    \"\"\"returns the one with the \"best of\" results\\n    (highest log probability per token)\\n    \"\"\"\\n    n: int = 1\\n    \"\"\"How many completions to generate for each prompt.\"\"\"\\n    logit_bias: Optional[Dict[int, float]] = None\\n    \"\"\"The logit bias allows to influence the likelihood of generating tokens.\"\"\"\\n    log_probs: Optional[int] = None\\n    \"\"\"Number of top log probabilities to be returned for each generated token.\"\"\"\\n    tokens: Optional[bool] = False\\n    \"\"\"return tokens of completion.\"\"\"\\n    disable_optimizations: Optional[bool] = False\\n    minimum_tokens: Optional[int] = 0\\n    \"\"\"Generate at least this number of tokens.\"\"\"\\n    echo: bool = False\\n    \"\"\"Echo the prompt in the completion.\"\"\"\\n    use_multiplicative_frequency_penalty: bool = False\\n    sequence_penalty: float = 0.0\\n    sequence_penalty_min_length: int = 2\\n    use_multiplicative_sequence_penalty: bool = False\\n    completion_bias_inclusion: Optional[Sequence[str]] = None\\n    completion_bias_inclusion_first_token_only: bool = False\\n    completion_bias_exclusion: Optional[Sequence[str]] = None\\n    completion_bias_exclusion_first_token_only: bool = False\\n    \"\"\"Only consider the first token for the completion_bias_exclusion.\"\"\"\\n    contextual_control_threshold: Optional[float] = None\\n    \"\"\"If set to None, attention control parameters only apply to those tokens that have\\n    explicitly been set in the request.\\n    If set to a non-None value, control parameters are also applied to similar tokens.\\n    \"\"\"\\n    control_log_additive: Optional[bool] = True\\n    \"\"\"True: apply control by adding the log(control_factor) to attention scores.\\n    False: (attention_scores - - attention_scores.min(-1)) * control_factor\\n    \"\"\"\\n    repetition_penalties_include_completion: bool = True\\n    \"\"\"Flag deciding whether presence penalty or frequency penalty\\n    are updated from the completion.\"\"\"\\n    raw_completion: bool = False\\n    \"\"\"Force the raw completion of the model to be returned.\"\"\"\\n    aleph_alpha_api_key: Optional[str] = None\\n    \"\"\"API key for Aleph Alpha API.\"\"\"\\n    stop_sequences: Optional[List[str]] = None\\n    \"\"\"Stop sequences to use.\"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        aleph_alpha_api_key = get_from_dict_or_env(\\n            values, \"aleph_alpha_api_key\", \"ALEPH_ALPHA_API_KEY\"\\n        )\\n        try:\\n            import aleph_alpha_client\\n            values[\"client\"] = aleph_alpha_client.Client(token=aleph_alpha_api_key)\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import aleph_alpha_client python package. \"\\n                \"Please install it with `pip install aleph_alpha_client`.\"\\n            )\\n        return values\\n    @property\\n    def _default_params(self) -> Dict[str, Any]:\\n        \"\"\"Get the default parameters for calling the Aleph Alpha API.\"\"\"\\n        return {\\n            \"maximum_tokens\": self.maximum_tokens,\\n            \"temperature\": self.temperature,\\n            \"top_k\": self.top_k,\\n            \"top_p\": self.top_p,\\n            \"presence_penalty\": self.presence_penalty,\\n            \"frequency_penalty\": self.frequency_penalty,\\n            \"n\": self.n,\\n            \"repetition_penalties_include_prompt\": self.repetition_penalties_include_prompt,  # noqa: E501\\n            \"use_multiplicative_presence_penalty\": self.use_multiplicative_presence_penalty,  # noqa: E501\\n            \"penalty_bias\": self.penalty_bias,\\n            \"penalty_exceptions\": self.penalty_exceptions,\\n            \"penalty_exceptions_include_stop_sequences\": self.penalty_exceptions_include_stop_sequences,  # noqa: E501\\n            \"best_of\": self.best_of,\\n            \"logit_bias\": self.logit_bias,\\n            \"log_probs\": self.log_probs,\\n            \"tokens\": self.tokens,\\n            \"disable_optimizations\": self.disable_optimizations,\\n            \"minimum_tokens\": self.minimum_tokens,\\n            \"echo\": self.echo,\\n            \"use_multiplicative_frequency_penalty\": self.use_multiplicative_frequency_penalty,  # noqa: E501\\n            \"sequence_penalty\": self.sequence_penalty,\\n            \"sequence_penalty_min_length\": self.sequence_penalty_min_length,\\n            \"use_multiplicative_sequence_penalty\": self.use_multiplicative_sequence_penalty,  # noqa: E501\\n            \"completion_bias_inclusion\": self.completion_bias_inclusion,\\n            \"completion_bias_inclusion_first_token_only\": self.completion_bias_inclusion_first_token_only,  # noqa: E501\\n            \"completion_bias_exclusion\": self.completion_bias_exclusion,\\n            \"completion_bias_exclusion_first_token_only\": self.completion_bias_exclusion_first_token_only,  # noqa: E501\\n            \"contextual_control_threshold\": self.contextual_control_threshold,\\n            \"control_log_additive\": self.control_log_additive,\\n            \"repetition_penalties_include_completion\": self.repetition_penalties_include_completion,  # noqa: E501\\n            \"raw_completion\": self.raw_completion,\\n        }\\n    @property\\n    def _identifying_params(self) -> Dict[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {**{\"model\": self.model}, **self._default_params}\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"alpeh_alpha\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call out to Aleph Alpha\\'s completion endpoint.\\n        Args:\\n            prompt: The prompt to pass into the model.\\n            stop: Optional list of stop words to use when generating.\\n        Returns:\\n            The string generated by the model.\\n        Example:\\n            .. code-block:: python\\n                response = alpeh_alpha(\"Tell me a joke.\")\\n        \"\"\"\\n        from aleph_alpha_client import CompletionRequest, Prompt\\n        params = self._default_params\\n        if self.stop_sequences is not None and stop is not None:\\n            raise ValueError(\\n                \"stop sequences found in both the input and default params.\"\\n            )\\n        elif self.stop_sequences is not None:\\n            params[\"stop_sequences\"] = self.stop_sequences\\n        else:\\n            params[\"stop_sequences\"] = stop\\n        request = CompletionRequest(prompt=Prompt.from_text(prompt), **params)\\n        response = self.client.complete(model=self.model, request=request)\\n        text = response.completions[0].completion\\n        # If stop tokens are provided, Aleph Alpha\\'s endpoint returns them.\\n        # In order to make this consistent with other endpoints, we strip them.\\n        if stop is not None or self.stop_sequences is not None:\\n            text = enforce_stop_tokens(text, params[\"stop_sequences\"])\\n        return text\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\aleph_alpha.html'}),\n",
              " Document(page_content='Source code for langchain.llms.anthropic\\n\"\"\"Wrapper around Anthropic APIs.\"\"\"\\nimport re\\nfrom typing import Any, Callable, Dict, Generator, List, Mapping, Optional\\nfrom pydantic import BaseModel, Extra, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.utils import get_from_dict_or_env\\nclass _AnthropicCommon(BaseModel):\\n    client: Any = None  #: :meta private:\\n    model: str = \"claude-v1\"\\n    \"\"\"Model name to use.\"\"\"\\n    max_tokens_to_sample: int = 256\\n    \"\"\"Denotes the number of tokens to predict per generation.\"\"\"\\n    temperature: Optional[float] = None\\n    \"\"\"A non-negative float that tunes the degree of randomness in generation.\"\"\"\\n    top_k: Optional[int] = None\\n    \"\"\"Number of most likely tokens to consider at each step.\"\"\"\\n    top_p: Optional[float] = None\\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\\n    streaming: bool = False\\n    \"\"\"Whether to stream the results.\"\"\"\\n    anthropic_api_key: Optional[str] = None\\n    HUMAN_PROMPT: Optional[str] = None\\n    AI_PROMPT: Optional[str] = None\\n    count_tokens: Optional[Callable[[str], int]] = None\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        anthropic_api_key = get_from_dict_or_env(\\n            values, \"anthropic_api_key\", \"ANTHROPIC_API_KEY\"\\n        )\\n        try:\\n            import anthropic\\n            values[\"client\"] = anthropic.Client(anthropic_api_key)\\n            values[\"HUMAN_PROMPT\"] = anthropic.HUMAN_PROMPT\\n            values[\"AI_PROMPT\"] = anthropic.AI_PROMPT\\n            values[\"count_tokens\"] = anthropic.count_tokens\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import anthropic python package. \"\\n                \"Please it install it with `pip install anthropic`.\"\\n            )\\n        return values\\n    @property\\n    def _default_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the default parameters for calling Anthropic API.\"\"\"\\n        d = {\\n            \"max_tokens_to_sample\": self.max_tokens_to_sample,\\n            \"model\": self.model,\\n        }\\n        if self.temperature is not None:\\n            d[\"temperature\"] = self.temperature\\n        if self.top_k is not None:\\n            d[\"top_k\"] = self.top_k\\n        if self.top_p is not None:\\n            d[\"top_p\"] = self.top_p\\n        return d\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {**{}, **self._default_params}\\n    def _get_anthropic_stop(self, stop: Optional[List[str]] = None) -> List[str]:\\n        if not self.HUMAN_PROMPT or not self.AI_PROMPT:\\n            raise NameError(\"Please ensure the anthropic package is loaded\")\\n        if stop is None:\\n            stop = []\\n        # Never want model to invent new turns of Human / Assistant dialog.\\n        stop.extend([self.HUMAN_PROMPT])\\n        return stop\\n    def get_num_tokens(self, text: str) -> int:\\n        \"\"\"Calculate number of tokens.\"\"\"\\n        if not self.count_tokens:\\n            raise NameError(\"Please ensure the anthropic package is loaded\")\\n        return self.count_tokens(text)\\n[docs]class Anthropic(LLM, _AnthropicCommon):\\n    r\"\"\"Wrapper around Anthropic\\'s large language models.\\n    To use, you should have the ``anthropic`` python package installed, and the\\n    environment variable ``ANTHROPIC_API_KEY`` set with your API key, or pass\\n    it as a named parameter to the constructor.\\n    Example:\\n        .. code-block:: python\\n            import anthropic\\n            from langchain.llms import Anthropic\\n            model = Anthropic(model=\"<model_name>\", anthropic_api_key=\"my-api-key\")\\n            # Simplest invocation, automatically wrapped with HUMAN_PROMPT\\n            # and AI_PROMPT.\\n            response = model(\"What are the biggest risks facing humanity?\")\\n            # Or if you want to use the chat mode, build a few-shot-prompt, or\\n            # put words in the Assistant\\'s mouth, use HUMAN_PROMPT and AI_PROMPT:\\n            raw_prompt = \"What are the biggest risks facing humanity?\"\\n            prompt = f\"{anthropic.HUMAN_PROMPT} {prompt}{anthropic.AI_PROMPT}\"\\n            response = model(prompt)\\n    \"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"anthropic-llm\"\\n    def _wrap_prompt(self, prompt: str) -> str:\\n        if not self.HUMAN_PROMPT or not self.AI_PROMPT:\\n            raise NameError(\"Please ensure the anthropic package is loaded\")\\n        if prompt.startswith(self.HUMAN_PROMPT):\\n            return prompt  # Already wrapped.\\n        # Guard against common errors in specifying wrong number of newlines.\\n        corrected_prompt, n_subs = re.subn(r\"^\\\\n*Human:\", self.HUMAN_PROMPT, prompt)\\n        if n_subs == 1:\\n            return corrected_prompt\\n        # As a last resort, wrap the prompt ourselves to emulate instruct-style.\\n        return f\"{self.HUMAN_PROMPT} {prompt}{self.AI_PROMPT} Sure, here you go:\\\\n\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        r\"\"\"Call out to Anthropic\\'s completion endpoint.\\n        Args:\\n            prompt: The prompt to pass into the model.\\n            stop: Optional list of stop words to use when generating.\\n        Returns:\\n            The string generated by the model.\\n        Example:\\n            .. code-block:: python\\n                prompt = \"What are the biggest risks facing humanity?\"\\n                prompt = f\"\\\\n\\\\nHuman: {prompt}\\\\n\\\\nAssistant:\"\\n                response = model(prompt)\\n        \"\"\"\\n        stop = self._get_anthropic_stop(stop)\\n        if self.streaming:\\n            stream_resp = self.client.completion_stream(\\n                prompt=self._wrap_prompt(prompt),\\n                stop_sequences=stop,\\n                **self._default_params,\\n            )\\n            current_completion = \"\"\\n            for data in stream_resp:\\n                delta = data[\"completion\"][len(current_completion) :]\\n                current_completion = data[\"completion\"]\\n                self.callback_manager.on_llm_new_token(\\n                    delta, verbose=self.verbose, **data\\n                )\\n            return current_completion\\n        response = self.client.completion(\\n            prompt=self._wrap_prompt(prompt),\\n            stop_sequences=stop,\\n            **self._default_params,\\n        )\\n        return response[\"completion\"]\\n    async def _acall(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call out to Anthropic\\'s completion endpoint asynchronously.\"\"\"\\n        stop = self._get_anthropic_stop(stop)\\n        if self.streaming:\\n            stream_resp = await self.client.acompletion_stream(\\n                prompt=self._wrap_prompt(prompt),\\n                stop_sequences=stop,\\n                **self._default_params,\\n            )\\n            current_completion = \"\"\\n            async for data in stream_resp:\\n                delta = data[\"completion\"][len(current_completion) :]\\n                current_completion = data[\"completion\"]\\n                if self.callback_manager.is_async:\\n                    await self.callback_manager.on_llm_new_token(\\n                        delta, verbose=self.verbose, **data\\n                    )\\n                else:\\n                    self.callback_manager.on_llm_new_token(\\n                        delta, verbose=self.verbose, **data\\n                    )\\n            return current_completion\\n        response = await self.client.acompletion(\\n            prompt=self._wrap_prompt(prompt),\\n            stop_sequences=stop,\\n            **self._default_params,\\n        )\\n        return response[\"completion\"]\\n[docs]    def stream(self, prompt: str, stop: Optional[List[str]] = None) -> Generator:\\n        r\"\"\"Call Anthropic completion_stream and return the resulting generator.\\n        BETA: this is a beta feature while we figure out the right abstraction.\\n        Once that happens, this interface could change.\\n        Args:\\n            prompt: The prompt to pass into the model.\\n            stop: Optional list of stop words to use when generating.\\n        Returns:\\n            A generator representing the stream of tokens from Anthropic.\\n        Example:\\n            .. code-block:: python\\n                prompt = \"Write a poem about a stream.\"\\n                prompt = f\"\\\\n\\\\nHuman: {prompt}\\\\n\\\\nAssistant:\"\\n                generator = anthropic.stream(prompt)\\n                for token in generator:\\n                    yield token\\n        \"\"\"\\n        stop = self._get_anthropic_stop(stop)\\n        return self.client.completion_stream(\\n            prompt=self._wrap_prompt(prompt),\\n            stop_sequences=stop,\\n            **self._default_params,\\n        )\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\anthropic.html'}),\n",
              " Document(page_content='Source code for langchain.llms.bananadev\\n\"\"\"Wrapper around Banana API.\"\"\"\\nimport logging\\nfrom typing import Any, Dict, List, Mapping, Optional\\nfrom pydantic import Extra, Field, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.llms.utils import enforce_stop_tokens\\nfrom langchain.utils import get_from_dict_or_env\\nlogger = logging.getLogger(__name__)\\n[docs]class Banana(LLM):\\n    \"\"\"Wrapper around Banana large language models.\\n    To use, you should have the ``banana-dev`` python package installed,\\n    and the environment variable ``BANANA_API_KEY`` set with your API key.\\n    Any parameters that are valid to be passed to the call can be passed\\n    in, even if not explicitly saved on this class.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import Banana\\n            banana = Banana(model_key=\"\")\\n    \"\"\"\\n    model_key: str = \"\"\\n    \"\"\"model endpoint to use\"\"\"\\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\\n    \"\"\"Holds any model parameters valid for `create` call not\\n    explicitly specified.\"\"\"\\n    banana_api_key: Optional[str] = None\\n    class Config:\\n        \"\"\"Configuration for this pydantic config.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator(pre=True)\\n    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Build extra kwargs from additional params that were passed in.\"\"\"\\n        all_required_field_names = {field.alias for field in cls.__fields__.values()}\\n        extra = values.get(\"model_kwargs\", {})\\n        for field_name in list(values):\\n            if field_name not in all_required_field_names:\\n                if field_name in extra:\\n                    raise ValueError(f\"Found {field_name} supplied twice.\")\\n                logger.warning(\\n                    f\"\"\"{field_name} was transfered to model_kwargs.\\n                    Please confirm that {field_name} is what you intended.\"\"\"\\n                )\\n                extra[field_name] = values.pop(field_name)\\n        values[\"model_kwargs\"] = extra\\n        return values\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        banana_api_key = get_from_dict_or_env(\\n            values, \"banana_api_key\", \"BANANA_API_KEY\"\\n        )\\n        values[\"banana_api_key\"] = banana_api_key\\n        return values\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {\\n            **{\"model_key\": self.model_key},\\n            **{\"model_kwargs\": self.model_kwargs},\\n        }\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"banana\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call to Banana endpoint.\"\"\"\\n        try:\\n            import banana_dev as banana\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import banana-dev python package. \"\\n                \"Please install it with `pip install banana-dev`.\"\\n            )\\n        params = self.model_kwargs or {}\\n        api_key = self.banana_api_key\\n        model_key = self.model_key\\n        model_inputs = {\\n            # a json specific to your model.\\n            \"prompt\": prompt,\\n            **params,\\n        }\\n        response = banana.run(api_key, model_key, model_inputs)\\n        try:\\n            text = response[\"modelOutputs\"][0][\"output\"]\\n        except (KeyError, TypeError):\\n            returned = response[\"modelOutputs\"][0]\\n            raise ValueError(\\n                \"Response should be of schema: {\\'output\\': \\'text\\'}.\"\\n                f\"\\\\nResponse was: {returned}\"\\n                \"\\\\nTo fix this:\"\\n                \"\\\\n- fork the source repo of the Banana model\"\\n                \"\\\\n- modify app.py to return the above schema\"\\n                \"\\\\n- deploy that as a custom repo\"\\n            )\\n        if stop is not None:\\n            # I believe this is required since the stop tokens\\n            # are not enforced by the model parameters\\n            text = enforce_stop_tokens(text, stop)\\n        return text\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\bananadev.html'}),\n",
              " Document(page_content='Source code for langchain.llms.cerebriumai\\n\"\"\"Wrapper around CerebriumAI API.\"\"\"\\nimport logging\\nfrom typing import Any, Dict, List, Mapping, Optional\\nfrom pydantic import Extra, Field, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.llms.utils import enforce_stop_tokens\\nfrom langchain.utils import get_from_dict_or_env\\nlogger = logging.getLogger(__name__)\\n[docs]class CerebriumAI(LLM):\\n    \"\"\"Wrapper around CerebriumAI large language models.\\n    To use, you should have the ``cerebrium`` python package installed, and the\\n    environment variable ``CEREBRIUMAI_API_KEY`` set with your API key.\\n    Any parameters that are valid to be passed to the call can be passed\\n    in, even if not explicitly saved on this class.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import CerebriumAI\\n            cerebrium = CerebriumAI(endpoint_url=\"\")\\n    \"\"\"\\n    endpoint_url: str = \"\"\\n    \"\"\"model endpoint to use\"\"\"\\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\\n    \"\"\"Holds any model parameters valid for `create` call not\\n    explicitly specified.\"\"\"\\n    cerebriumai_api_key: Optional[str] = None\\n    class Config:\\n        \"\"\"Configuration for this pydantic config.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator(pre=True)\\n    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Build extra kwargs from additional params that were passed in.\"\"\"\\n        all_required_field_names = {field.alias for field in cls.__fields__.values()}\\n        extra = values.get(\"model_kwargs\", {})\\n        for field_name in list(values):\\n            if field_name not in all_required_field_names:\\n                if field_name in extra:\\n                    raise ValueError(f\"Found {field_name} supplied twice.\")\\n                logger.warning(\\n                    f\"\"\"{field_name} was transfered to model_kwargs.\\n                    Please confirm that {field_name} is what you intended.\"\"\"\\n                )\\n                extra[field_name] = values.pop(field_name)\\n        values[\"model_kwargs\"] = extra\\n        return values\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        cerebriumai_api_key = get_from_dict_or_env(\\n            values, \"cerebriumai_api_key\", \"CEREBRIUMAI_API_KEY\"\\n        )\\n        values[\"cerebriumai_api_key\"] = cerebriumai_api_key\\n        return values\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {\\n            **{\"endpoint_url\": self.endpoint_url},\\n            **{\"model_kwargs\": self.model_kwargs},\\n        }\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"cerebriumai\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call to CerebriumAI endpoint.\"\"\"\\n        try:\\n            from cerebrium import model_api_request\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import cerebrium python package. \"\\n                \"Please install it with `pip install cerebrium`.\"\\n            )\\n        params = self.model_kwargs or {}\\n        response = model_api_request(\\n            self.endpoint_url, {\"prompt\": prompt, **params}, self.cerebriumai_api_key\\n        )\\n        text = response[\"data\"][\"result\"]\\n        if stop is not None:\\n            # I believe this is required since the stop tokens\\n            # are not enforced by the model parameters\\n            text = enforce_stop_tokens(text, stop)\\n        return text\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\cerebriumai.html'}),\n",
              " Document(page_content='Source code for langchain.llms.cohere\\n\"\"\"Wrapper around Cohere APIs.\"\"\"\\nimport logging\\nfrom typing import Any, Dict, List, Optional\\nfrom pydantic import Extra, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.llms.utils import enforce_stop_tokens\\nfrom langchain.utils import get_from_dict_or_env\\nlogger = logging.getLogger(__name__)\\n[docs]class Cohere(LLM):\\n    \"\"\"Wrapper around Cohere large language models.\\n    To use, you should have the ``cohere`` python package installed, and the\\n    environment variable ``COHERE_API_KEY`` set with your API key, or pass\\n    it as a named parameter to the constructor.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import Cohere\\n            cohere = Cohere(model=\"gptd-instruct-tft\", cohere_api_key=\"my-api-key\")\\n    \"\"\"\\n    client: Any  #: :meta private:\\n    model: Optional[str] = None\\n    \"\"\"Model name to use.\"\"\"\\n    max_tokens: int = 256\\n    \"\"\"Denotes the number of tokens to predict per generation.\"\"\"\\n    temperature: float = 0.75\\n    \"\"\"A non-negative float that tunes the degree of randomness in generation.\"\"\"\\n    k: int = 0\\n    \"\"\"Number of most likely tokens to consider at each step.\"\"\"\\n    p: int = 1\\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\\n    frequency_penalty: float = 0.0\\n    \"\"\"Penalizes repeated tokens according to frequency. Between 0 and 1.\"\"\"\\n    presence_penalty: float = 0.0\\n    \"\"\"Penalizes repeated tokens. Between 0 and 1.\"\"\"\\n    truncate: Optional[str] = None\\n    \"\"\"Specify how the client handles inputs longer than the maximum token\\n    length: Truncate from START, END or NONE\"\"\"\\n    cohere_api_key: Optional[str] = None\\n    stop: Optional[List[str]] = None\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        cohere_api_key = get_from_dict_or_env(\\n            values, \"cohere_api_key\", \"COHERE_API_KEY\"\\n        )\\n        try:\\n            import cohere\\n            values[\"client\"] = cohere.Client(cohere_api_key)\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import cohere python package. \"\\n                \"Please install it with `pip install cohere`.\"\\n            )\\n        return values\\n    @property\\n    def _default_params(self) -> Dict[str, Any]:\\n        \"\"\"Get the default parameters for calling Cohere API.\"\"\"\\n        return {\\n            \"max_tokens\": self.max_tokens,\\n            \"temperature\": self.temperature,\\n            \"k\": self.k,\\n            \"p\": self.p,\\n            \"frequency_penalty\": self.frequency_penalty,\\n            \"presence_penalty\": self.presence_penalty,\\n            \"truncate\": self.truncate,\\n        }\\n    @property\\n    def _identifying_params(self) -> Dict[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {**{\"model\": self.model}, **self._default_params}\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"cohere\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call out to Cohere\\'s generate endpoint.\\n        Args:\\n            prompt: The prompt to pass into the model.\\n            stop: Optional list of stop words to use when generating.\\n        Returns:\\n            The string generated by the model.\\n        Example:\\n            .. code-block:: python\\n                response = cohere(\"Tell me a joke.\")\\n        \"\"\"\\n        params = self._default_params\\n        if self.stop is not None and stop is not None:\\n            raise ValueError(\"`stop` found in both the input and default params.\")\\n        elif self.stop is not None:\\n            params[\"stop_sequences\"] = self.stop\\n        else:\\n            params[\"stop_sequences\"] = stop\\n        response = self.client.generate(model=self.model, prompt=prompt, **params)\\n        text = response.generations[0].text\\n        # If stop tokens are provided, Cohere\\'s endpoint returns them.\\n        # In order to make this consistent with other endpoints, we strip them.\\n        if stop is not None or self.stop is not None:\\n            text = enforce_stop_tokens(text, params[\"stop_sequences\"])\\n        return text\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\cohere.html'}),\n",
              " Document(page_content='Source code for langchain.llms.deepinfra\\n\"\"\"Wrapper around DeepInfra APIs.\"\"\"\\nfrom typing import Any, Dict, List, Mapping, Optional\\nimport requests\\nfrom pydantic import Extra, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.llms.utils import enforce_stop_tokens\\nfrom langchain.utils import get_from_dict_or_env\\nDEFAULT_MODEL_ID = \"google/flan-t5-xl\"\\n[docs]class DeepInfra(LLM):\\n    \"\"\"Wrapper around DeepInfra deployed models.\\n    To use, you should have the ``requests`` python package installed, and the\\n    environment variable ``DEEPINFRA_API_TOKEN`` set with your API token, or pass\\n    it as a named parameter to the constructor.\\n    Only supports `text-generation` and `text2text-generation` for now.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import DeepInfra\\n            di = DeepInfra(model_id=\"google/flan-t5-xl\",\\n                                deepinfra_api_token=\"my-api-key\")\\n    \"\"\"\\n    model_id: str = DEFAULT_MODEL_ID\\n    model_kwargs: Optional[dict] = None\\n    deepinfra_api_token: Optional[str] = None\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        deepinfra_api_token = get_from_dict_or_env(\\n            values, \"deepinfra_api_token\", \"DEEPINFRA_API_TOKEN\"\\n        )\\n        values[\"deepinfra_api_token\"] = deepinfra_api_token\\n        return values\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {\\n            **{\"model_id\": self.model_id},\\n            **{\"model_kwargs\": self.model_kwargs},\\n        }\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"deepinfra\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call out to DeepInfra\\'s inference API endpoint.\\n        Args:\\n            prompt: The prompt to pass into the model.\\n            stop: Optional list of stop words to use when generating.\\n        Returns:\\n            The string generated by the model.\\n        Example:\\n            .. code-block:: python\\n                response = di(\"Tell me a joke.\")\\n        \"\"\"\\n        _model_kwargs = self.model_kwargs or {}\\n        res = requests.post(\\n            f\"https://api.deepinfra.com/v1/inference/{self.model_id}\",\\n            headers={\\n                \"Authorization\": f\"bearer {self.deepinfra_api_token}\",\\n                \"Content-Type\": \"application/json\",\\n            },\\n            json={\"input\": prompt, **_model_kwargs},\\n        )\\n        if res.status_code != 200:\\n            raise ValueError(\"Error raised by inference API\")\\n        text = res.json()[0][\"generated_text\"]\\n        if stop is not None:\\n            # I believe this is required since the stop tokens\\n            # are not enforced by the model parameters\\n            text = enforce_stop_tokens(text, stop)\\n        return text\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\deepinfra.html'}),\n",
              " Document(page_content='Source code for langchain.llms.forefrontai\\n\"\"\"Wrapper around ForefrontAI APIs.\"\"\"\\nfrom typing import Any, Dict, List, Mapping, Optional\\nimport requests\\nfrom pydantic import Extra, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.llms.utils import enforce_stop_tokens\\nfrom langchain.utils import get_from_dict_or_env\\n[docs]class ForefrontAI(LLM):\\n    \"\"\"Wrapper around ForefrontAI large language models.\\n    To use, you should have the environment variable ``FOREFRONTAI_API_KEY``\\n    set with your API key.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import ForefrontAI\\n            forefrontai = ForefrontAI(endpoint_url=\"\")\\n    \"\"\"\\n    endpoint_url: str = \"\"\\n    \"\"\"Model name to use.\"\"\"\\n    temperature: float = 0.7\\n    \"\"\"What sampling temperature to use.\"\"\"\\n    length: int = 256\\n    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\\n    top_p: float = 1.0\\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\\n    top_k: int = 40\\n    \"\"\"The number of highest probability vocabulary tokens to\\n    keep for top-k-filtering.\"\"\"\\n    repetition_penalty: int = 1\\n    \"\"\"Penalizes repeated tokens according to frequency.\"\"\"\\n    forefrontai_api_key: Optional[str] = None\\n    base_url: Optional[str] = None\\n    \"\"\"Base url to use, if None decides based on model name.\"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key exists in environment.\"\"\"\\n        forefrontai_api_key = get_from_dict_or_env(\\n            values, \"forefrontai_api_key\", \"FOREFRONTAI_API_KEY\"\\n        )\\n        values[\"forefrontai_api_key\"] = forefrontai_api_key\\n        return values\\n    @property\\n    def _default_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the default parameters for calling ForefrontAI API.\"\"\"\\n        return {\\n            \"temperature\": self.temperature,\\n            \"length\": self.length,\\n            \"top_p\": self.top_p,\\n            \"top_k\": self.top_k,\\n            \"repetition_penalty\": self.repetition_penalty,\\n        }\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {**{\"endpoint_url\": self.endpoint_url}, **self._default_params}\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"forefrontai\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call out to ForefrontAI\\'s complete endpoint.\\n        Args:\\n            prompt: The prompt to pass into the model.\\n            stop: Optional list of stop words to use when generating.\\n        Returns:\\n            The string generated by the model.\\n        Example:\\n            .. code-block:: python\\n                response = ForefrontAI(\"Tell me a joke.\")\\n        \"\"\"\\n        response = requests.post(\\n            url=self.endpoint_url,\\n            headers={\\n                \"Authorization\": f\"Bearer {self.forefrontai_api_key}\",\\n                \"Content-Type\": \"application/json\",\\n            },\\n            json={\"text\": prompt, **self._default_params},\\n        )\\n        response_json = response.json()\\n        text = response_json[\"result\"][0][\"completion\"]\\n        if stop is not None:\\n            # I believe this is required since the stop tokens\\n            # are not enforced by the model parameters\\n            text = enforce_stop_tokens(text, stop)\\n        return text\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\forefrontai.html'}),\n",
              " Document(page_content='Source code for langchain.llms.gooseai\\n\"\"\"Wrapper around GooseAI API.\"\"\"\\nimport logging\\nfrom typing import Any, Dict, List, Mapping, Optional\\nfrom pydantic import Extra, Field, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.utils import get_from_dict_or_env\\nlogger = logging.getLogger(__name__)\\n[docs]class GooseAI(LLM):\\n    \"\"\"Wrapper around OpenAI large language models.\\n    To use, you should have the ``openai`` python package installed, and the\\n    environment variable ``GOOSEAI_API_KEY`` set with your API key.\\n    Any parameters that are valid to be passed to the openai.create call can be passed\\n    in, even if not explicitly saved on this class.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import GooseAI\\n            gooseai = GooseAI(model_name=\"gpt-neo-20b\")\\n    \"\"\"\\n    client: Any\\n    model_name: str = \"gpt-neo-20b\"\\n    \"\"\"Model name to use\"\"\"\\n    temperature: float = 0.7\\n    \"\"\"What sampling temperature to use\"\"\"\\n    max_tokens: int = 256\\n    \"\"\"The maximum number of tokens to generate in the completion.\\n    -1 returns as many tokens as possible given the prompt and\\n    the models maximal context size.\"\"\"\\n    top_p: float = 1\\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\\n    min_tokens: int = 1\\n    \"\"\"The minimum number of tokens to generate in the completion.\"\"\"\\n    frequency_penalty: float = 0\\n    \"\"\"Penalizes repeated tokens according to frequency.\"\"\"\\n    presence_penalty: float = 0\\n    \"\"\"Penalizes repeated tokens.\"\"\"\\n    n: int = 1\\n    \"\"\"How many completions to generate for each prompt.\"\"\"\\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\\n    \"\"\"Holds any model parameters valid for `create` call not explicitly specified.\"\"\"\\n    logit_bias: Optional[Dict[str, float]] = Field(default_factory=dict)\\n    \"\"\"Adjust the probability of specific tokens being generated.\"\"\"\\n    gooseai_api_key: Optional[str] = None\\n    class Config:\\n        \"\"\"Configuration for this pydantic config.\"\"\"\\n        extra = Extra.ignore\\n    @root_validator(pre=True)\\n    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Build extra kwargs from additional params that were passed in.\"\"\"\\n        all_required_field_names = {field.alias for field in cls.__fields__.values()}\\n        extra = values.get(\"model_kwargs\", {})\\n        for field_name in list(values):\\n            if field_name not in all_required_field_names:\\n                if field_name in extra:\\n                    raise ValueError(f\"Found {field_name} supplied twice.\")\\n                logger.warning(\\n                    f\"\"\"WARNING! {field_name} is not default parameter.\\n                    {field_name} was transfered to model_kwargs.\\n                    Please confirm that {field_name} is what you intended.\"\"\"\\n                )\\n                extra[field_name] = values.pop(field_name)\\n        values[\"model_kwargs\"] = extra\\n        return values\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        gooseai_api_key = get_from_dict_or_env(\\n            values, \"gooseai_api_key\", \"GOOSEAI_API_KEY\"\\n        )\\n        try:\\n            import openai\\n            openai.api_key = gooseai_api_key\\n            openai.api_base = \"https://api.goose.ai/v1\"\\n            values[\"client\"] = openai.Completion\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import openai python package. \"\\n                \"Please install it with `pip install openai`.\"\\n            )\\n        return values\\n    @property\\n    def _default_params(self) -> Dict[str, Any]:\\n        \"\"\"Get the default parameters for calling GooseAI API.\"\"\"\\n        normal_params = {\\n            \"temperature\": self.temperature,\\n            \"max_tokens\": self.max_tokens,\\n            \"top_p\": self.top_p,\\n            \"min_tokens\": self.min_tokens,\\n            \"frequency_penalty\": self.frequency_penalty,\\n            \"presence_penalty\": self.presence_penalty,\\n            \"n\": self.n,\\n            \"logit_bias\": self.logit_bias,\\n        }\\n        return {**normal_params, **self.model_kwargs}\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {**{\"model_name\": self.model_name}, **self._default_params}\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"gooseai\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call the GooseAI API.\"\"\"\\n        params = self._default_params\\n        if stop is not None:\\n            if \"stop\" in params:\\n                raise ValueError(\"`stop` found in both the input and default params.\")\\n            params[\"stop\"] = stop\\n        response = self.client.create(engine=self.model_name, prompt=prompt, **params)\\n        text = response.choices[0].text\\n        return text\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\gooseai.html'}),\n",
              " Document(page_content='Source code for langchain.llms.gpt4all\\n\"\"\"Wrapper for the GPT4All model.\"\"\"\\nfrom functools import partial\\nfrom typing import Any, Dict, List, Mapping, Optional, Set\\nfrom pydantic import Extra, Field, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.llms.utils import enforce_stop_tokens\\n[docs]class GPT4All(LLM):\\n    r\"\"\"Wrapper around GPT4All language models.\\n    To use, you should have the ``pyllamacpp`` python package installed, the\\n    pre-trained model file, and the model\\'s config information.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import GPT4All\\n            model = GPT4All(model=\"./models/gpt4all-model.bin\", n_ctx=512, n_threads=8)\\n            # Simplest invocation\\n            response = model(\"Once upon a time, \")\\n    \"\"\"\\n    model: str\\n    \"\"\"Path to the pre-trained GPT4All model file.\"\"\"\\n    n_ctx: int = Field(512, alias=\"n_ctx\")\\n    \"\"\"Token context window.\"\"\"\\n    n_parts: int = Field(-1, alias=\"n_parts\")\\n    \"\"\"Number of parts to split the model into. \\n    If -1, the number of parts is automatically determined.\"\"\"\\n    seed: int = Field(0, alias=\"seed\")\\n    \"\"\"Seed. If -1, a random seed is used.\"\"\"\\n    f16_kv: bool = Field(False, alias=\"f16_kv\")\\n    \"\"\"Use half-precision for key/value cache.\"\"\"\\n    logits_all: bool = Field(False, alias=\"logits_all\")\\n    \"\"\"Return logits for all tokens, not just the last token.\"\"\"\\n    vocab_only: bool = Field(False, alias=\"vocab_only\")\\n    \"\"\"Only load the vocabulary, no weights.\"\"\"\\n    use_mlock: bool = Field(False, alias=\"use_mlock\")\\n    \"\"\"Force system to keep model in RAM.\"\"\"\\n    embedding: bool = Field(False, alias=\"embedding\")\\n    \"\"\"Use embedding mode only.\"\"\"\\n    n_threads: Optional[int] = Field(4, alias=\"n_threads\")\\n    \"\"\"Number of threads to use.\"\"\"\\n    n_predict: Optional[int] = 256\\n    \"\"\"The maximum number of tokens to generate.\"\"\"\\n    temp: Optional[float] = 0.8\\n    \"\"\"The temperature to use for sampling.\"\"\"\\n    top_p: Optional[float] = 0.95\\n    \"\"\"The top-p value to use for sampling.\"\"\"\\n    top_k: Optional[int] = 40\\n    \"\"\"The top-k value to use for sampling.\"\"\"\\n    echo: Optional[bool] = False\\n    \"\"\"Whether to echo the prompt.\"\"\"\\n    stop: Optional[List[str]] = []\\n    \"\"\"A list of strings to stop generation when encountered.\"\"\"\\n    repeat_last_n: Optional[int] = 64\\n    \"Last n tokens to penalize\"\\n    repeat_penalty: Optional[float] = 1.3\\n    \"\"\"The penalty to apply to repeated tokens.\"\"\"\\n    n_batch: int = Field(1, alias=\"n_batch\")\\n    \"\"\"Batch size for prompt processing.\"\"\"\\n    streaming: bool = False\\n    \"\"\"Whether to stream the results or not.\"\"\"\\n    client: Any = None  #: :meta private:\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    @property\\n    def _default_params(self) -> Dict[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {\\n            \"seed\": self.seed,\\n            \"n_predict\": self.n_predict,\\n            \"n_threads\": self.n_threads,\\n            \"n_batch\": self.n_batch,\\n            \"repeat_last_n\": self.repeat_last_n,\\n            \"repeat_penalty\": self.repeat_penalty,\\n            \"top_k\": self.top_k,\\n            \"top_p\": self.top_p,\\n            \"temp\": self.temp,\\n        }\\n    @staticmethod\\n    def _llama_param_names() -> Set[str]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {\\n            \"seed\",\\n            \"n_ctx\",\\n            \"n_parts\",\\n            \"f16_kv\",\\n            \"logits_all\",\\n            \"vocab_only\",\\n            \"use_mlock\",\\n            \"embedding\",\\n        }\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that the python package exists in the environment.\"\"\"\\n        try:\\n            from pyllamacpp.model import Model as GPT4AllModel\\n            llama_keys = cls._llama_param_names()\\n            model_kwargs = {k: v for k, v in values.items() if k in llama_keys}\\n            values[\"client\"] = GPT4AllModel(\\n                ggml_model=values[\"model\"],\\n                **model_kwargs,\\n            )\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import pyllamacpp python package. \"\\n                \"Please install it with `pip install pyllamacpp`.\"\\n            )\\n        return values\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {\\n            \"model\": self.model,\\n            **self._default_params,\\n            **{\\n                k: v\\n                for k, v in self.__dict__.items()\\n                if k in GPT4All._llama_param_names()\\n            },\\n        }\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return the type of llm.\"\"\"\\n        return \"gpt4all\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        r\"\"\"Call out to GPT4All\\'s generate method.\\n        Args:\\n            prompt: The prompt to pass into the model.\\n            stop: A list of strings to stop generation when encountered.\\n        Returns:\\n            The string generated by the model.\\n        Example:\\n            .. code-block:: python\\n                prompt = \"Once upon a time, \"\\n                response = model(prompt, n_predict=55)\\n        \"\"\"\\n        text_callback = partial(\\n            self.callback_manager.on_llm_new_token, verbose=self.verbose\\n        )\\n        text = self.client.generate(\\n            prompt,\\n            new_text_callback=text_callback,\\n            **self._default_params,\\n        )\\n        if stop is not None:\\n            text = enforce_stop_tokens(text, stop)\\n        return text\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\gpt4all.html'}),\n",
              " Document(page_content='Source code for langchain.llms.huggingface_endpoint\\n\"\"\"Wrapper around HuggingFace APIs.\"\"\"\\nfrom typing import Any, Dict, List, Mapping, Optional\\nimport requests\\nfrom pydantic import Extra, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.llms.utils import enforce_stop_tokens\\nfrom langchain.utils import get_from_dict_or_env\\nVALID_TASKS = (\"text2text-generation\", \"text-generation\")\\n[docs]class HuggingFaceEndpoint(LLM):\\n    \"\"\"Wrapper around HuggingFaceHub Inference Endpoints.\\n    To use, you should have the ``huggingface_hub`` python package installed, and the\\n    environment variable ``HUGGINGFACEHUB_API_TOKEN`` set with your API token, or pass\\n    it as a named parameter to the constructor.\\n    Only supports `text-generation` and `text2text-generation` for now.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import HuggingFaceEndpoint\\n            endpoint_url = (\\n                \"https://abcdefghijklmnop.us-east-1.aws.endpoints.huggingface.cloud\"\\n            )\\n            hf = HuggingFaceEndpoint(\\n                endpoint_url=endpoint_url,\\n                huggingfacehub_api_token=\"my-api-key\"\\n            )\\n    \"\"\"\\n    endpoint_url: str = \"\"\\n    \"\"\"Endpoint URL to use.\"\"\"\\n    task: Optional[str] = None\\n    \"\"\"Task to call the model with. Should be a task that returns `generated_text`.\"\"\"\\n    model_kwargs: Optional[dict] = None\\n    \"\"\"Key word arguments to pass to the model.\"\"\"\\n    huggingfacehub_api_token: Optional[str] = None\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        huggingfacehub_api_token = get_from_dict_or_env(\\n            values, \"huggingfacehub_api_token\", \"HUGGINGFACEHUB_API_TOKEN\"\\n        )\\n        try:\\n            from huggingface_hub.hf_api import HfApi\\n            try:\\n                HfApi(\\n                    endpoint=\"https://huggingface.co\",  # Can be a Private Hub endpoint.\\n                    token=huggingfacehub_api_token,\\n                ).whoami()\\n            except Exception as e:\\n                raise ValueError(\\n                    \"Could not authenticate with huggingface_hub. \"\\n                    \"Please check your API token.\"\\n                ) from e\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import huggingface_hub python package. \"\\n                \"Please install it with `pip install huggingface_hub`.\"\\n            )\\n        return values\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        _model_kwargs = self.model_kwargs or {}\\n        return {\\n            **{\"endpoint_url\": self.endpoint_url, \"task\": self.task},\\n            **{\"model_kwargs\": _model_kwargs},\\n        }\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"huggingface_endpoint\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call out to HuggingFace Hub\\'s inference endpoint.\\n        Args:\\n            prompt: The prompt to pass into the model.\\n            stop: Optional list of stop words to use when generating.\\n        Returns:\\n            The string generated by the model.\\n        Example:\\n            .. code-block:: python\\n                response = hf(\"Tell me a joke.\")\\n        \"\"\"\\n        _model_kwargs = self.model_kwargs or {}\\n        # payload samples\\n        parameter_payload = {\"inputs\": prompt, \"parameters\": _model_kwargs}\\n        # HTTP headers for authorization\\n        headers = {\\n            \"Authorization\": f\"Bearer {self.huggingfacehub_api_token}\",\\n            \"Content-Type\": \"application/json\",\\n        }\\n        # send request\\n        try:\\n            response = requests.post(\\n                self.endpoint_url, headers=headers, json=parameter_payload\\n            )\\n        except requests.exceptions.RequestException as e:  # This is the correct syntax\\n            raise ValueError(f\"Error raised by inference endpoint: {e}\")\\n        generated_text = response.json()\\n        if \"error\" in generated_text:\\n            raise ValueError(\\n                f\"Error raised by inference API: {generated_text[\\'error\\']}\"\\n            )\\n        if self.task == \"text-generation\":\\n            # Text generation return includes the starter text.\\n            text = generated_text[0][\"generated_text\"][len(prompt) :]\\n        elif self.task == \"text2text-generation\":\\n            text = generated_text[0][\"generated_text\"]\\n        else:\\n            raise ValueError(\\n                f\"Got invalid task {self.task}, \"\\n                f\"currently only {VALID_TASKS} are supported\"\\n            )\\n        if stop is not None:\\n            # This is a bit hacky, but I can\\'t figure out a better way to enforce\\n            # stop tokens when making calls to huggingface_hub.\\n            text = enforce_stop_tokens(text, stop)\\n        return text\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\huggingface_endpoint.html'}),\n",
              " Document(page_content='Source code for langchain.llms.huggingface_hub\\n\"\"\"Wrapper around HuggingFace APIs.\"\"\"\\nfrom typing import Any, Dict, List, Mapping, Optional\\nfrom pydantic import Extra, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.llms.utils import enforce_stop_tokens\\nfrom langchain.utils import get_from_dict_or_env\\nDEFAULT_REPO_ID = \"gpt2\"\\nVALID_TASKS = (\"text2text-generation\", \"text-generation\")\\n[docs]class HuggingFaceHub(LLM):\\n    \"\"\"Wrapper around HuggingFaceHub  models.\\n    To use, you should have the ``huggingface_hub`` python package installed, and the\\n    environment variable ``HUGGINGFACEHUB_API_TOKEN`` set with your API token, or pass\\n    it as a named parameter to the constructor.\\n    Only supports `text-generation` and `text2text-generation` for now.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import HuggingFaceHub\\n            hf = HuggingFaceHub(repo_id=\"gpt2\", huggingfacehub_api_token=\"my-api-key\")\\n    \"\"\"\\n    client: Any  #: :meta private:\\n    repo_id: str = DEFAULT_REPO_ID\\n    \"\"\"Model name to use.\"\"\"\\n    task: Optional[str] = None\\n    \"\"\"Task to call the model with. Should be a task that returns `generated_text`.\"\"\"\\n    model_kwargs: Optional[dict] = None\\n    \"\"\"Key word arguments to pass to the model.\"\"\"\\n    huggingfacehub_api_token: Optional[str] = None\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        huggingfacehub_api_token = get_from_dict_or_env(\\n            values, \"huggingfacehub_api_token\", \"HUGGINGFACEHUB_API_TOKEN\"\\n        )\\n        try:\\n            from huggingface_hub.inference_api import InferenceApi\\n            repo_id = values[\"repo_id\"]\\n            client = InferenceApi(\\n                repo_id=repo_id,\\n                token=huggingfacehub_api_token,\\n                task=values.get(\"task\"),\\n            )\\n            if client.task not in VALID_TASKS:\\n                raise ValueError(\\n                    f\"Got invalid task {client.task}, \"\\n                    f\"currently only {VALID_TASKS} are supported\"\\n                )\\n            values[\"client\"] = client\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import huggingface_hub python package. \"\\n                \"Please install it with `pip install huggingface_hub`.\"\\n            )\\n        return values\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        _model_kwargs = self.model_kwargs or {}\\n        return {\\n            **{\"repo_id\": self.repo_id, \"task\": self.task},\\n            **{\"model_kwargs\": _model_kwargs},\\n        }\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"huggingface_hub\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call out to HuggingFace Hub\\'s inference endpoint.\\n        Args:\\n            prompt: The prompt to pass into the model.\\n            stop: Optional list of stop words to use when generating.\\n        Returns:\\n            The string generated by the model.\\n        Example:\\n            .. code-block:: python\\n                response = hf(\"Tell me a joke.\")\\n        \"\"\"\\n        _model_kwargs = self.model_kwargs or {}\\n        response = self.client(inputs=prompt, params=_model_kwargs)\\n        if \"error\" in response:\\n            raise ValueError(f\"Error raised by inference API: {response[\\'error\\']}\")\\n        if self.client.task == \"text-generation\":\\n            # Text generation return includes the starter text.\\n            text = response[0][\"generated_text\"][len(prompt) :]\\n        elif self.client.task == \"text2text-generation\":\\n            text = response[0][\"generated_text\"]\\n        else:\\n            raise ValueError(\\n                f\"Got invalid task {self.client.task}, \"\\n                f\"currently only {VALID_TASKS} are supported\"\\n            )\\n        if stop is not None:\\n            # This is a bit hacky, but I can\\'t figure out a better way to enforce\\n            # stop tokens when making calls to huggingface_hub.\\n            text = enforce_stop_tokens(text, stop)\\n        return text\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\huggingface_hub.html'}),\n",
              " Document(page_content='Source code for langchain.llms.huggingface_pipeline\\n\"\"\"Wrapper around HuggingFace Pipeline APIs.\"\"\"\\nimport importlib.util\\nimport logging\\nfrom typing import Any, List, Mapping, Optional\\nfrom pydantic import Extra\\nfrom langchain.llms.base import LLM\\nfrom langchain.llms.utils import enforce_stop_tokens\\nDEFAULT_MODEL_ID = \"gpt2\"\\nDEFAULT_TASK = \"text-generation\"\\nVALID_TASKS = (\"text2text-generation\", \"text-generation\")\\nlogger = logging.getLogger(__name__)\\n[docs]class HuggingFacePipeline(LLM):\\n    \"\"\"Wrapper around HuggingFace Pipeline API.\\n    To use, you should have the ``transformers`` python package installed.\\n    Only supports `text-generation` and `text2text-generation` for now.\\n    Example using from_model_id:\\n        .. code-block:: python\\n            from langchain.llms import HuggingFacePipeline\\n            hf = HuggingFacePipeline.from_model_id(\\n                model_id=\"gpt2\", task=\"text-generation\"\\n            )\\n    Example passing pipeline in directly:\\n        .. code-block:: python\\n            from langchain.llms import HuggingFacePipeline\\n            from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\\n            model_id = \"gpt2\"\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            model = AutoModelForCausalLM.from_pretrained(model_id)\\n            pipe = pipeline(\\n                \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10\\n            )\\n            hf = HuggingFacePipeline(pipeline=pipe)\\n    \"\"\"\\n    pipeline: Any  #: :meta private:\\n    model_id: str = DEFAULT_MODEL_ID\\n    \"\"\"Model name to use.\"\"\"\\n    model_kwargs: Optional[dict] = None\\n    \"\"\"Key word arguments to pass to the model.\"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n[docs]    @classmethod\\n    def from_model_id(\\n        cls,\\n        model_id: str,\\n        task: str,\\n        device: int = -1,\\n        model_kwargs: Optional[dict] = None,\\n        **kwargs: Any,\\n    ) -> LLM:\\n        \"\"\"Construct the pipeline object from model_id and task.\"\"\"\\n        try:\\n            from transformers import (\\n                AutoModelForCausalLM,\\n                AutoModelForSeq2SeqLM,\\n                AutoTokenizer,\\n            )\\n            from transformers import pipeline as hf_pipeline\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        _model_kwargs = model_kwargs or {}\\n        tokenizer = AutoTokenizer.from_pretrained(model_id, **_model_kwargs)\\n        try:\\n            if task == \"text-generation\":\\n                model = AutoModelForCausalLM.from_pretrained(model_id, **_model_kwargs)\\n            elif task == \"text2text-generation\":\\n                model = AutoModelForSeq2SeqLM.from_pretrained(model_id, **_model_kwargs)\\n            else:\\n                raise ValueError(\\n                    f\"Got invalid task {task}, \"\\n                    f\"currently only {VALID_TASKS} are supported\"\\n                )\\n        except ImportError as e:\\n            raise ValueError(\\n                f\"Could not load the {task} model due to missing dependencies.\"\\n            ) from e\\n        if importlib.util.find_spec(\"torch\") is not None:\\n            import torch\\n            cuda_device_count = torch.cuda.device_count()\\n            if device < -1 or (device >= cuda_device_count):\\n                raise ValueError(\\n                    f\"Got device=={device}, \"\\n                    f\"device is required to be within [-1, {cuda_device_count})\"\\n                )\\n            if device < 0 and cuda_device_count > 0:\\n                logger.warning(\\n                    \"Device has %d GPUs available. \"\\n                    \"Provide device={deviceId} to `from_model_id` to use available\"\\n                    \"GPUs for execution. deviceId is -1 (default) for CPU and \"\\n                    \"can be a positive integer associated with CUDA device id.\",\\n                    cuda_device_count,\\n                )\\n        pipeline = hf_pipeline(\\n            task=task,\\n            model=model,\\n            tokenizer=tokenizer,\\n            device=device,\\n            model_kwargs=_model_kwargs,\\n        )\\n        if pipeline.task not in VALID_TASKS:\\n            raise ValueError(\\n                f\"Got invalid task {pipeline.task}, \"\\n                f\"currently only {VALID_TASKS} are supported\"\\n            )\\n        return cls(\\n            pipeline=pipeline,\\n            model_id=model_id,\\n            model_kwargs=_model_kwargs,\\n            **kwargs,\\n        )\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {\\n            **{\"model_id\": self.model_id},\\n            **{\"model_kwargs\": self.model_kwargs},\\n        }\\n    @property\\n    def _llm_type(self) -> str:\\n        return \"huggingface_pipeline\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        response = self.pipeline(prompt)\\n        if self.pipeline.task == \"text-generation\":\\n            # Text generation return includes the starter text.\\n            text = response[0][\"generated_text\"][len(prompt) :]\\n        elif self.pipeline.task == \"text2text-generation\":\\n            text = response[0][\"generated_text\"]\\n        else:\\n            raise ValueError(\\n                f\"Got invalid task {self.pipeline.task}, \"\\n                f\"currently only {VALID_TASKS} are supported\"\\n            )\\n        if stop is not None:\\n            # This is a bit hacky, but I can\\'t figure out a better way to enforce\\n            # stop tokens when making calls to huggingface_hub.\\n            text = enforce_stop_tokens(text, stop)\\n        return text\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\huggingface_pipeline.html'}),\n",
              " Document(page_content='Source code for langchain.llms.llamacpp\\n\"\"\"Wrapper around llama.cpp.\"\"\"\\nimport logging\\nfrom typing import Any, Dict, List, Optional\\nfrom pydantic import Field, root_validator\\nfrom langchain.llms.base import LLM\\nlogger = logging.getLogger(__name__)\\n[docs]class LlamaCpp(LLM):\\n    \"\"\"Wrapper around the llama.cpp model.\\n    To use, you should have the llama-cpp-python library installed, and provide the\\n    path to the Llama model as a named parameter to the constructor.\\n    Check out: https://github.com/abetlen/llama-cpp-python\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import LlamaCppEmbeddings\\n            llm = LlamaCppEmbeddings(model_path=\"/path/to/llama/model\")\\n    \"\"\"\\n    client: Any  #: :meta private:\\n    model_path: str\\n    \"\"\"The path to the Llama model file.\"\"\"\\n    n_ctx: int = Field(512, alias=\"n_ctx\")\\n    \"\"\"Token context window.\"\"\"\\n    n_parts: int = Field(-1, alias=\"n_parts\")\\n    \"\"\"Number of parts to split the model into. \\n    If -1, the number of parts is automatically determined.\"\"\"\\n    seed: int = Field(-1, alias=\"seed\")\\n    \"\"\"Seed. If -1, a random seed is used.\"\"\"\\n    f16_kv: bool = Field(False, alias=\"f16_kv\")\\n    \"\"\"Use half-precision for key/value cache.\"\"\"\\n    logits_all: bool = Field(False, alias=\"logits_all\")\\n    \"\"\"Return logits for all tokens, not just the last token.\"\"\"\\n    vocab_only: bool = Field(False, alias=\"vocab_only\")\\n    \"\"\"Only load the vocabulary, no weights.\"\"\"\\n    use_mlock: bool = Field(False, alias=\"use_mlock\")\\n    \"\"\"Force system to keep model in RAM.\"\"\"\\n    n_threads: Optional[int] = Field(None, alias=\"n_threads\")\\n    \"\"\"Number of threads to use. \\n    If None, the number of threads is automatically determined.\"\"\"\\n    n_batch: Optional[int] = Field(8, alias=\"n_batch\")\\n    \"\"\"Number of tokens to process in parallel.\\n    Should be a number between 1 and n_ctx.\"\"\"\\n    suffix: Optional[str] = Field(None)\\n    \"\"\"A suffix to append to the generated text. If None, no suffix is appended.\"\"\"\\n    max_tokens: Optional[int] = 256\\n    \"\"\"The maximum number of tokens to generate.\"\"\"\\n    temperature: Optional[float] = 0.8\\n    \"\"\"The temperature to use for sampling.\"\"\"\\n    top_p: Optional[float] = 0.95\\n    \"\"\"The top-p value to use for sampling.\"\"\"\\n    logprobs: Optional[int] = Field(None)\\n    \"\"\"The number of logprobs to return. If None, no logprobs are returned.\"\"\"\\n    echo: Optional[bool] = False\\n    \"\"\"Whether to echo the prompt.\"\"\"\\n    stop: Optional[List[str]] = []\\n    \"\"\"A list of strings to stop generation when encountered.\"\"\"\\n    repeat_penalty: Optional[float] = 1.1\\n    \"\"\"The penalty to apply to repeated tokens.\"\"\"\\n    top_k: Optional[int] = 40\\n    \"\"\"The top-k value to use for sampling.\"\"\"\\n    last_n_tokens_size: Optional[int] = 64\\n    \"\"\"The number of tokens to look back when applying the repeat_penalty.\"\"\"\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that llama-cpp-python library is installed.\"\"\"\\n        model_path = values[\"model_path\"]\\n        n_ctx = values[\"n_ctx\"]\\n        n_parts = values[\"n_parts\"]\\n        seed = values[\"seed\"]\\n        f16_kv = values[\"f16_kv\"]\\n        logits_all = values[\"logits_all\"]\\n        vocab_only = values[\"vocab_only\"]\\n        use_mlock = values[\"use_mlock\"]\\n        n_threads = values[\"n_threads\"]\\n        n_batch = values[\"n_batch\"]\\n        last_n_tokens_size = values[\"last_n_tokens_size\"]\\n        try:\\n            from llama_cpp import Llama\\n            values[\"client\"] = Llama(\\n                model_path=model_path,\\n                n_ctx=n_ctx,\\n                n_parts=n_parts,\\n                seed=seed,\\n                f16_kv=f16_kv,\\n                logits_all=logits_all,\\n                vocab_only=vocab_only,\\n                use_mlock=use_mlock,\\n                n_threads=n_threads,\\n                n_batch=n_batch,\\n                last_n_tokens_size=last_n_tokens_size,\\n            )\\n        except ImportError:\\n            raise ModuleNotFoundError(\\n                \"Could not import llama-cpp-python library. \"\\n                \"Please install the llama-cpp-python library to \"\\n                \"use this embedding model: pip install llama-cpp-python\"\\n            )\\n        except Exception:\\n            raise NameError(f\"Could not load Llama model from path: {model_path}\")\\n        return values\\n    @property\\n    def _default_params(self) -> Dict[str, Any]:\\n        \"\"\"Get the default parameters for calling llama_cpp.\"\"\"\\n        return {\\n            \"suffix\": self.suffix,\\n            \"max_tokens\": self.max_tokens,\\n            \"temperature\": self.temperature,\\n            \"top_p\": self.top_p,\\n            \"logprobs\": self.logprobs,\\n            \"echo\": self.echo,\\n            \"stop_sequences\": self.stop,\\n            \"repeat_penalty\": self.repeat_penalty,\\n            \"top_k\": self.top_k,\\n        }\\n    @property\\n    def _identifying_params(self) -> Dict[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {**{\"model_path\": self.model_path}, **self._default_params}\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"llama.cpp\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call the Llama model and return the output.\\n        Args:\\n            prompt: The prompt to use for generation.\\n            stop: A list of strings to stop generation when encountered.\\n        Returns:\\n            The generated text.\\n        Example:\\n            .. code-block:: python\\n                from langchain.llms import LlamaCppEmbeddings\\n                llm = LlamaCppEmbeddings(model_path=\"/path/to/local/llama/model.bin\")\\n                llm(\"This is a prompt.\")\\n        \"\"\"\\n        params = self._default_params\\n        if self.stop and stop is not None:\\n            raise ValueError(\"`stop` found in both the input and default params.\")\\n        elif self.stop:\\n            params[\"stop_sequences\"] = self.stop\\n        elif stop:\\n            params[\"stop_sequences\"] = stop\\n        else:\\n            params[\"stop_sequences\"] = []\\n        \"\"\"Call the Llama model and return the output.\"\"\"\\n        text = self.client(\\n            prompt=prompt,\\n            max_tokens=params[\"max_tokens\"],\\n            temperature=params[\"temperature\"],\\n            top_p=params[\"top_p\"],\\n            logprobs=params[\"logprobs\"],\\n            echo=params[\"echo\"],\\n            stop=params[\"stop_sequences\"],\\n            repeat_penalty=params[\"repeat_penalty\"],\\n            top_k=params[\"top_k\"],\\n        )\\n        return text[\"choices\"][0][\"text\"]\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\llamacpp.html'}),\n",
              " Document(page_content='Source code for langchain.llms.modal\\n\"\"\"Wrapper around Modal API.\"\"\"\\nimport logging\\nfrom typing import Any, Dict, List, Mapping, Optional\\nimport requests\\nfrom pydantic import Extra, Field, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.llms.utils import enforce_stop_tokens\\nlogger = logging.getLogger(__name__)\\n[docs]class Modal(LLM):\\n    \"\"\"Wrapper around Modal large language models.\\n    To use, you should have the ``modal-client`` python package installed.\\n    Any parameters that are valid to be passed to the call can be passed\\n    in, even if not explicitly saved on this class.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import Modal\\n            modal = Modal(endpoint_url=\"\")\\n    \"\"\"\\n    endpoint_url: str = \"\"\\n    \"\"\"model endpoint to use\"\"\"\\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\\n    \"\"\"Holds any model parameters valid for `create` call not\\n    explicitly specified.\"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic config.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator(pre=True)\\n    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Build extra kwargs from additional params that were passed in.\"\"\"\\n        all_required_field_names = {field.alias for field in cls.__fields__.values()}\\n        extra = values.get(\"model_kwargs\", {})\\n        for field_name in list(values):\\n            if field_name not in all_required_field_names:\\n                if field_name in extra:\\n                    raise ValueError(f\"Found {field_name} supplied twice.\")\\n                logger.warning(\\n                    f\"\"\"{field_name} was transfered to model_kwargs.\\n                    Please confirm that {field_name} is what you intended.\"\"\"\\n                )\\n                extra[field_name] = values.pop(field_name)\\n        values[\"model_kwargs\"] = extra\\n        return values\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {\\n            **{\"endpoint_url\": self.endpoint_url},\\n            **{\"model_kwargs\": self.model_kwargs},\\n        }\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"modal\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call to Modal endpoint.\"\"\"\\n        params = self.model_kwargs or {}\\n        response = requests.post(\\n            url=self.endpoint_url,\\n            headers={\\n                \"Content-Type\": \"application/json\",\\n            },\\n            json={\"prompt\": prompt, **params},\\n        )\\n        try:\\n            if prompt in response.json()[\"prompt\"]:\\n                response_json = response.json()\\n        except KeyError:\\n            raise ValueError(\"LangChain requires \\'prompt\\' key in response.\")\\n        text = response_json[\"prompt\"]\\n        if stop is not None:\\n            # I believe this is required since the stop tokens\\n            # are not enforced by the model parameters\\n            text = enforce_stop_tokens(text, stop)\\n        return text\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\modal.html'}),\n",
              " Document(page_content='Source code for langchain.llms.nlpcloud\\n\"\"\"Wrapper around NLPCloud APIs.\"\"\"\\nfrom typing import Any, Dict, List, Mapping, Optional\\nfrom pydantic import Extra, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.utils import get_from_dict_or_env\\n[docs]class NLPCloud(LLM):\\n    \"\"\"Wrapper around NLPCloud large language models.\\n    To use, you should have the ``nlpcloud`` python package installed, and the\\n    environment variable ``NLPCLOUD_API_KEY`` set with your API key.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import NLPCloud\\n            nlpcloud = NLPCloud(model=\"gpt-neox-20b\")\\n    \"\"\"\\n    client: Any  #: :meta private:\\n    model_name: str = \"finetuned-gpt-neox-20b\"\\n    \"\"\"Model name to use.\"\"\"\\n    temperature: float = 0.7\\n    \"\"\"What sampling temperature to use.\"\"\"\\n    min_length: int = 1\\n    \"\"\"The minimum number of tokens to generate in the completion.\"\"\"\\n    max_length: int = 256\\n    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\\n    length_no_input: bool = True\\n    \"\"\"Whether min_length and max_length should include the length of the input.\"\"\"\\n    remove_input: bool = True\\n    \"\"\"Remove input text from API response\"\"\"\\n    remove_end_sequence: bool = True\\n    \"\"\"Whether or not to remove the end sequence token.\"\"\"\\n    bad_words: List[str] = []\\n    \"\"\"List of tokens not allowed to be generated.\"\"\"\\n    top_p: int = 1\\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\\n    top_k: int = 50\\n    \"\"\"The number of highest probability tokens to keep for top-k filtering.\"\"\"\\n    repetition_penalty: float = 1.0\\n    \"\"\"Penalizes repeated tokens. 1.0 means no penalty.\"\"\"\\n    length_penalty: float = 1.0\\n    \"\"\"Exponential penalty to the length.\"\"\"\\n    do_sample: bool = True\\n    \"\"\"Whether to use sampling (True) or greedy decoding.\"\"\"\\n    num_beams: int = 1\\n    \"\"\"Number of beams for beam search.\"\"\"\\n    early_stopping: bool = False\\n    \"\"\"Whether to stop beam search at num_beams sentences.\"\"\"\\n    num_return_sequences: int = 1\\n    \"\"\"How many completions to generate for each prompt.\"\"\"\\n    nlpcloud_api_key: Optional[str] = None\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        nlpcloud_api_key = get_from_dict_or_env(\\n            values, \"nlpcloud_api_key\", \"NLPCLOUD_API_KEY\"\\n        )\\n        try:\\n            import nlpcloud\\n            values[\"client\"] = nlpcloud.Client(\\n                values[\"model_name\"], nlpcloud_api_key, gpu=True, lang=\"en\"\\n            )\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import nlpcloud python package. \"\\n                \"Please install it with `pip install nlpcloud`.\"\\n            )\\n        return values\\n    @property\\n    def _default_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the default parameters for calling NLPCloud API.\"\"\"\\n        return {\\n            \"temperature\": self.temperature,\\n            \"min_length\": self.min_length,\\n            \"max_length\": self.max_length,\\n            \"length_no_input\": self.length_no_input,\\n            \"remove_input\": self.remove_input,\\n            \"remove_end_sequence\": self.remove_end_sequence,\\n            \"bad_words\": self.bad_words,\\n            \"top_p\": self.top_p,\\n            \"top_k\": self.top_k,\\n            \"repetition_penalty\": self.repetition_penalty,\\n            \"length_penalty\": self.length_penalty,\\n            \"do_sample\": self.do_sample,\\n            \"num_beams\": self.num_beams,\\n            \"early_stopping\": self.early_stopping,\\n            \"num_return_sequences\": self.num_return_sequences,\\n        }\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {**{\"model_name\": self.model_name}, **self._default_params}\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"nlpcloud\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call out to NLPCloud\\'s create endpoint.\\n        Args:\\n            prompt: The prompt to pass into the model.\\n            stop: Not supported by this interface (pass in init method)\\n        Returns:\\n            The string generated by the model.\\n        Example:\\n            .. code-block:: python\\n                response = nlpcloud(\"Tell me a joke.\")\\n        \"\"\"\\n        if stop and len(stop) > 1:\\n            raise ValueError(\\n                \"NLPCloud only supports a single stop sequence per generation.\"\\n                \"Pass in a list of length 1.\"\\n            )\\n        elif stop and len(stop) == 1:\\n            end_sequence = stop[0]\\n        else:\\n            end_sequence = None\\n        response = self.client.generation(\\n            prompt, end_sequence=end_sequence, **self._default_params\\n        )\\n        return response[\"generated_text\"]\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\nlpcloud.html'}),\n",
              " Document(page_content='Source code for langchain.llms.openai\\n\"\"\"Wrapper around OpenAI APIs.\"\"\"\\nfrom __future__ import annotations\\nimport logging\\nimport sys\\nimport warnings\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Generator,\\n    List,\\n    Literal,\\n    Mapping,\\n    Optional,\\n    Set,\\n    Tuple,\\n    Union,\\n)\\nfrom pydantic import Extra, Field, root_validator\\nfrom tenacity import (\\n    before_sleep_log,\\n    retry,\\n    retry_if_exception_type,\\n    stop_after_attempt,\\n    wait_exponential,\\n)\\nfrom langchain.llms.base import BaseLLM\\nfrom langchain.schema import Generation, LLMResult\\nfrom langchain.utils import get_from_dict_or_env\\nlogger = logging.getLogger(__name__)\\ndef update_token_usage(\\n    keys: Set[str], response: Dict[str, Any], token_usage: Dict[str, Any]\\n) -> None:\\n    \"\"\"Update token usage.\"\"\"\\n    _keys_to_use = keys.intersection(response[\"usage\"])\\n    for _key in _keys_to_use:\\n        if _key not in token_usage:\\n            token_usage[_key] = response[\"usage\"][_key]\\n        else:\\n            token_usage[_key] += response[\"usage\"][_key]\\ndef _update_response(response: Dict[str, Any], stream_response: Dict[str, Any]) -> None:\\n    \"\"\"Update response from the stream response.\"\"\"\\n    response[\"choices\"][0][\"text\"] += stream_response[\"choices\"][0][\"text\"]\\n    response[\"choices\"][0][\"finish_reason\"] = stream_response[\"choices\"][0][\\n        \"finish_reason\"\\n    ]\\n    response[\"choices\"][0][\"logprobs\"] = stream_response[\"choices\"][0][\"logprobs\"]\\ndef _streaming_response_template() -> Dict[str, Any]:\\n    return {\\n        \"choices\": [\\n            {\\n                \"text\": \"\",\\n                \"finish_reason\": None,\\n                \"logprobs\": None,\\n            }\\n        ]\\n    }\\ndef _create_retry_decorator(llm: Union[BaseOpenAI, OpenAIChat]) -> Callable[[Any], Any]:\\n    import openai\\n    min_seconds = 4\\n    max_seconds = 10\\n    # Wait 2^x * 1 second between each retry starting with\\n    # 4 seconds, then up to 10 seconds, then 10 seconds afterwards\\n    return retry(\\n        reraise=True,\\n        stop=stop_after_attempt(llm.max_retries),\\n        wait=wait_exponential(multiplier=1, min=min_seconds, max=max_seconds),\\n        retry=(\\n            retry_if_exception_type(openai.error.Timeout)\\n            | retry_if_exception_type(openai.error.APIError)\\n            | retry_if_exception_type(openai.error.APIConnectionError)\\n            | retry_if_exception_type(openai.error.RateLimitError)\\n            | retry_if_exception_type(openai.error.ServiceUnavailableError)\\n        ),\\n        before_sleep=before_sleep_log(logger, logging.WARNING),\\n    )\\ndef completion_with_retry(llm: Union[BaseOpenAI, OpenAIChat], **kwargs: Any) -> Any:\\n    \"\"\"Use tenacity to retry the completion call.\"\"\"\\n    retry_decorator = _create_retry_decorator(llm)\\n    @retry_decorator\\n    def _completion_with_retry(**kwargs: Any) -> Any:\\n        return llm.client.create(**kwargs)\\n    return _completion_with_retry(**kwargs)\\nasync def acompletion_with_retry(\\n    llm: Union[BaseOpenAI, OpenAIChat], **kwargs: Any\\n) -> Any:\\n    \"\"\"Use tenacity to retry the async completion call.\"\"\"\\n    retry_decorator = _create_retry_decorator(llm)\\n    @retry_decorator\\n    async def _completion_with_retry(**kwargs: Any) -> Any:\\n        # Use OpenAI\\'s async api https://github.com/openai/openai-python#async-api\\n        return await llm.client.acreate(**kwargs)\\n    return await _completion_with_retry(**kwargs)\\nclass BaseOpenAI(BaseLLM):\\n    \"\"\"Wrapper around OpenAI large language models.\"\"\"\\n    client: Any  #: :meta private:\\n    model_name: str = \"text-davinci-003\"\\n    \"\"\"Model name to use.\"\"\"\\n    temperature: float = 0.7\\n    \"\"\"What sampling temperature to use.\"\"\"\\n    max_tokens: int = 256\\n    \"\"\"The maximum number of tokens to generate in the completion.\\n    -1 returns as many tokens as possible given the prompt and\\n    the models maximal context size.\"\"\"\\n    top_p: float = 1\\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\\n    frequency_penalty: float = 0\\n    \"\"\"Penalizes repeated tokens according to frequency.\"\"\"\\n    presence_penalty: float = 0\\n    \"\"\"Penalizes repeated tokens.\"\"\"\\n    n: int = 1\\n    \"\"\"How many completions to generate for each prompt.\"\"\"\\n    best_of: int = 1\\n    \"\"\"Generates best_of completions server-side and returns the \"best\".\"\"\"\\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\\n    \"\"\"Holds any model parameters valid for `create` call not explicitly specified.\"\"\"\\n    openai_api_key: Optional[str] = None\\n    openai_api_base: Optional[str] = None\\n    openai_organization: Optional[str] = None\\n    batch_size: int = 20\\n    \"\"\"Batch size to use when passing multiple documents to generate.\"\"\"\\n    request_timeout: Optional[Union[float, Tuple[float, float]]] = None\\n    \"\"\"Timeout for requests to OpenAI completion API. Default is 600 seconds.\"\"\"\\n    logit_bias: Optional[Dict[str, float]] = Field(default_factory=dict)\\n    \"\"\"Adjust the probability of specific tokens being generated.\"\"\"\\n    max_retries: int = 6\\n    \"\"\"Maximum number of retries to make when generating.\"\"\"\\n    streaming: bool = False\\n    \"\"\"Whether to stream the results or not.\"\"\"\\n    allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set()\\n    \"\"\"Set of special tokens that are allowed。\"\"\"\\n    disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\"\\n    \"\"\"Set of special tokens that are not allowed。\"\"\"\\n    def __new__(cls, **data: Any) -> Union[OpenAIChat, BaseOpenAI]:  # type: ignore\\n        \"\"\"Initialize the OpenAI object.\"\"\"\\n        model_name = data.get(\"model_name\", \"\")\\n        if model_name.startswith(\"gpt-3.5-turbo\") or model_name.startswith(\"gpt-4\"):\\n            warnings.warn(\\n                \"You are trying to use a chat model. This way of initializing it is \"\\n                \"no longer supported. Instead, please use: \"\\n                \"`from langchain.chat_models import ChatOpenAI`\"\\n            )\\n            return OpenAIChat(**data)\\n        return super().__new__(cls)\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.ignore\\n    @root_validator(pre=True)\\n    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Build extra kwargs from additional params that were passed in.\"\"\"\\n        all_required_field_names = {field.alias for field in cls.__fields__.values()}\\n        extra = values.get(\"model_kwargs\", {})\\n        for field_name in list(values):\\n            if field_name not in all_required_field_names:\\n                if field_name in extra:\\n                    raise ValueError(f\"Found {field_name} supplied twice.\")\\n                logger.warning(\\n                    f\"\"\"WARNING! {field_name} is not default parameter.\\n                    {field_name} was transfered to model_kwargs.\\n                    Please confirm that {field_name} is what you intended.\"\"\"\\n                )\\n                extra[field_name] = values.pop(field_name)\\n        values[\"model_kwargs\"] = extra\\n        return values\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        openai_api_key = get_from_dict_or_env(\\n            values, \"openai_api_key\", \"OPENAI_API_KEY\"\\n        )\\n        openai_api_base = get_from_dict_or_env(\\n            values,\\n            \"openai_api_base\",\\n            \"OPENAI_API_BASE\",\\n            default=\"\",\\n        )\\n        openai_organization = get_from_dict_or_env(\\n            values,\\n            \"openai_organization\",\\n            \"OPENAI_ORGANIZATION\",\\n            default=\"\",\\n        )\\n        try:\\n            import openai\\n            openai.api_key = openai_api_key\\n            if openai_api_base:\\n                print(\"USING API_BASE: \")\\n                print(openai_api_base)\\n                openai.api_base = openai_api_base\\n            if openai_organization:\\n                print(\"USING ORGANIZATION: \")\\n                print(openai_organization)\\n                openai.organization = openai_organization\\n            values[\"client\"] = openai.Completion\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import openai python package. \"\\n                \"Please install it with `pip install openai`.\"\\n            )\\n        if values[\"streaming\"] and values[\"n\"] > 1:\\n            raise ValueError(\"Cannot stream results when n > 1.\")\\n        if values[\"streaming\"] and values[\"best_of\"] > 1:\\n            raise ValueError(\"Cannot stream results when best_of > 1.\")\\n        return values\\n    @property\\n    def _default_params(self) -> Dict[str, Any]:\\n        \"\"\"Get the default parameters for calling OpenAI API.\"\"\"\\n        normal_params = {\\n            \"temperature\": self.temperature,\\n            \"max_tokens\": self.max_tokens,\\n            \"top_p\": self.top_p,\\n            \"frequency_penalty\": self.frequency_penalty,\\n            \"presence_penalty\": self.presence_penalty,\\n            \"n\": self.n,\\n            \"best_of\": self.best_of,\\n            \"request_timeout\": self.request_timeout,\\n            \"logit_bias\": self.logit_bias,\\n        }\\n        return {**normal_params, **self.model_kwargs}\\n    def _generate(\\n        self, prompts: List[str], stop: Optional[List[str]] = None\\n    ) -> LLMResult:\\n        \"\"\"Call out to OpenAI\\'s endpoint with k unique prompts.\\n        Args:\\n            prompts: The prompts to pass into the model.\\n            stop: Optional list of stop words to use when generating.\\n        Returns:\\n            The full LLM output.\\n        Example:\\n            .. code-block:: python\\n                response = openai.generate([\"Tell me a joke.\"])\\n        \"\"\"\\n        # TODO: write a unit test for this\\n        params = self._invocation_params\\n        sub_prompts = self.get_sub_prompts(params, prompts, stop)\\n        choices = []\\n        token_usage: Dict[str, int] = {}\\n        # Get the token usage from the response.\\n        # Includes prompt, completion, and total tokens used.\\n        _keys = {\"completion_tokens\", \"prompt_tokens\", \"total_tokens\"}\\n        for _prompts in sub_prompts:\\n            if self.streaming:\\n                if len(_prompts) > 1:\\n                    raise ValueError(\"Cannot stream results with multiple prompts.\")\\n                params[\"stream\"] = True\\n                response = _streaming_response_template()\\n                for stream_resp in completion_with_retry(\\n                    self, prompt=_prompts, **params\\n                ):\\n                    self.callback_manager.on_llm_new_token(\\n                        stream_resp[\"choices\"][0][\"text\"],\\n                        verbose=self.verbose,\\n                        logprobs=stream_resp[\"choices\"][0][\"logprobs\"],\\n                    )\\n                    _update_response(response, stream_resp)\\n                choices.extend(response[\"choices\"])\\n            else:\\n                response = completion_with_retry(self, prompt=_prompts, **params)\\n                choices.extend(response[\"choices\"])\\n            if not self.streaming:\\n                # Can\\'t update token usage if streaming\\n                update_token_usage(_keys, response, token_usage)\\n        return self.create_llm_result(choices, prompts, token_usage)\\n    async def _agenerate(\\n        self, prompts: List[str], stop: Optional[List[str]] = None\\n    ) -> LLMResult:\\n        \"\"\"Call out to OpenAI\\'s endpoint async with k unique prompts.\"\"\"\\n        params = self._invocation_params\\n        sub_prompts = self.get_sub_prompts(params, prompts, stop)\\n        choices = []\\n        token_usage: Dict[str, int] = {}\\n        # Get the token usage from the response.\\n        # Includes prompt, completion, and total tokens used.\\n        _keys = {\"completion_tokens\", \"prompt_tokens\", \"total_tokens\"}\\n        for _prompts in sub_prompts:\\n            if self.streaming:\\n                if len(_prompts) > 1:\\n                    raise ValueError(\"Cannot stream results with multiple prompts.\")\\n                params[\"stream\"] = True\\n                response = _streaming_response_template()\\n                async for stream_resp in await acompletion_with_retry(\\n                    self, prompt=_prompts, **params\\n                ):\\n                    if self.callback_manager.is_async:\\n                        await self.callback_manager.on_llm_new_token(\\n                            stream_resp[\"choices\"][0][\"text\"],\\n                            verbose=self.verbose,\\n                            logprobs=stream_resp[\"choices\"][0][\"logprobs\"],\\n                        )\\n                    else:\\n                        self.callback_manager.on_llm_new_token(\\n                            stream_resp[\"choices\"][0][\"text\"],\\n                            verbose=self.verbose,\\n                            logprobs=stream_resp[\"choices\"][0][\"logprobs\"],\\n                        )\\n                    _update_response(response, stream_resp)\\n                choices.extend(response[\"choices\"])\\n            else:\\n                response = await acompletion_with_retry(self, prompt=_prompts, **params)\\n                choices.extend(response[\"choices\"])\\n            if not self.streaming:\\n                # Can\\'t update token usage if streaming\\n                update_token_usage(_keys, response, token_usage)\\n        return self.create_llm_result(choices, prompts, token_usage)\\n    def get_sub_prompts(\\n        self,\\n        params: Dict[str, Any],\\n        prompts: List[str],\\n        stop: Optional[List[str]] = None,\\n    ) -> List[List[str]]:\\n        \"\"\"Get the sub prompts for llm call.\"\"\"\\n        if stop is not None:\\n            if \"stop\" in params:\\n                raise ValueError(\"`stop` found in both the input and default params.\")\\n            params[\"stop\"] = stop\\n        if params[\"max_tokens\"] == -1:\\n            if len(prompts) != 1:\\n                raise ValueError(\\n                    \"max_tokens set to -1 not supported for multiple inputs.\"\\n                )\\n            params[\"max_tokens\"] = self.max_tokens_for_prompt(prompts[0])\\n        sub_prompts = [\\n            prompts[i : i + self.batch_size]\\n            for i in range(0, len(prompts), self.batch_size)\\n        ]\\n        return sub_prompts\\n    def create_llm_result(\\n        self, choices: Any, prompts: List[str], token_usage: Dict[str, int]\\n    ) -> LLMResult:\\n        \"\"\"Create the LLMResult from the choices and prompts.\"\"\"\\n        generations = []\\n        for i, _ in enumerate(prompts):\\n            sub_choices = choices[i * self.n : (i + 1) * self.n]\\n            generations.append(\\n                [\\n                    Generation(\\n                        text=choice[\"text\"],\\n                        generation_info=dict(\\n                            finish_reason=choice.get(\"finish_reason\"),\\n                            logprobs=choice.get(\"logprobs\"),\\n                        ),\\n                    )\\n                    for choice in sub_choices\\n                ]\\n            )\\n        llm_output = {\"token_usage\": token_usage, \"model_name\": self.model_name}\\n        return LLMResult(generations=generations, llm_output=llm_output)\\n    def stream(self, prompt: str, stop: Optional[List[str]] = None) -> Generator:\\n        \"\"\"Call OpenAI with streaming flag and return the resulting generator.\\n        BETA: this is a beta feature while we figure out the right abstraction.\\n        Once that happens, this interface could change.\\n        Args:\\n            prompt: The prompts to pass into the model.\\n            stop: Optional list of stop words to use when generating.\\n        Returns:\\n            A generator representing the stream of tokens from OpenAI.\\n        Example:\\n            .. code-block:: python\\n                generator = openai.stream(\"Tell me a joke.\")\\n                for token in generator:\\n                    yield token\\n        \"\"\"\\n        params = self.prep_streaming_params(stop)\\n        generator = self.client.create(prompt=prompt, **params)\\n        return generator\\n    def prep_streaming_params(self, stop: Optional[List[str]] = None) -> Dict[str, Any]:\\n        \"\"\"Prepare the params for streaming.\"\"\"\\n        params = self._invocation_params\\n        if params[\"best_of\"] != 1:\\n            raise ValueError(\"OpenAI only supports best_of == 1 for streaming\")\\n        if stop is not None:\\n            if \"stop\" in params:\\n                raise ValueError(\"`stop` found in both the input and default params.\")\\n            params[\"stop\"] = stop\\n        params[\"stream\"] = True\\n        return params\\n    @property\\n    def _invocation_params(self) -> Dict[str, Any]:\\n        \"\"\"Get the parameters used to invoke the model.\"\"\"\\n        return self._default_params\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {**{\"model_name\": self.model_name}, **self._default_params}\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"openai\"\\n    def get_num_tokens(self, text: str) -> int:\\n        \"\"\"Calculate num tokens with tiktoken package.\"\"\"\\n        # tiktoken NOT supported for Python < 3.8\\n        if sys.version_info[1] < 8:\\n            return super().get_num_tokens(text)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate get_num_tokens. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n        enc = tiktoken.encoding_for_model(self.model_name)\\n        tokenized_text = enc.encode(\\n            text,\\n            allowed_special=self.allowed_special,\\n            disallowed_special=self.disallowed_special,\\n        )\\n        # calculate the number of tokens in the encoded text\\n        return len(tokenized_text)\\n    def modelname_to_contextsize(self, modelname: str) -> int:\\n        \"\"\"Calculate the maximum number of tokens possible to generate for a model.\\n        Args:\\n            modelname: The modelname we want to know the context size for.\\n        Returns:\\n            The maximum context size\\n        Example:\\n            .. code-block:: python\\n                max_tokens = openai.modelname_to_contextsize(\"text-davinci-003\")\\n        \"\"\"\\n        model_token_mapping = {\\n            \"gpt-4\": 8192,\\n            \"gpt-4-0314\": 8192,\\n            \"gpt-4-32k\": 32768,\\n            \"gpt-4-32k-0314\": 32768,\\n            \"gpt-3.5-turbo\": 4096,\\n            \"gpt-3.5-turbo-0301\": 4096,\\n            \"text-ada-001\": 2049,\\n            \"ada\": 2049,\\n            \"text-babbage-001\": 2040,\\n            \"babbage\": 2049,\\n            \"text-curie-001\": 2049,\\n            \"curie\": 2049,\\n            \"davinci\": 2049,\\n            \"text-davinci-003\": 4097,\\n            \"text-davinci-002\": 4097,\\n            \"code-davinci-002\": 8001,\\n            \"code-davinci-001\": 8001,\\n            \"code-cushman-002\": 2048,\\n            \"code-cushman-001\": 2048,\\n        }\\n        context_size = model_token_mapping.get(modelname, None)\\n        if context_size is None:\\n            raise ValueError(\\n                f\"Unknown model: {modelname}. Please provide a valid OpenAI model name.\"\\n                \"Known models are: \" + \", \".join(model_token_mapping.keys())\\n            )\\n        return context_size\\n    def max_tokens_for_prompt(self, prompt: str) -> int:\\n        \"\"\"Calculate the maximum number of tokens possible to generate for a prompt.\\n        Args:\\n            prompt: The prompt to pass into the model.\\n        Returns:\\n            The maximum number of tokens to generate for a prompt.\\n        Example:\\n            .. code-block:: python\\n                max_tokens = openai.max_token_for_prompt(\"Tell me a joke.\")\\n        \"\"\"\\n        num_tokens = self.get_num_tokens(prompt)\\n        # get max context size for model by name\\n        max_size = self.modelname_to_contextsize(self.model_name)\\n        return max_size - num_tokens\\n[docs]class OpenAI(BaseOpenAI):\\n    \"\"\"Wrapper around OpenAI large language models.\\n    To use, you should have the ``openai`` python package installed, and the\\n    environment variable ``OPENAI_API_KEY`` set with your API key.\\n    Any parameters that are valid to be passed to the openai.create call can be passed\\n    in, even if not explicitly saved on this class.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import OpenAI\\n            openai = OpenAI(model_name=\"text-davinci-003\")\\n    \"\"\"\\n    @property\\n    def _invocation_params(self) -> Dict[str, Any]:\\n        return {**{\"model\": self.model_name}, **super()._invocation_params}\\n[docs]class AzureOpenAI(BaseOpenAI):\\n    \"\"\"Wrapper around Azure-specific OpenAI large language models.\\n    To use, you should have the ``openai`` python package installed, and the\\n    environment variable ``OPENAI_API_KEY`` set with your API key.\\n    Any parameters that are valid to be passed to the openai.create call can be passed\\n    in, even if not explicitly saved on this class.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import AzureOpenAI\\n            openai = AzureOpenAI(model_name=\"text-davinci-003\")\\n    \"\"\"\\n    deployment_name: str = \"\"\\n    \"\"\"Deployment name to use.\"\"\"\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        return {\\n            **{\"deployment_name\": self.deployment_name},\\n            **super()._identifying_params,\\n        }\\n    @property\\n    def _invocation_params(self) -> Dict[str, Any]:\\n        return {**{\"engine\": self.deployment_name}, **super()._invocation_params}\\n[docs]class OpenAIChat(BaseLLM):\\n    \"\"\"Wrapper around OpenAI Chat large language models.\\n    To use, you should have the ``openai`` python package installed, and the\\n    environment variable ``OPENAI_API_KEY`` set with your API key.\\n    Any parameters that are valid to be passed to the openai.create call can be passed\\n    in, even if not explicitly saved on this class.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import OpenAIChat\\n            openaichat = OpenAIChat(model_name=\"gpt-3.5-turbo\")\\n    \"\"\"\\n    client: Any  #: :meta private:\\n    model_name: str = \"gpt-3.5-turbo\"\\n    \"\"\"Model name to use.\"\"\"\\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\\n    \"\"\"Holds any model parameters valid for `create` call not explicitly specified.\"\"\"\\n    openai_api_key: Optional[str] = None\\n    openai_api_base: Optional[str] = None\\n    max_retries: int = 6\\n    \"\"\"Maximum number of retries to make when generating.\"\"\"\\n    prefix_messages: List = Field(default_factory=list)\\n    \"\"\"Series of messages for Chat input.\"\"\"\\n    streaming: bool = False\\n    \"\"\"Whether to stream the results or not.\"\"\"\\n    allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set()\\n    \"\"\"Set of special tokens that are allowed。\"\"\"\\n    disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\"\\n    \"\"\"Set of special tokens that are not allowed。\"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.ignore\\n    @root_validator(pre=True)\\n    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Build extra kwargs from additional params that were passed in.\"\"\"\\n        all_required_field_names = {field.alias for field in cls.__fields__.values()}\\n        extra = values.get(\"model_kwargs\", {})\\n        for field_name in list(values):\\n            if field_name not in all_required_field_names:\\n                if field_name in extra:\\n                    raise ValueError(f\"Found {field_name} supplied twice.\")\\n                extra[field_name] = values.pop(field_name)\\n        values[\"model_kwargs\"] = extra\\n        return values\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        openai_api_key = get_from_dict_or_env(\\n            values, \"openai_api_key\", \"OPENAI_API_KEY\"\\n        )\\n        openai_api_base = get_from_dict_or_env(\\n            values,\\n            \"openai_api_base\",\\n            \"OPENAI_API_BASE\",\\n            default=\"\",\\n        )\\n        openai_organization = get_from_dict_or_env(\\n            values, \"openai_organization\", \"OPENAI_ORGANIZATION\", default=\"\"\\n        )\\n        try:\\n            import openai\\n            openai.api_key = openai_api_key\\n            if openai_api_base:\\n                openai.api_base = openai_api_base\\n            if openai_organization:\\n                openai.organization = openai_organization\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import openai python package. \"\\n                \"Please install it with `pip install openai`.\"\\n            )\\n        try:\\n            values[\"client\"] = openai.ChatCompletion\\n        except AttributeError:\\n            raise ValueError(\\n                \"`openai` has no `ChatCompletion` attribute, this is likely \"\\n                \"due to an old version of the openai package. Try upgrading it \"\\n                \"with `pip install --upgrade openai`.\"\\n            )\\n        warnings.warn(\\n            \"You are trying to use a chat model. This way of initializing it is \"\\n            \"no longer supported. Instead, please use: \"\\n            \"`from langchain.chat_models import ChatOpenAI`\"\\n        )\\n        return values\\n    @property\\n    def _default_params(self) -> Dict[str, Any]:\\n        \"\"\"Get the default parameters for calling OpenAI API.\"\"\"\\n        return self.model_kwargs\\n    def _get_chat_params(\\n        self, prompts: List[str], stop: Optional[List[str]] = None\\n    ) -> Tuple:\\n        if len(prompts) > 1:\\n            raise ValueError(\\n                f\"OpenAIChat currently only supports single prompt, got {prompts}\"\\n            )\\n        messages = self.prefix_messages + [{\"role\": \"user\", \"content\": prompts[0]}]\\n        params: Dict[str, Any] = {**{\"model\": self.model_name}, **self._default_params}\\n        if stop is not None:\\n            if \"stop\" in params:\\n                raise ValueError(\"`stop` found in both the input and default params.\")\\n            params[\"stop\"] = stop\\n        if params.get(\"max_tokens\") == -1:\\n            # for ChatGPT api, omitting max_tokens is equivalent to having no limit\\n            del params[\"max_tokens\"]\\n        return messages, params\\n    def _generate(\\n        self, prompts: List[str], stop: Optional[List[str]] = None\\n    ) -> LLMResult:\\n        messages, params = self._get_chat_params(prompts, stop)\\n        if self.streaming:\\n            response = \"\"\\n            params[\"stream\"] = True\\n            for stream_resp in completion_with_retry(self, messages=messages, **params):\\n                token = stream_resp[\"choices\"][0][\"delta\"].get(\"content\", \"\")\\n                response += token\\n                self.callback_manager.on_llm_new_token(\\n                    token,\\n                    verbose=self.verbose,\\n                )\\n            return LLMResult(\\n                generations=[[Generation(text=response)]],\\n            )\\n        else:\\n            full_response = completion_with_retry(self, messages=messages, **params)\\n            llm_output = {\\n                \"token_usage\": full_response[\"usage\"],\\n                \"model_name\": self.model_name,\\n            }\\n            return LLMResult(\\n                generations=[\\n                    [Generation(text=full_response[\"choices\"][0][\"message\"][\"content\"])]\\n                ],\\n                llm_output=llm_output,\\n            )\\n    async def _agenerate(\\n        self, prompts: List[str], stop: Optional[List[str]] = None\\n    ) -> LLMResult:\\n        messages, params = self._get_chat_params(prompts, stop)\\n        if self.streaming:\\n            response = \"\"\\n            params[\"stream\"] = True\\n            async for stream_resp in await acompletion_with_retry(\\n                self, messages=messages, **params\\n            ):\\n                token = stream_resp[\"choices\"][0][\"delta\"].get(\"content\", \"\")\\n                response += token\\n                if self.callback_manager.is_async:\\n                    await self.callback_manager.on_llm_new_token(\\n                        token,\\n                        verbose=self.verbose,\\n                    )\\n                else:\\n                    self.callback_manager.on_llm_new_token(\\n                        token,\\n                        verbose=self.verbose,\\n                    )\\n            return LLMResult(\\n                generations=[[Generation(text=response)]],\\n            )\\n        else:\\n            full_response = await acompletion_with_retry(\\n                self, messages=messages, **params\\n            )\\n            llm_output = {\\n                \"token_usage\": full_response[\"usage\"],\\n                \"model_name\": self.model_name,\\n            }\\n            return LLMResult(\\n                generations=[\\n                    [Generation(text=full_response[\"choices\"][0][\"message\"][\"content\"])]\\n                ],\\n                llm_output=llm_output,\\n            )\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {**{\"model_name\": self.model_name}, **self._default_params}\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"openai-chat\"\\n[docs]    def get_num_tokens(self, text: str) -> int:\\n        \"\"\"Calculate num tokens with tiktoken package.\"\"\"\\n        # tiktoken NOT supported for Python < 3.8\\n        if sys.version_info[1] < 8:\\n            return super().get_num_tokens(text)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate get_num_tokens. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n        # create a GPT-3.5-Turbo encoder instance\\n        enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\\n        # encode the text using the GPT-3.5-Turbo encoder\\n        tokenized_text = enc.encode(\\n            text,\\n            allowed_special=self.allowed_special,\\n            disallowed_special=self.disallowed_special,\\n        )\\n        # calculate the number of tokens in the encoded text\\n        return len(tokenized_text)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\openai.html'}),\n",
              " Document(page_content='Source code for langchain.llms.petals\\n\"\"\"Wrapper around Petals API.\"\"\"\\nimport logging\\nfrom typing import Any, Dict, List, Mapping, Optional\\nfrom pydantic import Extra, Field, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.llms.utils import enforce_stop_tokens\\nfrom langchain.utils import get_from_dict_or_env\\nlogger = logging.getLogger(__name__)\\n[docs]class Petals(LLM):\\n    \"\"\"Wrapper around Petals Bloom models.\\n    To use, you should have the ``petals`` python package installed, and the\\n    environment variable ``HUGGINGFACE_API_KEY`` set with your API key.\\n    Any parameters that are valid to be passed to the call can be passed\\n    in, even if not explicitly saved on this class.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import petals\\n            petals = Petals()\\n    \"\"\"\\n    client: Any\\n    \"\"\"The client to use for the API calls.\"\"\"\\n    tokenizer: Any\\n    \"\"\"The tokenizer to use for the API calls.\"\"\"\\n    model_name: str = \"bigscience/bloom-petals\"\\n    \"\"\"The model to use.\"\"\"\\n    temperature: float = 0.7\\n    \"\"\"What sampling temperature to use\"\"\"\\n    max_new_tokens: int = 256\\n    \"\"\"The maximum number of new tokens to generate in the completion.\"\"\"\\n    top_p: float = 0.9\\n    \"\"\"The cumulative probability for top-p sampling.\"\"\"\\n    top_k: Optional[int] = None\\n    \"\"\"The number of highest probability vocabulary tokens\\n    to keep for top-k-filtering.\"\"\"\\n    do_sample: bool = True\\n    \"\"\"Whether or not to use sampling; use greedy decoding otherwise.\"\"\"\\n    max_length: Optional[int] = None\\n    \"\"\"The maximum length of the sequence to be generated.\"\"\"\\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\\n    \"\"\"Holds any model parameters valid for `create` call\\n    not explicitly specified.\"\"\"\\n    huggingface_api_key: Optional[str] = None\\n    class Config:\\n        \"\"\"Configuration for this pydantic config.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator(pre=True)\\n    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Build extra kwargs from additional params that were passed in.\"\"\"\\n        all_required_field_names = {field.alias for field in cls.__fields__.values()}\\n        extra = values.get(\"model_kwargs\", {})\\n        for field_name in list(values):\\n            if field_name not in all_required_field_names:\\n                if field_name in extra:\\n                    raise ValueError(f\"Found {field_name} supplied twice.\")\\n                logger.warning(\\n                    f\"\"\"WARNING! {field_name} is not default parameter.\\n                    {field_name} was transfered to model_kwargs.\\n                    Please confirm that {field_name} is what you intended.\"\"\"\\n                )\\n                extra[field_name] = values.pop(field_name)\\n        values[\"model_kwargs\"] = extra\\n        return values\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        huggingface_api_key = get_from_dict_or_env(\\n            values, \"huggingface_api_key\", \"HUGGINGFACE_API_KEY\"\\n        )\\n        try:\\n            from petals import DistributedBloomForCausalLM\\n            from transformers import BloomTokenizerFast\\n            model_name = values[\"model_name\"]\\n            values[\"tokenizer\"] = BloomTokenizerFast.from_pretrained(model_name)\\n            values[\"client\"] = DistributedBloomForCausalLM.from_pretrained(model_name)\\n            values[\"huggingface_api_key\"] = huggingface_api_key\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers or petals python package.\"\\n                \"Please install with `pip install -U transformers petals`.\"\\n            )\\n        return values\\n    @property\\n    def _default_params(self) -> Dict[str, Any]:\\n        \"\"\"Get the default parameters for calling Petals API.\"\"\"\\n        normal_params = {\\n            \"temperature\": self.temperature,\\n            \"max_new_tokens\": self.max_new_tokens,\\n            \"top_p\": self.top_p,\\n            \"top_k\": self.top_k,\\n            \"do_sample\": self.do_sample,\\n            \"max_length\": self.max_length,\\n        }\\n        return {**normal_params, **self.model_kwargs}\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {**{\"model_name\": self.model_name}, **self._default_params}\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"petals\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call the Petals API.\"\"\"\\n        params = self._default_params\\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\\n        outputs = self.client.generate(inputs, **params)\\n        text = self.tokenizer.decode(outputs[0])\\n        if stop is not None:\\n            # I believe this is required since the stop tokens\\n            # are not enforced by the model parameters\\n            text = enforce_stop_tokens(text, stop)\\n        return text\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\petals.html'}),\n",
              " Document(page_content='Source code for langchain.llms.promptlayer_openai\\n\"\"\"PromptLayer wrapper.\"\"\"\\nimport datetime\\nfrom typing import List, Optional\\nfrom langchain.llms import OpenAI, OpenAIChat\\nfrom langchain.schema import LLMResult\\n[docs]class PromptLayerOpenAI(OpenAI):\\n    \"\"\"Wrapper around OpenAI large language models.\\n    To use, you should have the ``openai`` and ``promptlayer`` python\\n    package installed, and the environment variable ``OPENAI_API_KEY``\\n    and ``PROMPTLAYER_API_KEY`` set with your openAI API key and\\n    promptlayer key respectively.\\n    All parameters that can be passed to the OpenAI LLM can also\\n    be passed here. The PromptLayerOpenAI LLM adds two optional\\n    parameters:\\n        ``pl_tags``: List of strings to tag the request with.\\n        ``return_pl_id``: If True, the PromptLayer request ID will be\\n            returned in the ``generation_info`` field of the\\n            ``Generation`` object.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import PromptLayerOpenAI\\n            openai = PromptLayerOpenAI(model_name=\"text-davinci-003\")\\n    \"\"\"\\n    pl_tags: Optional[List[str]]\\n    return_pl_id: Optional[bool] = False\\n    def _generate(\\n        self, prompts: List[str], stop: Optional[List[str]] = None\\n    ) -> LLMResult:\\n        \"\"\"Call OpenAI generate and then call PromptLayer API to log the request.\"\"\"\\n        from promptlayer.utils import get_api_key, promptlayer_api_request\\n        request_start_time = datetime.datetime.now().timestamp()\\n        generated_responses = super()._generate(prompts, stop)\\n        request_end_time = datetime.datetime.now().timestamp()\\n        for i in range(len(prompts)):\\n            prompt = prompts[i]\\n            generation = generated_responses.generations[i][0]\\n            resp = {\\n                \"text\": generation.text,\\n                \"llm_output\": generated_responses.llm_output,\\n            }\\n            pl_request_id = promptlayer_api_request(\\n                \"langchain.PromptLayerOpenAI\",\\n                \"langchain\",\\n                [prompt],\\n                self._identifying_params,\\n                self.pl_tags,\\n                resp,\\n                request_start_time,\\n                request_end_time,\\n                get_api_key(),\\n                return_pl_id=self.return_pl_id,\\n            )\\n            if self.return_pl_id:\\n                if generation.generation_info is None or not isinstance(\\n                    generation.generation_info, dict\\n                ):\\n                    generation.generation_info = {}\\n                generation.generation_info[\"pl_request_id\"] = pl_request_id\\n        return generated_responses\\n    async def _agenerate(\\n        self, prompts: List[str], stop: Optional[List[str]] = None\\n    ) -> LLMResult:\\n        from promptlayer.utils import get_api_key, promptlayer_api_request_async\\n        request_start_time = datetime.datetime.now().timestamp()\\n        generated_responses = await super()._agenerate(prompts, stop)\\n        request_end_time = datetime.datetime.now().timestamp()\\n        for i in range(len(prompts)):\\n            prompt = prompts[i]\\n            generation = generated_responses.generations[i][0]\\n            resp = {\\n                \"text\": generation.text,\\n                \"llm_output\": generated_responses.llm_output,\\n            }\\n            pl_request_id = await promptlayer_api_request_async(\\n                \"langchain.PromptLayerOpenAI.async\",\\n                \"langchain\",\\n                [prompt],\\n                self._identifying_params,\\n                self.pl_tags,\\n                resp,\\n                request_start_time,\\n                request_end_time,\\n                get_api_key(),\\n                return_pl_id=self.return_pl_id,\\n            )\\n            if self.return_pl_id:\\n                if generation.generation_info is None or not isinstance(\\n                    generation.generation_info, dict\\n                ):\\n                    generation.generation_info = {}\\n                generation.generation_info[\"pl_request_id\"] = pl_request_id\\n        return generated_responses\\n[docs]class PromptLayerOpenAIChat(OpenAIChat):\\n    \"\"\"Wrapper around OpenAI large language models.\\n    To use, you should have the ``openai`` and ``promptlayer`` python\\n    package installed, and the environment variable ``OPENAI_API_KEY``\\n    and ``PROMPTLAYER_API_KEY`` set with your openAI API key and\\n    promptlayer key respectively.\\n    All parameters that can be passed to the OpenAIChat LLM can also\\n    be passed here. The PromptLayerOpenAIChat adds two optional\\n    parameters:\\n        ``pl_tags``: List of strings to tag the request with.\\n        ``return_pl_id``: If True, the PromptLayer request ID will be\\n            returned in the ``generation_info`` field of the\\n            ``Generation`` object.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import PromptLayerOpenAIChat\\n            openaichat = PromptLayerOpenAIChat(model_name=\"gpt-3.5-turbo\")\\n    \"\"\"\\n    pl_tags: Optional[List[str]]\\n    return_pl_id: Optional[bool] = False\\n    def _generate(\\n        self, prompts: List[str], stop: Optional[List[str]] = None\\n    ) -> LLMResult:\\n        \"\"\"Call OpenAI generate and then call PromptLayer API to log the request.\"\"\"\\n        from promptlayer.utils import get_api_key, promptlayer_api_request\\n        request_start_time = datetime.datetime.now().timestamp()\\n        generated_responses = super()._generate(prompts, stop)\\n        request_end_time = datetime.datetime.now().timestamp()\\n        for i in range(len(prompts)):\\n            prompt = prompts[i]\\n            generation = generated_responses.generations[i][0]\\n            resp = {\\n                \"text\": generation.text,\\n                \"llm_output\": generated_responses.llm_output,\\n            }\\n            pl_request_id = promptlayer_api_request(\\n                \"langchain.PromptLayerOpenAIChat\",\\n                \"langchain\",\\n                [prompt],\\n                self._identifying_params,\\n                self.pl_tags,\\n                resp,\\n                request_start_time,\\n                request_end_time,\\n                get_api_key(),\\n                return_pl_id=self.return_pl_id,\\n            )\\n            if self.return_pl_id:\\n                if generation.generation_info is None or not isinstance(\\n                    generation.generation_info, dict\\n                ):\\n                    generation.generation_info = {}\\n                generation.generation_info[\"pl_request_id\"] = pl_request_id\\n        return generated_responses\\n    async def _agenerate(\\n        self, prompts: List[str], stop: Optional[List[str]] = None\\n    ) -> LLMResult:\\n        from promptlayer.utils import get_api_key, promptlayer_api_request_async\\n        request_start_time = datetime.datetime.now().timestamp()\\n        generated_responses = await super()._agenerate(prompts, stop)\\n        request_end_time = datetime.datetime.now().timestamp()\\n        for i in range(len(prompts)):\\n            prompt = prompts[i]\\n            generation = generated_responses.generations[i][0]\\n            resp = {\\n                \"text\": generation.text,\\n                \"llm_output\": generated_responses.llm_output,\\n            }\\n            pl_request_id = await promptlayer_api_request_async(\\n                \"langchain.PromptLayerOpenAIChat.async\",\\n                \"langchain\",\\n                [prompt],\\n                self._identifying_params,\\n                self.pl_tags,\\n                resp,\\n                request_start_time,\\n                request_end_time,\\n                get_api_key(),\\n                return_pl_id=self.return_pl_id,\\n            )\\n            if self.return_pl_id:\\n                if generation.generation_info is None or not isinstance(\\n                    generation.generation_info, dict\\n                ):\\n                    generation.generation_info = {}\\n                generation.generation_info[\"pl_request_id\"] = pl_request_id\\n        return generated_responses\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\promptlayer_openai.html'}),\n",
              " Document(page_content='Source code for langchain.llms.replicate\\n\"\"\"Wrapper around Replicate API.\"\"\"\\nimport logging\\nfrom typing import Any, Dict, List, Mapping, Optional\\nfrom pydantic import Extra, Field, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.utils import get_from_dict_or_env\\nlogger = logging.getLogger(__name__)\\n[docs]class Replicate(LLM):\\n    \"\"\"Wrapper around Replicate models.\\n    To use, you should have the ``replicate`` python package installed,\\n    and the environment variable ``REPLICATE_API_TOKEN`` set with your API token.\\n    You can find your token here: https://replicate.com/account\\n    The model param is required, but any other model parameters can also\\n    be passed in with the format input={model_param: value, ...}\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import Replicate\\n            replicate = Replicate(model=\"stability-ai/stable-diffusion: \\\\\\n                                         27b93a2413e7f36cd83da926f365628\\\\\\n                                         0b2931564ff050bf9575f1fdf9bcd7478\",\\n                                  input={\"image_dimensions\": \"512x512\"})\\n    \"\"\"\\n    model: str\\n    input: Dict[str, Any] = Field(default_factory=dict)\\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\\n    replicate_api_token: Optional[str] = None\\n    class Config:\\n        \"\"\"Configuration for this pydantic config.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator(pre=True)\\n    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Build extra kwargs from additional params that were passed in.\"\"\"\\n        all_required_field_names = {field.alias for field in cls.__fields__.values()}\\n        extra = values.get(\"model_kwargs\", {})\\n        for field_name in list(values):\\n            if field_name not in all_required_field_names:\\n                if field_name in extra:\\n                    raise ValueError(f\"Found {field_name} supplied twice.\")\\n                logger.warning(\\n                    f\"\"\"{field_name} was transfered to model_kwargs.\\n                    Please confirm that {field_name} is what you intended.\"\"\"\\n                )\\n                extra[field_name] = values.pop(field_name)\\n        values[\"model_kwargs\"] = extra\\n        return values\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        replicate_api_token = get_from_dict_or_env(\\n            values, \"REPLICATE_API_TOKEN\", \"REPLICATE_API_TOKEN\"\\n        )\\n        values[\"replicate_api_token\"] = replicate_api_token\\n        return values\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {\\n            **{\"model_kwargs\": self.model_kwargs},\\n        }\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of model.\"\"\"\\n        return \"replicate\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call to replicate endpoint.\"\"\"\\n        try:\\n            import replicate as replicate_python\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import replicate python package. \"\\n                \"Please install it with `pip install replicate`.\"\\n            )\\n        # get the model and version\\n        model_str, version_str = self.model.split(\":\")\\n        model = replicate_python.models.get(model_str)\\n        version = model.versions.get(version_str)\\n        # sort through the openapi schema to get the name of the first input\\n        input_properties = sorted(\\n            version.openapi_schema[\"components\"][\"schemas\"][\"Input\"][\\n                \"properties\"\\n            ].items(),\\n            key=lambda item: item[1].get(\"x-order\", 0),\\n        )\\n        first_input_name = input_properties[0][0]\\n        inputs = {first_input_name: prompt, **self.input}\\n        outputs = replicate_python.run(self.model, input={**inputs})\\n        return outputs[0]\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\replicate.html'}),\n",
              " Document(page_content='Source code for langchain.llms.rwkv\\n\"\"\"Wrapper for the RWKV model.\\nBased on https://github.com/saharNooby/rwkv.cpp/blob/master/rwkv/chat_with_bot.py\\n         https://github.com/BlinkDL/ChatRWKV/blob/main/v2/chat.py\\n\"\"\"\\nfrom typing import Any, Dict, List, Mapping, Optional, Set\\nfrom pydantic import BaseModel, Extra, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.llms.utils import enforce_stop_tokens\\n[docs]class RWKV(LLM, BaseModel):\\n    r\"\"\"Wrapper around RWKV language models.\\n    To use, you should have the ``rwkv`` python package installed, the\\n    pre-trained model file, and the model\\'s config information.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import RWKV\\n            model = RWKV(model=\"./models/rwkv-3b-fp16.bin\", strategy=\"cpu fp32\")\\n            # Simplest invocation\\n            response = model(\"Once upon a time, \")\\n    \"\"\"\\n    model: str\\n    \"\"\"Path to the pre-trained RWKV model file.\"\"\"\\n    tokens_path: str\\n    \"\"\"Path to the RWKV tokens file.\"\"\"\\n    strategy: str = \"cpu fp32\"\\n    \"\"\"Token context window.\"\"\"\\n    rwkv_verbose: bool = True\\n    \"\"\"Print debug information.\"\"\"\\n    temperature: float = 1.0\\n    \"\"\"The temperature to use for sampling.\"\"\"\\n    top_p: float = 0.5\\n    \"\"\"The top-p value to use for sampling.\"\"\"\\n    penalty_alpha_frequency: float = 0.4\\n    \"\"\"Positive values penalize new tokens based on their existing frequency\\n    in the text so far, decreasing the model\\'s likelihood to repeat the same\\n    line verbatim..\"\"\"\\n    penalty_alpha_presence: float = 0.4\\n    \"\"\"Positive values penalize new tokens based on whether they appear\\n    in the text so far, increasing the model\\'s likelihood to talk about\\n    new topics..\"\"\"\\n    CHUNK_LEN: int = 256\\n    \"\"\"Batch size for prompt processing.\"\"\"\\n    max_tokens_per_generation: int = 256\\n    \"\"\"Maximum number of tokens to generate.\"\"\"\\n    client: Any = None  #: :meta private:\\n    tokenizer: Any = None  #: :meta private:\\n    pipeline: Any = None  #: :meta private:\\n    model_tokens: Any = None  #: :meta private:\\n    model_state: Any = None  #: :meta private:\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    @property\\n    def _default_params(self) -> Dict[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {\\n            \"verbose\": self.verbose,\\n            \"top_p\": self.top_p,\\n            \"temperature\": self.temperature,\\n            \"penalty_alpha_frequency\": self.penalty_alpha_frequency,\\n            \"penalty_alpha_presence\": self.penalty_alpha_presence,\\n            \"CHUNK_LEN\": self.CHUNK_LEN,\\n            \"max_tokens_per_generation\": self.max_tokens_per_generation,\\n        }\\n    @staticmethod\\n    def _rwkv_param_names() -> Set[str]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {\\n            \"verbose\",\\n        }\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that the python package exists in the environment.\"\"\"\\n        try:\\n            import tokenizers\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import tokenizers python package. \"\\n                \"Please install it with `pip install tokenizers`.\"\\n            )\\n        try:\\n            from rwkv.model import RWKV as RWKVMODEL\\n            from rwkv.utils import PIPELINE\\n            values[\"tokenizer\"] = tokenizers.Tokenizer.from_file(values[\"tokens_path\"])\\n            rwkv_keys = cls._rwkv_param_names()\\n            model_kwargs = {k: v for k, v in values.items() if k in rwkv_keys}\\n            model_kwargs[\"verbose\"] = values[\"rwkv_verbose\"]\\n            values[\"client\"] = RWKVMODEL(\\n                values[\"model\"], strategy=values[\"strategy\"], **model_kwargs\\n            )\\n            values[\"pipeline\"] = PIPELINE(values[\"client\"], values[\"tokens_path\"])\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import rwkv python package. \"\\n                \"Please install it with `pip install rwkv`.\"\\n            )\\n        return values\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {\\n            \"model\": self.model,\\n            **self._default_params,\\n            **{k: v for k, v in self.__dict__.items() if k in RWKV._rwkv_param_names()},\\n        }\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return the type of llm.\"\"\"\\n        return \"rwkv-4\"\\n    def run_rnn(self, _tokens: List[str], newline_adj: int = 0) -> Any:\\n        AVOID_REPEAT_TOKENS = []\\n        AVOID_REPEAT = \"，：？！\"\\n        for i in AVOID_REPEAT:\\n            dd = self.pipeline.encode(i)\\n            assert len(dd) == 1\\n            AVOID_REPEAT_TOKENS += dd\\n        tokens = [int(x) for x in _tokens]\\n        self.model_tokens += tokens\\n        out: Any = None\\n        while len(tokens) > 0:\\n            out, self.model_state = self.client.forward(\\n                tokens[: self.CHUNK_LEN], self.model_state\\n            )\\n            tokens = tokens[self.CHUNK_LEN :]\\n        END_OF_LINE = 187\\n        out[END_OF_LINE] += newline_adj  # adjust \\\\n probability\\n        if self.model_tokens[-1] in AVOID_REPEAT_TOKENS:\\n            out[self.model_tokens[-1]] = -999999999\\n        return out\\n    def rwkv_generate(self, prompt: str) -> str:\\n        self.model_state = None\\n        self.model_tokens = []\\n        logits = self.run_rnn(self.tokenizer.encode(prompt).ids)\\n        begin = len(self.model_tokens)\\n        out_last = begin\\n        occurrence: Dict = {}\\n        decoded = \"\"\\n        for i in range(self.max_tokens_per_generation):\\n            for n in occurrence:\\n                logits[n] -= (\\n                    self.penalty_alpha_presence\\n                    + occurrence[n] * self.penalty_alpha_frequency\\n                )\\n            token = self.pipeline.sample_logits(\\n                logits, temperature=self.temperature, top_p=self.top_p\\n            )\\n            END_OF_TEXT = 0\\n            if token == END_OF_TEXT:\\n                break\\n            if token not in occurrence:\\n                occurrence[token] = 1\\n            else:\\n                occurrence[token] += 1\\n            logits = self.run_rnn([token])\\n            xxx = self.tokenizer.decode(self.model_tokens[out_last:])\\n            if \"\\\\ufffd\" not in xxx:  # avoid utf-8 display issues\\n                decoded += xxx\\n                out_last = begin + i + 1\\n                if i >= self.max_tokens_per_generation - 100:\\n                    break\\n        return decoded\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        r\"\"\"RWKV generation\\n        Args:\\n            prompt: The prompt to pass into the model.\\n            stop: A list of strings to stop generation when encountered.\\n        Returns:\\n            The string generated by the model.\\n        Example:\\n            .. code-block:: python\\n                prompt = \"Once upon a time, \"\\n                response = model(prompt, n_predict=55)\\n        \"\"\"\\n        text = self.rwkv_generate(prompt)\\n        if stop is not None:\\n            text = enforce_stop_tokens(text, stop)\\n        return text\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\rwkv.html'}),\n",
              " Document(page_content='Source code for langchain.llms.sagemaker_endpoint\\n\"\"\"Wrapper around Sagemaker InvokeEndpoint API.\"\"\"\\nfrom abc import ABC, abstractmethod\\nfrom typing import Any, Dict, List, Mapping, Optional, Union\\nfrom pydantic import Extra, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.llms.utils import enforce_stop_tokens\\nclass ContentHandlerBase(ABC):\\n    \"\"\"A handler class to transform input from LLM to a\\n    format that SageMaker endpoint expects. Similarily,\\n    the class also handles transforming output from the\\n    SageMaker endpoint to a format that LLM class expects.\\n    \"\"\"\\n    \"\"\"\\n    Example:\\n        .. code-block:: python\\n            class ContentHandler(ContentHandlerBase):\\n                content_type = \"application/json\"\\n                accepts = \"application/json\"\\n                def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\\n                    input_str = json.dumps({prompt: prompt, **model_kwargs})\\n                    return input_str.encode(\\'utf-8\\')\\n                \\n                def transform_output(self, output: bytes) -> str:\\n                    response_json = json.loads(output.read().decode(\"utf-8\"))\\n                    return response_json[0][\"generated_text\"]\\n    \"\"\"\\n    content_type: Optional[str] = \"text/plain\"\\n    \"\"\"The MIME type of the input data passed to endpoint\"\"\"\\n    accepts: Optional[str] = \"text/plain\"\\n    \"\"\"The MIME type of the response data returned from endpoint\"\"\"\\n    @abstractmethod\\n    def transform_input(\\n        self, prompt: Union[str, List[str]], model_kwargs: Dict\\n    ) -> bytes:\\n        \"\"\"Transforms the input to a format that model can accept\\n        as the request Body. Should return bytes or seekable file\\n        like object in the format specified in the content_type\\n        request header.\\n        \"\"\"\\n    @abstractmethod\\n    def transform_output(self, output: bytes) -> Any:\\n        \"\"\"Transforms the output from the model to string that\\n        the LLM class expects.\\n        \"\"\"\\n[docs]class SagemakerEndpoint(LLM):\\n    \"\"\"Wrapper around custom Sagemaker Inference Endpoints.\\n    To use, you must supply the endpoint name from your deployed\\n    Sagemaker model & the region where it is deployed.\\n    To authenticate, the AWS client uses the following methods to\\n    automatically load credentials:\\n    https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\\n    If a specific credential profile should be used, you must pass\\n    the name of the profile from the ~/.aws/credentials file that is to be used.\\n    Make sure the credentials / roles used have the required policies to\\n    access the Sagemaker endpoint.\\n    See: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\\n    \"\"\"\\n    \"\"\"\\n    Example:\\n        .. code-block:: python\\n            from langchain import SagemakerEndpoint\\n            endpoint_name = (\\n                \"my-endpoint-name\"\\n            )\\n            region_name = (\\n                \"us-west-2\"\\n            )\\n            credentials_profile_name = (\\n                \"default\"\\n            )\\n            se = SagemakerEndpoint(\\n                endpoint_name=endpoint_name,\\n                region_name=region_name,\\n                credentials_profile_name=credentials_profile_name\\n            )\\n    \"\"\"\\n    client: Any  #: :meta private:\\n    endpoint_name: str = \"\"\\n    \"\"\"The name of the endpoint from the deployed Sagemaker model.\\n    Must be unique within an AWS Region.\"\"\"\\n    region_name: str = \"\"\\n    \"\"\"The aws region where the Sagemaker model is deployed, eg. `us-west-2`.\"\"\"\\n    credentials_profile_name: Optional[str] = None\\n    \"\"\"The name of the profile in the ~/.aws/credentials or ~/.aws/config files, which\\n    has either access keys or role information specified.\\n    If not specified, the default credential profile or, if on an EC2 instance,\\n    credentials from IMDS will be used.\\n    See: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\\n    \"\"\"\\n    content_handler: ContentHandlerBase\\n    \"\"\"The content handler class that provides an input and\\n    output transform functions to handle formats between LLM\\n    and the endpoint.\\n    \"\"\"\\n    \"\"\"\\n     Example:\\n        .. code-block:: python\\n        class ContentHandler(ContentHandlerBase):\\n                content_type = \"application/json\"\\n                accepts = \"application/json\"\\n                def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\\n                    input_str = json.dumps({prompt: prompt, **model_kwargs})\\n                    return input_str.encode(\\'utf-8\\')\\n                \\n                def transform_output(self, output: bytes) -> str:\\n                    response_json = json.loads(output.read().decode(\"utf-8\"))\\n                    return response_json[0][\"generated_text\"]\\n    \"\"\"\\n    model_kwargs: Optional[Dict] = None\\n    \"\"\"Key word arguments to pass to the model.\"\"\"\\n    endpoint_kwargs: Optional[Dict] = None\\n    \"\"\"Optional attributes passed to the invoke_endpoint\\n    function. See `boto3`_. docs for more info.\\n    .. _boto3: <https://boto3.amazonaws.com/v1/documentation/api/latest/index.html>\\n    \"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that AWS credentials to and python package exists in environment.\"\"\"\\n        try:\\n            import boto3\\n            try:\\n                if values[\"credentials_profile_name\"] is not None:\\n                    session = boto3.Session(\\n                        profile_name=values[\"credentials_profile_name\"]\\n                    )\\n                else:\\n                    # use default credentials\\n                    session = boto3.Session()\\n                values[\"client\"] = session.client(\\n                    \"sagemaker-runtime\", region_name=values[\"region_name\"]\\n                )\\n            except Exception as e:\\n                raise ValueError(\\n                    \"Could not load credentials to authenticate with AWS client. \"\\n                    \"Please check that credentials in the specified \"\\n                    \"profile name are valid.\"\\n                ) from e\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import boto3 python package. \"\\n                \"Please install it with `pip install boto3`.\"\\n            )\\n        return values\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        _model_kwargs = self.model_kwargs or {}\\n        return {\\n            **{\"endpoint_name\": self.endpoint_name},\\n            **{\"model_kwargs\": _model_kwargs},\\n        }\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"sagemaker_endpoint\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call out to Sagemaker inference endpoint.\\n        Args:\\n            prompt: The prompt to pass into the model.\\n            stop: Optional list of stop words to use when generating.\\n        Returns:\\n            The string generated by the model.\\n        Example:\\n            .. code-block:: python\\n                response = se(\"Tell me a joke.\")\\n        \"\"\"\\n        _model_kwargs = self.model_kwargs or {}\\n        _endpoint_kwargs = self.endpoint_kwargs or {}\\n        body = self.content_handler.transform_input(prompt, _model_kwargs)\\n        content_type = self.content_handler.content_type\\n        accepts = self.content_handler.accepts\\n        # send request\\n        try:\\n            response = self.client.invoke_endpoint(\\n                EndpointName=self.endpoint_name,\\n                Body=body,\\n                ContentType=content_type,\\n                Accept=accepts,\\n                **_endpoint_kwargs,\\n            )\\n        except Exception as e:\\n            raise ValueError(f\"Error raised by inference endpoint: {e}\")\\n        text = self.content_handler.transform_output(response[\"Body\"])\\n        if stop is not None:\\n            # This is a bit hacky, but I can\\'t figure out a better way to enforce\\n            # stop tokens when making calls to the sagemaker endpoint.\\n            text = enforce_stop_tokens(text, stop)\\n        return text\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\sagemaker_endpoint.html'}),\n",
              " Document(page_content='Source code for langchain.llms.self_hosted\\n\"\"\"Run model inference on self-hosted remote hardware.\"\"\"\\nimport importlib.util\\nimport logging\\nimport pickle\\nfrom typing import Any, Callable, List, Mapping, Optional\\nfrom pydantic import Extra\\nfrom langchain.llms.base import LLM\\nfrom langchain.llms.utils import enforce_stop_tokens\\nlogger = logging.getLogger(__name__)\\ndef _generate_text(\\n    pipeline: Any,\\n    prompt: str,\\n    *args: Any,\\n    stop: Optional[List[str]] = None,\\n    **kwargs: Any,\\n) -> str:\\n    \"\"\"Inference function to send to the remote hardware.\\n    Accepts a pipeline callable (or, more likely,\\n    a key pointing to the model on the cluster\\'s object store)\\n    and returns text predictions for each document\\n    in the batch.\\n    \"\"\"\\n    text = pipeline(prompt, *args, **kwargs)\\n    if stop is not None:\\n        text = enforce_stop_tokens(text, stop)\\n    return text\\ndef _send_pipeline_to_device(pipeline: Any, device: int) -> Any:\\n    \"\"\"Send a pipeline to a device on the cluster.\"\"\"\\n    if isinstance(pipeline, str):\\n        with open(pipeline, \"rb\") as f:\\n            pipeline = pickle.load(f)\\n    if importlib.util.find_spec(\"torch\") is not None:\\n        import torch\\n        cuda_device_count = torch.cuda.device_count()\\n        if device < -1 or (device >= cuda_device_count):\\n            raise ValueError(\\n                f\"Got device=={device}, \"\\n                f\"device is required to be within [-1, {cuda_device_count})\"\\n            )\\n        if device < 0 and cuda_device_count > 0:\\n            logger.warning(\\n                \"Device has %d GPUs available. \"\\n                \"Provide device={deviceId} to `from_model_id` to use available\"\\n                \"GPUs for execution. deviceId is -1 for CPU and \"\\n                \"can be a positive integer associated with CUDA device id.\",\\n                cuda_device_count,\\n            )\\n        pipeline.device = torch.device(device)\\n        pipeline.model = pipeline.model.to(pipeline.device)\\n    return pipeline\\n[docs]class SelfHostedPipeline(LLM):\\n    \"\"\"Run model inference on self-hosted remote hardware.\\n    Supported hardware includes auto-launched instances on AWS, GCP, Azure,\\n    and Lambda, as well as servers specified\\n    by IP address and SSH credentials (such as on-prem, or another\\n    cloud like Paperspace, Coreweave, etc.).\\n    To use, you should have the ``runhouse`` python package installed.\\n    Example for custom pipeline and inference functions:\\n        .. code-block:: python\\n            from langchain.llms import SelfHostedPipeline\\n            from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\\n            import runhouse as rh\\n            def load_pipeline():\\n                tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\\n                model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\\n                return pipeline(\\n                    \"text-generation\", model=model, tokenizer=tokenizer,\\n                    max_new_tokens=10\\n                )\\n            def inference_fn(pipeline, prompt, stop = None):\\n                return pipeline(prompt)[0][\"generated_text\"]\\n            gpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\")\\n            llm = SelfHostedPipeline(\\n                model_load_fn=load_pipeline,\\n                hardware=gpu,\\n                model_reqs=model_reqs, inference_fn=inference_fn\\n            )\\n    Example for <2GB model (can be serialized and sent directly to the server):\\n        .. code-block:: python\\n            from langchain.llms import SelfHostedPipeline\\n            import runhouse as rh\\n            gpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\")\\n            my_model = ...\\n            llm = SelfHostedPipeline.from_pipeline(\\n                pipeline=my_model,\\n                hardware=gpu,\\n                model_reqs=[\"./\", \"torch\", \"transformers\"],\\n            )\\n    Example passing model path for larger models:\\n        .. code-block:: python\\n            from langchain.llms import SelfHostedPipeline\\n            import runhouse as rh\\n            import pickle\\n            from transformers import pipeline\\n            generator = pipeline(model=\"gpt2\")\\n            rh.blob(pickle.dumps(generator), path=\"models/pipeline.pkl\"\\n                ).save().to(gpu, path=\"models\")\\n            llm = SelfHostedPipeline.from_pipeline(\\n                pipeline=\"models/pipeline.pkl\",\\n                hardware=gpu,\\n                model_reqs=[\"./\", \"torch\", \"transformers\"],\\n            )\\n    \"\"\"\\n    pipeline_ref: Any  #: :meta private:\\n    client: Any  #: :meta private:\\n    inference_fn: Callable = _generate_text  #: :meta private:\\n    \"\"\"Inference function to send to the remote hardware.\"\"\"\\n    hardware: Any\\n    \"\"\"Remote hardware to send the inference function to.\"\"\"\\n    model_load_fn: Callable\\n    \"\"\"Function to load the model remotely on the server.\"\"\"\\n    load_fn_kwargs: Optional[dict] = None\\n    \"\"\"Key word arguments to pass to the model load function.\"\"\"\\n    model_reqs: List[str] = [\"./\", \"torch\"]\\n    \"\"\"Requirements to install on hardware to inference the model.\"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    def __init__(self, **kwargs: Any):\\n        \"\"\"Init the pipeline with an auxiliary function.\\n        The load function must be in global scope to be imported\\n        and run on the server, i.e. in a module and not a REPL or closure.\\n        Then, initialize the remote inference function.\\n        \"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import runhouse as rh\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import runhouse python package. \"\\n                \"Please install it with `pip install runhouse`.\"\\n            )\\n        remote_load_fn = rh.function(fn=self.model_load_fn).to(\\n            self.hardware, reqs=self.model_reqs\\n        )\\n        _load_fn_kwargs = self.load_fn_kwargs or {}\\n        self.pipeline_ref = remote_load_fn.remote(**_load_fn_kwargs)\\n        self.client = rh.function(fn=self.inference_fn).to(\\n            self.hardware, reqs=self.model_reqs\\n        )\\n[docs]    @classmethod\\n    def from_pipeline(\\n        cls,\\n        pipeline: Any,\\n        hardware: Any,\\n        model_reqs: Optional[List[str]] = None,\\n        device: int = 0,\\n        **kwargs: Any,\\n    ) -> LLM:\\n        \"\"\"Init the SelfHostedPipeline from a pipeline object or string.\"\"\"\\n        if not isinstance(pipeline, str):\\n            logger.warning(\\n                \"Serializing pipeline to send to remote hardware. \"\\n                \"Note, it can be quite slow\"\\n                \"to serialize and send large models with each execution. \"\\n                \"Consider sending the pipeline\"\\n                \"to the cluster and passing the path to the pipeline instead.\"\\n            )\\n        load_fn_kwargs = {\"pipeline\": pipeline, \"device\": device}\\n        return cls(\\n            load_fn_kwargs=load_fn_kwargs,\\n            model_load_fn=_send_pipeline_to_device,\\n            hardware=hardware,\\n            model_reqs=[\"transformers\", \"torch\"] + (model_reqs or []),\\n            **kwargs,\\n        )\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {\\n            **{\"hardware\": self.hardware},\\n        }\\n    @property\\n    def _llm_type(self) -> str:\\n        return \"self_hosted_llm\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        return self.client(pipeline=self.pipeline_ref, prompt=prompt, stop=stop)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\self_hosted.html'}),\n",
              " Document(page_content='Source code for langchain.llms.self_hosted_hugging_face\\n\"\"\"Wrapper around HuggingFace Pipeline API to run on self-hosted remote hardware.\"\"\"\\nimport importlib.util\\nimport logging\\nfrom typing import Any, Callable, List, Mapping, Optional\\nfrom pydantic import Extra\\nfrom langchain.llms.self_hosted import SelfHostedPipeline\\nfrom langchain.llms.utils import enforce_stop_tokens\\nDEFAULT_MODEL_ID = \"gpt2\"\\nDEFAULT_TASK = \"text-generation\"\\nVALID_TASKS = (\"text2text-generation\", \"text-generation\")\\nlogger = logging.getLogger(__name__)\\ndef _generate_text(\\n    pipeline: Any,\\n    prompt: str,\\n    *args: Any,\\n    stop: Optional[List[str]] = None,\\n    **kwargs: Any,\\n) -> str:\\n    \"\"\"Inference function to send to the remote hardware.\\n    Accepts a Hugging Face pipeline (or more likely,\\n    a key pointing to such a pipeline on the cluster\\'s object store)\\n    and returns generated text.\\n    \"\"\"\\n    response = pipeline(prompt, *args, **kwargs)\\n    if pipeline.task == \"text-generation\":\\n        # Text generation return includes the starter text.\\n        text = response[0][\"generated_text\"][len(prompt) :]\\n    elif pipeline.task == \"text2text-generation\":\\n        text = response[0][\"generated_text\"]\\n    else:\\n        raise ValueError(\\n            f\"Got invalid task {pipeline.task}, \"\\n            f\"currently only {VALID_TASKS} are supported\"\\n        )\\n    if stop is not None:\\n        text = enforce_stop_tokens(text, stop)\\n    return text\\ndef _load_transformer(\\n    model_id: str = DEFAULT_MODEL_ID,\\n    task: str = DEFAULT_TASK,\\n    device: int = 0,\\n    model_kwargs: Optional[dict] = None,\\n) -> Any:\\n    \"\"\"Inference function to send to the remote hardware.\\n    Accepts a huggingface model_id and returns a pipeline for the task.\\n    \"\"\"\\n    from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer\\n    from transformers import pipeline as hf_pipeline\\n    _model_kwargs = model_kwargs or {}\\n    tokenizer = AutoTokenizer.from_pretrained(model_id, **_model_kwargs)\\n    try:\\n        if task == \"text-generation\":\\n            model = AutoModelForCausalLM.from_pretrained(model_id, **_model_kwargs)\\n        elif task == \"text2text-generation\":\\n            model = AutoModelForSeq2SeqLM.from_pretrained(model_id, **_model_kwargs)\\n        else:\\n            raise ValueError(\\n                f\"Got invalid task {task}, \"\\n                f\"currently only {VALID_TASKS} are supported\"\\n            )\\n    except ImportError as e:\\n        raise ValueError(\\n            f\"Could not load the {task} model due to missing dependencies.\"\\n        ) from e\\n    if importlib.util.find_spec(\"torch\") is not None:\\n        import torch\\n        cuda_device_count = torch.cuda.device_count()\\n        if device < -1 or (device >= cuda_device_count):\\n            raise ValueError(\\n                f\"Got device=={device}, \"\\n                f\"device is required to be within [-1, {cuda_device_count})\"\\n            )\\n        if device < 0 and cuda_device_count > 0:\\n            logger.warning(\\n                \"Device has %d GPUs available. \"\\n                \"Provide device={deviceId} to `from_model_id` to use available\"\\n                \"GPUs for execution. deviceId is -1 for CPU and \"\\n                \"can be a positive integer associated with CUDA device id.\",\\n                cuda_device_count,\\n            )\\n    pipeline = hf_pipeline(\\n        task=task,\\n        model=model,\\n        tokenizer=tokenizer,\\n        device=device,\\n        model_kwargs=_model_kwargs,\\n    )\\n    if pipeline.task not in VALID_TASKS:\\n        raise ValueError(\\n            f\"Got invalid task {pipeline.task}, \"\\n            f\"currently only {VALID_TASKS} are supported\"\\n        )\\n    return pipeline\\n[docs]class SelfHostedHuggingFaceLLM(SelfHostedPipeline):\\n    \"\"\"Wrapper around HuggingFace Pipeline API to run on self-hosted remote hardware.\\n    Supported hardware includes auto-launched instances on AWS, GCP, Azure,\\n    and Lambda, as well as servers specified\\n    by IP address and SSH credentials (such as on-prem, or another cloud\\n    like Paperspace, Coreweave, etc.).\\n    To use, you should have the ``runhouse`` python package installed.\\n    Only supports `text-generation` and `text2text-generation` for now.\\n    Example using from_model_id:\\n        .. code-block:: python\\n            from langchain.llms import SelfHostedHuggingFaceLLM\\n            import runhouse as rh\\n            gpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\")\\n            hf = SelfHostedHuggingFaceLLM(\\n                model_id=\"google/flan-t5-large\", task=\"text2text-generation\",\\n                hardware=gpu\\n            )\\n    Example passing fn that generates a pipeline (bc the pipeline is not serializable):\\n        .. code-block:: python\\n            from langchain.llms import SelfHostedHuggingFaceLLM\\n            from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\\n            import runhouse as rh\\n            def get_pipeline():\\n                model_id = \"gpt2\"\\n                tokenizer = AutoTokenizer.from_pretrained(model_id)\\n                model = AutoModelForCausalLM.from_pretrained(model_id)\\n                pipe = pipeline(\\n                    \"text-generation\", model=model, tokenizer=tokenizer\\n                )\\n                return pipe\\n            hf = SelfHostedHuggingFaceLLM(\\n                model_load_fn=get_pipeline, model_id=\"gpt2\", hardware=gpu)\\n    \"\"\"\\n    model_id: str = DEFAULT_MODEL_ID\\n    \"\"\"Hugging Face model_id to load the model.\"\"\"\\n    task: str = DEFAULT_TASK\\n    \"\"\"Hugging Face task (either \"text-generation\" or \"text2text-generation\").\"\"\"\\n    device: int = 0\\n    \"\"\"Device to use for inference. -1 for CPU, 0 for GPU, 1 for second GPU, etc.\"\"\"\\n    model_kwargs: Optional[dict] = None\\n    \"\"\"Key word arguments to pass to the model.\"\"\"\\n    hardware: Any\\n    \"\"\"Remote hardware to send the inference function to.\"\"\"\\n    model_reqs: List[str] = [\"./\", \"transformers\", \"torch\"]\\n    \"\"\"Requirements to install on hardware to inference the model.\"\"\"\\n    model_load_fn: Callable = _load_transformer\\n    \"\"\"Function to load the model remotely on the server.\"\"\"\\n    inference_fn: Callable = _generate_text  #: :meta private:\\n    \"\"\"Inference function to send to the remote hardware.\"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    def __init__(self, **kwargs: Any):\\n        \"\"\"Construct the pipeline remotely using an auxiliary function.\\n        The load function needs to be importable to be imported\\n        and run on the server, i.e. in a module and not a REPL or closure.\\n        Then, initialize the remote inference function.\\n        \"\"\"\\n        load_fn_kwargs = {\\n            \"model_id\": kwargs.get(\"model_id\", DEFAULT_MODEL_ID),\\n            \"task\": kwargs.get(\"task\", DEFAULT_TASK),\\n            \"device\": kwargs.get(\"device\", 0),\\n            \"model_kwargs\": kwargs.get(\"model_kwargs\", None),\\n        }\\n        super().__init__(load_fn_kwargs=load_fn_kwargs, **kwargs)\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {\\n            **{\"model_id\": self.model_id},\\n            **{\"model_kwargs\": self.model_kwargs},\\n        }\\n    @property\\n    def _llm_type(self) -> str:\\n        return \"selfhosted_huggingface_pipeline\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        return self.client(pipeline=self.pipeline_ref, prompt=prompt, stop=stop)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\self_hosted_hugging_face.html'}),\n",
              " Document(page_content='Source code for langchain.llms.stochasticai\\n\"\"\"Wrapper around StochasticAI APIs.\"\"\"\\nimport logging\\nimport time\\nfrom typing import Any, Dict, List, Mapping, Optional\\nimport requests\\nfrom pydantic import Extra, Field, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.llms.utils import enforce_stop_tokens\\nfrom langchain.utils import get_from_dict_or_env\\nlogger = logging.getLogger(__name__)\\n[docs]class StochasticAI(LLM):\\n    \"\"\"Wrapper around StochasticAI large language models.\\n    To use, you should have the environment variable ``STOCHASTICAI_API_KEY``\\n    set with your API key.\\n    Example:\\n        .. code-block:: python\\n            from langchain.llms import StochasticAI\\n            stochasticai = StochasticAI(api_url=\"\")\\n    \"\"\"\\n    api_url: str = \"\"\\n    \"\"\"Model name to use.\"\"\"\\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\\n    \"\"\"Holds any model parameters valid for `create` call not\\n    explicitly specified.\"\"\"\\n    stochasticai_api_key: Optional[str] = None\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator(pre=True)\\n    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Build extra kwargs from additional params that were passed in.\"\"\"\\n        all_required_field_names = {field.alias for field in cls.__fields__.values()}\\n        extra = values.get(\"model_kwargs\", {})\\n        for field_name in list(values):\\n            if field_name not in all_required_field_names:\\n                if field_name in extra:\\n                    raise ValueError(f\"Found {field_name} supplied twice.\")\\n                logger.warning(\\n                    f\"\"\"{field_name} was transfered to model_kwargs.\\n                    Please confirm that {field_name} is what you intended.\"\"\"\\n                )\\n                extra[field_name] = values.pop(field_name)\\n        values[\"model_kwargs\"] = extra\\n        return values\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key exists in environment.\"\"\"\\n        stochasticai_api_key = get_from_dict_or_env(\\n            values, \"stochasticai_api_key\", \"STOCHASTICAI_API_KEY\"\\n        )\\n        values[\"stochasticai_api_key\"] = stochasticai_api_key\\n        return values\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {\\n            **{\"endpoint_url\": self.api_url},\\n            **{\"model_kwargs\": self.model_kwargs},\\n        }\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"stochasticai\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call out to StochasticAI\\'s complete endpoint.\\n        Args:\\n            prompt: The prompt to pass into the model.\\n            stop: Optional list of stop words to use when generating.\\n        Returns:\\n            The string generated by the model.\\n        Example:\\n            .. code-block:: python\\n                response = StochasticAI(\"Tell me a joke.\")\\n        \"\"\"\\n        params = self.model_kwargs or {}\\n        response_post = requests.post(\\n            url=self.api_url,\\n            json={\"prompt\": prompt, \"params\": params},\\n            headers={\\n                \"apiKey\": f\"{self.stochasticai_api_key}\",\\n                \"Accept\": \"application/json\",\\n                \"Content-Type\": \"application/json\",\\n            },\\n        )\\n        response_post.raise_for_status()\\n        response_post_json = response_post.json()\\n        completed = False\\n        while not completed:\\n            response_get = requests.get(\\n                url=response_post_json[\"data\"][\"responseUrl\"],\\n                headers={\\n                    \"apiKey\": f\"{self.stochasticai_api_key}\",\\n                    \"Accept\": \"application/json\",\\n                    \"Content-Type\": \"application/json\",\\n                },\\n            )\\n            response_get.raise_for_status()\\n            response_get_json = response_get.json()[\"data\"]\\n            text = response_get_json.get(\"completion\")\\n            completed = text is not None\\n            time.sleep(0.5)\\n        text = text[0]\\n        if stop is not None:\\n            # I believe this is required since the stop tokens\\n            # are not enforced by the model parameters\\n            text = enforce_stop_tokens(text, stop)\\n        return text\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\stochasticai.html'}),\n",
              " Document(page_content='Source code for langchain.llms.writer\\n\"\"\"Wrapper around Writer APIs.\"\"\"\\nfrom typing import Any, Dict, List, Mapping, Optional\\nimport requests\\nfrom pydantic import Extra, root_validator\\nfrom langchain.llms.base import LLM\\nfrom langchain.llms.utils import enforce_stop_tokens\\nfrom langchain.utils import get_from_dict_or_env\\n[docs]class Writer(LLM):\\n    \"\"\"Wrapper around Writer large language models.\\n    To use, you should have the environment variable ``WRITER_API_KEY``\\n    set with your API key.\\n    Example:\\n        .. code-block:: python\\n            from langchain import Writer\\n            writer = Writer(model_id=\"palmyra-base\")\\n    \"\"\"\\n    model_id: str = \"palmyra-base\"\\n    \"\"\"Model name to use.\"\"\"\\n    tokens_to_generate: int = 24\\n    \"\"\"Max number of tokens to generate.\"\"\"\\n    logprobs: bool = False\\n    \"\"\"Whether to return log probabilities.\"\"\"\\n    temperature: float = 1.0\\n    \"\"\"What sampling temperature to use.\"\"\"\\n    length: int = 256\\n    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\\n    top_p: float = 1.0\\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\\n    top_k: int = 1\\n    \"\"\"The number of highest probability vocabulary tokens to\\n    keep for top-k-filtering.\"\"\"\\n    repetition_penalty: float = 1.0\\n    \"\"\"Penalizes repeated tokens according to frequency.\"\"\"\\n    random_seed: int = 0\\n    \"\"\"The model generates random results.\\n    Changing the random seed alone will produce a different response\\n    with similar characteristics. It is possible to reproduce results\\n    by fixing the random seed (assuming all other hyperparameters\\n    are also fixed)\"\"\"\\n    beam_search_diversity_rate: float = 1.0\\n    \"\"\"Only applies to beam search, i.e. when the beam width is >1.\\n    A higher value encourages beam search to return a more diverse\\n    set of candidates\"\"\"\\n    beam_width: Optional[int] = None\\n    \"\"\"The number of concurrent candidates to keep track of during\\n    beam search\"\"\"\\n    length_pentaly: float = 1.0\\n    \"\"\"Only applies to beam search, i.e. when the beam width is >1.\\n    Larger values penalize long candidates more heavily, thus preferring\\n    shorter candidates\"\"\"\\n    writer_api_key: Optional[str] = None\\n    stop: Optional[List[str]] = None\\n    \"\"\"Sequences when completion generation will stop\"\"\"\\n    base_url: Optional[str] = None\\n    \"\"\"Base url to use, if None decides based on model name.\"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key exists in environment.\"\"\"\\n        writer_api_key = get_from_dict_or_env(\\n            values, \"writer_api_key\", \"WRITER_API_KEY\"\\n        )\\n        values[\"writer_api_key\"] = writer_api_key\\n        return values\\n    @property\\n    def _default_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the default parameters for calling Writer API.\"\"\"\\n        return {\\n            \"tokens_to_generate\": self.tokens_to_generate,\\n            \"stop\": self.stop,\\n            \"logprobs\": self.logprobs,\\n            \"temperature\": self.temperature,\\n            \"top_p\": self.top_p,\\n            \"top_k\": self.top_k,\\n            \"repetition_penalty\": self.repetition_penalty,\\n            \"random_seed\": self.random_seed,\\n            \"beam_search_diversity_rate\": self.beam_search_diversity_rate,\\n            \"beam_width\": self.beam_width,\\n            \"length_pentaly\": self.length_pentaly,\\n        }\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        \"\"\"Get the identifying parameters.\"\"\"\\n        return {**{\"model_id\": self.model_id}, **self._default_params}\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"writer\"\\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\\n        \"\"\"Call out to Writer\\'s complete endpoint.\\n        Args:\\n            prompt: The prompt to pass into the model.\\n            stop: Optional list of stop words to use when generating.\\n        Returns:\\n            The string generated by the model.\\n        Example:\\n            .. code-block:: python\\n                response = Writer(\"Tell me a joke.\")\\n        \"\"\"\\n        if self.base_url is not None:\\n            base_url = self.base_url\\n        else:\\n            base_url = (\\n                \"https://api.llm.writer.com/v1/models/{self.model_id}/completions\"\\n            )\\n        response = requests.post(\\n            url=base_url,\\n            headers={\\n                \"Authorization\": f\"Bearer {self.writer_api_key}\",\\n                \"Content-Type\": \"application/json\",\\n                \"Accept\": \"application/json\",\\n            },\\n            json={\"prompt\": prompt, **self._default_params},\\n        )\\n        text = response.text\\n        if stop is not None:\\n            # I believe this is required since the stop tokens\\n            # are not enforced by the model parameters\\n            text = enforce_stop_tokens(text, stop)\\n        return text\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\llms\\\\writer.html'}),\n",
              " Document(page_content='Source code for langchain.prompts.base\\n\"\"\"BasePrompt schema definition.\"\"\"\\nfrom __future__ import annotations\\nimport json\\nfrom abc import ABC, abstractmethod\\nfrom pathlib import Path\\nfrom typing import Any, Callable, Dict, List, Mapping, Optional, Set, Union\\nimport yaml\\nfrom pydantic import BaseModel, Extra, Field, root_validator\\nfrom langchain.formatting import formatter\\nfrom langchain.schema import BaseMessage, BaseOutputParser, HumanMessage, PromptValue\\ndef jinja2_formatter(template: str, **kwargs: Any) -> str:\\n    \"\"\"Format a template using jinja2.\"\"\"\\n    try:\\n        from jinja2 import Template\\n    except ImportError:\\n        raise ImportError(\\n            \"jinja2 not installed, which is needed to use the jinja2_formatter. \"\\n            \"Please install it with `pip install jinja2`.\"\\n        )\\n    return Template(template).render(**kwargs)\\ndef validate_jinja2(template: str, input_variables: List[str]) -> None:\\n    input_variables_set = set(input_variables)\\n    valid_variables = _get_jinja2_variables_from_template(template)\\n    missing_variables = valid_variables - input_variables_set\\n    extra_variables = input_variables_set - valid_variables\\n    error_message = \"\"\\n    if missing_variables:\\n        error_message += f\"Missing variables: {missing_variables} \"\\n    if extra_variables:\\n        error_message += f\"Extra variables: {extra_variables}\"\\n    if error_message:\\n        raise KeyError(error_message.strip())\\ndef _get_jinja2_variables_from_template(template: str) -> Set[str]:\\n    try:\\n        from jinja2 import Environment, meta\\n    except ImportError:\\n        raise ImportError(\\n            \"jinja2 not installed, which is needed to use the jinja2_formatter. \"\\n            \"Please install it with `pip install jinja2`.\"\\n        )\\n    env = Environment()\\n    ast = env.parse(template)\\n    variables = meta.find_undeclared_variables(ast)\\n    return variables\\nDEFAULT_FORMATTER_MAPPING: Dict[str, Callable] = {\\n    \"f-string\": formatter.format,\\n    \"jinja2\": jinja2_formatter,\\n}\\nDEFAULT_VALIDATOR_MAPPING: Dict[str, Callable] = {\\n    \"f-string\": formatter.validate_input_variables,\\n    \"jinja2\": validate_jinja2,\\n}\\ndef check_valid_template(\\n    template: str, template_format: str, input_variables: List[str]\\n) -> None:\\n    \"\"\"Check that template string is valid.\"\"\"\\n    if template_format not in DEFAULT_FORMATTER_MAPPING:\\n        valid_formats = list(DEFAULT_FORMATTER_MAPPING)\\n        raise ValueError(\\n            f\"Invalid template format. Got `{template_format}`;\"\\n            f\" should be one of {valid_formats}\"\\n        )\\n    try:\\n        validator_func = DEFAULT_VALIDATOR_MAPPING[template_format]\\n        validator_func(template, input_variables)\\n    except KeyError as e:\\n        raise ValueError(\\n            \"Invalid prompt schema; check for mismatched or missing input parameters. \"\\n            + str(e)\\n        )\\nclass StringPromptValue(PromptValue):\\n    text: str\\n    def to_string(self) -> str:\\n        \"\"\"Return prompt as string.\"\"\"\\n        return self.text\\n    def to_messages(self) -> List[BaseMessage]:\\n        \"\"\"Return prompt as messages.\"\"\"\\n        return [HumanMessage(content=self.text)]\\n[docs]class BasePromptTemplate(BaseModel, ABC):\\n    \"\"\"Base class for all prompt templates, returning a prompt.\"\"\"\\n    input_variables: List[str]\\n    \"\"\"A list of the names of the variables the prompt template expects.\"\"\"\\n    output_parser: Optional[BaseOutputParser] = None\\n    \"\"\"How to parse the output of calling an LLM on this formatted prompt.\"\"\"\\n    partial_variables: Mapping[str, Union[str, Callable[[], str]]] = Field(\\n        default_factory=dict\\n    )\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n[docs]    @abstractmethod\\n    def format_prompt(self, **kwargs: Any) -> PromptValue:\\n        \"\"\"Create Chat Messages.\"\"\"\\n    @root_validator()\\n    def validate_variable_names(cls, values: Dict) -> Dict:\\n        \"\"\"Validate variable names do not include restricted names.\"\"\"\\n        if \"stop\" in values[\"input_variables\"]:\\n            raise ValueError(\\n                \"Cannot have an input variable named \\'stop\\', as it is used internally,\"\\n                \" please rename.\"\\n            )\\n        if \"stop\" in values[\"partial_variables\"]:\\n            raise ValueError(\\n                \"Cannot have an partial variable named \\'stop\\', as it is used \"\\n                \"internally, please rename.\"\\n            )\\n        overall = set(values[\"input_variables\"]).intersection(\\n            values[\"partial_variables\"]\\n        )\\n        if overall:\\n            raise ValueError(\\n                f\"Found overlapping input and partial variables: {overall}\"\\n            )\\n        return values\\n[docs]    def partial(self, **kwargs: Union[str, Callable[[], str]]) -> BasePromptTemplate:\\n        \"\"\"Return a partial of the prompt template.\"\"\"\\n        prompt_dict = self.__dict__.copy()\\n        prompt_dict[\"input_variables\"] = list(\\n            set(self.input_variables).difference(kwargs)\\n        )\\n        prompt_dict[\"partial_variables\"] = {**self.partial_variables, **kwargs}\\n        return type(self)(**prompt_dict)\\n    def _merge_partial_and_user_variables(self, **kwargs: Any) -> Dict[str, Any]:\\n        # Get partial params:\\n        partial_kwargs = {\\n            k: v if isinstance(v, str) else v()\\n            for k, v in self.partial_variables.items()\\n        }\\n        return {**partial_kwargs, **kwargs}\\n[docs]    @abstractmethod\\n    def format(self, **kwargs: Any) -> str:\\n        \"\"\"Format the prompt with the inputs.\\n        Args:\\n            kwargs: Any arguments to be passed to the prompt template.\\n        Returns:\\n            A formatted string.\\n        Example:\\n        .. code-block:: python\\n            prompt.format(variable1=\"foo\")\\n        \"\"\"\\n    @property\\n    def _prompt_type(self) -> str:\\n        \"\"\"Return the prompt type key.\"\"\"\\n        raise NotImplementedError\\n[docs]    def dict(self, **kwargs: Any) -> Dict:\\n        \"\"\"Return dictionary representation of prompt.\"\"\"\\n        prompt_dict = super().dict(**kwargs)\\n        prompt_dict[\"_type\"] = self._prompt_type\\n        return prompt_dict\\n[docs]    def save(self, file_path: Union[Path, str]) -> None:\\n        \"\"\"Save the prompt.\\n        Args:\\n            file_path: Path to directory to save prompt to.\\n        Example:\\n        .. code-block:: python\\n            prompt.save(file_path=\"path/prompt.yaml\")\\n        \"\"\"\\n        if self.partial_variables:\\n            raise ValueError(\"Cannot save prompt with partial variables.\")\\n        # Convert file to Path object.\\n        if isinstance(file_path, str):\\n            save_path = Path(file_path)\\n        else:\\n            save_path = file_path\\n        directory_path = save_path.parent\\n        directory_path.mkdir(parents=True, exist_ok=True)\\n        # Fetch dictionary to save\\n        prompt_dict = self.dict()\\n        if save_path.suffix == \".json\":\\n            with open(file_path, \"w\") as f:\\n                json.dump(prompt_dict, f, indent=4)\\n        elif save_path.suffix == \".yaml\":\\n            with open(file_path, \"w\") as f:\\n                yaml.dump(prompt_dict, f, default_flow_style=False)\\n        else:\\n            raise ValueError(f\"{save_path} must be json or yaml\")\\n[docs]class StringPromptTemplate(BasePromptTemplate, ABC):\\n    \"\"\"String prompt should expose the format method, returning a prompt.\"\"\"\\n[docs]    def format_prompt(self, **kwargs: Any) -> PromptValue:\\n        \"\"\"Create Chat Messages.\"\"\"\\n        return StringPromptValue(text=self.format(**kwargs))\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\prompts\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.prompts.chat\\n\"\"\"Chat prompt template.\"\"\"\\nfrom __future__ import annotations\\nfrom abc import ABC, abstractmethod\\nfrom pathlib import Path\\nfrom typing import Any, Callable, List, Sequence, Tuple, Type, Union\\nfrom pydantic import BaseModel, Field\\nfrom langchain.memory.buffer import get_buffer_string\\nfrom langchain.prompts.base import BasePromptTemplate, StringPromptTemplate\\nfrom langchain.prompts.prompt import PromptTemplate\\nfrom langchain.schema import (\\n    AIMessage,\\n    BaseMessage,\\n    ChatMessage,\\n    HumanMessage,\\n    PromptValue,\\n    SystemMessage,\\n)\\nclass BaseMessagePromptTemplate(BaseModel, ABC):\\n    @abstractmethod\\n    def format_messages(self, **kwargs: Any) -> List[BaseMessage]:\\n        \"\"\"To messages.\"\"\"\\n    @property\\n    @abstractmethod\\n    def input_variables(self) -> List[str]:\\n        \"\"\"Input variables for this prompt template.\"\"\"\\n[docs]class MessagesPlaceholder(BaseMessagePromptTemplate):\\n    \"\"\"Prompt template that assumes variable is already list of messages.\"\"\"\\n    variable_name: str\\n[docs]    def format_messages(self, **kwargs: Any) -> List[BaseMessage]:\\n        \"\"\"To a BaseMessage.\"\"\"\\n        value = kwargs[self.variable_name]\\n        if not isinstance(value, list):\\n            raise ValueError(\\n                f\"variable {self.variable_name} should be a list of base messages, \"\\n                f\"got {value}\"\\n            )\\n        for v in value:\\n            if not isinstance(v, BaseMessage):\\n                raise ValueError(\\n                    f\"variable {self.variable_name} should be a list of base messages,\"\\n                    f\" got {value}\"\\n                )\\n        return value\\n    @property\\n    def input_variables(self) -> List[str]:\\n        \"\"\"Input variables for this prompt template.\"\"\"\\n        return [self.variable_name]\\nclass BaseStringMessagePromptTemplate(BaseMessagePromptTemplate, ABC):\\n    prompt: StringPromptTemplate\\n    additional_kwargs: dict = Field(default_factory=dict)\\n    @classmethod\\n    def from_template(cls, template: str, **kwargs: Any) -> BaseMessagePromptTemplate:\\n        prompt = PromptTemplate.from_template(template)\\n        return cls(prompt=prompt, **kwargs)\\n    @abstractmethod\\n    def format(self, **kwargs: Any) -> BaseMessage:\\n        \"\"\"To a BaseMessage.\"\"\"\\n    def format_messages(self, **kwargs: Any) -> List[BaseMessage]:\\n        return [self.format(**kwargs)]\\n    @property\\n    def input_variables(self) -> List[str]:\\n        return self.prompt.input_variables\\nclass ChatMessagePromptTemplate(BaseStringMessagePromptTemplate):\\n    role: str\\n    def format(self, **kwargs: Any) -> BaseMessage:\\n        text = self.prompt.format(**kwargs)\\n        return ChatMessage(\\n            content=text, role=self.role, additional_kwargs=self.additional_kwargs\\n        )\\nclass HumanMessagePromptTemplate(BaseStringMessagePromptTemplate):\\n    def format(self, **kwargs: Any) -> BaseMessage:\\n        text = self.prompt.format(**kwargs)\\n        return HumanMessage(content=text, additional_kwargs=self.additional_kwargs)\\nclass AIMessagePromptTemplate(BaseStringMessagePromptTemplate):\\n    def format(self, **kwargs: Any) -> BaseMessage:\\n        text = self.prompt.format(**kwargs)\\n        return AIMessage(content=text, additional_kwargs=self.additional_kwargs)\\nclass SystemMessagePromptTemplate(BaseStringMessagePromptTemplate):\\n    def format(self, **kwargs: Any) -> BaseMessage:\\n        text = self.prompt.format(**kwargs)\\n        return SystemMessage(content=text, additional_kwargs=self.additional_kwargs)\\nclass ChatPromptValue(PromptValue):\\n    messages: List[BaseMessage]\\n    def to_string(self) -> str:\\n        \"\"\"Return prompt as string.\"\"\"\\n        return get_buffer_string(self.messages)\\n    def to_messages(self) -> List[BaseMessage]:\\n        \"\"\"Return prompt as messages.\"\"\"\\n        return self.messages\\n[docs]class BaseChatPromptTemplate(BasePromptTemplate, ABC):\\n[docs]    def format(self, **kwargs: Any) -> str:\\n        return self.format_prompt(**kwargs).to_string()\\n[docs]    def format_prompt(self, **kwargs: Any) -> PromptValue:\\n        messages = self.format_messages(**kwargs)\\n        return ChatPromptValue(messages=messages)\\n[docs]    @abstractmethod\\n    def format_messages(self, **kwargs: Any) -> List[BaseMessage]:\\n        \"\"\"Format kwargs into a list of messages.\"\"\"\\n[docs]class ChatPromptTemplate(BaseChatPromptTemplate, ABC):\\n    input_variables: List[str]\\n    messages: List[Union[BaseMessagePromptTemplate, BaseMessage]]\\n    @classmethod\\n    def from_role_strings(\\n        cls, string_messages: List[Tuple[str, str]]\\n    ) -> ChatPromptTemplate:\\n        messages = [\\n            ChatMessagePromptTemplate(\\n                content=PromptTemplate.from_template(template), role=role\\n            )\\n            for role, template in string_messages\\n        ]\\n        return cls.from_messages(messages)\\n    @classmethod\\n    def from_strings(\\n        cls, string_messages: List[Tuple[Type[BaseMessagePromptTemplate], str]]\\n    ) -> ChatPromptTemplate:\\n        messages = [\\n            role(content=PromptTemplate.from_template(template))\\n            for role, template in string_messages\\n        ]\\n        return cls.from_messages(messages)\\n    @classmethod\\n    def from_messages(\\n        cls, messages: Sequence[Union[BaseMessagePromptTemplate, BaseMessage]]\\n    ) -> ChatPromptTemplate:\\n        input_vars = set()\\n        for message in messages:\\n            if isinstance(message, BaseMessagePromptTemplate):\\n                input_vars.update(message.input_variables)\\n        return cls(input_variables=list(input_vars), messages=messages)\\n[docs]    def format(self, **kwargs: Any) -> str:\\n        return self.format_prompt(**kwargs).to_string()\\n[docs]    def format_messages(self, **kwargs: Any) -> List[BaseMessage]:\\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\\n        result = []\\n        for message_template in self.messages:\\n            if isinstance(message_template, BaseMessage):\\n                result.extend([message_template])\\n            elif isinstance(message_template, BaseMessagePromptTemplate):\\n                rel_params = {\\n                    k: v\\n                    for k, v in kwargs.items()\\n                    if k in message_template.input_variables\\n                }\\n                message = message_template.format_messages(**rel_params)\\n                result.extend(message)\\n            else:\\n                raise ValueError(f\"Unexpected input: {message_template}\")\\n        return result\\n[docs]    def partial(self, **kwargs: Union[str, Callable[[], str]]) -> BasePromptTemplate:\\n        raise NotImplementedError\\n    @property\\n    def _prompt_type(self) -> str:\\n        raise NotImplementedError\\n[docs]    def save(self, file_path: Union[Path, str]) -> None:\\n        raise NotImplementedError\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\prompts\\\\chat.html'}),\n",
              " Document(page_content='Source code for langchain.prompts.few_shot\\n\"\"\"Prompt template that contains few shot examples.\"\"\"\\nfrom typing import Any, Dict, List, Optional\\nfrom pydantic import Extra, root_validator\\nfrom langchain.prompts.base import (\\n    DEFAULT_FORMATTER_MAPPING,\\n    StringPromptTemplate,\\n    check_valid_template,\\n)\\nfrom langchain.prompts.example_selector.base import BaseExampleSelector\\nfrom langchain.prompts.prompt import PromptTemplate\\n[docs]class FewShotPromptTemplate(StringPromptTemplate):\\n    \"\"\"Prompt template that contains few shot examples.\"\"\"\\n    examples: Optional[List[dict]] = None\\n    \"\"\"Examples to format into the prompt.\\n    Either this or example_selector should be provided.\"\"\"\\n    example_selector: Optional[BaseExampleSelector] = None\\n    \"\"\"ExampleSelector to choose the examples to format into the prompt.\\n    Either this or examples should be provided.\"\"\"\\n    example_prompt: PromptTemplate\\n    \"\"\"PromptTemplate used to format an individual example.\"\"\"\\n    suffix: str\\n    \"\"\"A prompt template string to put after the examples.\"\"\"\\n    input_variables: List[str]\\n    \"\"\"A list of the names of the variables the prompt template expects.\"\"\"\\n    example_separator: str = \"\\\\n\\\\n\"\\n    \"\"\"String separator used to join the prefix, the examples, and suffix.\"\"\"\\n    prefix: str = \"\"\\n    \"\"\"A prompt template string to put before the examples.\"\"\"\\n    template_format: str = \"f-string\"\\n    \"\"\"The format of the prompt template. Options are: \\'f-string\\', \\'jinja2\\'.\"\"\"\\n    validate_template: bool = True\\n    \"\"\"Whether or not to try validating the template.\"\"\"\\n    @root_validator(pre=True)\\n    def check_examples_and_selector(cls, values: Dict) -> Dict:\\n        \"\"\"Check that one and only one of examples/example_selector are provided.\"\"\"\\n        examples = values.get(\"examples\", None)\\n        example_selector = values.get(\"example_selector\", None)\\n        if examples and example_selector:\\n            raise ValueError(\\n                \"Only one of \\'examples\\' and \\'example_selector\\' should be provided\"\\n            )\\n        if examples is None and example_selector is None:\\n            raise ValueError(\\n                \"One of \\'examples\\' and \\'example_selector\\' should be provided\"\\n            )\\n        return values\\n    @root_validator()\\n    def template_is_valid(cls, values: Dict) -> Dict:\\n        \"\"\"Check that prefix, suffix and input variables are consistent.\"\"\"\\n        if values[\"validate_template\"]:\\n            check_valid_template(\\n                values[\"prefix\"] + values[\"suffix\"],\\n                values[\"template_format\"],\\n                values[\"input_variables\"] + list(values[\"partial_variables\"]),\\n            )\\n        return values\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n    def _get_examples(self, **kwargs: Any) -> List[dict]:\\n        if self.examples is not None:\\n            return self.examples\\n        elif self.example_selector is not None:\\n            return self.example_selector.select_examples(kwargs)\\n        else:\\n            raise ValueError\\n[docs]    def format(self, **kwargs: Any) -> str:\\n        \"\"\"Format the prompt with the inputs.\\n        Args:\\n            kwargs: Any arguments to be passed to the prompt template.\\n        Returns:\\n            A formatted string.\\n        Example:\\n        .. code-block:: python\\n            prompt.format(variable1=\"foo\")\\n        \"\"\"\\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\\n        # Get the examples to use.\\n        examples = self._get_examples(**kwargs)\\n        # Format the examples.\\n        example_strings = [\\n            self.example_prompt.format(**example) for example in examples\\n        ]\\n        # Create the overall template.\\n        pieces = [self.prefix, *example_strings, self.suffix]\\n        template = self.example_separator.join([piece for piece in pieces if piece])\\n        # Format the template with the input variables.\\n        return DEFAULT_FORMATTER_MAPPING[self.template_format](template, **kwargs)\\n    @property\\n    def _prompt_type(self) -> str:\\n        \"\"\"Return the prompt type key.\"\"\"\\n        return \"few_shot\"\\n[docs]    def dict(self, **kwargs: Any) -> Dict:\\n        \"\"\"Return a dictionary of the prompt.\"\"\"\\n        if self.example_selector:\\n            raise ValueError(\"Saving an example selector is not currently supported\")\\n        return super().dict(**kwargs)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\prompts\\\\few_shot.html'}),\n",
              " Document(page_content='Source code for langchain.prompts.few_shot_with_templates\\n\"\"\"Prompt template that contains few shot examples.\"\"\"\\nfrom typing import Any, Dict, List, Optional\\nfrom pydantic import Extra, root_validator\\nfrom langchain.prompts.base import DEFAULT_FORMATTER_MAPPING, StringPromptTemplate\\nfrom langchain.prompts.example_selector.base import BaseExampleSelector\\nfrom langchain.prompts.prompt import PromptTemplate\\n[docs]class FewShotPromptWithTemplates(StringPromptTemplate):\\n    \"\"\"Prompt template that contains few shot examples.\"\"\"\\n    examples: Optional[List[dict]] = None\\n    \"\"\"Examples to format into the prompt.\\n    Either this or example_selector should be provided.\"\"\"\\n    example_selector: Optional[BaseExampleSelector] = None\\n    \"\"\"ExampleSelector to choose the examples to format into the prompt.\\n    Either this or examples should be provided.\"\"\"\\n    example_prompt: PromptTemplate\\n    \"\"\"PromptTemplate used to format an individual example.\"\"\"\\n    suffix: StringPromptTemplate\\n    \"\"\"A PromptTemplate to put after the examples.\"\"\"\\n    input_variables: List[str]\\n    \"\"\"A list of the names of the variables the prompt template expects.\"\"\"\\n    example_separator: str = \"\\\\n\\\\n\"\\n    \"\"\"String separator used to join the prefix, the examples, and suffix.\"\"\"\\n    prefix: Optional[StringPromptTemplate] = None\\n    \"\"\"A PromptTemplate to put before the examples.\"\"\"\\n    template_format: str = \"f-string\"\\n    \"\"\"The format of the prompt template. Options are: \\'f-string\\', \\'jinja2\\'.\"\"\"\\n    validate_template: bool = True\\n    \"\"\"Whether or not to try validating the template.\"\"\"\\n    @root_validator(pre=True)\\n    def check_examples_and_selector(cls, values: Dict) -> Dict:\\n        \"\"\"Check that one and only one of examples/example_selector are provided.\"\"\"\\n        examples = values.get(\"examples\", None)\\n        example_selector = values.get(\"example_selector\", None)\\n        if examples and example_selector:\\n            raise ValueError(\\n                \"Only one of \\'examples\\' and \\'example_selector\\' should be provided\"\\n            )\\n        if examples is None and example_selector is None:\\n            raise ValueError(\\n                \"One of \\'examples\\' and \\'example_selector\\' should be provided\"\\n            )\\n        return values\\n    @root_validator()\\n    def template_is_valid(cls, values: Dict) -> Dict:\\n        \"\"\"Check that prefix, suffix and input variables are consistent.\"\"\"\\n        if values[\"validate_template\"]:\\n            input_variables = values[\"input_variables\"]\\n            expected_input_variables = set(values[\"suffix\"].input_variables)\\n            expected_input_variables |= set(values[\"partial_variables\"])\\n            if values[\"prefix\"] is not None:\\n                expected_input_variables |= set(values[\"prefix\"].input_variables)\\n            missing_vars = expected_input_variables.difference(input_variables)\\n            if missing_vars:\\n                raise ValueError(\\n                    f\"Got input_variables={input_variables}, but based on \"\\n                    f\"prefix/suffix expected {expected_input_variables}\"\\n                )\\n        return values\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n    def _get_examples(self, **kwargs: Any) -> List[dict]:\\n        if self.examples is not None:\\n            return self.examples\\n        elif self.example_selector is not None:\\n            return self.example_selector.select_examples(kwargs)\\n        else:\\n            raise ValueError\\n[docs]    def format(self, **kwargs: Any) -> str:\\n        \"\"\"Format the prompt with the inputs.\\n        Args:\\n            kwargs: Any arguments to be passed to the prompt template.\\n        Returns:\\n            A formatted string.\\n        Example:\\n        .. code-block:: python\\n            prompt.format(variable1=\"foo\")\\n        \"\"\"\\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\\n        # Get the examples to use.\\n        examples = self._get_examples(**kwargs)\\n        # Format the examples.\\n        example_strings = [\\n            self.example_prompt.format(**example) for example in examples\\n        ]\\n        # Create the overall prefix.\\n        if self.prefix is None:\\n            prefix = \"\"\\n        else:\\n            prefix_kwargs = {\\n                k: v for k, v in kwargs.items() if k in self.prefix.input_variables\\n            }\\n            for k in prefix_kwargs.keys():\\n                kwargs.pop(k)\\n            prefix = self.prefix.format(**prefix_kwargs)\\n        # Create the overall suffix\\n        suffix_kwargs = {\\n            k: v for k, v in kwargs.items() if k in self.suffix.input_variables\\n        }\\n        for k in suffix_kwargs.keys():\\n            kwargs.pop(k)\\n        suffix = self.suffix.format(\\n            **suffix_kwargs,\\n        )\\n        pieces = [prefix, *example_strings, suffix]\\n        template = self.example_separator.join([piece for piece in pieces if piece])\\n        # Format the template with the input variables.\\n        return DEFAULT_FORMATTER_MAPPING[self.template_format](template, **kwargs)\\n    @property\\n    def _prompt_type(self) -> str:\\n        \"\"\"Return the prompt type key.\"\"\"\\n        return \"few_shot_with_templates\"\\n[docs]    def dict(self, **kwargs: Any) -> Dict:\\n        \"\"\"Return a dictionary of the prompt.\"\"\"\\n        if self.example_selector:\\n            raise ValueError(\"Saving an example selector is not currently supported\")\\n        return super().dict(**kwargs)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\prompts\\\\few_shot_with_templates.html'}),\n",
              " Document(page_content='Source code for langchain.prompts.loading\\n\"\"\"Load prompts from disk.\"\"\"\\nimport importlib\\nimport json\\nimport logging\\nfrom pathlib import Path\\nfrom typing import Union\\nimport yaml\\nfrom langchain.output_parsers.regex import RegexParser\\nfrom langchain.prompts.base import BasePromptTemplate\\nfrom langchain.prompts.few_shot import FewShotPromptTemplate\\nfrom langchain.prompts.prompt import PromptTemplate\\nfrom langchain.utilities.loading import try_load_from_hub\\nURL_BASE = \"https://raw.githubusercontent.com/hwchase17/langchain-hub/master/prompts/\"\\nlogger = logging.getLogger(__name__)\\ndef load_prompt_from_config(config: dict) -> BasePromptTemplate:\\n    \"\"\"Load prompt from Config Dict.\"\"\"\\n    if \"_type\" not in config:\\n        logger.warning(\"No `_type` key found, defaulting to `prompt`.\")\\n    config_type = config.pop(\"_type\", \"prompt\")\\n    if config_type not in type_to_loader_dict:\\n        raise ValueError(f\"Loading {config_type} prompt not supported\")\\n    prompt_loader = type_to_loader_dict[config_type]\\n    return prompt_loader(config)\\ndef _load_template(var_name: str, config: dict) -> dict:\\n    \"\"\"Load template from disk if applicable.\"\"\"\\n    # Check if template_path exists in config.\\n    if f\"{var_name}_path\" in config:\\n        # If it does, make sure template variable doesn\\'t also exist.\\n        if var_name in config:\\n            raise ValueError(\\n                f\"Both `{var_name}_path` and `{var_name}` cannot be provided.\"\\n            )\\n        # Pop the template path from the config.\\n        template_path = Path(config.pop(f\"{var_name}_path\"))\\n        # Load the template.\\n        if template_path.suffix == \".txt\":\\n            with open(template_path) as f:\\n                template = f.read()\\n        else:\\n            raise ValueError\\n        # Set the template variable to the extracted variable.\\n        config[var_name] = template\\n    return config\\ndef _load_examples(config: dict) -> dict:\\n    \"\"\"Load examples if necessary.\"\"\"\\n    if isinstance(config[\"examples\"], list):\\n        pass\\n    elif isinstance(config[\"examples\"], str):\\n        with open(config[\"examples\"]) as f:\\n            if config[\"examples\"].endswith(\".json\"):\\n                examples = json.load(f)\\n            elif config[\"examples\"].endswith((\".yaml\", \".yml\")):\\n                examples = yaml.safe_load(f)\\n            else:\\n                raise ValueError(\\n                    \"Invalid file format. Only json or yaml formats are supported.\"\\n                )\\n        config[\"examples\"] = examples\\n    else:\\n        raise ValueError(\"Invalid examples format. Only list or string are supported.\")\\n    return config\\ndef _load_output_parser(config: dict) -> dict:\\n    \"\"\"Load output parser.\"\"\"\\n    if \"output_parsers\" in config:\\n        if config[\"output_parsers\"] is not None:\\n            _config = config[\"output_parsers\"]\\n            output_parser_type = _config[\"_type\"]\\n            if output_parser_type == \"regex_parser\":\\n                output_parser = RegexParser(**_config)\\n            else:\\n                raise ValueError(f\"Unsupported output parser {output_parser_type}\")\\n            config[\"output_parsers\"] = output_parser\\n    return config\\ndef _load_few_shot_prompt(config: dict) -> FewShotPromptTemplate:\\n    \"\"\"Load the few shot prompt from the config.\"\"\"\\n    # Load the suffix and prefix templates.\\n    config = _load_template(\"suffix\", config)\\n    config = _load_template(\"prefix\", config)\\n    # Load the example prompt.\\n    if \"example_prompt_path\" in config:\\n        if \"example_prompt\" in config:\\n            raise ValueError(\\n                \"Only one of example_prompt and example_prompt_path should \"\\n                \"be specified.\"\\n            )\\n        config[\"example_prompt\"] = load_prompt(config.pop(\"example_prompt_path\"))\\n    else:\\n        config[\"example_prompt\"] = load_prompt_from_config(config[\"example_prompt\"])\\n    # Load the examples.\\n    config = _load_examples(config)\\n    config = _load_output_parser(config)\\n    return FewShotPromptTemplate(**config)\\ndef _load_prompt(config: dict) -> PromptTemplate:\\n    \"\"\"Load the prompt template from config.\"\"\"\\n    # Load the template from disk if necessary.\\n    config = _load_template(\"template\", config)\\n    config = _load_output_parser(config)\\n    return PromptTemplate(**config)\\n[docs]def load_prompt(path: Union[str, Path]) -> BasePromptTemplate:\\n    \"\"\"Unified method for loading a prompt from LangChainHub or local fs.\"\"\"\\n    if hub_result := try_load_from_hub(\\n        path, _load_prompt_from_file, \"prompts\", {\"py\", \"json\", \"yaml\"}\\n    ):\\n        return hub_result\\n    else:\\n        return _load_prompt_from_file(path)\\ndef _load_prompt_from_file(file: Union[str, Path]) -> BasePromptTemplate:\\n    \"\"\"Load prompt from file.\"\"\"\\n    # Convert file to Path object.\\n    if isinstance(file, str):\\n        file_path = Path(file)\\n    else:\\n        file_path = file\\n    # Load from either json or yaml.\\n    if file_path.suffix == \".json\":\\n        with open(file_path) as f:\\n            config = json.load(f)\\n    elif file_path.suffix == \".yaml\":\\n        with open(file_path, \"r\") as f:\\n            config = yaml.safe_load(f)\\n    elif file_path.suffix == \".py\":\\n        spec = importlib.util.spec_from_loader(\\n            \"prompt\", loader=None, origin=str(file_path)\\n        )\\n        if spec is None:\\n            raise ValueError(\"could not load spec\")\\n        helper = importlib.util.module_from_spec(spec)\\n        with open(file_path, \"rb\") as f:\\n            exec(f.read(), helper.__dict__)\\n        if not isinstance(helper.PROMPT, BasePromptTemplate):\\n            raise ValueError(\"Did not get object of type BasePromptTemplate.\")\\n        return helper.PROMPT\\n    else:\\n        raise ValueError(f\"Got unsupported file type {file_path.suffix}\")\\n    # Load the prompt from the config now.\\n    return load_prompt_from_config(config)\\ntype_to_loader_dict = {\\n    \"prompt\": _load_prompt,\\n    \"few_shot\": _load_few_shot_prompt,\\n    # \"few_shot_with_templates\": _load_few_shot_with_templates_prompt,\\n}\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\prompts\\\\loading.html'}),\n",
              " Document(page_content='Source code for langchain.prompts.prompt\\n\"\"\"Prompt schema definition.\"\"\"\\nfrom __future__ import annotations\\nfrom pathlib import Path\\nfrom string import Formatter\\nfrom typing import Any, Dict, List, Union\\nfrom pydantic import Extra, root_validator\\nfrom langchain.prompts.base import (\\n    DEFAULT_FORMATTER_MAPPING,\\n    StringPromptTemplate,\\n    _get_jinja2_variables_from_template,\\n    check_valid_template,\\n)\\n[docs]class PromptTemplate(StringPromptTemplate):\\n    \"\"\"Schema to represent a prompt for an LLM.\\n    Example:\\n        .. code-block:: python\\n            from langchain import PromptTemplate\\n            prompt = PromptTemplate(input_variables=[\"foo\"], template=\"Say {foo}\")\\n    \"\"\"\\n    input_variables: List[str]\\n    \"\"\"A list of the names of the variables the prompt template expects.\"\"\"\\n    template: str\\n    \"\"\"The prompt template.\"\"\"\\n    template_format: str = \"f-string\"\\n    \"\"\"The format of the prompt template. Options are: \\'f-string\\', \\'jinja2\\'.\"\"\"\\n    validate_template: bool = True\\n    \"\"\"Whether or not to try validating the template.\"\"\"\\n    @property\\n    def _prompt_type(self) -> str:\\n        \"\"\"Return the prompt type key.\"\"\"\\n        return \"prompt\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n[docs]    def format(self, **kwargs: Any) -> str:\\n        \"\"\"Format the prompt with the inputs.\\n        Args:\\n            kwargs: Any arguments to be passed to the prompt template.\\n        Returns:\\n            A formatted string.\\n        Example:\\n        .. code-block:: python\\n            prompt.format(variable1=\"foo\")\\n        \"\"\"\\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\\n        return DEFAULT_FORMATTER_MAPPING[self.template_format](self.template, **kwargs)\\n    @root_validator()\\n    def template_is_valid(cls, values: Dict) -> Dict:\\n        \"\"\"Check that template and input variables are consistent.\"\"\"\\n        if values[\"validate_template\"]:\\n            all_inputs = values[\"input_variables\"] + list(values[\"partial_variables\"])\\n            check_valid_template(\\n                values[\"template\"], values[\"template_format\"], all_inputs\\n            )\\n        return values\\n[docs]    @classmethod\\n    def from_examples(\\n        cls,\\n        examples: List[str],\\n        suffix: str,\\n        input_variables: List[str],\\n        example_separator: str = \"\\\\n\\\\n\",\\n        prefix: str = \"\",\\n        **kwargs: Any,\\n    ) -> PromptTemplate:\\n        \"\"\"Take examples in list format with prefix and suffix to create a prompt.\\n        Intended to be used as a way to dynamically create a prompt from examples.\\n        Args:\\n            examples: List of examples to use in the prompt.\\n            suffix: String to go after the list of examples. Should generally\\n                set up the user\\'s input.\\n            input_variables: A list of variable names the final prompt template\\n                will expect.\\n            example_separator: The separator to use in between examples. Defaults\\n                to two new line characters.\\n            prefix: String that should go before any examples. Generally includes\\n                examples. Default to an empty string.\\n        Returns:\\n            The final prompt generated.\\n        \"\"\"\\n        template = example_separator.join([prefix, *examples, suffix])\\n        return cls(input_variables=input_variables, template=template, **kwargs)\\n[docs]    @classmethod\\n    def from_file(\\n        cls, template_file: Union[str, Path], input_variables: List[str], **kwargs: Any\\n    ) -> PromptTemplate:\\n        \"\"\"Load a prompt from a file.\\n        Args:\\n            template_file: The path to the file containing the prompt template.\\n            input_variables: A list of variable names the final prompt template\\n                will expect.\\n        Returns:\\n            The prompt loaded from the file.\\n        \"\"\"\\n        with open(str(template_file), \"r\") as f:\\n            template = f.read()\\n        return cls(input_variables=input_variables, template=template, **kwargs)\\n[docs]    @classmethod\\n    def from_template(cls, template: str, **kwargs: Any) -> PromptTemplate:\\n        \"\"\"Load a prompt template from a template.\"\"\"\\n        if \"template_format\" in kwargs and kwargs[\"template_format\"] == \"jinja2\":\\n            # Get the variables for the template\\n            input_variables = _get_jinja2_variables_from_template(template)\\n        else:\\n            input_variables = {\\n                v for _, v, _, _ in Formatter().parse(template) if v is not None\\n            }\\n        return cls(\\n            input_variables=list(sorted(input_variables)), template=template, **kwargs\\n        )\\n# For backwards compatibility.\\nPrompt = PromptTemplate\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\prompts\\\\prompt.html'}),\n",
              " Document(page_content='Source code for langchain.prompts.example_selector.length_based\\n\"\"\"Select examples based on length.\"\"\"\\nimport re\\nfrom typing import Callable, Dict, List\\nfrom pydantic import BaseModel, validator\\nfrom langchain.prompts.example_selector.base import BaseExampleSelector\\nfrom langchain.prompts.prompt import PromptTemplate\\ndef _get_length_based(text: str) -> int:\\n    return len(re.split(\"\\\\n| \", text))\\n[docs]class LengthBasedExampleSelector(BaseExampleSelector, BaseModel):\\n    \"\"\"Select examples based on length.\"\"\"\\n    examples: List[dict]\\n    \"\"\"A list of the examples that the prompt template expects.\"\"\"\\n    example_prompt: PromptTemplate\\n    \"\"\"Prompt template used to format the examples.\"\"\"\\n    get_text_length: Callable[[str], int] = _get_length_based\\n    \"\"\"Function to measure prompt length. Defaults to word count.\"\"\"\\n    max_length: int = 2048\\n    \"\"\"Max length for the prompt, beyond which examples are cut.\"\"\"\\n    example_text_lengths: List[int] = []  #: :meta private:\\n[docs]    def add_example(self, example: Dict[str, str]) -> None:\\n        \"\"\"Add new example to list.\"\"\"\\n        self.examples.append(example)\\n        string_example = self.example_prompt.format(**example)\\n        self.example_text_lengths.append(self.get_text_length(string_example))\\n    @validator(\"example_text_lengths\", always=True)\\n    def calculate_example_text_lengths(cls, v: List[int], values: Dict) -> List[int]:\\n        \"\"\"Calculate text lengths if they don\\'t exist.\"\"\"\\n        # Check if text lengths were passed in\\n        if v:\\n            return v\\n        # If they were not, calculate them\\n        example_prompt = values[\"example_prompt\"]\\n        get_text_length = values[\"get_text_length\"]\\n        string_examples = [example_prompt.format(**eg) for eg in values[\"examples\"]]\\n        return [get_text_length(eg) for eg in string_examples]\\n[docs]    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\\n        \"\"\"Select which examples to use based on the input lengths.\"\"\"\\n        inputs = \" \".join(input_variables.values())\\n        remaining_length = self.max_length - self.get_text_length(inputs)\\n        i = 0\\n        examples = []\\n        while remaining_length > 0 and i < len(self.examples):\\n            new_length = remaining_length - self.example_text_lengths[i]\\n            if new_length < 0:\\n                break\\n            else:\\n                examples.append(self.examples[i])\\n                remaining_length = new_length\\n            i += 1\\n        return examples\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\prompts\\\\example_selector\\\\length_based.html'}),\n",
              " Document(page_content='Source code for langchain.prompts.example_selector.semantic_similarity\\n\"\"\"Example selector that selects examples based on SemanticSimilarity.\"\"\"\\nfrom __future__ import annotations\\nfrom typing import Any, Dict, List, Optional, Type\\nfrom pydantic import BaseModel, Extra\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.prompts.example_selector.base import BaseExampleSelector\\nfrom langchain.vectorstores.base import VectorStore\\ndef sorted_values(values: Dict[str, str]) -> List[Any]:\\n    \"\"\"Return a list of values in dict sorted by key.\"\"\"\\n    return [values[val] for val in sorted(values)]\\n[docs]class SemanticSimilarityExampleSelector(BaseExampleSelector, BaseModel):\\n    \"\"\"Example selector that selects examples based on SemanticSimilarity.\"\"\"\\n    vectorstore: VectorStore\\n    \"\"\"VectorStore than contains information about examples.\"\"\"\\n    k: int = 4\\n    \"\"\"Number of examples to select.\"\"\"\\n    example_keys: Optional[List[str]] = None\\n    \"\"\"Optional keys to filter examples to.\"\"\"\\n    input_keys: Optional[List[str]] = None\\n    \"\"\"Optional keys to filter input to. If provided, the search is based on\\n    the input variables instead of all variables.\"\"\"\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n[docs]    def add_example(self, example: Dict[str, str]) -> str:\\n        \"\"\"Add new example to vectorstore.\"\"\"\\n        if self.input_keys:\\n            string_example = \" \".join(\\n                sorted_values({key: example[key] for key in self.input_keys})\\n            )\\n        else:\\n            string_example = \" \".join(sorted_values(example))\\n        ids = self.vectorstore.add_texts([string_example], metadatas=[example])\\n        return ids[0]\\n[docs]    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\\n        \"\"\"Select which examples to use based on semantic similarity.\"\"\"\\n        # Get the docs with the highest similarity.\\n        if self.input_keys:\\n            input_variables = {key: input_variables[key] for key in self.input_keys}\\n        query = \" \".join(sorted_values(input_variables))\\n        example_docs = self.vectorstore.similarity_search(query, k=self.k)\\n        # Get the examples from the metadata.\\n        # This assumes that examples are stored in metadata.\\n        examples = [dict(e.metadata) for e in example_docs]\\n        # If example keys are provided, filter examples to those keys.\\n        if self.example_keys:\\n            examples = [{k: eg[k] for k in self.example_keys} for eg in examples]\\n        return examples\\n[docs]    @classmethod\\n    def from_examples(\\n        cls,\\n        examples: List[dict],\\n        embeddings: Embeddings,\\n        vectorstore_cls: Type[VectorStore],\\n        k: int = 4,\\n        input_keys: Optional[List[str]] = None,\\n        **vectorstore_cls_kwargs: Any,\\n    ) -> SemanticSimilarityExampleSelector:\\n        \"\"\"Create k-shot example selector using example list and embeddings.\\n        Reshuffles examples dynamically based on query similarity.\\n        Args:\\n            examples: List of examples to use in the prompt.\\n            embeddings: An initialized embedding API interface, e.g. OpenAIEmbeddings().\\n            vectorstore_cls: A vector store DB interface class, e.g. FAISS.\\n            k: Number of examples to select\\n            input_keys: If provided, the search is based on the input variables\\n                instead of all variables.\\n            vectorstore_cls_kwargs: optional kwargs containing url for vector store\\n        Returns:\\n            The ExampleSelector instantiated, backed by a vector store.\\n        \"\"\"\\n        if input_keys:\\n            string_examples = [\\n                \" \".join(sorted_values({k: eg[k] for k in input_keys}))\\n                for eg in examples\\n            ]\\n        else:\\n            string_examples = [\" \".join(sorted_values(eg)) for eg in examples]\\n        vectorstore = vectorstore_cls.from_texts(\\n            string_examples, embeddings, metadatas=examples, **vectorstore_cls_kwargs\\n        )\\n        return cls(vectorstore=vectorstore, k=k, input_keys=input_keys)\\n[docs]class MaxMarginalRelevanceExampleSelector(SemanticSimilarityExampleSelector):\\n    \"\"\"ExampleSelector that selects examples based on Max Marginal Relevance.\\n    This was shown to improve performance in this paper:\\n    https://arxiv.org/pdf/2211.13892.pdf\\n    \"\"\"\\n    fetch_k: int = 20\\n    \"\"\"Number of examples to fetch to rerank.\"\"\"\\n[docs]    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\\n        \"\"\"Select which examples to use based on semantic similarity.\"\"\"\\n        # Get the docs with the highest similarity.\\n        if self.input_keys:\\n            input_variables = {key: input_variables[key] for key in self.input_keys}\\n        query = \" \".join(sorted_values(input_variables))\\n        example_docs = self.vectorstore.max_marginal_relevance_search(\\n            query, k=self.k, fetch_k=self.fetch_k\\n        )\\n        # Get the examples from the metadata.\\n        # This assumes that examples are stored in metadata.\\n        examples = [dict(e.metadata) for e in example_docs]\\n        # If example keys are provided, filter examples to those keys.\\n        if self.example_keys:\\n            examples = [{k: eg[k] for k in self.example_keys} for eg in examples]\\n        return examples\\n[docs]    @classmethod\\n    def from_examples(\\n        cls,\\n        examples: List[dict],\\n        embeddings: Embeddings,\\n        vectorstore_cls: Type[VectorStore],\\n        k: int = 4,\\n        input_keys: Optional[List[str]] = None,\\n        fetch_k: int = 20,\\n        **vectorstore_cls_kwargs: Any,\\n    ) -> MaxMarginalRelevanceExampleSelector:\\n        \"\"\"Create k-shot example selector using example list and embeddings.\\n        Reshuffles examples dynamically based on query similarity.\\n        Args:\\n            examples: List of examples to use in the prompt.\\n            embeddings: An iniialized embedding API interface, e.g. OpenAIEmbeddings().\\n            vectorstore_cls: A vector store DB interface class, e.g. FAISS.\\n            k: Number of examples to select\\n            input_keys: If provided, the search is based on the input variables\\n                instead of all variables.\\n            vectorstore_cls_kwargs: optional kwargs containing url for vector store\\n        Returns:\\n            The ExampleSelector instantiated, backed by a vector store.\\n        \"\"\"\\n        if input_keys:\\n            string_examples = [\\n                \" \".join(sorted_values({k: eg[k] for k in input_keys}))\\n                for eg in examples\\n            ]\\n        else:\\n            string_examples = [\" \".join(sorted_values(eg)) for eg in examples]\\n        vectorstore = vectorstore_cls.from_texts(\\n            string_examples, embeddings, metadatas=examples, **vectorstore_cls_kwargs\\n        )\\n        return cls(vectorstore=vectorstore, k=k, fetch_k=fetch_k, input_keys=input_keys)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\prompts\\\\example_selector\\\\semantic_similarity.html'}),\n",
              " Document(page_content='Source code for langchain.utilities.python\\nimport sys\\nfrom io import StringIO\\nfrom typing import Dict, Optional\\nfrom pydantic import BaseModel, Field\\n[docs]class PythonREPL(BaseModel):\\n    \"\"\"Simulates a standalone Python REPL.\"\"\"\\n    globals: Optional[Dict] = Field(default_factory=dict, alias=\"_globals\")\\n    locals: Optional[Dict] = Field(default_factory=dict, alias=\"_locals\")\\n[docs]    def run(self, command: str) -> str:\\n        \"\"\"Run command with own globals/locals and returns anything printed.\"\"\"\\n        old_stdout = sys.stdout\\n        sys.stdout = mystdout = StringIO()\\n        try:\\n            exec(command, self.globals, self.locals)\\n            sys.stdout = old_stdout\\n            output = mystdout.getvalue()\\n        except Exception as e:\\n            sys.stdout = old_stdout\\n            output = str(e)\\n        return output\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\utilities\\\\python.html'}),\n",
              " Document(page_content='Source code for langchain.utilities.searx_search\\n\"\"\"Utility for using SearxNG meta search API.\\nSearxNG is a privacy-friendly free metasearch engine that aggregates results from\\n`multiple search engines\\n<https://docs.searxng.org/admin/engines/configured_engines.html>`_ and databases and\\nsupports the `OpenSearch \\n<https://github.com/dewitt/opensearch/blob/master/opensearch-1-1-draft-6.md>`_\\nspecification.\\nMore detailes on the installtion instructions `here. <../../ecosystem/searx.html>`_\\nFor the search API refer to https://docs.searxng.org/dev/search_api.html\\nQuick Start\\n-----------\\nIn order to use this utility you need to provide the searx host. This can be done\\nby passing the named parameter :attr:`searx_host <SearxSearchWrapper.searx_host>`\\nor exporting the environment variable SEARX_HOST.\\nNote: this is the only required parameter.\\nThen create a searx search instance like this:\\n    .. code-block:: python\\n        from langchain.utilities import SearxSearchWrapper\\n        # when the host starts with `http` SSL is disabled and the connection\\n        # is assumed to be on a private network\\n        searx_host=\\'http://self.hosted\\'\\n        search = SearxSearchWrapper(searx_host=searx_host)\\nYou can now use the ``search`` instance to query the searx API.\\nSearching\\n---------\\nUse the :meth:`run() <SearxSearchWrapper.run>` and\\n:meth:`results() <SearxSearchWrapper.results>` methods to query the searx API.\\nOther methods are are available for convenience.\\n:class:`SearxResults` is a convenience wrapper around the raw json result.\\nExample usage of the ``run`` method to make a search:\\n    .. code-block:: python\\n        s.run(query=\"what is the best search engine?\")\\nEngine Parameters\\n-----------------\\nYou can pass any `accepted searx search API\\n<https://docs.searxng.org/dev/search_api.html>`_ parameters to the\\n:py:class:`SearxSearchWrapper` instance.\\nIn the following example we are using the\\n:attr:`engines <SearxSearchWrapper.engines>` and the ``language`` parameters:\\n    .. code-block:: python\\n        # assuming the searx host is set as above or exported as an env variable\\n        s = SearxSearchWrapper(engines=[\\'google\\', \\'bing\\'],\\n                            language=\\'es\\')\\nSearch Tips\\n-----------\\nSearx offers a special\\n`search syntax <https://docs.searxng.org/user/index.html#search-syntax>`_\\nthat can also be used instead of passing engine parameters.\\nFor example the following query:\\n    .. code-block:: python\\n        s = SearxSearchWrapper(\"langchain library\", engines=[\\'github\\'])\\n        # can also be written as:\\n        s = SearxSearchWrapper(\"langchain library !github\")\\n        # or even:\\n        s = SearxSearchWrapper(\"langchain library !gh\")\\nIn some situations you might want to pass an extra string to the search query.\\nFor example when the `run()` method is called by an agent. The search suffix can\\nalso be used as a way to pass extra parameters to searx or the underlying search\\nengines.\\n    .. code-block:: python\\n        # select the github engine and pass the search suffix\\n        s = SearchWrapper(\"langchain library\", query_suffix=\"!gh\")\\n        s = SearchWrapper(\"langchain library\")\\n        # select github the conventional google search syntax\\n        s.run(\"large language models\", query_suffix=\"site:github.com\")\\n*NOTE*: A search suffix can be defined on both the instance and the method level.\\nThe resulting query will be the concatenation of the two with the former taking\\nprecedence.\\nSee `SearxNG Configured Engines\\n<https://docs.searxng.org/admin/engines/configured_engines.html>`_ and\\n`SearxNG Search Syntax <https://docs.searxng.org/user/index.html#id1>`_\\nfor more details.\\nNotes\\n-----\\nThis wrapper is based on the SearxNG fork https://github.com/searxng/searxng which is\\nbetter maintained than the original Searx project and offers more features.\\nPublic searxNG instances often use a rate limiter for API usage, so you might want to\\nuse a self hosted instance and disable the rate limiter.\\nIf you are self-hosting an instance you can customize the rate limiter for your\\nown network as described `here <https://github.com/searxng/searxng/pull/2129>`_.\\nFor a list of public SearxNG instances see https://searx.space/\\n\"\"\"\\nimport json\\nfrom typing import Any, Dict, List, Optional\\nimport aiohttp\\nimport requests\\nfrom pydantic import BaseModel, Extra, Field, PrivateAttr, root_validator, validator\\nfrom langchain.utils import get_from_dict_or_env\\ndef _get_default_params() -> dict:\\n    return {\"language\": \"en\", \"format\": \"json\"}\\n[docs]class SearxResults(dict):\\n    \"\"\"Dict like wrapper around search api results.\"\"\"\\n    _data = \"\"\\n    def __init__(self, data: str):\\n        \"\"\"Take a raw result from Searx and make it into a dict like object.\"\"\"\\n        json_data = json.loads(data)\\n        super().__init__(json_data)\\n        self.__dict__ = self\\n    def __str__(self) -> str:\\n        \"\"\"Text representation of searx result.\"\"\"\\n        return self._data\\n    @property\\n    def results(self) -> Any:\\n        \"\"\"Silence mypy for accessing this field.\\n        :meta private:\\n        \"\"\"\\n        return self.get(\"results\")\\n    @property\\n    def answers(self) -> Any:\\n        \"\"\"Helper accessor on the json result.\"\"\"\\n        return self.get(\"answers\")\\n[docs]class SearxSearchWrapper(BaseModel):\\n    \"\"\"Wrapper for Searx API.\\n    To use you need to provide the searx host by passing the named parameter\\n    ``searx_host`` or exporting the environment variable ``SEARX_HOST``.\\n    In some situations you might want to disable SSL verification, for example\\n    if you are running searx locally. You can do this by passing the named parameter\\n    ``unsecure``. You can also pass the host url scheme as ``http`` to disable SSL.\\n    Example:\\n        .. code-block:: python\\n            from langchain.utilities import SearxSearchWrapper\\n            searx = SearxSearchWrapper(searx_host=\"http://localhost:8888\")\\n    Example with SSL disabled:\\n        .. code-block:: python\\n            from langchain.utilities import SearxSearchWrapper\\n            # note the unsecure parameter is not needed if you pass the url scheme as\\n            # http\\n            searx = SearxSearchWrapper(searx_host=\"http://localhost:8888\",\\n                                                    unsecure=True)\\n    \"\"\"\\n    _result: SearxResults = PrivateAttr()\\n    searx_host: str = \"\"\\n    unsecure: bool = False\\n    params: dict = Field(default_factory=_get_default_params)\\n    headers: Optional[dict] = None\\n    engines: Optional[List[str]] = []\\n    categories: Optional[List[str]] = []\\n    query_suffix: Optional[str] = \"\"\\n    k: int = 10\\n    aiosession: Optional[Any] = None\\n    @validator(\"unsecure\")\\n    def disable_ssl_warnings(cls, v: bool) -> bool:\\n        \"\"\"Disable SSL warnings.\"\"\"\\n        if v:\\n            # requests.urllib3.disable_warnings()\\n            try:\\n                import urllib3\\n                urllib3.disable_warnings()\\n            except ImportError as e:\\n                print(e)\\n        return v\\n    @root_validator()\\n    def validate_params(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that custom searx params are merged with default ones.\"\"\"\\n        user_params = values[\"params\"]\\n        default = _get_default_params()\\n        values[\"params\"] = {**default, **user_params}\\n        engines = values.get(\"engines\")\\n        if engines:\\n            values[\"params\"][\"engines\"] = \",\".join(engines)\\n        categories = values.get(\"categories\")\\n        if categories:\\n            values[\"params\"][\"categories\"] = \",\".join(categories)\\n        searx_host = get_from_dict_or_env(values, \"searx_host\", \"SEARX_HOST\")\\n        if not searx_host.startswith(\"http\"):\\n            print(\\n                f\"Warning: missing the url scheme on host \\\\\\n                ! assuming secure https://{searx_host} \"\\n            )\\n            searx_host = \"https://\" + searx_host\\n        elif searx_host.startswith(\"http://\"):\\n            values[\"unsecure\"] = True\\n            cls.disable_ssl_warnings(True)\\n        values[\"searx_host\"] = searx_host\\n        return values\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n    def _searx_api_query(self, params: dict) -> SearxResults:\\n        \"\"\"Actual request to searx API.\"\"\"\\n        raw_result = requests.get(\\n            self.searx_host,\\n            headers=self.headers,\\n            params=params,\\n            verify=not self.unsecure,\\n        )\\n        # test if http result is ok\\n        if not raw_result.ok:\\n            raise ValueError(\"Searx API returned an error: \", raw_result.text)\\n        res = SearxResults(raw_result.text)\\n        self._result = res\\n        return res\\n    async def _asearx_api_query(self, params: dict) -> SearxResults:\\n        if not self.aiosession:\\n            async with aiohttp.ClientSession() as session:\\n                async with session.get(\\n                    self.searx_host,\\n                    headers=self.headers,\\n                    params=params,\\n                    ssl=(lambda: False if self.unsecure else None)(),\\n                ) as response:\\n                    if not response.ok:\\n                        raise ValueError(\"Searx API returned an error: \", response.text)\\n                    result = SearxResults(await response.text())\\n                    self._result = result\\n        else:\\n            async with self.aiosession.get(\\n                self.searx_host,\\n                headers=self.headers,\\n                params=params,\\n                verify=not self.unsecure,\\n            ) as response:\\n                if not response.ok:\\n                    raise ValueError(\"Searx API returned an error: \", response.text)\\n                result = SearxResults(await response.text())\\n                self._result = result\\n        return result\\n[docs]    def run(\\n        self,\\n        query: str,\\n        engines: Optional[List[str]] = None,\\n        categories: Optional[List[str]] = None,\\n        query_suffix: Optional[str] = \"\",\\n        **kwargs: Any,\\n    ) -> str:\\n        \"\"\"Run query through Searx API and parse results.\\n        You can pass any other params to the searx query API.\\n        Args:\\n            query: The query to search for.\\n            query_suffix: Extra suffix appended to the query.\\n            engines: List of engines to use for the query.\\n            categories: List of categories to use for the query.\\n            **kwargs: extra parameters to pass to the searx API.\\n        Returns:\\n            str: The result of the query.\\n        Raises:\\n            ValueError: If an error occured with the query.\\n        Example:\\n            This will make a query to the qwant engine:\\n            .. code-block:: python\\n                from langchain.utilities import SearxSearchWrapper\\n                searx = SearxSearchWrapper(searx_host=\"http://my.searx.host\")\\n                searx.run(\"what is the weather in France ?\", engine=\"qwant\")\\n                # the same result can be achieved using the `!` syntax of searx\\n                # to select the engine using `query_suffix`\\n                searx.run(\"what is the weather in France ?\", query_suffix=\"!qwant\")\\n        \"\"\"\\n        _params = {\\n            \"q\": query,\\n        }\\n        params = {**self.params, **_params, **kwargs}\\n        if self.query_suffix and len(self.query_suffix) > 0:\\n            params[\"q\"] += \" \" + self.query_suffix\\n        if isinstance(query_suffix, str) and len(query_suffix) > 0:\\n            params[\"q\"] += \" \" + query_suffix\\n        if isinstance(engines, list) and len(engines) > 0:\\n            params[\"engines\"] = \",\".join(engines)\\n        if isinstance(categories, list) and len(categories) > 0:\\n            params[\"categories\"] = \",\".join(categories)\\n        res = self._searx_api_query(params)\\n        if len(res.answers) > 0:\\n            toret = res.answers[0]\\n        # only return the content of the results list\\n        elif len(res.results) > 0:\\n            toret = \"\\\\n\\\\n\".join([r.get(\"content\", \"\") for r in res.results[: self.k]])\\n        else:\\n            toret = \"No good search result found\"\\n        return toret\\n[docs]    async def arun(\\n        self,\\n        query: str,\\n        engines: Optional[List[str]] = None,\\n        query_suffix: Optional[str] = \"\",\\n        **kwargs: Any,\\n    ) -> str:\\n        \"\"\"Asynchronously version of `run`.\"\"\"\\n        _params = {\\n            \"q\": query,\\n        }\\n        params = {**self.params, **_params, **kwargs}\\n        if self.query_suffix and len(self.query_suffix) > 0:\\n            params[\"q\"] += \" \" + self.query_suffix\\n        if isinstance(query_suffix, str) and len(query_suffix) > 0:\\n            params[\"q\"] += \" \" + query_suffix\\n        if isinstance(engines, list) and len(engines) > 0:\\n            params[\"engines\"] = \",\".join(engines)\\n        res = await self._asearx_api_query(params)\\n        if len(res.answers) > 0:\\n            toret = res.answers[0]\\n        # only return the content of the results list\\n        elif len(res.results) > 0:\\n            toret = \"\\\\n\\\\n\".join([r.get(\"content\", \"\") for r in res.results[: self.k]])\\n        else:\\n            toret = \"No good search result found\"\\n        return toret\\n[docs]    def results(\\n        self,\\n        query: str,\\n        num_results: int,\\n        engines: Optional[List[str]] = None,\\n        categories: Optional[List[str]] = None,\\n        query_suffix: Optional[str] = \"\",\\n        **kwargs: Any,\\n    ) -> List[Dict]:\\n        \"\"\"Run query through Searx API and returns the results with metadata.\\n        Args:\\n            query: The query to search for.\\n            query_suffix: Extra suffix appended to the query.\\n            num_results: Limit the number of results to return.\\n            engines: List of engines to use for the query.\\n            categories: List of categories to use for the query.\\n            **kwargs: extra parameters to pass to the searx API.\\n        Returns:\\n            Dict with the following keys:\\n            {\\n                snippet:  The description of the result.\\n                title:  The title of the result.\\n                link: The link to the result.\\n                engines: The engines used for the result.\\n                category: Searx category of the result.\\n            }\\n        \"\"\"\\n        _params = {\\n            \"q\": query,\\n        }\\n        params = {**self.params, **_params, **kwargs}\\n        if self.query_suffix and len(self.query_suffix) > 0:\\n            params[\"q\"] += \" \" + self.query_suffix\\n        if isinstance(query_suffix, str) and len(query_suffix) > 0:\\n            params[\"q\"] += \" \" + query_suffix\\n        if isinstance(engines, list) and len(engines) > 0:\\n            params[\"engines\"] = \",\".join(engines)\\n        if isinstance(categories, list) and len(categories) > 0:\\n            params[\"categories\"] = \",\".join(categories)\\n        results = self._searx_api_query(params).results[:num_results]\\n        if len(results) == 0:\\n            return [{\"Result\": \"No good Search Result was found\"}]\\n        return [\\n            {\\n                \"snippet\": result.get(\"content\", \"\"),\\n                \"title\": result[\"title\"],\\n                \"link\": result[\"url\"],\\n                \"engines\": result[\"engines\"],\\n                \"category\": result[\"category\"],\\n            }\\n            for result in results\\n        ]\\n[docs]    async def aresults(\\n        self,\\n        query: str,\\n        num_results: int,\\n        engines: Optional[List[str]] = None,\\n        query_suffix: Optional[str] = \"\",\\n        **kwargs: Any,\\n    ) -> List[Dict]:\\n        \"\"\"Asynchronously query with json results.\\n        Uses aiohttp. See `results` for more info.\\n        \"\"\"\\n        _params = {\\n            \"q\": query,\\n        }\\n        params = {**self.params, **_params, **kwargs}\\n        if self.query_suffix and len(self.query_suffix) > 0:\\n            params[\"q\"] += \" \" + self.query_suffix\\n        if isinstance(query_suffix, str) and len(query_suffix) > 0:\\n            params[\"q\"] += \" \" + query_suffix\\n        if isinstance(engines, list) and len(engines) > 0:\\n            params[\"engines\"] = \",\".join(engines)\\n        results = (await self._asearx_api_query(params)).results[:num_results]\\n        if len(results) == 0:\\n            return [{\"Result\": \"No good Search Result was found\"}]\\n        return [\\n            {\\n                \"snippet\": result.get(\"content\", \"\"),\\n                \"title\": result[\"title\"],\\n                \"link\": result[\"url\"],\\n                \"engines\": result[\"engines\"],\\n                \"category\": result[\"category\"],\\n            }\\n            for result in results\\n        ]\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\utilities\\\\searx_search.html'}),\n",
              " Document(page_content='Source code for langchain.utilities.serpapi\\n\"\"\"Chain that calls SerpAPI.\\nHeavily borrowed from https://github.com/ofirpress/self-ask\\n\"\"\"\\nimport os\\nimport sys\\nfrom typing import Any, Dict, Optional, Tuple\\nimport aiohttp\\nfrom pydantic import BaseModel, Extra, Field, root_validator\\nfrom langchain.utils import get_from_dict_or_env\\nclass HiddenPrints:\\n    \"\"\"Context manager to hide prints.\"\"\"\\n    def __enter__(self) -> None:\\n        \"\"\"Open file to pipe stdout to.\"\"\"\\n        self._original_stdout = sys.stdout\\n        sys.stdout = open(os.devnull, \"w\")\\n    def __exit__(self, *_: Any) -> None:\\n        \"\"\"Close file that stdout was piped to.\"\"\"\\n        sys.stdout.close()\\n        sys.stdout = self._original_stdout\\n[docs]class SerpAPIWrapper(BaseModel):\\n    \"\"\"Wrapper around SerpAPI.\\n    To use, you should have the ``google-search-results`` python package installed,\\n    and the environment variable ``SERPAPI_API_KEY`` set with your API key, or pass\\n    `serpapi_api_key` as a named parameter to the constructor.\\n    Example:\\n        .. code-block:: python\\n            from langchain import SerpAPIWrapper\\n            serpapi = SerpAPIWrapper()\\n    \"\"\"\\n    search_engine: Any  #: :meta private:\\n    params: dict = Field(\\n        default={\\n            \"engine\": \"google\",\\n            \"google_domain\": \"google.com\",\\n            \"gl\": \"us\",\\n            \"hl\": \"en\",\\n        }\\n    )\\n    serpapi_api_key: Optional[str] = None\\n    aiosession: Optional[aiohttp.ClientSession] = None\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        serpapi_api_key = get_from_dict_or_env(\\n            values, \"serpapi_api_key\", \"SERPAPI_API_KEY\"\\n        )\\n        values[\"serpapi_api_key\"] = serpapi_api_key\\n        try:\\n            from serpapi import GoogleSearch\\n            values[\"search_engine\"] = GoogleSearch\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import serpapi python package. \"\\n                \"Please install it with `pip install google-search-results`.\"\\n            )\\n        return values\\n[docs]    async def arun(self, query: str) -> str:\\n        \"\"\"Use aiohttp to run query through SerpAPI and parse result.\"\"\"\\n        def construct_url_and_params() -> Tuple[str, Dict[str, str]]:\\n            params = self.get_params(query)\\n            params[\"source\"] = \"python\"\\n            if self.serpapi_api_key:\\n                params[\"serp_api_key\"] = self.serpapi_api_key\\n            params[\"output\"] = \"json\"\\n            url = \"https://serpapi.com/search\"\\n            return url, params\\n        url, params = construct_url_and_params()\\n        if not self.aiosession:\\n            async with aiohttp.ClientSession() as session:\\n                async with session.get(url, params=params) as response:\\n                    res = await response.json()\\n        else:\\n            async with self.aiosession.get(url, params=params) as response:\\n                res = await response.json()\\n        return self._process_response(res)\\n[docs]    def run(self, query: str) -> str:\\n        \"\"\"Run query through SerpAPI and parse result.\"\"\"\\n        return self._process_response(self.results(query))\\n[docs]    def results(self, query: str) -> dict:\\n        \"\"\"Run query through SerpAPI and return the raw result.\"\"\"\\n        params = self.get_params(query)\\n        with HiddenPrints():\\n            search = self.search_engine(params)\\n            res = search.get_dict()\\n        return res\\n[docs]    def get_params(self, query: str) -> Dict[str, str]:\\n        \"\"\"Get parameters for SerpAPI.\"\"\"\\n        _params = {\\n            \"api_key\": self.serpapi_api_key,\\n            \"q\": query,\\n        }\\n        params = {**self.params, **_params}\\n        return params\\n    @staticmethod\\n    def _process_response(res: dict) -> str:\\n        \"\"\"Process response from SerpAPI.\"\"\"\\n        if \"error\" in res.keys():\\n            raise ValueError(f\"Got error from SerpAPI: {res[\\'error\\']}\")\\n        if \"answer_box\" in res.keys() and \"answer\" in res[\"answer_box\"].keys():\\n            toret = res[\"answer_box\"][\"answer\"]\\n        elif \"answer_box\" in res.keys() and \"snippet\" in res[\"answer_box\"].keys():\\n            toret = res[\"answer_box\"][\"snippet\"]\\n        elif (\\n            \"answer_box\" in res.keys()\\n            and \"snippet_highlighted_words\" in res[\"answer_box\"].keys()\\n        ):\\n            toret = res[\"answer_box\"][\"snippet_highlighted_words\"][0]\\n        elif (\\n            \"sports_results\" in res.keys()\\n            and \"game_spotlight\" in res[\"sports_results\"].keys()\\n        ):\\n            toret = res[\"sports_results\"][\"game_spotlight\"]\\n        elif (\\n            \"knowledge_graph\" in res.keys()\\n            and \"description\" in res[\"knowledge_graph\"].keys()\\n        ):\\n            toret = res[\"knowledge_graph\"][\"description\"]\\n        elif \"snippet\" in res[\"organic_results\"][0].keys():\\n            toret = res[\"organic_results\"][0][\"snippet\"]\\n        else:\\n            toret = \"No good search result found\"\\n        return toret\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\utilities\\\\serpapi.html'}),\n",
              " Document(page_content='Source code for langchain.vectorstores.annoy\\n\"\"\"Wrapper around Annoy vector database.\"\"\"\\nfrom __future__ import annotations\\nimport os\\nimport pickle\\nimport uuid\\nfrom configparser import ConfigParser\\nfrom pathlib import Path\\nfrom typing import Any, Callable, Dict, Iterable, List, Optional, Tuple\\nimport numpy as np\\nfrom langchain.docstore.base import Docstore\\nfrom langchain.docstore.document import Document\\nfrom langchain.docstore.in_memory import InMemoryDocstore\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.vectorstores.base import VectorStore\\nfrom langchain.vectorstores.utils import maximal_marginal_relevance\\nINDEX_METRICS = frozenset([\"angular\", \"euclidean\", \"manhattan\", \"hamming\", \"dot\"])\\nDEFAULT_METRIC = \"angular\"\\ndef dependable_annoy_import() -> Any:\\n    \"\"\"Import annoy if available, otherwise raise error.\"\"\"\\n    try:\\n        import annoy\\n    except ImportError:\\n        raise ValueError(\\n            \"Could not import annoy python package. \"\\n            \"Please install it with `pip install --user annoy` \"\\n        )\\n    return annoy\\n[docs]class Annoy(VectorStore):\\n    \"\"\"Wrapper around Annoy vector database.\\n    To use, you should have the ``annoy`` python package installed.\\n    Example:\\n        .. code-block:: python\\n            from langchain import Annoy\\n            db = Annoy(embedding_function, index, docstore, index_to_docstore_id)\\n    \"\"\"\\n    def __init__(\\n        self,\\n        embedding_function: Callable,\\n        index: Any,\\n        metric: str,\\n        docstore: Docstore,\\n        index_to_docstore_id: Dict[int, str],\\n    ):\\n        \"\"\"Initialize with necessary components.\"\"\"\\n        self.embedding_function = embedding_function\\n        self.index = index\\n        self.metric = metric\\n        self.docstore = docstore\\n        self.index_to_docstore_id = index_to_docstore_id\\n[docs]    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        raise NotImplementedError(\\n            \"Annoy does not allow to add new data once the index is build.\"\\n        )\\n[docs]    def process_index_results(\\n        self, idxs: List[int], dists: List[float]\\n    ) -> List[Tuple[Document, float]]:\\n        \"\"\"Turns annoy results into a list of documents and scores.\\n        Args:\\n            idxs: List of indices of the documents in the index.\\n            dists: List of distances of the documents in the index.\\n        Returns:\\n            List of Documents and scores.\\n        \"\"\"\\n        docs = []\\n        for idx, dist in zip(idxs, dists):\\n            _id = self.index_to_docstore_id[idx]\\n            doc = self.docstore.search(_id)\\n            if not isinstance(doc, Document):\\n                raise ValueError(f\"Could not find document for id {_id}, got {doc}\")\\n            docs.append((doc, dist))\\n        return docs\\n[docs]    def similarity_search_with_score_by_vector(\\n        self, embedding: List[float], k: int = 4, search_k: int = -1\\n    ) -> List[Tuple[Document, float]]:\\n        \"\"\"Return docs most similar to query.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            search_k: inspect up to search_k nodes which defaults\\n                to n_trees * n if not provided\\n        Returns:\\n            List of Documents most similar to the query and score for each\\n        \"\"\"\\n        idxs, dists = self.index.get_nns_by_vector(\\n            embedding, k, search_k=search_k, include_distances=True\\n        )\\n        return self.process_index_results(idxs, dists)\\n[docs]    def similarity_search_with_score_by_index(\\n        self, docstore_index: int, k: int = 4, search_k: int = -1\\n    ) -> List[Tuple[Document, float]]:\\n        \"\"\"Return docs most similar to query.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            search_k: inspect up to search_k nodes which defaults\\n                to n_trees * n if not provided\\n        Returns:\\n            List of Documents most similar to the query and score for each\\n        \"\"\"\\n        idxs, dists = self.index.get_nns_by_item(\\n            docstore_index, k, search_k=search_k, include_distances=True\\n        )\\n        return self.process_index_results(idxs, dists)\\n[docs]    def similarity_search_with_score(\\n        self, query: str, k: int = 4, search_k: int = -1\\n    ) -> List[Tuple[Document, float]]:\\n        \"\"\"Return docs most similar to query.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            search_k: inspect up to search_k nodes which defaults\\n                to n_trees * n if not provided\\n        Returns:\\n            List of Documents most similar to the query and score for each\\n        \"\"\"\\n        embedding = self.embedding_function(query)\\n        docs = self.similarity_search_with_score_by_vector(embedding, k, search_k)\\n        return docs\\n[docs]    def similarity_search_by_vector(\\n        self, embedding: List[float], k: int = 4, search_k: int = -1, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs most similar to embedding vector.\\n        Args:\\n            embedding: Embedding to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            search_k: inspect up to search_k nodes which defaults\\n                to n_trees * n if not provided\\n        Returns:\\n            List of Documents most similar to the embedding.\\n        \"\"\"\\n        docs_and_scores = self.similarity_search_with_score_by_vector(\\n            embedding, k, search_k\\n        )\\n        return [doc for doc, _ in docs_and_scores]\\n[docs]    def similarity_search_by_index(\\n        self, docstore_index: int, k: int = 4, search_k: int = -1, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs most similar to docstore_index.\\n        Args:\\n            docstore_index: Index of document in docstore\\n            k: Number of Documents to return. Defaults to 4.\\n            search_k: inspect up to search_k nodes which defaults\\n                to n_trees * n if not provided\\n        Returns:\\n            List of Documents most similar to the embedding.\\n        \"\"\"\\n        docs_and_scores = self.similarity_search_with_score_by_index(\\n            docstore_index, k, search_k\\n        )\\n        return [doc for doc, _ in docs_and_scores]\\n[docs]    def similarity_search(\\n        self, query: str, k: int = 4, search_k: int = -1, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs most similar to query.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            search_k: inspect up to search_k nodes which defaults\\n                to n_trees * n if not provided\\n        Returns:\\n            List of Documents most similar to the query.\\n        \"\"\"\\n        docs_and_scores = self.similarity_search_with_score(query, k, search_k)\\n        return [doc for doc, _ in docs_and_scores]\\n[docs]    def max_marginal_relevance_search_by_vector(\\n        self, embedding: List[float], k: int = 4, fetch_k: int = 20, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs selected using the maximal marginal relevance.\\n        Maximal marginal relevance optimizes for similarity to query AND diversity\\n        among selected documents.\\n        Args:\\n            embedding: Embedding to look up documents similar to.\\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n            k: Number of Documents to return. Defaults to 4.\\n        Returns:\\n            List of Documents selected by maximal marginal relevance.\\n        \"\"\"\\n        idxs = self.index.get_nns_by_vector(\\n            embedding, fetch_k, search_k=-1, include_distances=False\\n        )\\n        embeddings = [self.index.get_item_vector(i) for i in idxs]\\n        mmr_selected = maximal_marginal_relevance(\\n            np.array([embedding], dtype=np.float32), embeddings, k=k\\n        )\\n        # ignore the -1\\'s if not enough docs are returned/indexed\\n        selected_indices = [idxs[i] for i in mmr_selected if i != -1]\\n        docs = []\\n        for i in selected_indices:\\n            _id = self.index_to_docstore_id[i]\\n            doc = self.docstore.search(_id)\\n            if not isinstance(doc, Document):\\n                raise ValueError(f\"Could not find document for id {_id}, got {doc}\")\\n            docs.append(doc)\\n        return docs\\n[docs]    def max_marginal_relevance_search(\\n        self, query: str, k: int = 4, fetch_k: int = 20, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs selected using the maximal marginal relevance.\\n        Maximal marginal relevance optimizes for similarity to query AND diversity\\n        among selected documents.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n        Returns:\\n            List of Documents selected by maximal marginal relevance.\\n        \"\"\"\\n        embedding = self.embedding_function(query)\\n        docs = self.max_marginal_relevance_search_by_vector(embedding, k, fetch_k)\\n        return docs\\n    @classmethod\\n    def __from(\\n        cls,\\n        texts: List[str],\\n        embeddings: List[List[float]],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        metric: str = DEFAULT_METRIC,\\n        trees: int = 100,\\n        n_jobs: int = -1,\\n        **kwargs: Any,\\n    ) -> Annoy:\\n        if metric not in INDEX_METRICS:\\n            raise ValueError(\\n                (\\n                    f\"Unsupported distance metric: {metric}. \"\\n                    f\"Expected one of {list(INDEX_METRICS)}\"\\n                )\\n            )\\n        annoy = dependable_annoy_import()\\n        if not embeddings:\\n            raise ValueError(\"embeddings must be provided to build AnnoyIndex\")\\n        f = len(embeddings[0])\\n        index = annoy.AnnoyIndex(f, metric=metric)\\n        for i, emb in enumerate(embeddings):\\n            index.add_item(i, emb)\\n        index.build(trees, n_jobs=n_jobs)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            metadata = metadatas[i] if metadatas else {}\\n            documents.append(Document(page_content=text, metadata=metadata))\\n        index_to_id = {i: str(uuid.uuid4()) for i in range(len(documents))}\\n        docstore = InMemoryDocstore(\\n            {index_to_id[i]: doc for i, doc in enumerate(documents)}\\n        )\\n        return cls(embedding.embed_query, index, metric, docstore, index_to_id)\\n[docs]    @classmethod\\n    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        metric: str = DEFAULT_METRIC,\\n        trees: int = 100,\\n        n_jobs: int = -1,\\n        **kwargs: Any,\\n    ) -> Annoy:\\n        \"\"\"Construct Annoy wrapper from raw documents.\\n        Args:\\n            texts: List of documents to index.\\n            embedding: Embedding function to use.\\n            metadatas: List of metadata dictionaries to associate with documents.\\n            metric: Metric to use for indexing. Defaults to \"angular\".\\n            trees: Number of trees to use for indexing. Defaults to 100.\\n            n_jobs: Number of jobs to use for indexing. Defaults to -1.\\n        This is a user friendly interface that:\\n            1. Embeds documents.\\n            2. Creates an in memory docstore\\n            3. Initializes the Annoy database\\n        This is intended to be a quick way to get started.\\n        Example:\\n            .. code-block:: python\\n                from langchain import Annoy\\n                from langchain.embeddings import OpenAIEmbeddings\\n                embeddings = OpenAIEmbeddings()\\n                index = Annoy.from_texts(texts, embeddings)\\n        \"\"\"\\n        embeddings = embedding.embed_documents(texts)\\n        return cls.__from(\\n            texts, embeddings, embedding, metadatas, metric, trees, n_jobs, **kwargs\\n        )\\n[docs]    @classmethod\\n    def from_embeddings(\\n        cls,\\n        text_embeddings: List[Tuple[str, List[float]]],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        metric: str = DEFAULT_METRIC,\\n        trees: int = 100,\\n        n_jobs: int = -1,\\n        **kwargs: Any,\\n    ) -> Annoy:\\n        \"\"\"Construct Annoy wrapper from embeddings.\\n        Args:\\n            text_embeddings: List of tuples of (text, embedding)\\n            embedding: Embedding function to use.\\n            metadatas: List of metadata dictionaries to associate with documents.\\n            metric: Metric to use for indexing. Defaults to \"angular\".\\n            trees: Number of trees to use for indexing. Defaults to 100.\\n            n_jobs: Number of jobs to use for indexing. Defaults to -1\\n        This is a user friendly interface that:\\n            1. Creates an in memory docstore with provided embeddings\\n            2. Initializes the Annoy database\\n        This is intended to be a quick way to get started.\\n        Example:\\n            .. code-block:: python\\n                from langchain import Annoy\\n                from langchain.embeddings import OpenAIEmbeddings\\n                embeddings = OpenAIEmbeddings()\\n                text_embeddings = embeddings.embed_documents(texts)\\n                text_embedding_pairs = list(zip(texts, text_embeddings))\\n                db = Annoy.from_embeddings(text_embedding_pairs, embeddings)\\n        \"\"\"\\n        texts = [t[0] for t in text_embeddings]\\n        embeddings = [t[1] for t in text_embeddings]\\n        return cls.__from(\\n            texts, embeddings, embedding, metadatas, metric, trees, n_jobs, **kwargs\\n        )\\n[docs]    def save_local(self, folder_path: str, prefault: bool = False) -> None:\\n        \"\"\"Save Annoy index, docstore, and index_to_docstore_id to disk.\\n        Args:\\n            folder_path: folder path to save index, docstore,\\n                and index_to_docstore_id to.\\n            prefault: Whether to pre-load the index into memory.\\n        \"\"\"\\n        path = Path(folder_path)\\n        os.makedirs(path, exist_ok=True)\\n        # save index, index config, docstore and index_to_docstore_id\\n        config_object = ConfigParser()\\n        config_object[\"ANNOY\"] = {\\n            \"f\": self.index.f,\\n            \"metric\": self.metric,\\n        }\\n        self.index.save(str(path / \"index.annoy\"), prefault=prefault)\\n        with open(path / \"index.pkl\", \"wb\") as file:\\n            pickle.dump((self.docstore, self.index_to_docstore_id, config_object), file)\\n[docs]    @classmethod\\n    def load_local(\\n        cls,\\n        folder_path: str,\\n        embeddings: Embeddings,\\n    ) -> Annoy:\\n        \"\"\"Load Annoy index, docstore, and index_to_docstore_id to disk.\\n        Args:\\n            folder_path: folder path to load index, docstore,\\n                and index_to_docstore_id from.\\n            embeddings: Embeddings to use when generating queries.\\n        \"\"\"\\n        path = Path(folder_path)\\n        # load index separately since it is not picklable\\n        annoy = dependable_annoy_import()\\n        # load docstore and index_to_docstore_id\\n        with open(path / \"index.pkl\", \"rb\") as file:\\n            docstore, index_to_docstore_id, config_object = pickle.load(file)\\n        f = int(config_object[\"ANNOY\"][\"f\"])\\n        metric = config_object[\"ANNOY\"][\"metric\"]\\n        index = annoy.AnnoyIndex(f, metric=metric)\\n        index.load(str(path / \"index.annoy\"))\\n        return cls(\\n            embeddings.embed_query, index, metric, docstore, index_to_docstore_id\\n        )\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\vectorstores\\\\annoy.html'}),\n",
              " Document(page_content='Source code for langchain.vectorstores.atlas\\n\"\"\"Wrapper around Atlas by Nomic.\"\"\"\\nfrom __future__ import annotations\\nimport logging\\nimport uuid\\nfrom typing import Any, Iterable, List, Optional, Type\\nimport numpy as np\\nfrom langchain.docstore.document import Document\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.vectorstores.base import VectorStore\\nlogger = logging.getLogger(__name__)\\n[docs]class AtlasDB(VectorStore):\\n    \"\"\"Wrapper around Atlas: Nomic\\'s neural database and rhizomatic instrument.\\n    To use, you should have the ``nomic`` python package installed.\\n    Example:\\n        .. code-block:: python\\n                from langchain.vectorstores import AtlasDB\\n                from langchain.embeddings.openai import OpenAIEmbeddings\\n                embeddings = OpenAIEmbeddings()\\n                vectorstore = AtlasDB(\"my_project\", embeddings.embed_query)\\n    \"\"\"\\n    _ATLAS_DEFAULT_ID_FIELD = \"atlas_id\"\\n    def __init__(\\n        self,\\n        name: str,\\n        embedding_function: Optional[Embeddings] = None,\\n        api_key: Optional[str] = None,\\n        description: str = \"A description for your project\",\\n        is_public: bool = True,\\n        reset_project_if_exists: bool = False,\\n    ) -> None:\\n        \"\"\"\\n        Initialize the Atlas Client\\n        Args:\\n            name (str): The name of your project. If the project already exists,\\n                it will be loaded.\\n            embedding_function (Optional[Callable]): An optional function used for\\n                embedding your data. If None, data will be embedded with\\n                Nomic\\'s embed model.\\n            api_key (str): Your nomic API key\\n            description (str): A description for your project.\\n            is_public (bool): Whether your project is publicly accessible.\\n                True by default.\\n            reset_project_if_exists (bool): Whether to reset this project if it\\n                already exists. Default False.\\n                Generally userful during development and testing.\\n        \"\"\"\\n        try:\\n            import nomic\\n            from nomic import AtlasProject\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import nomic python package. \"\\n                \"Please install it with `pip install nomic`.\"\\n            )\\n        if api_key is None:\\n            raise ValueError(\"No API key provided. Sign up at atlas.nomic.ai!\")\\n        nomic.login(api_key)\\n        self._embedding_function = embedding_function\\n        modality = \"text\"\\n        if self._embedding_function is not None:\\n            modality = \"embedding\"\\n        # Check if the project exists, create it if not\\n        self.project = AtlasProject(\\n            name=name,\\n            description=description,\\n            modality=modality,\\n            is_public=is_public,\\n            reset_project_if_exists=reset_project_if_exists,\\n            unique_id_field=AtlasDB._ATLAS_DEFAULT_ID_FIELD,\\n        )\\n        self.project._latest_project_state()\\n[docs]    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        refresh: bool = True,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\\n        Args:\\n            texts (Iterable[str]): Texts to add to the vectorstore.\\n            metadatas (Optional[List[dict]], optional): Optional list of metadatas.\\n            ids (Optional[List[str]]): An optional list of ids.\\n            refresh(bool): Whether or not to refresh indices with the updated data.\\n                Default True.\\n        Returns:\\n            List[str]: List of IDs of the added texts.\\n        \"\"\"\\n        if (\\n            metadatas is not None\\n            and len(metadatas) > 0\\n            and \"text\" in metadatas[0].keys()\\n        ):\\n            raise ValueError(\"Cannot accept key text in metadata!\")\\n        texts = list(texts)\\n        if ids is None:\\n            ids = [str(uuid.uuid1()) for _ in texts]\\n        # Embedding upload case\\n        if self._embedding_function is not None:\\n            _embeddings = self._embedding_function.embed_documents(texts)\\n            embeddings = np.stack(_embeddings)\\n            if metadatas is None:\\n                data = [\\n                    {AtlasDB._ATLAS_DEFAULT_ID_FIELD: ids[i], \"text\": texts[i]}\\n                    for i, _ in enumerate(texts)\\n                ]\\n            else:\\n                for i in range(len(metadatas)):\\n                    metadatas[i][AtlasDB._ATLAS_DEFAULT_ID_FIELD] = ids[i]\\n                    metadatas[i][\"text\"] = texts[i]\\n                data = metadatas\\n            self.project._validate_map_data_inputs(\\n                [], id_field=AtlasDB._ATLAS_DEFAULT_ID_FIELD, data=data\\n            )\\n            with self.project.wait_for_project_lock():\\n                self.project.add_embeddings(embeddings=embeddings, data=data)\\n        # Text upload case\\n        else:\\n            if metadatas is None:\\n                data = [\\n                    {\"text\": text, AtlasDB._ATLAS_DEFAULT_ID_FIELD: ids[i]}\\n                    for i, text in enumerate(texts)\\n                ]\\n            else:\\n                for i, text in enumerate(texts):\\n                    metadatas[i][\"text\"] = texts\\n                    metadatas[i][AtlasDB._ATLAS_DEFAULT_ID_FIELD] = ids[i]\\n                data = metadatas\\n            self.project._validate_map_data_inputs(\\n                [], id_field=AtlasDB._ATLAS_DEFAULT_ID_FIELD, data=data\\n            )\\n            with self.project.wait_for_project_lock():\\n                self.project.add_text(data)\\n        if refresh:\\n            if len(self.project.indices) > 0:\\n                with self.project.wait_for_project_lock():\\n                    self.project.rebuild_maps()\\n        return ids\\n[docs]    def create_index(self, **kwargs: Any) -> Any:\\n        \"\"\"Creates an index in your project.\\n        See\\n        https://docs.nomic.ai/atlas_api.html#nomic.project.AtlasProject.create_index\\n        for full detail.\\n        \"\"\"\\n        with self.project.wait_for_project_lock():\\n            return self.project.create_index(**kwargs)\\n[docs]    def similarity_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        \"\"\"Run similarity search with AtlasDB\\n        Args:\\n            query (str): Query text to search for.\\n            k (int): Number of results to return. Defaults to 4.\\n        Returns:\\n            List[Document]: List of documents most similar to the query text.\\n        \"\"\"\\n        if self._embedding_function is None:\\n            raise NotImplementedError(\\n                \"AtlasDB requires an embedding_function for text similarity search!\"\\n            )\\n        _embedding = self._embedding_function.embed_documents([query])[0]\\n        embedding = np.array(_embedding).reshape(1, -1)\\n        with self.project.wait_for_project_lock():\\n            neighbors, _ = self.project.projections[0].vector_search(\\n                queries=embedding, k=k\\n            )\\n            datas = self.project.get_data(ids=neighbors[0])\\n        docs = [\\n            Document(page_content=datas[i][\"text\"], metadata=datas[i])\\n            for i, neighbor in enumerate(neighbors)\\n        ]\\n        return docs\\n[docs]    @classmethod\\n    def from_texts(\\n        cls: Type[AtlasDB],\\n        texts: List[str],\\n        embedding: Optional[Embeddings] = None,\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        name: Optional[str] = None,\\n        api_key: Optional[str] = None,\\n        description: str = \"A description for your project\",\\n        is_public: bool = True,\\n        reset_project_if_exists: bool = False,\\n        index_kwargs: Optional[dict] = None,\\n        **kwargs: Any,\\n    ) -> AtlasDB:\\n        \"\"\"Create an AtlasDB vectorstore from a raw documents.\\n        Args:\\n            texts (List[str]): The list of texts to ingest.\\n            name (str): Name of the project to create.\\n            api_key (str): Your nomic API key,\\n            embedding (Optional[Embeddings]): Embedding function. Defaults to None.\\n            metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.\\n            ids (Optional[List[str]]): Optional list of document IDs. If None,\\n                ids will be auto created\\n            description (str): A description for your project.\\n            is_public (bool): Whether your project is publicly accessible.\\n                True by default.\\n            reset_project_if_exists (bool): Whether to reset this project if it\\n                already exists. Default False.\\n                Generally userful during development and testing.\\n            index_kwargs (Optional[dict]): Dict of kwargs for index creation.\\n                See https://docs.nomic.ai/atlas_api.html\\n        Returns:\\n            AtlasDB: Nomic\\'s neural database and finest rhizomatic instrument\\n        \"\"\"\\n        if name is None or api_key is None:\\n            raise ValueError(\"`name` and `api_key` cannot be None.\")\\n        # Inject relevant kwargs\\n        all_index_kwargs = {\"name\": name + \"_index\", \"indexed_field\": \"text\"}\\n        if index_kwargs is not None:\\n            for k, v in index_kwargs.items():\\n                all_index_kwargs[k] = v\\n        # Build project\\n        atlasDB = cls(\\n            name,\\n            embedding_function=embedding,\\n            api_key=api_key,\\n            description=\"A description for your project\",\\n            is_public=is_public,\\n            reset_project_if_exists=reset_project_if_exists,\\n        )\\n        with atlasDB.project.wait_for_project_lock():\\n            atlasDB.add_texts(texts=texts, metadatas=metadatas, ids=ids)\\n            atlasDB.create_index(**all_index_kwargs)\\n        return atlasDB\\n[docs]    @classmethod\\n    def from_documents(\\n        cls: Type[AtlasDB],\\n        documents: List[Document],\\n        embedding: Optional[Embeddings] = None,\\n        ids: Optional[List[str]] = None,\\n        name: Optional[str] = None,\\n        api_key: Optional[str] = None,\\n        persist_directory: Optional[str] = None,\\n        description: str = \"A description for your project\",\\n        is_public: bool = True,\\n        reset_project_if_exists: bool = False,\\n        index_kwargs: Optional[dict] = None,\\n        **kwargs: Any,\\n    ) -> AtlasDB:\\n        \"\"\"Create an AtlasDB vectorstore from a list of documents.\\n        Args:\\n            name (str): Name of the collection to create.\\n            api_key (str): Your nomic API key,\\n            documents (List[Document]): List of documents to add to the vectorstore.\\n            embedding (Optional[Embeddings]): Embedding function. Defaults to None.\\n            ids (Optional[List[str]]): Optional list of document IDs. If None,\\n                ids will be auto created\\n            description (str): A description for your project.\\n            is_public (bool): Whether your project is publicly accessible.\\n                True by default.\\n            reset_project_if_exists (bool): Whether to reset this project if\\n                it already exists. Default False.\\n                Generally userful during development and testing.\\n            index_kwargs (Optional[dict]): Dict of kwargs for index creation.\\n                See https://docs.nomic.ai/atlas_api.html\\n        Returns:\\n            AtlasDB: Nomic\\'s neural database and finest rhizomatic instrument\\n        \"\"\"\\n        if name is None or api_key is None:\\n            raise ValueError(\"`name` and `api_key` cannot be None.\")\\n        texts = [doc.page_content for doc in documents]\\n        metadatas = [doc.metadata for doc in documents]\\n        return cls.from_texts(\\n            name=name,\\n            api_key=api_key,\\n            texts=texts,\\n            embedding=embedding,\\n            metadatas=metadatas,\\n            ids=ids,\\n            description=description,\\n            is_public=is_public,\\n            reset_project_if_exists=reset_project_if_exists,\\n            index_kwargs=index_kwargs,\\n        )\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\vectorstores\\\\atlas.html'}),\n",
              " Document(page_content='Source code for langchain.vectorstores.base\\n\"\"\"Interface for vector stores.\"\"\"\\nfrom __future__ import annotations\\nimport asyncio\\nfrom abc import ABC, abstractmethod\\nfrom functools import partial\\nfrom typing import Any, Dict, Iterable, List, Optional, Tuple, Type, TypeVar\\nfrom pydantic import BaseModel, Field, root_validator\\nfrom langchain.docstore.document import Document\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.schema import BaseRetriever\\nVST = TypeVar(\"VST\", bound=\"VectorStore\")\\n[docs]class VectorStore(ABC):\\n    \"\"\"Interface for vector stores.\"\"\"\\n[docs]    @abstractmethod\\n    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\\n        Args:\\n            texts: Iterable of strings to add to the vectorstore.\\n            metadatas: Optional list of metadatas associated with the texts.\\n            kwargs: vectorstore specific parameters\\n        Returns:\\n            List of ids from adding the texts into the vectorstore.\\n        \"\"\"\\n[docs]    async def aadd_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\"\"\"\\n        raise NotImplementedError\\n[docs]    def add_documents(self, documents: List[Document], **kwargs: Any) -> List[str]:\\n        \"\"\"Run more documents through the embeddings and add to the vectorstore.\\n        Args:\\n            documents (List[Document]: Documents to add to the vectorstore.\\n        Returns:\\n            List[str]: List of IDs of the added texts.\\n        \"\"\"\\n        # TODO: Handle the case where the user doesn\\'t provide ids on the Collection\\n        texts = [doc.page_content for doc in documents]\\n        metadatas = [doc.metadata for doc in documents]\\n        return self.add_texts(texts, metadatas, **kwargs)\\n[docs]    async def aadd_documents(\\n        self, documents: List[Document], **kwargs: Any\\n    ) -> List[str]:\\n        \"\"\"Run more documents through the embeddings and add to the vectorstore.\\n        Args:\\n            documents (List[Document]: Documents to add to the vectorstore.\\n        Returns:\\n            List[str]: List of IDs of the added texts.\\n        \"\"\"\\n        texts = [doc.page_content for doc in documents]\\n        metadatas = [doc.metadata for doc in documents]\\n        return await self.aadd_texts(texts, metadatas, **kwargs)\\n[docs]    @abstractmethod\\n    def similarity_search(\\n        self, query: str, k: int = 4, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs most similar to query.\"\"\"\\n[docs]    def similarity_search_with_relevance_scores(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        **kwargs: Any,\\n    ) -> List[Tuple[Document, float]]:\\n        \"\"\"Return docs and relevance scores in the range [0, 1].\\n        0 is dissimilar, 1 is most similar.\\n        \"\"\"\\n        docs_and_similarities = self._similarity_search_with_relevance_scores(\\n            query, k=k, **kwargs\\n        )\\n        if any(\\n            similarity < 0.0 or similarity > 1.0\\n            for _, similarity in docs_and_similarities\\n        ):\\n            raise ValueError(\\n                \"Relevance scores must be between\"\\n                f\" 0 and 1, got {docs_and_similarities}\"\\n            )\\n        return docs_and_similarities\\n    def _similarity_search_with_relevance_scores(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        **kwargs: Any,\\n    ) -> List[Tuple[Document, float]]:\\n        \"\"\"Return docs and relevance scores, normalized on a scale from 0 to 1.\\n        0 is dissimilar, 1 is most similar.\\n        \"\"\"\\n        raise NotImplementedError\\n[docs]    async def asimilarity_search(\\n        self, query: str, k: int = 4, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs most similar to query.\"\"\"\\n        # This is a temporary workaround to make the similarity search\\n        # asynchronous. The proper solution is to make the similarity search\\n        # asynchronous in the vector store implementations.\\n        func = partial(self.similarity_search, query, k, **kwargs)\\n        return await asyncio.get_event_loop().run_in_executor(None, func)\\n[docs]    def similarity_search_by_vector(\\n        self, embedding: List[float], k: int = 4, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs most similar to embedding vector.\\n        Args:\\n            embedding: Embedding to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n        Returns:\\n            List of Documents most similar to the query vector.\\n        \"\"\"\\n        raise NotImplementedError\\n[docs]    async def asimilarity_search_by_vector(\\n        self, embedding: List[float], k: int = 4, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs most similar to embedding vector.\"\"\"\\n        # This is a temporary workaround to make the similarity search\\n        # asynchronous. The proper solution is to make the similarity search\\n        # asynchronous in the vector store implementations.\\n        func = partial(self.similarity_search_by_vector, embedding, k, **kwargs)\\n        return await asyncio.get_event_loop().run_in_executor(None, func)\\n[docs]    def max_marginal_relevance_search(\\n        self, query: str, k: int = 4, fetch_k: int = 20, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs selected using the maximal marginal relevance.\\n        Maximal marginal relevance optimizes for similarity to query AND diversity\\n        among selected documents.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n        Returns:\\n            List of Documents selected by maximal marginal relevance.\\n        \"\"\"\\n        raise NotImplementedError\\n[docs]    async def amax_marginal_relevance_search(\\n        self, query: str, k: int = 4, fetch_k: int = 20, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs selected using the maximal marginal relevance.\"\"\"\\n        # This is a temporary workaround to make the similarity search\\n        # asynchronous. The proper solution is to make the similarity search\\n        # asynchronous in the vector store implementations.\\n        func = partial(self.max_marginal_relevance_search, query, k, fetch_k, **kwargs)\\n        return await asyncio.get_event_loop().run_in_executor(None, func)\\n[docs]    def max_marginal_relevance_search_by_vector(\\n        self, embedding: List[float], k: int = 4, fetch_k: int = 20, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs selected using the maximal marginal relevance.\\n        Maximal marginal relevance optimizes for similarity to query AND diversity\\n        among selected documents.\\n        Args:\\n            embedding: Embedding to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n        Returns:\\n            List of Documents selected by maximal marginal relevance.\\n        \"\"\"\\n        raise NotImplementedError\\n[docs]    async def amax_marginal_relevance_search_by_vector(\\n        self, embedding: List[float], k: int = 4, fetch_k: int = 20, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs selected using the maximal marginal relevance.\"\"\"\\n        raise NotImplementedError\\n[docs]    @classmethod\\n    def from_documents(\\n        cls: Type[VST],\\n        documents: List[Document],\\n        embedding: Embeddings,\\n        **kwargs: Any,\\n    ) -> VST:\\n        \"\"\"Return VectorStore initialized from documents and embeddings.\"\"\"\\n        texts = [d.page_content for d in documents]\\n        metadatas = [d.metadata for d in documents]\\n        return cls.from_texts(texts, embedding, metadatas=metadatas, **kwargs)\\n[docs]    @classmethod\\n    async def afrom_documents(\\n        cls: Type[VST],\\n        documents: List[Document],\\n        embedding: Embeddings,\\n        **kwargs: Any,\\n    ) -> VST:\\n        \"\"\"Return VectorStore initialized from documents and embeddings.\"\"\"\\n        texts = [d.page_content for d in documents]\\n        metadatas = [d.metadata for d in documents]\\n        return await cls.afrom_texts(texts, embedding, metadatas=metadatas, **kwargs)\\n[docs]    @classmethod\\n    @abstractmethod\\n    def from_texts(\\n        cls: Type[VST],\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        **kwargs: Any,\\n    ) -> VST:\\n        \"\"\"Return VectorStore initialized from texts and embeddings.\"\"\"\\n[docs]    @classmethod\\n    async def afrom_texts(\\n        cls: Type[VST],\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        **kwargs: Any,\\n    ) -> VST:\\n        \"\"\"Return VectorStore initialized from texts and embeddings.\"\"\"\\n        raise NotImplementedError\\n[docs]    def as_retriever(self, **kwargs: Any) -> BaseRetriever:\\n        return VectorStoreRetriever(vectorstore=self, **kwargs)\\nclass VectorStoreRetriever(BaseRetriever, BaseModel):\\n    vectorstore: VectorStore\\n    search_type: str = \"similarity\"\\n    search_kwargs: dict = Field(default_factory=dict)\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n        arbitrary_types_allowed = True\\n    @root_validator()\\n    def validate_search_type(cls, values: Dict) -> Dict:\\n        \"\"\"Validate search type.\"\"\"\\n        if \"search_type\" in values:\\n            search_type = values[\"search_type\"]\\n            if search_type not in (\"similarity\", \"mmr\"):\\n                raise ValueError(f\"search_type of {search_type} not allowed.\")\\n        return values\\n    def get_relevant_documents(self, query: str) -> List[Document]:\\n        if self.search_type == \"similarity\":\\n            docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\\n        elif self.search_type == \"mmr\":\\n            docs = self.vectorstore.max_marginal_relevance_search(\\n                query, **self.search_kwargs\\n            )\\n        else:\\n            raise ValueError(f\"search_type of {self.search_type} not allowed.\")\\n        return docs\\n    async def aget_relevant_documents(self, query: str) -> List[Document]:\\n        if self.search_type == \"similarity\":\\n            docs = await self.vectorstore.asimilarity_search(\\n                query, **self.search_kwargs\\n            )\\n        elif self.search_type == \"mmr\":\\n            docs = await self.vectorstore.amax_marginal_relevance_search(\\n                query, **self.search_kwargs\\n            )\\n        else:\\n            raise ValueError(f\"search_type of {self.search_type} not allowed.\")\\n        return docs\\n    def add_documents(self, documents: List[Document], **kwargs: Any) -> List[str]:\\n        \"\"\"Add documents to vectorstore.\"\"\"\\n        return self.vectorstore.add_documents(documents, **kwargs)\\n    async def aadd_documents(\\n        self, documents: List[Document], **kwargs: Any\\n    ) -> List[str]:\\n        \"\"\"Add documents to vectorstore.\"\"\"\\n        return await self.vectorstore.aadd_documents(documents, **kwargs)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\vectorstores\\\\base.html'}),\n",
              " Document(page_content='Source code for langchain.vectorstores.chroma\\n\"\"\"Wrapper around ChromaDB embeddings platform.\"\"\"\\nfrom __future__ import annotations\\nimport logging\\nimport uuid\\nfrom typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Tuple, Type\\nimport numpy as np\\nfrom langchain.docstore.document import Document\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.vectorstores.base import VectorStore\\nfrom langchain.vectorstores.utils import maximal_marginal_relevance\\nif TYPE_CHECKING:\\n    import chromadb\\n    import chromadb.config\\nlogger = logging.getLogger(__name__)\\ndef _results_to_docs(results: Any) -> List[Document]:\\n    return [doc for doc, _ in _results_to_docs_and_scores(results)]\\ndef _results_to_docs_and_scores(results: Any) -> List[Tuple[Document, float]]:\\n    return [\\n        # TODO: Chroma can do batch querying,\\n        # we shouldn\\'t hard code to the 1st result\\n        (Document(page_content=result[0], metadata=result[1] or {}), result[2])\\n        for result in zip(\\n            results[\"documents\"][0],\\n            results[\"metadatas\"][0],\\n            results[\"distances\"][0],\\n        )\\n    ]\\n[docs]class Chroma(VectorStore):\\n    \"\"\"Wrapper around ChromaDB embeddings platform.\\n    To use, you should have the ``chromadb`` python package installed.\\n    Example:\\n        .. code-block:: python\\n                from langchain.vectorstores import Chroma\\n                from langchain.embeddings.openai import OpenAIEmbeddings\\n                embeddings = OpenAIEmbeddings()\\n                vectorstore = Chroma(\"langchain_store\", embeddings.embed_query)\\n    \"\"\"\\n    _LANGCHAIN_DEFAULT_COLLECTION_NAME = \"langchain\"\\n    def __init__(\\n        self,\\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\\n        embedding_function: Optional[Embeddings] = None,\\n        persist_directory: Optional[str] = None,\\n        client_settings: Optional[chromadb.config.Settings] = None,\\n        collection_metadata: Optional[Dict] = None,\\n        client: Optional[chromadb.Client] = None,\\n    ) -> None:\\n        \"\"\"Initialize with Chroma client.\"\"\"\\n        try:\\n            import chromadb\\n            import chromadb.config\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import chromadb python package. \"\\n                \"Please install it with `pip install chromadb`.\"\\n            )\\n        if client is not None:\\n            self._client = client\\n        else:\\n            if client_settings:\\n                self._client_settings = client_settings\\n            else:\\n                self._client_settings = chromadb.config.Settings()\\n                if persist_directory is not None:\\n                    self._client_settings = chromadb.config.Settings(\\n                        chroma_db_impl=\"duckdb+parquet\",\\n                        persist_directory=persist_directory,\\n                    )\\n            self._client = chromadb.Client(self._client_settings)\\n        self._embedding_function = embedding_function\\n        self._persist_directory = persist_directory\\n        self._collection = self._client.get_or_create_collection(\\n            name=collection_name,\\n            embedding_function=self._embedding_function.embed_documents\\n            if self._embedding_function is not None\\n            else None,\\n            metadata=collection_metadata,\\n        )\\n[docs]    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\\n        Args:\\n            texts (Iterable[str]): Texts to add to the vectorstore.\\n            metadatas (Optional[List[dict]], optional): Optional list of metadatas.\\n            ids (Optional[List[str]], optional): Optional list of IDs.\\n        Returns:\\n            List[str]: List of IDs of the added texts.\\n        \"\"\"\\n        # TODO: Handle the case where the user doesn\\'t provide ids on the Collection\\n        if ids is None:\\n            ids = [str(uuid.uuid1()) for _ in texts]\\n        embeddings = None\\n        if self._embedding_function is not None:\\n            embeddings = self._embedding_function.embed_documents(list(texts))\\n        self._collection.add(\\n            metadatas=metadatas, embeddings=embeddings, documents=texts, ids=ids\\n        )\\n        return ids\\n[docs]    def similarity_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        filter: Optional[Dict[str, str]] = None,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        \"\"\"Run similarity search with Chroma.\\n        Args:\\n            query (str): Query text to search for.\\n            k (int): Number of results to return. Defaults to 4.\\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\\n        Returns:\\n            List[Document]: List of documents most similar to the query text.\\n        \"\"\"\\n        docs_and_scores = self.similarity_search_with_score(query, k, filter=filter)\\n        return [doc for doc, _ in docs_and_scores]\\n[docs]    def similarity_search_by_vector(\\n        self,\\n        embedding: List[float],\\n        k: int = 4,\\n        filter: Optional[Dict[str, str]] = None,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        \"\"\"Return docs most similar to embedding vector.\\n        Args:\\n            embedding: Embedding to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n        Returns:\\n            List of Documents most similar to the query vector.\\n        \"\"\"\\n        results = self._collection.query(\\n            query_embeddings=embedding, n_results=k, where=filter\\n        )\\n        return _results_to_docs(results)\\n[docs]    def similarity_search_with_score(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        filter: Optional[Dict[str, str]] = None,\\n        **kwargs: Any,\\n    ) -> List[Tuple[Document, float]]:\\n        \"\"\"Run similarity search with Chroma with distance.\\n        Args:\\n            query (str): Query text to search for.\\n            k (int): Number of results to return. Defaults to 4.\\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\\n        Returns:\\n            List[Tuple[Document, float]]: List of documents most similar to the query\\n                text with distance in float.\\n        \"\"\"\\n        if self._embedding_function is None:\\n            results = self._collection.query(\\n                query_texts=[query], n_results=k, where=filter\\n            )\\n        else:\\n            query_embedding = self._embedding_function.embed_query(query)\\n            results = self._collection.query(\\n                query_embeddings=[query_embedding], n_results=k, where=filter\\n            )\\n        return _results_to_docs_and_scores(results)\\n[docs]    def max_marginal_relevance_search_by_vector(\\n        self,\\n        embedding: List[float],\\n        k: int = 4,\\n        fetch_k: int = 20,\\n        filter: Optional[Dict[str, str]] = None,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        \"\"\"Return docs selected using the maximal marginal relevance.\\n        Maximal marginal relevance optimizes for similarity to query AND diversity\\n        among selected documents.\\n        Args:\\n            embedding: Embedding to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\\n        Returns:\\n            List of Documents selected by maximal marginal relevance.\\n        \"\"\"\\n        results = self._collection.query(\\n            query_embeddings=embedding,\\n            n_results=fetch_k,\\n            where=filter,\\n            include=[\"metadatas\", \"documents\", \"distances\", \"embeddings\"],\\n        )\\n        mmr_selected = maximal_marginal_relevance(\\n            np.array(embedding, dtype=np.float32), results[\"embeddings\"][0], k=k\\n        )\\n        candidates = _results_to_docs(results)\\n        selected_results = [r for i, r in enumerate(candidates) if i in mmr_selected]\\n        return selected_results\\n[docs]    def max_marginal_relevance_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        fetch_k: int = 20,\\n        filter: Optional[Dict[str, str]] = None,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        \"\"\"Return docs selected using the maximal marginal relevance.\\n        Maximal marginal relevance optimizes for similarity to query AND diversity\\n        among selected documents.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\\n        Returns:\\n            List of Documents selected by maximal marginal relevance.\\n        \"\"\"\\n        if self._embedding_function is None:\\n            raise ValueError(\\n                \"For MMR search, you must specify an embedding function on\" \"creation.\"\\n            )\\n        embedding = self._embedding_function.embed_query(query)\\n        docs = self.max_marginal_relevance_search_by_vector(\\n            embedding, k, fetch_k, filter\\n        )\\n        return docs\\n[docs]    def delete_collection(self) -> None:\\n        \"\"\"Delete the collection.\"\"\"\\n        self._client.delete_collection(self._collection.name)\\n[docs]    def persist(self) -> None:\\n        \"\"\"Persist the collection.\\n        This can be used to explicitly persist the data to disk.\\n        It will also be called automatically when the object is destroyed.\\n        \"\"\"\\n        if self._persist_directory is None:\\n            raise ValueError(\\n                \"You must specify a persist_directory on\"\\n                \"creation to persist the collection.\"\\n            )\\n        self._client.persist()\\n[docs]    @classmethod\\n    def from_texts(\\n        cls: Type[Chroma],\\n        texts: List[str],\\n        embedding: Optional[Embeddings] = None,\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\\n        persist_directory: Optional[str] = None,\\n        client_settings: Optional[chromadb.config.Settings] = None,\\n        client: Optional[chromadb.Client] = None,\\n        **kwargs: Any,\\n    ) -> Chroma:\\n        \"\"\"Create a Chroma vectorstore from a raw documents.\\n        If a persist_directory is specified, the collection will be persisted there.\\n        Otherwise, the data will be ephemeral in-memory.\\n        Args:\\n            texts (List[str]): List of texts to add to the collection.\\n            collection_name (str): Name of the collection to create.\\n            persist_directory (Optional[str]): Directory to persist the collection.\\n            embedding (Optional[Embeddings]): Embedding function. Defaults to None.\\n            metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.\\n            ids (Optional[List[str]]): List of document IDs. Defaults to None.\\n            client_settings (Optional[chromadb.config.Settings]): Chroma client settings\\n        Returns:\\n            Chroma: Chroma vectorstore.\\n        \"\"\"\\n        chroma_collection = cls(\\n            collection_name=collection_name,\\n            embedding_function=embedding,\\n            persist_directory=persist_directory,\\n            client_settings=client_settings,\\n            client=client,\\n        )\\n        chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\\n        return chroma_collection\\n[docs]    @classmethod\\n    def from_documents(\\n        cls: Type[Chroma],\\n        documents: List[Document],\\n        embedding: Optional[Embeddings] = None,\\n        ids: Optional[List[str]] = None,\\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\\n        persist_directory: Optional[str] = None,\\n        client_settings: Optional[chromadb.config.Settings] = None,\\n        client: Optional[chromadb.Client] = None,  # Add this line\\n        **kwargs: Any,\\n    ) -> Chroma:\\n        \"\"\"Create a Chroma vectorstore from a list of documents.\\n        If a persist_directory is specified, the collection will be persisted there.\\n        Otherwise, the data will be ephemeral in-memory.\\n        Args:\\n            collection_name (str): Name of the collection to create.\\n            persist_directory (Optional[str]): Directory to persist the collection.\\n            ids (Optional[List[str]]): List of document IDs. Defaults to None.\\n            documents (List[Document]): List of documents to add to the vectorstore.\\n            embedding (Optional[Embeddings]): Embedding function. Defaults to None.\\n            client_settings (Optional[chromadb.config.Settings]): Chroma client settings\\n        Returns:\\n            Chroma: Chroma vectorstore.\\n        \"\"\"\\n        texts = [doc.page_content for doc in documents]\\n        metadatas = [doc.metadata for doc in documents]\\n        return cls.from_texts(\\n            texts=texts,\\n            embedding=embedding,\\n            metadatas=metadatas,\\n            ids=ids,\\n            collection_name=collection_name,\\n            persist_directory=persist_directory,\\n            client_settings=client_settings,\\n            client=client,\\n        )\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\vectorstores\\\\chroma.html'}),\n",
              " Document(page_content='Source code for langchain.vectorstores.deeplake\\n\"\"\"Wrapper around Activeloop Deep Lake.\"\"\"\\nfrom __future__ import annotations\\nimport logging\\nimport uuid\\nfrom functools import partial\\nfrom typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple\\nimport numpy as np\\nfrom langchain.docstore.document import Document\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.vectorstores.base import VectorStore\\nfrom langchain.vectorstores.utils import maximal_marginal_relevance\\nlogger = logging.getLogger(__name__)\\ndistance_metric_map = {\\n    \"l2\": lambda a, b: np.linalg.norm(a - b, axis=1, ord=2),\\n    \"l1\": lambda a, b: np.linalg.norm(a - b, axis=1, ord=1),\\n    \"max\": lambda a, b: np.linalg.norm(a - b, axis=1, ord=np.inf),\\n    \"cos\": lambda a, b: np.dot(a, b.T)\\n    / (np.linalg.norm(a) * np.linalg.norm(b, axis=1)),\\n    \"dot\": lambda a, b: np.dot(a, b.T),\\n}\\ndef vector_search(\\n    query_embedding: np.ndarray,\\n    data_vectors: np.ndarray,\\n    distance_metric: str = \"L2\",\\n    k: Optional[int] = 4,\\n) -> Tuple[List, List]:\\n    \"\"\"Naive search for nearest neighbors\\n    args:\\n        query_embedding: np.ndarray\\n        data_vectors: np.ndarray\\n        k (int): number of nearest neighbors\\n        distance_metric: distance function \\'L2\\' for Euclidean, \\'L1\\' for Nuclear, \\'Max\\'\\n            l-infinity distnace, \\'cos\\' for cosine similarity, \\'dot\\' for dot product\\n    returns:\\n        nearest_indices: List, indices of nearest neighbors\\n    \"\"\"\\n    # Calculate the distance between the query_vector and all data_vectors\\n    distances = distance_metric_map[distance_metric](query_embedding, data_vectors)\\n    nearest_indices = np.argsort(distances)\\n    nearest_indices = (\\n        nearest_indices[::-1][:k] if distance_metric in [\"cos\"] else nearest_indices[:k]\\n    )\\n    return nearest_indices.tolist(), distances[nearest_indices].tolist()\\ndef dp_filter(x: dict, filter: Dict[str, str]) -> bool:\\n    \"\"\"Filter helper function for Deep Lake\"\"\"\\n    metadata = x[\"metadata\"].data()[\"value\"]\\n    return all(k in metadata and v == metadata[k] for k, v in filter.items())\\n[docs]class DeepLake(VectorStore):\\n    \"\"\"Wrapper around Deep Lake, a data lake for deep learning applications.\\n    We implement naive similarity search and filtering for fast prototyping,\\n    but it can be extended with Tensor Query Language (TQL) for production use cases\\n    over billion rows.\\n    Why Deep Lake?\\n    - Not only stores embeddings, but also the original data with version control.\\n    - Serverless, doesn\\'t require another service and can be used with major\\n        cloud providers (S3, GCS, etc.)\\n    - More than just a multi-modal vector store. You can use the dataset\\n        to fine-tune your own LLM models.\\n    To use, you should have the ``deeplake`` python package installed.\\n    Example:\\n        .. code-block:: python\\n                from langchain.vectorstores import DeepLake\\n                from langchain.embeddings.openai import OpenAIEmbeddings\\n                embeddings = OpenAIEmbeddings()\\n                vectorstore = DeepLake(\"langchain_store\", embeddings.embed_query)\\n    \"\"\"\\n    _LANGCHAIN_DEFAULT_DEEPLAKE_PATH = \"mem://langchain\"\\n    def __init__(\\n        self,\\n        dataset_path: str = _LANGCHAIN_DEFAULT_DEEPLAKE_PATH,\\n        token: Optional[str] = None,\\n        embedding_function: Optional[Embeddings] = None,\\n        read_only: Optional[bool] = False,\\n        ingestion_batch_size: int = 1024,\\n        num_workers: int = 4,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize with Deep Lake client.\"\"\"\\n        self.ingestion_batch_size = ingestion_batch_size\\n        self.num_workers = num_workers\\n        try:\\n            import deeplake\\n            from deeplake.constants import MB\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import deeplake python package. \"\\n                \"Please install it with `pip install deeplake`.\"\\n            )\\n        self._deeplake = deeplake\\n        if deeplake.exists(dataset_path, token=token):\\n            self.ds = deeplake.load(\\n                dataset_path, token=token, read_only=read_only, **kwargs\\n            )\\n            logger.warning(\\n                f\"Deep Lake Dataset in {dataset_path} already exists, \"\\n                f\"loading from the storage\"\\n            )\\n            self.ds.summary()\\n        else:\\n            self.ds = deeplake.empty(\\n                dataset_path, token=token, overwrite=True, **kwargs\\n            )\\n            with self.ds:\\n                self.ds.create_tensor(\\n                    \"text\",\\n                    htype=\"text\",\\n                    create_id_tensor=False,\\n                    create_sample_info_tensor=False,\\n                    create_shape_tensor=False,\\n                    chunk_compression=\"lz4\",\\n                )\\n                self.ds.create_tensor(\\n                    \"metadata\",\\n                    htype=\"json\",\\n                    create_id_tensor=False,\\n                    create_sample_info_tensor=False,\\n                    create_shape_tensor=False,\\n                    chunk_compression=\"lz4\",\\n                )\\n                self.ds.create_tensor(\\n                    \"embedding\",\\n                    htype=\"generic\",\\n                    dtype=np.float32,\\n                    create_id_tensor=False,\\n                    create_sample_info_tensor=False,\\n                    max_chunk_size=64 * MB,\\n                    create_shape_tensor=True,\\n                )\\n                self.ds.create_tensor(\\n                    \"ids\",\\n                    htype=\"text\",\\n                    create_id_tensor=False,\\n                    create_sample_info_tensor=False,\\n                    create_shape_tensor=False,\\n                    chunk_compression=\"lz4\",\\n                )\\n        self._embedding_function = embedding_function\\n[docs]    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\\n        Args:\\n            texts (Iterable[str]): Texts to add to the vectorstore.\\n            metadatas (Optional[List[dict]], optional): Optional list of metadatas.\\n            ids (Optional[List[str]], optional): Optional list of IDs.\\n        Returns:\\n            List[str]: List of IDs of the added texts.\\n        \"\"\"\\n        if ids is None:\\n            ids = [str(uuid.uuid1()) for _ in texts]\\n        text_list = list(texts)\\n        if metadatas is None:\\n            metadatas = [{}] * len(text_list)\\n        elements = list(zip(text_list, metadatas, ids))\\n        @self._deeplake.compute\\n        def ingest(sample_in: list, sample_out: list) -> None:\\n            text_list = [s[0] for s in sample_in]\\n            embeds: Sequence[Optional[np.ndarray]] = []\\n            if self._embedding_function is not None:\\n                embeddings = self._embedding_function.embed_documents(text_list)\\n                embeds = [np.array(e, dtype=np.float32) for e in embeddings]\\n            else:\\n                embeds = [None] * len(text_list)\\n            for s, e in zip(sample_in, embeds):\\n                sample_out.append(\\n                    {\\n                        \"text\": s[0],\\n                        \"metadata\": s[1],\\n                        \"ids\": s[2],\\n                        \"embedding\": e,\\n                    }\\n                )\\n        batch_size = min(self.ingestion_batch_size, len(elements))\\n        batched = [\\n            elements[i : i + batch_size] for i in range(0, len(elements), batch_size)\\n        ]\\n        ingest().eval(\\n            batched,\\n            self.ds,\\n            num_workers=min(self.num_workers, len(batched) // self.num_workers),\\n        )\\n        self.ds.commit(allow_empty=True)\\n        self.ds.summary()\\n        return ids\\n[docs]    def search(\\n        self,\\n        query: Any[str, None] = None,\\n        embedding: Any[float, None] = None,\\n        k: int = 4,\\n        distance_metric: str = \"L2\",\\n        use_maximal_marginal_relevance: Optional[bool] = False,\\n        fetch_k: Optional[int] = 20,\\n        filter: Optional[Any[Dict[str, str], Callable, str]] = None,\\n        return_score: Optional[bool] = False,\\n        **kwargs: Any,\\n    ) -> Any[List[Document], List[Tuple[Document, float]]]:\\n        \"\"\"Return docs most similar to query.\\n        Args:\\n            query: Text to look up documents similar to.\\n            embedding: Embedding function to use. Defaults to None.\\n            k: Number of Documents to return. Defaults to 4.\\n            distance_metric: `L2` for Euclidean, `L1` for Nuclear,\\n                `max` L-infinity distance, `cos` for cosine similarity,\\n                \\'dot\\' for dot product. Defaults to `L2`.\\n            filter: Attribute filter by metadata example {\\'key\\': \\'value\\'}. It can also\\n            take [Deep Lake filter]\\n            (https://docs.deeplake.ai/en/latest/deeplake.core.dataset.html#deeplake.core.dataset.Dataset.filter)\\n                Defaults to None.\\n            maximal_marginal_relevance: Whether to use maximal marginal relevance.\\n                Defaults to False.\\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n                Defaults to 20.\\n            return_score: Whether to return the score. Defaults to False.\\n        Returns:\\n            List of Documents selected by the specified distance metric,\\n            if return_score True, return a tuple of (Document, score)\\n        \"\"\"\\n        view = self.ds\\n        # attribute based filtering\\n        if filter is not None:\\n            if isinstance(filter, dict):\\n                filter = partial(dp_filter, filter=filter)\\n            view = view.filter(filter)\\n            if len(view) == 0:\\n                return []\\n        if self._embedding_function is None:\\n            view = view.filter(lambda x: query in x[\"text\"].data()[\"value\"])\\n            scores = [1.0] * len(view)\\n            if use_maximal_marginal_relevance:\\n                raise ValueError(\\n                    \"For MMR search, you must specify an embedding function on\"\\n                    \"creation.\"\\n                )\\n        else:\\n            emb = embedding or self._embedding_function.embed_query(\\n                query\\n            )  # type: ignore\\n            query_emb = np.array(emb, dtype=np.float32)\\n            embeddings = view.embedding.numpy(fetch_chunks=True)\\n            k_search = fetch_k if use_maximal_marginal_relevance else k\\n            indices, scores = vector_search(\\n                query_emb,\\n                embeddings,\\n                k=k_search,\\n                distance_metric=distance_metric.lower(),\\n            )\\n            view = view[indices]\\n            if use_maximal_marginal_relevance:\\n                indices = maximal_marginal_relevance(\\n                    query_emb, embeddings[indices], k=min(k, len(indices))\\n                )\\n                view = view[indices]\\n                scores = [scores[i] for i in indices]\\n        docs = [\\n            Document(\\n                page_content=el[\"text\"].data()[\"value\"],\\n                metadata=el[\"metadata\"].data()[\"value\"],\\n            )\\n            for el in view\\n        ]\\n        if return_score:\\n            return [(doc, score) for doc, score in zip(docs, scores)]\\n        return docs\\n[docs]    def similarity_search(\\n        self, query: str, k: int = 4, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs most similar to query.\\n        Args:\\n            query: text to embed and run the query on.\\n            k: Number of Documents to return.\\n                Defaults to 4.\\n            query: Text to look up documents similar to.\\n            embedding: Embedding function to use.\\n                Defaults to None.\\n            k: Number of Documents to return.\\n                Defaults to 4.\\n            distance_metric: `L2` for Euclidean, `L1` for Nuclear, `max`\\n                L-infinity distance, `cos` for cosine similarity, \\'dot\\' for dot product\\n                Defaults to `L2`.\\n            filter: Attribute filter by metadata example {\\'key\\': \\'value\\'}.\\n                Defaults to None.\\n            maximal_marginal_relevance: Whether to use maximal marginal relevance.\\n                Defaults to False.\\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n                Defaults to 20.\\n            return_score: Whether to return the score. Defaults to False.\\n        Returns:\\n            List of Documents most similar to the query vector.\\n        \"\"\"\\n        return self.search(query=query, k=k, **kwargs)\\n[docs]    def similarity_search_by_vector(\\n        self, embedding: List[float], k: int = 4, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs most similar to embedding vector.\\n        Args:\\n            embedding: Embedding to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n        Returns:\\n            List of Documents most similar to the query vector.\\n        \"\"\"\\n        return self.search(embedding=embedding, k=k, **kwargs)\\n[docs]    def similarity_search_with_score(\\n        self,\\n        query: str,\\n        distance_metric: str = \"L2\",\\n        k: int = 4,\\n        filter: Optional[Dict[str, str]] = None,\\n    ) -> List[Tuple[Document, float]]:\\n        \"\"\"Run similarity search with Deep Lake with distance returned.\\n        Args:\\n            query (str): Query text to search for.\\n            distance_metric: `L2` for Euclidean, `L1` for Nuclear, `max` L-infinity\\n                distance, `cos` for cosine similarity, \\'dot\\' for dot product.\\n                Defaults to `L2`.\\n            k (int): Number of results to return. Defaults to 4.\\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\\n        Returns:\\n            List[Tuple[Document, float]]: List of documents most similar to the query\\n                text with distance in float.\\n        \"\"\"\\n        return self.search(\\n            query=query,\\n            k=k,\\n            filter=filter,\\n            return_score=True,\\n            distance_metric=distance_metric,\\n        )\\n[docs]    def max_marginal_relevance_search_by_vector(\\n        self, embedding: List[float], k: int = 4, fetch_k: int = 20, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs selected using the maximal marginal relevance.\\n        Maximal marginal relevance optimizes for similarity to query AND diversity\\n        among selected documents.\\n        Args:\\n            embedding: Embedding to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n        Returns:\\n            List of Documents selected by maximal marginal relevance.\\n        \"\"\"\\n        return self.search(\\n            embedding=embedding,\\n            k=k,\\n            fetch_k=fetch_k,\\n            use_maximal_marginal_relevance=True,\\n        )\\n[docs]    def max_marginal_relevance_search(\\n        self, query: str, k: int = 4, fetch_k: int = 20, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs selected using the maximal marginal relevance.\\n        Maximal marginal relevance optimizes for similarity to query AND diversity\\n        among selected documents.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n        Returns:\\n            List of Documents selected by maximal marginal relevance.\\n        \"\"\"\\n        if self._embedding_function is None:\\n            raise ValueError(\\n                \"For MMR search, you must specify an embedding function on\" \"creation.\"\\n            )\\n        return self.search(\\n            query=query, k=k, fetch_k=fetch_k, use_maximal_marginal_relevance=True\\n        )\\n[docs]    @classmethod\\n    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Optional[Embeddings] = None,\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        dataset_path: str = _LANGCHAIN_DEFAULT_DEEPLAKE_PATH,\\n        **kwargs: Any,\\n    ) -> DeepLake:\\n        \"\"\"Create a Deep Lake dataset from a raw documents.\\n        If a dataset_path is specified, the dataset will be persisted there.\\n        Otherwise, the data will be ephemeral in-memory.\\n        Args:\\n            path (str, pathlib.Path): - The full path to the dataset. Can be:\\n                - Deep Lake cloud path of the form ``hub://username/dataset_name``.\\n                    To write to Deep Lake cloud datasets,\\n                    ensure that you are logged in to Deep Lake\\n                    (use \\'activeloop login\\' from command line)\\n                - AWS S3 path of the form ``s3://bucketname/path/to/dataset``.\\n                    Credentials are required in either the environment\\n                - Google Cloud Storage path of the form\\n                    ``gcs://bucketname/path/to/dataset``Credentials are required\\n                    in either the environment\\n                - Local file system path of the form ``./path/to/dataset`` or\\n                    ``~/path/to/dataset`` or ``path/to/dataset``.\\n                - In-memory path of the form ``mem://path/to/dataset`` which doesn\\'t\\n                    save the dataset, but keeps it in memory instead.\\n                    Should be used only for testing as it does not persist.\\n            documents (List[Document]): List of documents to add.\\n            embedding (Optional[Embeddings]): Embedding function. Defaults to None.\\n            metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.\\n            ids (Optional[List[str]]): List of document IDs. Defaults to None.\\n        Returns:\\n            DeepLake: Deep Lake dataset.\\n        \"\"\"\\n        deeplake_dataset = cls(\\n            dataset_path=dataset_path, embedding_function=embedding, **kwargs\\n        )\\n        deeplake_dataset.add_texts(texts=texts, metadatas=metadatas, ids=ids)\\n        return deeplake_dataset\\n[docs]    def delete(\\n        self,\\n        ids: Any[List[str], None] = None,\\n        filter: Any[Dict[str, str], None] = None,\\n        delete_all: Any[bool, None] = None,\\n    ) -> bool:\\n        \"\"\"Delete the entities in the dataset\\n        Args:\\n            ids (Optional[List[str]], optional): The document_ids to delete.\\n                Defaults to None.\\n            filter (Optional[Dict[str, str]], optional): The filter to delete by.\\n                Defaults to None.\\n            delete_all (Optional[bool], optional): Whether to drop the dataset.\\n                Defaults to None.\\n        \"\"\"\\n        if delete_all:\\n            self.ds.delete()\\n            return True\\n        view = None\\n        if ids:\\n            view = self.ds.filter(lambda x: x[\"ids\"].data()[\"value\"] in ids)\\n            ids = list(view.sample_indices)\\n        if filter:\\n            if view is None:\\n                view = self.ds\\n            view = view.filter(partial(dp_filter, filter=filter))\\n            ids = list(view.sample_indices)\\n        with self.ds:\\n            for id in sorted(ids)[::-1]:\\n                self.ds.pop(id)\\n            self.ds.commit(f\"deleted {len(ids)} samples\", allow_empty=True)\\n        return True\\n[docs]    def delete_dataset(self) -> None:\\n        \"\"\"Delete the collection.\"\"\"\\n        self.delete(delete_all=True)\\n[docs]    def persist(self) -> None:\\n        \"\"\"Persist the collection.\"\"\"\\n        self.ds.flush()\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\vectorstores\\\\deeplake.html'}),\n",
              " Document(page_content='Source code for langchain.vectorstores.elastic_vector_search\\n\"\"\"Wrapper around Elasticsearch vector database.\"\"\"\\nfrom __future__ import annotations\\nimport uuid\\nfrom abc import ABC\\nfrom typing import Any, Dict, Iterable, List, Optional\\nfrom langchain.docstore.document import Document\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.utils import get_from_dict_or_env\\nfrom langchain.vectorstores.base import VectorStore\\ndef _default_text_mapping(dim: int) -> Dict:\\n    return {\\n        \"properties\": {\\n            \"text\": {\"type\": \"text\"},\\n            \"vector\": {\"type\": \"dense_vector\", \"dims\": dim},\\n        }\\n    }\\ndef _default_script_query(query_vector: List[float]) -> Dict:\\n    return {\\n        \"script_score\": {\\n            \"query\": {\"match_all\": {}},\\n            \"script\": {\\n                \"source\": \"cosineSimilarity(params.query_vector, \\'vector\\') + 1.0\",\\n                \"params\": {\"query_vector\": query_vector},\\n            },\\n        }\\n    }\\n# ElasticVectorSearch is a concrete implementation of the abstract base class\\n# VectorStore, which defines a common interface for all vector database\\n# implementations. By inheriting from the ABC class, ElasticVectorSearch can be\\n# defined as an abstract base class itself, allowing the creation of subclasses with\\n# their own specific implementations. If you plan to subclass ElasticVectorSearch,\\n# you can inherit from it and define your own implementation of the necessary methods\\n# and attributes.\\n[docs]class ElasticVectorSearch(VectorStore, ABC):\\n    \"\"\"Wrapper around Elasticsearch as a vector database.\\n    To connect to an Elasticsearch instance that does not require\\n    login credentials, pass the Elasticsearch URL and index name along with the\\n    embedding object to the constructor.\\n    Example:\\n        .. code-block:: python\\n            from langchain import ElasticVectorSearch\\n            from langchain.embeddings import OpenAIEmbeddings\\n            embedding = OpenAIEmbeddings()\\n            elastic_vector_search = ElasticVectorSearch(\\n                elasticsearch_url=\"http://localhost:9200\",\\n                index_name=\"test_index\",\\n                embedding=embedding\\n            )\\n    To connect to an Elasticsearch instance that requires login credentials,\\n    including Elastic Cloud, use the Elasticsearch URL format\\n    https://username:password@es_host:9243. For example, to connect to Elastic\\n    Cloud, create the Elasticsearch URL with the required authentication details and\\n    pass it to the ElasticVectorSearch constructor as the named parameter\\n    elasticsearch_url.\\n    You can obtain your Elastic Cloud URL and login credentials by logging in to the\\n    Elastic Cloud console at https://cloud.elastic.co, selecting your deployment, and\\n    navigating to the \"Deployments\" page.\\n    To obtain your Elastic Cloud password for the default \"elastic\" user:\\n    1. Log in to the Elastic Cloud console at https://cloud.elastic.co\\n    2. Go to \"Security\" > \"Users\"\\n    3. Locate the \"elastic\" user and click \"Edit\"\\n    4. Click \"Reset password\"\\n    5. Follow the prompts to reset the password\\n    The format for Elastic Cloud URLs is\\n    https://username:password@cluster_id.region_id.gcp.cloud.es.io:9243.\\n    Example:\\n        .. code-block:: python\\n            from langchain import ElasticVectorSearch\\n            from langchain.embeddings import OpenAIEmbeddings\\n            embedding = OpenAIEmbeddings()\\n            elastic_host = \"cluster_id.region_id.gcp.cloud.es.io\"\\n            elasticsearch_url = f\"https://username:password@{elastic_host}:9243\"\\n            elastic_vector_search = ElasticVectorSearch(\\n                elasticsearch_url=elasticsearch_url,\\n                index_name=\"test_index\",\\n                embedding=embedding\\n            )\\n    Args:\\n        elasticsearch_url (str): The URL for the Elasticsearch instance.\\n        index_name (str): The name of the Elasticsearch index for the embeddings.\\n        embedding (Embeddings): An object that provides the ability to embed text.\\n                It should be an instance of a class that subclasses the Embeddings\\n                abstract base class, such as OpenAIEmbeddings()\\n    Raises:\\n        ValueError: If the elasticsearch python package is not installed.\\n    \"\"\"\\n    def __init__(self, elasticsearch_url: str, index_name: str, embedding: Embeddings):\\n        \"\"\"Initialize with necessary components.\"\"\"\\n        try:\\n            import elasticsearch\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import elasticsearch python package. \"\\n                \"Please install it with `pip install elasticsearch`.\"\\n            )\\n        self.embedding = embedding\\n        self.index_name = index_name\\n        try:\\n            es_client = elasticsearch.Elasticsearch(elasticsearch_url)  # noqa\\n        except ValueError as e:\\n            raise ValueError(\\n                f\"Your elasticsearch client string is misformatted. Got error: {e} \"\\n            )\\n        self.client = es_client\\n[docs]    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        refresh_indices: bool = True,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\\n        Args:\\n            texts: Iterable of strings to add to the vectorstore.\\n            metadatas: Optional list of metadatas associated with the texts.\\n            refresh_indices: bool to refresh ElasticSearch indices\\n        Returns:\\n            List of ids from adding the texts into the vectorstore.\\n        \"\"\"\\n        try:\\n            from elasticsearch.exceptions import NotFoundError\\n            from elasticsearch.helpers import bulk\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import elasticsearch python package. \"\\n                \"Please install it with `pip install elasticsearch`.\"\\n            )\\n        requests = []\\n        ids = []\\n        embeddings = self.embedding.embed_documents(list(texts))\\n        dim = len(embeddings[0])\\n        mapping = _default_text_mapping(dim)\\n        # check to see if the index already exists\\n        try:\\n            self.client.indices.get(index=self.index_name)\\n        except NotFoundError:\\n            # TODO would be nice to create index before embedding,\\n            # just to save expensive steps for last\\n            self.client.indices.create(index=self.index_name, mappings=mapping)\\n        for i, text in enumerate(texts):\\n            metadata = metadatas[i] if metadatas else {}\\n            _id = str(uuid.uuid4())\\n            request = {\\n                \"_op_type\": \"index\",\\n                \"_index\": self.index_name,\\n                \"vector\": embeddings[i],\\n                \"text\": text,\\n                \"metadata\": metadata,\\n                \"_id\": _id,\\n            }\\n            ids.append(_id)\\n            requests.append(request)\\n        bulk(self.client, requests)\\n        if refresh_indices:\\n            self.client.indices.refresh(index=self.index_name)\\n        return ids\\n[docs]    def similarity_search(\\n        self, query: str, k: int = 4, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs most similar to query.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n        Returns:\\n            List of Documents most similar to the query.\\n        \"\"\"\\n        embedding = self.embedding.embed_query(query)\\n        script_query = _default_script_query(embedding)\\n        response = self.client.search(index=self.index_name, query=script_query, size=k)\\n        hits = [hit[\"_source\"] for hit in response[\"hits\"][\"hits\"]]\\n        documents = [\\n            Document(page_content=hit[\"text\"], metadata=hit[\"metadata\"]) for hit in hits\\n        ]\\n        return documents\\n[docs]    @classmethod\\n    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        **kwargs: Any,\\n    ) -> ElasticVectorSearch:\\n        \"\"\"Construct ElasticVectorSearch wrapper from raw documents.\\n        This is a user-friendly interface that:\\n            1. Embeds documents.\\n            2. Creates a new index for the embeddings in the Elasticsearch instance.\\n            3. Adds the documents to the newly created Elasticsearch index.\\n        This is intended to be a quick way to get started.\\n        Example:\\n            .. code-block:: python\\n                from langchain import ElasticVectorSearch\\n                from langchain.embeddings import OpenAIEmbeddings\\n                embeddings = OpenAIEmbeddings()\\n                elastic_vector_search = ElasticVectorSearch.from_texts(\\n                    texts,\\n                    embeddings,\\n                    elasticsearch_url=\"http://localhost:9200\"\\n                )\\n        \"\"\"\\n        elasticsearch_url = get_from_dict_or_env(\\n            kwargs, \"elasticsearch_url\", \"ELASTICSEARCH_URL\"\\n        )\\n        try:\\n            import elasticsearch\\n            from elasticsearch.exceptions import NotFoundError\\n            from elasticsearch.helpers import bulk\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import elasticsearch python package. \"\\n                \"Please install it with `pip install elasticsearch`.\"\\n            )\\n        try:\\n            client = elasticsearch.Elasticsearch(elasticsearch_url)\\n        except ValueError as e:\\n            raise ValueError(\\n                \"Your elasticsearch client string is misformatted. \" f\"Got error: {e} \"\\n            )\\n        index_name = kwargs.get(\"index_name\", uuid.uuid4().hex)\\n        embeddings = embedding.embed_documents(texts)\\n        dim = len(embeddings[0])\\n        mapping = _default_text_mapping(dim)\\n        # check to see if the index already exists\\n        try:\\n            client.indices.get(index=index_name)\\n        except NotFoundError:\\n            # TODO would be nice to create index before embedding,\\n            # just to save expensive steps for last\\n            client.indices.create(index=index_name, mappings=mapping)\\n        requests = []\\n        for i, text in enumerate(texts):\\n            metadata = metadatas[i] if metadatas else {}\\n            request = {\\n                \"_op_type\": \"index\",\\n                \"_index\": index_name,\\n                \"vector\": embeddings[i],\\n                \"text\": text,\\n                \"metadata\": metadata,\\n            }\\n            requests.append(request)\\n        bulk(client, requests)\\n        client.indices.refresh(index=index_name)\\n        return cls(elasticsearch_url, index_name, embedding)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\vectorstores\\\\elastic_vector_search.html'}),\n",
              " Document(page_content='Source code for langchain.vectorstores.faiss\\n\"\"\"Wrapper around FAISS vector database.\"\"\"\\nfrom __future__ import annotations\\nimport math\\nimport pickle\\nimport uuid\\nfrom pathlib import Path\\nfrom typing import Any, Callable, Dict, Iterable, List, Optional, Tuple\\nimport numpy as np\\nfrom langchain.docstore.base import AddableMixin, Docstore\\nfrom langchain.docstore.document import Document\\nfrom langchain.docstore.in_memory import InMemoryDocstore\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.vectorstores.base import VectorStore\\nfrom langchain.vectorstores.utils import maximal_marginal_relevance\\ndef dependable_faiss_import() -> Any:\\n    \"\"\"Import faiss if available, otherwise raise error.\"\"\"\\n    try:\\n        import faiss\\n    except ImportError:\\n        raise ValueError(\\n            \"Could not import faiss python package. \"\\n            \"Please install it with `pip install faiss` \"\\n            \"or `pip install faiss-cpu` (depending on Python version).\"\\n        )\\n    return faiss\\ndef _default_relevance_score_fn(score: float) -> float:\\n    \"\"\"Return a similarity score on a scale [0, 1].\"\"\"\\n    # The \\'correct\\' relevance function\\n    # may differ depending on a few things, including:\\n    # - the distance / similarity metric used by the VectorStore\\n    # - the scale of your embeddings (OpenAI\\'s are unit normed. Many others are not!)\\n    # - embedding dimensionality\\n    # - etc.\\n    # This function converts the euclidean norm of normalized embeddings\\n    # (0 is most similar, sqrt(2) most dissimilar)\\n    # to a similarity function (0 to 1)\\n    return 1.0 - score / math.sqrt(2)\\n[docs]class FAISS(VectorStore):\\n    \"\"\"Wrapper around FAISS vector database.\\n    To use, you should have the ``faiss`` python package installed.\\n    Example:\\n        .. code-block:: python\\n            from langchain import FAISS\\n            faiss = FAISS(embedding_function, index, docstore, index_to_docstore_id)\\n    \"\"\"\\n    def __init__(\\n        self,\\n        embedding_function: Callable,\\n        index: Any,\\n        docstore: Docstore,\\n        index_to_docstore_id: Dict[int, str],\\n        relevance_score_fn: Optional[\\n            Callable[[float], float]\\n        ] = _default_relevance_score_fn,\\n    ):\\n        \"\"\"Initialize with necessary components.\"\"\"\\n        self.embedding_function = embedding_function\\n        self.index = index\\n        self.docstore = docstore\\n        self.index_to_docstore_id = index_to_docstore_id\\n        self.relevance_score_fn = relevance_score_fn\\n    def __add(\\n        self,\\n        texts: Iterable[str],\\n        embeddings: Iterable[List[float]],\\n        metadatas: Optional[List[dict]] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        if not isinstance(self.docstore, AddableMixin):\\n            raise ValueError(\\n                \"If trying to add texts, the underlying docstore should support \"\\n                f\"adding items, which {self.docstore} does not\"\\n            )\\n        documents = []\\n        for i, text in enumerate(texts):\\n            metadata = metadatas[i] if metadatas else {}\\n            documents.append(Document(page_content=text, metadata=metadata))\\n        # Add to the index, the index_to_id mapping, and the docstore.\\n        starting_len = len(self.index_to_docstore_id)\\n        self.index.add(np.array(embeddings, dtype=np.float32))\\n        # Get list of index, id, and docs.\\n        full_info = [\\n            (starting_len + i, str(uuid.uuid4()), doc)\\n            for i, doc in enumerate(documents)\\n        ]\\n        # Add information to docstore and index.\\n        self.docstore.add({_id: doc for _, _id, doc in full_info})\\n        index_to_id = {index: _id for index, _id, _ in full_info}\\n        self.index_to_docstore_id.update(index_to_id)\\n        return [_id for _, _id, _ in full_info]\\n[docs]    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\\n        Args:\\n            texts: Iterable of strings to add to the vectorstore.\\n            metadatas: Optional list of metadatas associated with the texts.\\n        Returns:\\n            List of ids from adding the texts into the vectorstore.\\n        \"\"\"\\n        if not isinstance(self.docstore, AddableMixin):\\n            raise ValueError(\\n                \"If trying to add texts, the underlying docstore should support \"\\n                f\"adding items, which {self.docstore} does not\"\\n            )\\n        # Embed and create the documents.\\n        embeddings = [self.embedding_function(text) for text in texts]\\n        return self.__add(texts, embeddings, metadatas, **kwargs)\\n[docs]    def add_embeddings(\\n        self,\\n        text_embeddings: Iterable[Tuple[str, List[float]]],\\n        metadatas: Optional[List[dict]] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\\n        Args:\\n            text_embeddings: Iterable pairs of string and embedding to\\n                add to the vectorstore.\\n            metadatas: Optional list of metadatas associated with the texts.\\n        Returns:\\n            List of ids from adding the texts into the vectorstore.\\n        \"\"\"\\n        if not isinstance(self.docstore, AddableMixin):\\n            raise ValueError(\\n                \"If trying to add texts, the underlying docstore should support \"\\n                f\"adding items, which {self.docstore} does not\"\\n            )\\n        # Embed and create the documents.\\n        texts = [te[0] for te in text_embeddings]\\n        embeddings = [te[1] for te in text_embeddings]\\n        return self.__add(texts, embeddings, metadatas, **kwargs)\\n[docs]    def similarity_search_with_score_by_vector(\\n        self, embedding: List[float], k: int = 4\\n    ) -> List[Tuple[Document, float]]:\\n        \"\"\"Return docs most similar to query.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n        Returns:\\n            List of Documents most similar to the query and score for each\\n        \"\"\"\\n        scores, indices = self.index.search(np.array([embedding], dtype=np.float32), k)\\n        docs = []\\n        for j, i in enumerate(indices[0]):\\n            if i == -1:\\n                # This happens when not enough docs are returned.\\n                continue\\n            _id = self.index_to_docstore_id[i]\\n            doc = self.docstore.search(_id)\\n            if not isinstance(doc, Document):\\n                raise ValueError(f\"Could not find document for id {_id}, got {doc}\")\\n            docs.append((doc, scores[0][j]))\\n        return docs\\n[docs]    def similarity_search_with_score(\\n        self, query: str, k: int = 4\\n    ) -> List[Tuple[Document, float]]:\\n        \"\"\"Return docs most similar to query.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n        Returns:\\n            List of Documents most similar to the query and score for each\\n        \"\"\"\\n        embedding = self.embedding_function(query)\\n        docs = self.similarity_search_with_score_by_vector(embedding, k)\\n        return docs\\n[docs]    def similarity_search_by_vector(\\n        self, embedding: List[float], k: int = 4, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs most similar to embedding vector.\\n        Args:\\n            embedding: Embedding to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n        Returns:\\n            List of Documents most similar to the embedding.\\n        \"\"\"\\n        docs_and_scores = self.similarity_search_with_score_by_vector(embedding, k)\\n        return [doc for doc, _ in docs_and_scores]\\n[docs]    def similarity_search(\\n        self, query: str, k: int = 4, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs most similar to query.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n        Returns:\\n            List of Documents most similar to the query.\\n        \"\"\"\\n        docs_and_scores = self.similarity_search_with_score(query, k)\\n        return [doc for doc, _ in docs_and_scores]\\n[docs]    def max_marginal_relevance_search_by_vector(\\n        self, embedding: List[float], k: int = 4, fetch_k: int = 20, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs selected using the maximal marginal relevance.\\n        Maximal marginal relevance optimizes for similarity to query AND diversity\\n        among selected documents.\\n        Args:\\n            embedding: Embedding to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n        Returns:\\n            List of Documents selected by maximal marginal relevance.\\n        \"\"\"\\n        _, indices = self.index.search(np.array([embedding], dtype=np.float32), fetch_k)\\n        # -1 happens when not enough docs are returned.\\n        embeddings = [self.index.reconstruct(int(i)) for i in indices[0] if i != -1]\\n        mmr_selected = maximal_marginal_relevance(\\n            np.array([embedding], dtype=np.float32), embeddings, k=k\\n        )\\n        selected_indices = [indices[0][i] for i in mmr_selected]\\n        docs = []\\n        for i in selected_indices:\\n            if i == -1:\\n                # This happens when not enough docs are returned.\\n                continue\\n            _id = self.index_to_docstore_id[i]\\n            doc = self.docstore.search(_id)\\n            if not isinstance(doc, Document):\\n                raise ValueError(f\"Could not find document for id {_id}, got {doc}\")\\n            docs.append(doc)\\n        return docs\\n[docs]    def max_marginal_relevance_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        fetch_k: int = 20,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        \"\"\"Return docs selected using the maximal marginal relevance.\\n        Maximal marginal relevance optimizes for similarity to query AND diversity\\n        among selected documents.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n        Returns:\\n            List of Documents selected by maximal marginal relevance.\\n        \"\"\"\\n        embedding = self.embedding_function(query)\\n        docs = self.max_marginal_relevance_search_by_vector(embedding, k, fetch_k)\\n        return docs\\n[docs]    def merge_from(self, target: FAISS) -> None:\\n        \"\"\"Merge another FAISS object with the current one.\\n        Add the target FAISS to the current one.\\n        Args:\\n            target: FAISS object you wish to merge into the current one\\n        Returns:\\n            None.\\n        \"\"\"\\n        if not isinstance(self.docstore, AddableMixin):\\n            raise ValueError(\"Cannot merge with this type of docstore\")\\n        # Numerical index for target docs are incremental on existing ones\\n        starting_len = len(self.index_to_docstore_id)\\n        # Merge two IndexFlatL2\\n        self.index.merge_from(target.index)\\n        # Create new id for docs from target FAISS object\\n        full_info = []\\n        for i in target.index_to_docstore_id:\\n            doc = target.docstore.search(target.index_to_docstore_id[i])\\n            if not isinstance(doc, Document):\\n                raise ValueError(\"Document should be returned\")\\n            full_info.append((starting_len + i, str(uuid.uuid4()), doc))\\n        # Add information to docstore and index_to_docstore_id.\\n        self.docstore.add({_id: doc for _, _id, doc in full_info})\\n        index_to_id = {index: _id for index, _id, _ in full_info}\\n        self.index_to_docstore_id.update(index_to_id)\\n    @classmethod\\n    def __from(\\n        cls,\\n        texts: List[str],\\n        embeddings: List[List[float]],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        **kwargs: Any,\\n    ) -> FAISS:\\n        faiss = dependable_faiss_import()\\n        index = faiss.IndexFlatL2(len(embeddings[0]))\\n        index.add(np.array(embeddings, dtype=np.float32))\\n        documents = []\\n        for i, text in enumerate(texts):\\n            metadata = metadatas[i] if metadatas else {}\\n            documents.append(Document(page_content=text, metadata=metadata))\\n        index_to_id = {i: str(uuid.uuid4()) for i in range(len(documents))}\\n        docstore = InMemoryDocstore(\\n            {index_to_id[i]: doc for i, doc in enumerate(documents)}\\n        )\\n        return cls(embedding.embed_query, index, docstore, index_to_id, **kwargs)\\n[docs]    @classmethod\\n    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        **kwargs: Any,\\n    ) -> FAISS:\\n        \"\"\"Construct FAISS wrapper from raw documents.\\n        This is a user friendly interface that:\\n            1. Embeds documents.\\n            2. Creates an in memory docstore\\n            3. Initializes the FAISS database\\n        This is intended to be a quick way to get started.\\n        Example:\\n            .. code-block:: python\\n                from langchain import FAISS\\n                from langchain.embeddings import OpenAIEmbeddings\\n                embeddings = OpenAIEmbeddings()\\n                faiss = FAISS.from_texts(texts, embeddings)\\n        \"\"\"\\n        embeddings = embedding.embed_documents(texts)\\n        return cls.__from(\\n            texts,\\n            embeddings,\\n            embedding,\\n            metadatas,\\n            **kwargs,\\n        )\\n[docs]    @classmethod\\n    def from_embeddings(\\n        cls,\\n        text_embeddings: List[Tuple[str, List[float]]],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        **kwargs: Any,\\n    ) -> FAISS:\\n        \"\"\"Construct FAISS wrapper from raw documents.\\n        This is a user friendly interface that:\\n            1. Embeds documents.\\n            2. Creates an in memory docstore\\n            3. Initializes the FAISS database\\n        This is intended to be a quick way to get started.\\n        Example:\\n            .. code-block:: python\\n                from langchain import FAISS\\n                from langchain.embeddings import OpenAIEmbeddings\\n                embeddings = OpenAIEmbeddings()\\n                text_embeddings = embeddings.embed_documents(texts)\\n                text_embedding_pairs = list(zip(texts, text_embeddings))\\n                faiss = FAISS.from_embeddings(text_embedding_pairs, embeddings)\\n        \"\"\"\\n        texts = [t[0] for t in text_embeddings]\\n        embeddings = [t[1] for t in text_embeddings]\\n        return cls.__from(\\n            texts,\\n            embeddings,\\n            embedding,\\n            metadatas,\\n            **kwargs,\\n        )\\n[docs]    def save_local(self, folder_path: str, index_name: str = \"index\") -> None:\\n        \"\"\"Save FAISS index, docstore, and index_to_docstore_id to disk.\\n        Args:\\n            folder_path: folder path to save index, docstore,\\n                and index_to_docstore_id to.\\n            index_name: for saving with a specific index file name\\n        \"\"\"\\n        path = Path(folder_path)\\n        path.mkdir(exist_ok=True, parents=True)\\n        # save index separately since it is not picklable\\n        faiss = dependable_faiss_import()\\n        faiss.write_index(\\n            self.index, str(path / \"{index_name}.faiss\".format(index_name=index_name))\\n        )\\n        # save docstore and index_to_docstore_id\\n        with open(path / \"{index_name}.pkl\".format(index_name=index_name), \"wb\") as f:\\n            pickle.dump((self.docstore, self.index_to_docstore_id), f)\\n[docs]    @classmethod\\n    def load_local(\\n        cls, folder_path: str, embeddings: Embeddings, index_name: str = \"index\"\\n    ) -> FAISS:\\n        \"\"\"Load FAISS index, docstore, and index_to_docstore_id to disk.\\n        Args:\\n            folder_path: folder path to load index, docstore,\\n                and index_to_docstore_id from.\\n            embeddings: Embeddings to use when generating queries\\n            index_name: for saving with a specific index file name\\n        \"\"\"\\n        path = Path(folder_path)\\n        # load index separately since it is not picklable\\n        faiss = dependable_faiss_import()\\n        index = faiss.read_index(\\n            str(path / \"{index_name}.faiss\".format(index_name=index_name))\\n        )\\n        # load docstore and index_to_docstore_id\\n        with open(path / \"{index_name}.pkl\".format(index_name=index_name), \"rb\") as f:\\n            docstore, index_to_docstore_id = pickle.load(f)\\n        return cls(embeddings.embed_query, index, docstore, index_to_docstore_id)\\n    def _similarity_search_with_relevance_scores(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        **kwargs: Any,\\n    ) -> List[Tuple[Document, float]]:\\n        \"\"\"Return docs and their similarity scores on a scale from 0 to 1.\"\"\"\\n        if self.relevance_score_fn is None:\\n            raise ValueError(\\n                \"normalize_score_fn must be provided to\"\\n                \" FAISS constructor to normalize scores\"\\n            )\\n        docs_and_scores = self.similarity_search_with_score(query, k=k)\\n        return [(doc, self.relevance_score_fn(score)) for doc, score in docs_and_scores]\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\vectorstores\\\\faiss.html'}),\n",
              " Document(page_content='Source code for langchain.vectorstores.milvus\\n\"\"\"Wrapper around the Milvus vector database.\"\"\"\\nfrom __future__ import annotations\\nimport uuid\\nfrom typing import Any, Iterable, List, Optional, Tuple\\nimport numpy as np\\nfrom langchain.docstore.document import Document\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.vectorstores.base import VectorStore\\nfrom langchain.vectorstores.utils import maximal_marginal_relevance\\n[docs]class Milvus(VectorStore):\\n    \"\"\"Wrapper around the Milvus vector database.\"\"\"\\n    def __init__(\\n        self,\\n        embedding_function: Embeddings,\\n        connection_args: dict,\\n        collection_name: str,\\n        text_field: str,\\n    ):\\n        \"\"\"Initialize wrapper around the milvus vector database.\\n        In order to use this you need to have `pymilvus` installed and a\\n        running Milvus instance.\\n        See the following documentation for how to run a Milvus instance:\\n        https://milvus.io/docs/install_standalone-docker.md\\n        Args:\\n            embedding_function (Embeddings): Function used to embed the text\\n            connection_args (dict): Arguments for pymilvus connections.connect()\\n            collection_name (str): The name of the collection to search.\\n            text_field (str): The field in Milvus schema where the\\n                original text is stored.\\n        \"\"\"\\n        try:\\n            from pymilvus import Collection, DataType, connections\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import pymilvus python package. \"\\n                \"Please install it with `pip install pymilvus`.\"\\n            )\\n        # Connecting to Milvus instance\\n        if not connections.has_connection(\"default\"):\\n            connections.connect(**connection_args)\\n        self.embedding_func = embedding_function\\n        self.collection_name = collection_name\\n        self.text_field = text_field\\n        self.auto_id = False\\n        self.primary_field = None\\n        self.vector_field = None\\n        self.fields = []\\n        self.col = Collection(self.collection_name)\\n        schema = self.col.schema\\n        # Grabbing the fields for the existing collection.\\n        for x in schema.fields:\\n            self.fields.append(x.name)\\n            if x.auto_id:\\n                self.fields.remove(x.name)\\n            if x.is_primary:\\n                self.primary_field = x.name\\n            if x.dtype == DataType.FLOAT_VECTOR or x.dtype == DataType.BINARY_VECTOR:\\n                self.vector_field = x.name\\n        # Default search params when one is not provided.\\n        self.index_params = {\\n            \"IVF_FLAT\": {\"params\": {\"nprobe\": 10}},\\n            \"IVF_SQ8\": {\"params\": {\"nprobe\": 10}},\\n            \"IVF_PQ\": {\"params\": {\"nprobe\": 10}},\\n            \"HNSW\": {\"params\": {\"ef\": 10}},\\n            \"RHNSW_FLAT\": {\"params\": {\"ef\": 10}},\\n            \"RHNSW_SQ\": {\"params\": {\"ef\": 10}},\\n            \"RHNSW_PQ\": {\"params\": {\"ef\": 10}},\\n            \"IVF_HNSW\": {\"params\": {\"nprobe\": 10, \"ef\": 10}},\\n            \"ANNOY\": {\"params\": {\"search_k\": 10}},\\n        }\\n[docs]    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        partition_name: Optional[str] = None,\\n        timeout: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        \"\"\"Insert text data into Milvus.\\n        When using add_texts() it is assumed that a collecton has already\\n        been made and indexed. If metadata is included, it is assumed that\\n        it is ordered correctly to match the schema provided to the Collection\\n        and that the embedding vector is the first schema field.\\n        Args:\\n            texts (Iterable[str]): The text being embedded and inserted.\\n            metadatas (Optional[List[dict]], optional): The metadata that\\n                corresponds to each insert. Defaults to None.\\n            partition_name (str, optional): The partition of the collection\\n                to insert data into. Defaults to None.\\n            timeout: specified timeout.\\n        Returns:\\n            List[str]: The resulting keys for each inserted element.\\n        \"\"\"\\n        insert_dict: Any = {self.text_field: list(texts)}\\n        try:\\n            insert_dict[self.vector_field] = self.embedding_func.embed_documents(\\n                list(texts)\\n            )\\n        except NotImplementedError:\\n            insert_dict[self.vector_field] = [\\n                self.embedding_func.embed_query(x) for x in texts\\n            ]\\n        # Collect the metadata into the insert dict.\\n        if len(self.fields) > 2 and metadatas is not None:\\n            for d in metadatas:\\n                for key, value in d.items():\\n                    if key in self.fields:\\n                        insert_dict.setdefault(key, []).append(value)\\n        # Convert dict to list of lists for insertion\\n        insert_list = [insert_dict[x] for x in self.fields]\\n        # Insert into the collection.\\n        res = self.col.insert(\\n            insert_list, partition_name=partition_name, timeout=timeout\\n        )\\n        # Flush to make sure newly inserted is immediately searchable.\\n        self.col.flush()\\n        return res.primary_keys\\n    def _worker_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        param: Optional[dict] = None,\\n        expr: Optional[str] = None,\\n        partition_names: Optional[List[str]] = None,\\n        round_decimal: int = -1,\\n        timeout: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> Tuple[List[float], List[Tuple[Document, Any, Any]]]:\\n        # Load the collection into memory for searching.\\n        self.col.load()\\n        # Decide to use default params if not passed in.\\n        if param is None:\\n            index_type = self.col.indexes[0].params[\"index_type\"]\\n            param = self.index_params[index_type]\\n        # Embed the query text.\\n        data = [self.embedding_func.embed_query(query)]\\n        # Determine result metadata fields.\\n        output_fields = self.fields[:]\\n        output_fields.remove(self.vector_field)\\n        # Perform the search.\\n        res = self.col.search(\\n            data,\\n            self.vector_field,\\n            param,\\n            k,\\n            expr=expr,\\n            output_fields=output_fields,\\n            partition_names=partition_names,\\n            round_decimal=round_decimal,\\n            timeout=timeout,\\n            **kwargs,\\n        )\\n        # Organize results.\\n        ret = []\\n        for result in res[0]:\\n            meta = {x: result.entity.get(x) for x in output_fields}\\n            ret.append(\\n                (\\n                    Document(page_content=meta.pop(self.text_field), metadata=meta),\\n                    result.distance,\\n                    result.id,\\n                )\\n            )\\n        return data[0], ret\\n[docs]    def similarity_search_with_score(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        param: Optional[dict] = None,\\n        expr: Optional[str] = None,\\n        partition_names: Optional[List[str]] = None,\\n        round_decimal: int = -1,\\n        timeout: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> List[Tuple[Document, float]]:\\n        \"\"\"Perform a search on a query string and return results.\\n        Args:\\n            query (str): The text being searched.\\n            k (int, optional): The amount of results ot return. Defaults to 4.\\n            param (dict, optional): The search params for the specified index.\\n                Defaults to None.\\n            expr (str, optional): Filtering expression. Defaults to None.\\n            partition_names (List[str], optional): Partitions to search through.\\n                Defaults to None.\\n            round_decimal (int, optional): Round the resulting distance. Defaults\\n                to -1.\\n            timeout (int, optional): Amount to wait before timeout error. Defaults\\n                to None.\\n            kwargs: Collection.search() keyword arguments.\\n        Returns:\\n            List[float], List[Tuple[Document, any, any]]: search_embedding,\\n                (Document, distance, primary_field) results.\\n        \"\"\"\\n        _, result = self._worker_search(\\n            query, k, param, expr, partition_names, round_decimal, timeout, **kwargs\\n        )\\n        return [(x, y) for x, y, _ in result]\\n[docs]    def max_marginal_relevance_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        fetch_k: int = 20,\\n        param: Optional[dict] = None,\\n        expr: Optional[str] = None,\\n        partition_names: Optional[List[str]] = None,\\n        round_decimal: int = -1,\\n        timeout: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        \"\"\"Perform a search and return results that are reordered by MMR.\\n        Args:\\n            query (str): The text being searched.\\n            k (int, optional): How many results to give. Defaults to 4.\\n            fetch_k (int, optional): Total results to select k from.\\n                Defaults to 20.\\n            param (dict, optional): The search params for the specified index.\\n                Defaults to None.\\n            expr (str, optional): Filtering expression. Defaults to None.\\n            partition_names (List[str], optional): What partitions to search.\\n                Defaults to None.\\n            round_decimal (int, optional): Round the resulting distance. Defaults\\n                to -1.\\n            timeout (int, optional): Amount to wait before timeout error. Defaults\\n                to None.\\n        Returns:\\n            List[Document]: Document results for search.\\n        \"\"\"\\n        data, res = self._worker_search(\\n            query,\\n            fetch_k,\\n            param,\\n            expr,\\n            partition_names,\\n            round_decimal,\\n            timeout,\\n            **kwargs,\\n        )\\n        # Extract result IDs.\\n        ids = [x for _, _, x in res]\\n        # Get the raw vectors from Milvus.\\n        vectors = self.col.query(\\n            expr=f\"{self.primary_field} in {ids}\",\\n            output_fields=[self.primary_field, self.vector_field],\\n        )\\n        # Reorganize the results from query to match result order.\\n        vectors = {x[self.primary_field]: x[self.vector_field] for x in vectors}\\n        search_embedding = data\\n        ordered_result_embeddings = [vectors[x] for x in ids]\\n        # Get the new order of results.\\n        new_ordering = maximal_marginal_relevance(\\n            np.array(search_embedding), ordered_result_embeddings, k=k\\n        )\\n        # Reorder the values and return.\\n        ret = []\\n        for x in new_ordering:\\n            if x == -1:\\n                break\\n            else:\\n                ret.append(res[x][0])\\n        return ret\\n[docs]    def similarity_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        param: Optional[dict] = None,\\n        expr: Optional[str] = None,\\n        partition_names: Optional[List[str]] = None,\\n        round_decimal: int = -1,\\n        timeout: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        \"\"\"Perform a similarity search against the query string.\\n        Args:\\n            query (str): The text to search.\\n            k (int, optional): How many results to return. Defaults to 4.\\n            param (dict, optional): The search params for the index type.\\n                Defaults to None.\\n            expr (str, optional): Filtering expression. Defaults to None.\\n            partition_names (List[str], optional): What partitions to search.\\n                Defaults to None.\\n            round_decimal (int, optional): What decimal point to round to.\\n                Defaults to -1.\\n            timeout (int, optional): How long to wait before timeout error.\\n                Defaults to None.\\n        Returns:\\n            List[Document]: Document results for search.\\n        \"\"\"\\n        _, docs_and_scores = self._worker_search(\\n            query, k, param, expr, partition_names, round_decimal, timeout, **kwargs\\n        )\\n        return [doc for doc, _, _ in docs_and_scores]\\n[docs]    @classmethod\\n    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        **kwargs: Any,\\n    ) -> Milvus:\\n        \"\"\"Create a Milvus collection, indexes it with HNSW, and insert data.\\n        Args:\\n            texts (List[str]): Text to insert.\\n            embedding (Embeddings): Embedding function to use.\\n            metadatas (Optional[List[dict]], optional): Dict metatadata.\\n                Defaults to None.\\n        Returns:\\n            VectorStore: The Milvus vector store.\\n        \"\"\"\\n        try:\\n            from pymilvus import (\\n                Collection,\\n                CollectionSchema,\\n                DataType,\\n                FieldSchema,\\n                connections,\\n            )\\n            from pymilvus.orm.types import infer_dtype_bydata\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import pymilvus python package. \"\\n                \"Please install it with `pip install pymilvus`.\"\\n            )\\n        # Connect to Milvus instance\\n        if not connections.has_connection(\"default\"):\\n            connections.connect(**kwargs.get(\"connection_args\", {\"port\": 19530}))\\n        # Determine embedding dim\\n        embeddings = embedding.embed_query(texts[0])\\n        dim = len(embeddings)\\n        # Generate unique names\\n        primary_field = \"c\" + str(uuid.uuid4().hex)\\n        vector_field = \"c\" + str(uuid.uuid4().hex)\\n        text_field = \"c\" + str(uuid.uuid4().hex)\\n        collection_name = \"c\" + str(uuid.uuid4().hex)\\n        fields = []\\n        # Determine metadata schema\\n        if metadatas:\\n            # Check if all metadata keys line up\\n            key = metadatas[0].keys()\\n            for x in metadatas:\\n                if key != x.keys():\\n                    raise ValueError(\\n                        \"Mismatched metadata. \"\\n                        \"Make sure all metadata has the same keys and datatype.\"\\n                    )\\n            # Create FieldSchema for each entry in singular metadata.\\n            for key, value in metadatas[0].items():\\n                # Infer the corresponding datatype of the metadata\\n                dtype = infer_dtype_bydata(value)\\n                if dtype == DataType.UNKNOWN:\\n                    raise ValueError(f\"Unrecognized datatype for {key}.\")\\n                elif dtype == DataType.VARCHAR:\\n                    # Find out max length text based metadata\\n                    max_length = 0\\n                    for subvalues in metadatas:\\n                        max_length = max(max_length, len(subvalues[key]))\\n                    fields.append(\\n                        FieldSchema(key, DataType.VARCHAR, max_length=max_length + 1)\\n                    )\\n                else:\\n                    fields.append(FieldSchema(key, dtype))\\n        # Find out max length of texts\\n        max_length = 0\\n        for y in texts:\\n            max_length = max(max_length, len(y))\\n        # Create the text field\\n        fields.append(\\n            FieldSchema(text_field, DataType.VARCHAR, max_length=max_length + 1)\\n        )\\n        # Create the primary key field\\n        fields.append(\\n            FieldSchema(primary_field, DataType.INT64, is_primary=True, auto_id=True)\\n        )\\n        # Create the vector field\\n        fields.append(FieldSchema(vector_field, DataType.FLOAT_VECTOR, dim=dim))\\n        # Create the schema for the collection\\n        schema = CollectionSchema(fields)\\n        # Create the collection\\n        collection = Collection(collection_name, schema)\\n        # Index parameters for the collection\\n        index = {\\n            \"index_type\": \"HNSW\",\\n            \"metric_type\": \"L2\",\\n            \"params\": {\"M\": 8, \"efConstruction\": 64},\\n        }\\n        # Create the index\\n        collection.create_index(vector_field, index)\\n        # Create the VectorStore\\n        milvus = cls(\\n            embedding,\\n            kwargs.get(\"connection_args\", {\"port\": 19530}),\\n            collection_name,\\n            text_field,\\n        )\\n        # Add the texts.\\n        milvus.add_texts(texts, metadatas)\\n        return milvus\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\vectorstores\\\\milvus.html'}),\n",
              " Document(page_content='Source code for langchain.vectorstores.opensearch_vector_search\\n\"\"\"Wrapper around OpenSearch vector database.\"\"\"\\nfrom __future__ import annotations\\nimport uuid\\nfrom typing import Any, Dict, Iterable, List, Optional\\nfrom langchain.docstore.document import Document\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.utils import get_from_dict_or_env\\nfrom langchain.vectorstores.base import VectorStore\\nIMPORT_OPENSEARCH_PY_ERROR = (\\n    \"Could not import OpenSearch. Please install it with `pip install opensearch-py`.\"\\n)\\nSCRIPT_SCORING_SEARCH = \"script_scoring\"\\nPAINLESS_SCRIPTING_SEARCH = \"painless_scripting\"\\nMATCH_ALL_QUERY = {\"match_all\": {}}  # type: Dict\\ndef _import_opensearch() -> Any:\\n    \"\"\"Import OpenSearch if available, otherwise raise error.\"\"\"\\n    try:\\n        from opensearchpy import OpenSearch\\n    except ImportError:\\n        raise ValueError(IMPORT_OPENSEARCH_PY_ERROR)\\n    return OpenSearch\\ndef _import_bulk() -> Any:\\n    \"\"\"Import bulk if available, otherwise raise error.\"\"\"\\n    try:\\n        from opensearchpy.helpers import bulk\\n    except ImportError:\\n        raise ValueError(IMPORT_OPENSEARCH_PY_ERROR)\\n    return bulk\\ndef _get_opensearch_client(opensearch_url: str, **kwargs: Any) -> Any:\\n    \"\"\"Get OpenSearch client from the opensearch_url, otherwise raise error.\"\"\"\\n    try:\\n        opensearch = _import_opensearch()\\n        client = opensearch(opensearch_url, **kwargs)\\n    except ValueError as e:\\n        raise ValueError(\\n            f\"OpenSearch client string provided is not in proper format. \"\\n            f\"Got error: {e} \"\\n        )\\n    return client\\ndef _validate_embeddings_and_bulk_size(embeddings_length: int, bulk_size: int) -> None:\\n    \"\"\"Validate Embeddings Length and Bulk Size.\"\"\"\\n    if embeddings_length == 0:\\n        raise RuntimeError(\"Embeddings size is zero\")\\n    if bulk_size < embeddings_length:\\n        raise RuntimeError(\\n            f\"The embeddings count, {embeddings_length} is more than the \"\\n            f\"[bulk_size], {bulk_size}. Increase the value of [bulk_size].\"\\n        )\\ndef _bulk_ingest_embeddings(\\n    client: Any,\\n    index_name: str,\\n    embeddings: List[List[float]],\\n    texts: Iterable[str],\\n    metadatas: Optional[List[dict]] = None,\\n    vector_field: str = \"vector_field\",\\n    text_field: str = \"text\",\\n) -> List[str]:\\n    \"\"\"Bulk Ingest Embeddings into given index.\"\"\"\\n    bulk = _import_bulk()\\n    requests = []\\n    ids = []\\n    for i, text in enumerate(texts):\\n        metadata = metadatas[i] if metadatas else {}\\n        _id = str(uuid.uuid4())\\n        request = {\\n            \"_op_type\": \"index\",\\n            \"_index\": index_name,\\n            vector_field: embeddings[i],\\n            text_field: text,\\n            \"metadata\": metadata,\\n            \"_id\": _id,\\n        }\\n        requests.append(request)\\n        ids.append(_id)\\n    bulk(client, requests)\\n    client.indices.refresh(index=index_name)\\n    return ids\\ndef _default_scripting_text_mapping(\\n    dim: int,\\n    vector_field: str = \"vector_field\",\\n) -> Dict:\\n    \"\"\"For Painless Scripting or Script Scoring,the default mapping to create index.\"\"\"\\n    return {\\n        \"mappings\": {\\n            \"properties\": {\\n                vector_field: {\"type\": \"knn_vector\", \"dimension\": dim},\\n            }\\n        }\\n    }\\ndef _default_text_mapping(\\n    dim: int,\\n    engine: str = \"nmslib\",\\n    space_type: str = \"l2\",\\n    ef_search: int = 512,\\n    ef_construction: int = 512,\\n    m: int = 16,\\n    vector_field: str = \"vector_field\",\\n) -> Dict:\\n    \"\"\"For Approximate k-NN Search, this is the default mapping to create index.\"\"\"\\n    return {\\n        \"settings\": {\"index\": {\"knn\": True, \"knn.algo_param.ef_search\": ef_search}},\\n        \"mappings\": {\\n            \"properties\": {\\n                vector_field: {\\n                    \"type\": \"knn_vector\",\\n                    \"dimension\": dim,\\n                    \"method\": {\\n                        \"name\": \"hnsw\",\\n                        \"space_type\": space_type,\\n                        \"engine\": engine,\\n                        \"parameters\": {\"ef_construction\": ef_construction, \"m\": m},\\n                    },\\n                }\\n            }\\n        },\\n    }\\ndef _default_approximate_search_query(\\n    query_vector: List[float],\\n    size: int = 4,\\n    k: int = 4,\\n    vector_field: str = \"vector_field\",\\n) -> Dict:\\n    \"\"\"For Approximate k-NN Search, this is the default query.\"\"\"\\n    return {\\n        \"size\": size,\\n        \"query\": {\"knn\": {vector_field: {\"vector\": query_vector, \"k\": k}}},\\n    }\\ndef _approximate_search_query_with_boolean_filter(\\n    query_vector: List[float],\\n    boolean_filter: Dict,\\n    size: int = 4,\\n    k: int = 4,\\n    vector_field: str = \"vector_field\",\\n    subquery_clause: str = \"must\",\\n) -> Dict:\\n    \"\"\"For Approximate k-NN Search, with Boolean Filter.\"\"\"\\n    return {\\n        \"size\": size,\\n        \"query\": {\\n            \"bool\": {\\n                \"filter\": boolean_filter,\\n                subquery_clause: [\\n                    {\"knn\": {vector_field: {\"vector\": query_vector, \"k\": k}}}\\n                ],\\n            }\\n        },\\n    }\\ndef _approximate_search_query_with_lucene_filter(\\n    query_vector: List[float],\\n    lucene_filter: Dict,\\n    size: int = 4,\\n    k: int = 4,\\n    vector_field: str = \"vector_field\",\\n) -> Dict:\\n    \"\"\"For Approximate k-NN Search, with Lucene Filter.\"\"\"\\n    search_query = _default_approximate_search_query(\\n        query_vector, size, k, vector_field\\n    )\\n    search_query[\"query\"][\"knn\"][vector_field][\"filter\"] = lucene_filter\\n    return search_query\\ndef _default_script_query(\\n    query_vector: List[float],\\n    space_type: str = \"l2\",\\n    pre_filter: Dict = MATCH_ALL_QUERY,\\n    vector_field: str = \"vector_field\",\\n) -> Dict:\\n    \"\"\"For Script Scoring Search, this is the default query.\"\"\"\\n    return {\\n        \"query\": {\\n            \"script_score\": {\\n                \"query\": pre_filter,\\n                \"script\": {\\n                    \"source\": \"knn_score\",\\n                    \"lang\": \"knn\",\\n                    \"params\": {\\n                        \"field\": vector_field,\\n                        \"query_value\": query_vector,\\n                        \"space_type\": space_type,\\n                    },\\n                },\\n            }\\n        }\\n    }\\ndef __get_painless_scripting_source(\\n    space_type: str, query_vector: List[float], vector_field: str = \"vector_field\"\\n) -> str:\\n    \"\"\"For Painless Scripting, it returns the script source based on space type.\"\"\"\\n    source_value = (\\n        \"(1.0 + \"\\n        + space_type\\n        + \"(\"\\n        + str(query_vector)\\n        + \", doc[\\'\"\\n        + vector_field\\n        + \"\\']))\"\\n    )\\n    if space_type == \"cosineSimilarity\":\\n        return source_value\\n    else:\\n        return \"1/\" + source_value\\ndef _default_painless_scripting_query(\\n    query_vector: List[float],\\n    space_type: str = \"l2Squared\",\\n    pre_filter: Dict = MATCH_ALL_QUERY,\\n    vector_field: str = \"vector_field\",\\n) -> Dict:\\n    \"\"\"For Painless Scripting Search, this is the default query.\"\"\"\\n    source = __get_painless_scripting_source(space_type, query_vector)\\n    return {\\n        \"query\": {\\n            \"script_score\": {\\n                \"query\": pre_filter,\\n                \"script\": {\\n                    \"source\": source,\\n                    \"params\": {\\n                        \"field\": vector_field,\\n                        \"query_value\": query_vector,\\n                    },\\n                },\\n            }\\n        }\\n    }\\ndef _get_kwargs_value(kwargs: Any, key: str, default_value: Any) -> Any:\\n    \"\"\"Get the value of the key if present. Else get the default_value.\"\"\"\\n    if key in kwargs:\\n        return kwargs.get(key)\\n    return default_value\\n[docs]class OpenSearchVectorSearch(VectorStore):\\n    \"\"\"Wrapper around OpenSearch as a vector database.\\n    Example:\\n        .. code-block:: python\\n            from langchain import OpenSearchVectorSearch\\n            opensearch_vector_search = OpenSearchVectorSearch(\\n                \"http://localhost:9200\",\\n                \"embeddings\",\\n                embedding_function\\n            )\\n    \"\"\"\\n    def __init__(\\n        self,\\n        opensearch_url: str,\\n        index_name: str,\\n        embedding_function: Embeddings,\\n        **kwargs: Any,\\n    ):\\n        \"\"\"Initialize with necessary components.\"\"\"\\n        self.embedding_function = embedding_function\\n        self.index_name = index_name\\n        self.client = _get_opensearch_client(opensearch_url, **kwargs)\\n[docs]    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        bulk_size: int = 500,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\\n        Args:\\n            texts: Iterable of strings to add to the vectorstore.\\n            metadatas: Optional list of metadatas associated with the texts.\\n            bulk_size: Bulk API request count; Default: 500\\n        Returns:\\n            List of ids from adding the texts into the vectorstore.\\n        Optional Args:\\n            vector_field: Document field embeddings are stored in. Defaults to\\n            \"vector_field\".\\n            text_field: Document field the text of the document is stored in. Defaults\\n            to \"text\".\\n        \"\"\"\\n        embeddings = self.embedding_function.embed_documents(list(texts))\\n        _validate_embeddings_and_bulk_size(len(embeddings), bulk_size)\\n        vector_field = _get_kwargs_value(kwargs, \"vector_field\", \"vector_field\")\\n        text_field = _get_kwargs_value(kwargs, \"text_field\", \"text\")\\n        return _bulk_ingest_embeddings(\\n            self.client,\\n            self.index_name,\\n            embeddings,\\n            texts,\\n            metadatas,\\n            vector_field,\\n            text_field,\\n        )\\n[docs]    def similarity_search(\\n        self, query: str, k: int = 4, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs most similar to query.\\n        By default supports Approximate Search.\\n        Also supports Script Scoring and Painless Scripting.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n        Returns:\\n            List of Documents most similar to the query.\\n        Optional Args:\\n            vector_field: Document field embeddings are stored in. Defaults to\\n            \"vector_field\".\\n            text_field: Document field the text of the document is stored in. Defaults\\n            to \"text\".\\n            metadata_field: Document field that metadata is stored in. Defaults to\\n            \"metadata\".\\n            Can be set to a special value \"*\" to include the entire document.\\n        Optional Args for Approximate Search:\\n            search_type: \"approximate_search\"; default: \"approximate_search\"\\n            size: number of results the query actually returns; default: 4\\n            boolean_filter: A Boolean filter consists of a Boolean query that\\n            contains a k-NN query and a filter.\\n            subquery_clause: Query clause on the knn vector field; default: \"must\"\\n            lucene_filter: the Lucene algorithm decides whether to perform an exact\\n            k-NN search with pre-filtering or an approximate search with modified\\n            post-filtering.\\n        Optional Args for Script Scoring Search:\\n            search_type: \"script_scoring\"; default: \"approximate_search\"\\n            space_type: \"l2\", \"l1\", \"linf\", \"cosinesimil\", \"innerproduct\",\\n            \"hammingbit\"; default: \"l2\"\\n            pre_filter: script_score query to pre-filter documents before identifying\\n            nearest neighbors; default: {\"match_all\": {}}\\n        Optional Args for Painless Scripting Search:\\n            search_type: \"painless_scripting\"; default: \"approximate_search\"\\n            space_type: \"l2Squared\", \"l1Norm\", \"cosineSimilarity\"; default: \"l2Squared\"\\n            pre_filter: script_score query to pre-filter documents before identifying\\n            nearest neighbors; default: {\"match_all\": {}}\\n        \"\"\"\\n        embedding = self.embedding_function.embed_query(query)\\n        search_type = _get_kwargs_value(kwargs, \"search_type\", \"approximate_search\")\\n        text_field = _get_kwargs_value(kwargs, \"text_field\", \"text\")\\n        metadata_field = _get_kwargs_value(kwargs, \"metadata_field\", \"metadata\")\\n        vector_field = _get_kwargs_value(kwargs, \"vector_field\", \"vector_field\")\\n        if search_type == \"approximate_search\":\\n            size = _get_kwargs_value(kwargs, \"size\", 4)\\n            boolean_filter = _get_kwargs_value(kwargs, \"boolean_filter\", {})\\n            subquery_clause = _get_kwargs_value(kwargs, \"subquery_clause\", \"must\")\\n            lucene_filter = _get_kwargs_value(kwargs, \"lucene_filter\", {})\\n            if boolean_filter != {} and lucene_filter != {}:\\n                raise ValueError(\\n                    \"Both `boolean_filter` and `lucene_filter` are provided which \"\\n                    \"is invalid\"\\n                )\\n            if boolean_filter != {}:\\n                search_query = _approximate_search_query_with_boolean_filter(\\n                    embedding, boolean_filter, size, k, vector_field, subquery_clause\\n                )\\n            elif lucene_filter != {}:\\n                search_query = _approximate_search_query_with_lucene_filter(\\n                    embedding, lucene_filter, size, k, vector_field\\n                )\\n            else:\\n                search_query = _default_approximate_search_query(\\n                    embedding, size, k, vector_field\\n                )\\n        elif search_type == SCRIPT_SCORING_SEARCH:\\n            space_type = _get_kwargs_value(kwargs, \"space_type\", \"l2\")\\n            pre_filter = _get_kwargs_value(kwargs, \"pre_filter\", MATCH_ALL_QUERY)\\n            search_query = _default_script_query(\\n                embedding, space_type, pre_filter, vector_field\\n            )\\n        elif search_type == PAINLESS_SCRIPTING_SEARCH:\\n            space_type = _get_kwargs_value(kwargs, \"space_type\", \"l2Squared\")\\n            pre_filter = _get_kwargs_value(kwargs, \"pre_filter\", MATCH_ALL_QUERY)\\n            search_query = _default_painless_scripting_query(\\n                embedding, space_type, pre_filter, vector_field\\n            )\\n        else:\\n            raise ValueError(\"Invalid `search_type` provided as an argument\")\\n        response = self.client.search(index=self.index_name, body=search_query)\\n        hits = [hit[\"_source\"] for hit in response[\"hits\"][\"hits\"][:k]]\\n        documents = [\\n            Document(\\n                page_content=hit[text_field],\\n                metadata=hit\\n                if metadata_field == \"*\" or metadata_field not in hit\\n                else hit[metadata_field],\\n            )\\n            for hit in hits\\n        ]\\n        return documents\\n[docs]    @classmethod\\n    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        bulk_size: int = 500,\\n        **kwargs: Any,\\n    ) -> OpenSearchVectorSearch:\\n        \"\"\"Construct OpenSearchVectorSearch wrapper from raw documents.\\n        Example:\\n            .. code-block:: python\\n                from langchain import OpenSearchVectorSearch\\n                from langchain.embeddings import OpenAIEmbeddings\\n                embeddings = OpenAIEmbeddings()\\n                opensearch_vector_search = OpenSearchVectorSearch.from_texts(\\n                    texts,\\n                    embeddings,\\n                    opensearch_url=\"http://localhost:9200\"\\n                )\\n        OpenSearch by default supports Approximate Search powered by nmslib, faiss\\n        and lucene engines recommended for large datasets. Also supports brute force\\n        search through Script Scoring and Painless Scripting.\\n        Optional Args:\\n            vector_field: Document field embeddings are stored in. Defaults to\\n            \"vector_field\".\\n            text_field: Document field the text of the document is stored in. Defaults\\n            to \"text\".\\n        Optional Keyword Args for Approximate Search:\\n            engine: \"nmslib\", \"faiss\", \"lucene\"; default: \"nmslib\"\\n            space_type: \"l2\", \"l1\", \"cosinesimil\", \"linf\", \"innerproduct\"; default: \"l2\"\\n            ef_search: Size of the dynamic list used during k-NN searches. Higher values\\n            lead to more accurate but slower searches; default: 512\\n            ef_construction: Size of the dynamic list used during k-NN graph creation.\\n            Higher values lead to more accurate graph but slower indexing speed;\\n            default: 512\\n            m: Number of bidirectional links created for each new element. Large impact\\n            on memory consumption. Between 2 and 100; default: 16\\n        Keyword Args for Script Scoring or Painless Scripting:\\n            is_appx_search: False\\n        \"\"\"\\n        opensearch_url = get_from_dict_or_env(\\n            kwargs, \"opensearch_url\", \"OPENSEARCH_URL\"\\n        )\\n        # List of arguments that needs to be removed from kwargs\\n        # before passing kwargs to get opensearch client\\n        keys_list = [\\n            \"opensearch_url\",\\n            \"index_name\",\\n            \"is_appx_search\",\\n            \"vector_field\",\\n            \"text_field\",\\n            \"engine\",\\n            \"space_type\",\\n            \"ef_search\",\\n            \"ef_construction\",\\n            \"m\",\\n        ]\\n        embeddings = embedding.embed_documents(texts)\\n        _validate_embeddings_and_bulk_size(len(embeddings), bulk_size)\\n        dim = len(embeddings[0])\\n        # Get the index name from either from kwargs or ENV Variable\\n        # before falling back to random generation\\n        index_name = get_from_dict_or_env(\\n            kwargs, \"index_name\", \"OPENSEARCH_INDEX_NAME\", default=uuid.uuid4().hex\\n        )\\n        is_appx_search = _get_kwargs_value(kwargs, \"is_appx_search\", True)\\n        vector_field = _get_kwargs_value(kwargs, \"vector_field\", \"vector_field\")\\n        text_field = _get_kwargs_value(kwargs, \"text_field\", \"text\")\\n        if is_appx_search:\\n            engine = _get_kwargs_value(kwargs, \"engine\", \"nmslib\")\\n            space_type = _get_kwargs_value(kwargs, \"space_type\", \"l2\")\\n            ef_search = _get_kwargs_value(kwargs, \"ef_search\", 512)\\n            ef_construction = _get_kwargs_value(kwargs, \"ef_construction\", 512)\\n            m = _get_kwargs_value(kwargs, \"m\", 16)\\n            mapping = _default_text_mapping(\\n                dim, engine, space_type, ef_search, ef_construction, m, vector_field\\n            )\\n        else:\\n            mapping = _default_scripting_text_mapping(dim)\\n        [kwargs.pop(key, None) for key in keys_list]\\n        client = _get_opensearch_client(opensearch_url, **kwargs)\\n        client.indices.create(index=index_name, body=mapping)\\n        _bulk_ingest_embeddings(\\n            client, index_name, embeddings, texts, metadatas, vector_field, text_field\\n        )\\n        return cls(opensearch_url, index_name, embedding)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\vectorstores\\\\opensearch_vector_search.html'}),\n",
              " Document(page_content='Source code for langchain.vectorstores.pinecone\\n\"\"\"Wrapper around Pinecone vector database.\"\"\"\\nfrom __future__ import annotations\\nimport uuid\\nfrom typing import Any, Callable, Iterable, List, Optional, Tuple\\nfrom langchain.docstore.document import Document\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.vectorstores.base import VectorStore\\n[docs]class Pinecone(VectorStore):\\n    \"\"\"Wrapper around Pinecone vector database.\\n    To use, you should have the ``pinecone-client`` python package installed.\\n    Example:\\n        .. code-block:: python\\n            from langchain.vectorstores import Pinecone\\n            from langchain.embeddings.openai import OpenAIEmbeddings\\n            import pinecone\\n            # The environment should be the one specified next to the API key\\n            # in your Pinecone console\\n            pinecone.init(api_key=\"***\", environment=\"...\")\\n            index = pinecone.Index(\"langchain-demo\")\\n            embeddings = OpenAIEmbeddings()\\n            vectorstore = Pinecone(index, embeddings.embed_query, \"text\")\\n    \"\"\"\\n    def __init__(\\n        self,\\n        index: Any,\\n        embedding_function: Callable,\\n        text_key: str,\\n        namespace: Optional[str] = None,\\n    ):\\n        \"\"\"Initialize with Pinecone client.\"\"\"\\n        try:\\n            import pinecone\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import pinecone python package. \"\\n                \"Please install it with `pip install pinecone-client`.\"\\n            )\\n        if not isinstance(index, pinecone.index.Index):\\n            raise ValueError(\\n                f\"client should be an instance of pinecone.index.Index, \"\\n                f\"got {type(index)}\"\\n            )\\n        self._index = index\\n        self._embedding_function = embedding_function\\n        self._text_key = text_key\\n        self._namespace = namespace\\n[docs]    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        namespace: Optional[str] = None,\\n        batch_size: int = 32,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\\n        Args:\\n            texts: Iterable of strings to add to the vectorstore.\\n            metadatas: Optional list of metadatas associated with the texts.\\n            ids: Optional list of ids to associate with the texts.\\n            namespace: Optional pinecone namespace to add the texts to.\\n        Returns:\\n            List of ids from adding the texts into the vectorstore.\\n        \"\"\"\\n        if namespace is None:\\n            namespace = self._namespace\\n        # Embed and create the documents\\n        docs = []\\n        ids = ids or [str(uuid.uuid4()) for _ in texts]\\n        for i, text in enumerate(texts):\\n            embedding = self._embedding_function(text)\\n            metadata = metadatas[i] if metadatas else {}\\n            metadata[self._text_key] = text\\n            docs.append((ids[i], embedding, metadata))\\n        # upsert to Pinecone\\n        self._index.upsert(vectors=docs, namespace=namespace, batch_size=batch_size)\\n        return ids\\n[docs]    def similarity_search_with_score(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        filter: Optional[dict] = None,\\n        namespace: Optional[str] = None,\\n    ) -> List[Tuple[Document, float]]:\\n        \"\"\"Return pinecone documents most similar to query, along with scores.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            filter: Dictionary of argument(s) to filter on metadata\\n            namespace: Namespace to search in. Default will search in \\'\\' namespace.\\n        Returns:\\n            List of Documents most similar to the query and score for each\\n        \"\"\"\\n        if namespace is None:\\n            namespace = self._namespace\\n        query_obj = self._embedding_function(query)\\n        docs = []\\n        results = self._index.query(\\n            [query_obj],\\n            top_k=k,\\n            include_metadata=True,\\n            namespace=namespace,\\n            filter=filter,\\n        )\\n        for res in results[\"matches\"]:\\n            metadata = res[\"metadata\"]\\n            text = metadata.pop(self._text_key)\\n            docs.append((Document(page_content=text, metadata=metadata), res[\"score\"]))\\n        return docs\\n[docs]    def similarity_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        filter: Optional[dict] = None,\\n        namespace: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        \"\"\"Return pinecone documents most similar to query.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            filter: Dictionary of argument(s) to filter on metadata\\n            namespace: Namespace to search in. Default will search in \\'\\' namespace.\\n        Returns:\\n            List of Documents most similar to the query and score for each\\n        \"\"\"\\n        if namespace is None:\\n            namespace = self._namespace\\n        query_obj = self._embedding_function(query)\\n        docs = []\\n        results = self._index.query(\\n            [query_obj],\\n            top_k=k,\\n            include_metadata=True,\\n            namespace=namespace,\\n            filter=filter,\\n        )\\n        for res in results[\"matches\"]:\\n            metadata = res[\"metadata\"]\\n            text = metadata.pop(self._text_key)\\n            docs.append(Document(page_content=text, metadata=metadata))\\n        return docs\\n[docs]    @classmethod\\n    def from_texts(\\n        cls,\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        batch_size: int = 32,\\n        text_key: str = \"text\",\\n        index_name: Optional[str] = None,\\n        namespace: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> Pinecone:\\n        \"\"\"Construct Pinecone wrapper from raw documents.\\n        This is a user friendly interface that:\\n            1. Embeds documents.\\n            2. Adds the documents to a provided Pinecone index\\n        This is intended to be a quick way to get started.\\n        Example:\\n            .. code-block:: python\\n                from langchain import Pinecone\\n                from langchain.embeddings import OpenAIEmbeddings\\n                import pinecone\\n                # The environment should be the one specified next to the API key\\n                # in your Pinecone console\\n                pinecone.init(api_key=\"***\", environment=\"...\")\\n                embeddings = OpenAIEmbeddings()\\n                pinecone = Pinecone.from_texts(\\n                    texts,\\n                    embeddings,\\n                    index_name=\"langchain-demo\"\\n                )\\n        \"\"\"\\n        try:\\n            import pinecone\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import pinecone python package. \"\\n                \"Please install it with `pip install pinecone-client`.\"\\n            )\\n        indexes = pinecone.list_indexes()  # checks if provided index exists\\n        if index_name in indexes:\\n            index = pinecone.Index(index_name)\\n        elif len(indexes) == 0:\\n            raise ValueError(\\n                \"No active indexes found in your Pinecone project, \"\\n                \"are you sure you\\'re using the right API key and environment?\"\\n            )\\n        else:\\n            raise ValueError(\\n                f\"Index \\'{index_name}\\' not found in your Pinecone project. \"\\n                f\"Did you mean one of the following indexes: {\\', \\'.join(indexes)}\"\\n            )\\n        for i in range(0, len(texts), batch_size):\\n            # set end position of batch\\n            i_end = min(i + batch_size, len(texts))\\n            # get batch of texts and ids\\n            lines_batch = texts[i:i_end]\\n            # create ids if not provided\\n            if ids:\\n                ids_batch = ids[i:i_end]\\n            else:\\n                ids_batch = [str(uuid.uuid4()) for n in range(i, i_end)]\\n            # create embeddings\\n            embeds = embedding.embed_documents(lines_batch)\\n            # prep metadata and upsert batch\\n            if metadatas:\\n                metadata = metadatas[i:i_end]\\n            else:\\n                metadata = [{} for _ in range(i, i_end)]\\n            for j, line in enumerate(lines_batch):\\n                metadata[j][text_key] = line\\n            to_upsert = zip(ids_batch, embeds, metadata)\\n            # upsert to Pinecone\\n            index.upsert(vectors=list(to_upsert), namespace=namespace)\\n        return cls(index, embedding.embed_query, text_key, namespace)\\n[docs]    @classmethod\\n    def from_existing_index(\\n        cls,\\n        index_name: str,\\n        embedding: Embeddings,\\n        text_key: str = \"text\",\\n        namespace: Optional[str] = None,\\n    ) -> Pinecone:\\n        \"\"\"Load pinecone vectorstore from index name.\"\"\"\\n        try:\\n            import pinecone\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import pinecone python package. \"\\n                \"Please install it with `pip install pinecone-client`.\"\\n            )\\n        return cls(\\n            pinecone.Index(index_name), embedding.embed_query, text_key, namespace\\n        )\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\vectorstores\\\\pinecone.html'}),\n",
              " Document(page_content='Source code for langchain.vectorstores.qdrant\\n\"\"\"Wrapper around Qdrant vector database.\"\"\"\\nfrom __future__ import annotations\\nimport uuid\\nfrom operator import itemgetter\\nfrom typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Type, Union\\nfrom langchain.docstore.document import Document\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.vectorstores import VectorStore\\nfrom langchain.vectorstores.utils import maximal_marginal_relevance\\nMetadataFilter = Dict[str, Union[str, int, bool]]\\n[docs]class Qdrant(VectorStore):\\n    \"\"\"Wrapper around Qdrant vector database.\\n    To use you should have the ``qdrant-client`` package installed.\\n    Example:\\n        .. code-block:: python\\n            from qdrant_client import QdrantClient\\n            from langchain import Qdrant\\n            client = QdrantClient()\\n            collection_name = \"MyCollection\"\\n            qdrant = Qdrant(client, collection_name, embedding_function)\\n    \"\"\"\\n    CONTENT_KEY = \"page_content\"\\n    METADATA_KEY = \"metadata\"\\n    def __init__(\\n        self,\\n        client: Any,\\n        collection_name: str,\\n        embedding_function: Callable,\\n        content_payload_key: str = CONTENT_KEY,\\n        metadata_payload_key: str = METADATA_KEY,\\n    ):\\n        \"\"\"Initialize with necessary components.\"\"\"\\n        try:\\n            import qdrant_client\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import qdrant-client python package. \"\\n                \"Please install it with `pip install qdrant-client`.\"\\n            )\\n        if not isinstance(client, qdrant_client.QdrantClient):\\n            raise ValueError(\\n                f\"client should be an instance of qdrant_client.QdrantClient, \"\\n                f\"got {type(client)}\"\\n            )\\n        self.client: qdrant_client.QdrantClient = client\\n        self.collection_name = collection_name\\n        self.embedding_function = embedding_function\\n        self.content_payload_key = content_payload_key or self.CONTENT_KEY\\n        self.metadata_payload_key = metadata_payload_key or self.METADATA_KEY\\n[docs]    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\\n        Args:\\n            texts: Iterable of strings to add to the vectorstore.\\n            metadatas: Optional list of metadatas associated with the texts.\\n        Returns:\\n            List of ids from adding the texts into the vectorstore.\\n        \"\"\"\\n        from qdrant_client.http import models as rest\\n        ids = [uuid.uuid4().hex for _ in texts]\\n        self.client.upsert(\\n            collection_name=self.collection_name,\\n            points=rest.Batch.construct(\\n                ids=ids,\\n                vectors=[self.embedding_function(text) for text in texts],\\n                payloads=self._build_payloads(\\n                    texts,\\n                    metadatas,\\n                    self.content_payload_key,\\n                    self.metadata_payload_key,\\n                ),\\n            ),\\n        )\\n        return ids\\n[docs]    def similarity_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        filter: Optional[MetadataFilter] = None,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        \"\"\"Return docs most similar to query.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            filter: Filter by metadata. Defaults to None.\\n        Returns:\\n            List of Documents most similar to the query.\\n        \"\"\"\\n        results = self.similarity_search_with_score(query, k, filter)\\n        return list(map(itemgetter(0), results))\\n[docs]    def similarity_search_with_score(\\n        self, query: str, k: int = 4, filter: Optional[MetadataFilter] = None\\n    ) -> List[Tuple[Document, float]]:\\n        \"\"\"Return docs most similar to query.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            filter: Filter by metadata. Defaults to None.\\n        Returns:\\n            List of Documents most similar to the query and score for each.\\n        \"\"\"\\n        embedding = self.embedding_function(query)\\n        results = self.client.search(\\n            collection_name=self.collection_name,\\n            query_vector=embedding,\\n            query_filter=self._qdrant_filter_from_dict(filter),\\n            with_payload=True,\\n            limit=k,\\n        )\\n        return [\\n            (\\n                self._document_from_scored_point(\\n                    result, self.content_payload_key, self.metadata_payload_key\\n                ),\\n                result.score,\\n            )\\n            for result in results\\n        ]\\n[docs]    def max_marginal_relevance_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        fetch_k: int = 20,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        \"\"\"Return docs selected using the maximal marginal relevance.\\n        Maximal marginal relevance optimizes for similarity to query AND diversity\\n        among selected documents.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n                     Defaults to 20.\\n        Returns:\\n            List of Documents selected by maximal marginal relevance.\\n        \"\"\"\\n        embedding = self.embedding_function(query)\\n        results = self.client.search(\\n            collection_name=self.collection_name,\\n            query_vector=embedding,\\n            with_payload=True,\\n            with_vectors=True,\\n            limit=fetch_k,\\n        )\\n        embeddings = [result.vector for result in results]\\n        mmr_selected = maximal_marginal_relevance(embedding, embeddings, k=k)\\n        return [\\n            self._document_from_scored_point(\\n                results[i], self.content_payload_key, self.metadata_payload_key\\n            )\\n            for i in mmr_selected\\n        ]\\n[docs]    @classmethod\\n    def from_texts(\\n        cls: Type[Qdrant],\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        location: Optional[str] = None,\\n        url: Optional[str] = None,\\n        port: Optional[int] = 6333,\\n        grpc_port: int = 6334,\\n        prefer_grpc: bool = False,\\n        https: Optional[bool] = None,\\n        api_key: Optional[str] = None,\\n        prefix: Optional[str] = None,\\n        timeout: Optional[float] = None,\\n        host: Optional[str] = None,\\n        path: Optional[str] = None,\\n        collection_name: Optional[str] = None,\\n        distance_func: str = \"Cosine\",\\n        content_payload_key: str = CONTENT_KEY,\\n        metadata_payload_key: str = METADATA_KEY,\\n        **kwargs: Any,\\n    ) -> Qdrant:\\n        \"\"\"Construct Qdrant wrapper from a list of texts.\\n        Args:\\n            texts: A list of texts to be indexed in Qdrant.\\n            embedding: A subclass of `Embeddings`, responsible for text vectorization.\\n            metadatas:\\n                An optional list of metadata. If provided it has to be of the same\\n                length as a list of texts.\\n            location:\\n                If `:memory:` - use in-memory Qdrant instance.\\n                If `str` - use it as a `url` parameter.\\n                If `None` - fallback to relying on `host` and `port` parameters.\\n            url: either host or str of \"Optional[scheme], host, Optional[port],\\n                Optional[prefix]\". Default: `None`\\n            port: Port of the REST API interface. Default: 6333\\n            grpc_port: Port of the gRPC interface. Default: 6334\\n            prefer_grpc:\\n                If true - use gPRC interface whenever possible in custom methods.\\n                Default: False\\n            https: If true - use HTTPS(SSL) protocol. Default: None\\n            api_key: API key for authentication in Qdrant Cloud. Default: None\\n            prefix:\\n                If not None - add prefix to the REST URL path.\\n                Example: service/v1 will result in\\n                    http://localhost:6333/service/v1/{qdrant-endpoint} for REST API.\\n                Default: None\\n            timeout:\\n                Timeout for REST and gRPC API requests.\\n                Default: 5.0 seconds for REST and unlimited for gRPC\\n            host:\\n                Host name of Qdrant service. If url and host are None, set to\\n                \\'localhost\\'. Default: None\\n            path:\\n                Path in which the vectors will be stored while using local mode.\\n                Default: None\\n            collection_name:\\n                Name of the Qdrant collection to be used. If not provided,\\n                it will be created randomly. Default: None\\n            distance_func:\\n                Distance function. One of: \"Cosine\" / \"Euclid\" / \"Dot\".\\n                Default: \"Cosine\"\\n            content_payload_key:\\n                A payload key used to store the content of the document.\\n                Default: \"page_content\"\\n            metadata_payload_key:\\n                A payload key used to store the metadata of the document.\\n                Default: \"metadata\"\\n            **kwargs:\\n                Additional arguments passed directly into REST client initialization\\n        This is a user friendly interface that:\\n            1. Creates embeddings, one for each text\\n            2. Initializes the Qdrant database as an in-memory docstore by default\\n               (and overridable to a remote docstore)\\n            3. Adds the text embeddings to the Qdrant database\\n        This is intended to be a quick way to get started.\\n        Example:\\n            .. code-block:: python\\n                from langchain import Qdrant\\n                from langchain.embeddings import OpenAIEmbeddings\\n                embeddings = OpenAIEmbeddings()\\n                qdrant = Qdrant.from_texts(texts, embeddings, \"localhost\")\\n        \"\"\"\\n        try:\\n            import qdrant_client\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import qdrant-client python package. \"\\n                \"Please install it with `pip install qdrant-client`.\"\\n            )\\n        from qdrant_client.http import models as rest\\n        # Just do a single quick embedding to get vector size\\n        partial_embeddings = embedding.embed_documents(texts[:1])\\n        vector_size = len(partial_embeddings[0])\\n        collection_name = collection_name or uuid.uuid4().hex\\n        distance_func = distance_func.upper()\\n        client = qdrant_client.QdrantClient(\\n            location=location,\\n            url=url,\\n            port=port,\\n            grpc_port=grpc_port,\\n            prefer_grpc=prefer_grpc,\\n            https=https,\\n            api_key=api_key,\\n            prefix=prefix,\\n            timeout=timeout,\\n            host=host,\\n            path=path,\\n            **kwargs,\\n        )\\n        client.recreate_collection(\\n            collection_name=collection_name,\\n            vectors_config=rest.VectorParams(\\n                size=vector_size,\\n                distance=rest.Distance[distance_func],\\n            ),\\n        )\\n        # Now generate the embeddings for all the texts\\n        embeddings = embedding.embed_documents(texts)\\n        client.upsert(\\n            collection_name=collection_name,\\n            points=rest.Batch.construct(\\n                ids=[uuid.uuid4().hex for _ in texts],\\n                vectors=embeddings,\\n                payloads=cls._build_payloads(\\n                    texts, metadatas, content_payload_key, metadata_payload_key\\n                ),\\n            ),\\n        )\\n        return cls(\\n            client=client,\\n            collection_name=collection_name,\\n            embedding_function=embedding.embed_query,\\n            content_payload_key=content_payload_key,\\n            metadata_payload_key=metadata_payload_key,\\n        )\\n    @classmethod\\n    def _build_payloads(\\n        cls,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]],\\n        content_payload_key: str,\\n        metadata_payload_key: str,\\n    ) -> List[dict]:\\n        payloads = []\\n        for i, text in enumerate(texts):\\n            if text is None:\\n                raise ValueError(\\n                    \"At least one of the texts is None. Please remove it before \"\\n                    \"calling .from_texts or .add_texts on Qdrant instance.\"\\n                )\\n            metadata = metadatas[i] if metadatas is not None else None\\n            payloads.append(\\n                {\\n                    content_payload_key: text,\\n                    metadata_payload_key: metadata,\\n                }\\n            )\\n        return payloads\\n    @classmethod\\n    def _document_from_scored_point(\\n        cls,\\n        scored_point: Any,\\n        content_payload_key: str,\\n        metadata_payload_key: str,\\n    ) -> Document:\\n        return Document(\\n            page_content=scored_point.payload.get(content_payload_key),\\n            metadata=scored_point.payload.get(metadata_payload_key) or {},\\n        )\\n    def _qdrant_filter_from_dict(self, filter: Optional[MetadataFilter]) -> Any:\\n        if filter is None or 0 == len(filter):\\n            return None\\n        from qdrant_client.http import models as rest\\n        return rest.Filter(\\n            must=[\\n                rest.FieldCondition(\\n                    key=f\"{self.metadata_payload_key}.{key}\",\\n                    match=rest.MatchValue(value=value),\\n                )\\n                for key, value in filter.items()\\n            ]\\n        )\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\vectorstores\\\\qdrant.html'}),\n",
              " Document(page_content='Source code for langchain.vectorstores.supabase\\nfrom __future__ import annotations\\nfrom itertools import repeat\\nfrom typing import (\\n    TYPE_CHECKING,\\n    Any,\\n    Iterable,\\n    List,\\n    Optional,\\n    Tuple,\\n    Type,\\n    Union,\\n)\\nimport numpy as np\\nfrom langchain.docstore.document import Document\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.vectorstores.base import VectorStore\\nfrom langchain.vectorstores.utils import maximal_marginal_relevance\\nif TYPE_CHECKING:\\n    import supabase\\n[docs]class SupabaseVectorStore(VectorStore):\\n    \"\"\"VectorStore for a Supabase postgres database. Assumes you have the `pgvector`\\n    extension installed and a `match_documents` (or similar) function. For more details:\\n    https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase\\n    You can implement your own `match_documents` function in order to limit the search\\n    space to a subset of documents based on your own authorization or business logic.\\n    Note that the Supabase Python client does not yet support async operations.\\n    If you\\'d like to use `max_marginal_relevance_search`, please review the instructions\\n    below on modifying the `match_documents` function to return matched embeddings.\\n    \"\"\"\\n    _client: supabase.client.Client\\n    # This is the embedding function. Don\\'t confuse with the embedding vectors.\\n    # We should perhaps rename the underlying Embedding base class to EmbeddingFunction\\n    # or something\\n    _embedding: Embeddings\\n    table_name: str\\n    query_name: str\\n    def __init__(\\n        self,\\n        client: supabase.client.Client,\\n        embedding: Embeddings,\\n        table_name: str,\\n        query_name: Union[str, None] = None,\\n    ) -> None:\\n        \"\"\"Initialize with supabase client.\"\"\"\\n        try:\\n            import supabase  # noqa: F401\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import supabase python package. \"\\n                \"Please install it with `pip install supabase`.\"\\n            )\\n        self._client = client\\n        self._embedding: Embeddings = embedding\\n        self.table_name = table_name or \"documents\"\\n        self.query_name = query_name or \"match_documents\"\\n[docs]    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict[Any, Any]]] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        docs = self._texts_to_documents(texts, metadatas)\\n        vectors = self._embedding.embed_documents(list(texts))\\n        return self.add_vectors(vectors, docs)\\n[docs]    @classmethod\\n    def from_texts(\\n        cls: Type[\"SupabaseVectorStore\"],\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        client: Optional[supabase.client.Client] = None,\\n        table_name: Optional[str] = \"documents\",\\n        query_name: Union[str, None] = \"match_documents\",\\n        **kwargs: Any,\\n    ) -> \"SupabaseVectorStore\":\\n        \"\"\"Return VectorStore initialized from texts and embeddings.\"\"\"\\n        if not client:\\n            raise ValueError(\"Supabase client is required.\")\\n        if not table_name:\\n            raise ValueError(\"Supabase document table_name is required.\")\\n        embeddings = embedding.embed_documents(texts)\\n        docs = cls._texts_to_documents(texts, metadatas)\\n        _ids = cls._add_vectors(client, table_name, embeddings, docs)\\n        return cls(\\n            client=client,\\n            embedding=embedding,\\n            table_name=table_name,\\n            query_name=query_name,\\n        )\\n[docs]    def add_vectors(\\n        self, vectors: List[List[float]], documents: List[Document]\\n    ) -> List[str]:\\n        return self._add_vectors(self._client, self.table_name, vectors, documents)\\n[docs]    def similarity_search(\\n        self, query: str, k: int = 4, **kwargs: Any\\n    ) -> List[Document]:\\n        vectors = self._embedding.embed_documents([query])\\n        return self.similarity_search_by_vector(vectors[0], k)\\n[docs]    def similarity_search_by_vector(\\n        self, embedding: List[float], k: int = 4, **kwargs: Any\\n    ) -> List[Document]:\\n        result = self.similarity_search_by_vector_with_relevance_scores(embedding, k)\\n        documents = [doc for doc, _ in result]\\n        return documents\\n[docs]    def similarity_search_with_relevance_scores(\\n        self, query: str, k: int = 4, **kwargs: Any\\n    ) -> List[Tuple[Document, float]]:\\n        vectors = self._embedding.embed_documents([query])\\n        return self.similarity_search_by_vector_with_relevance_scores(vectors[0], k)\\n[docs]    def similarity_search_by_vector_with_relevance_scores(\\n        self, query: List[float], k: int\\n    ) -> List[Tuple[Document, float]]:\\n        match_documents_params = dict(query_embedding=query, match_count=k)\\n        res = self._client.rpc(self.query_name, match_documents_params).execute()\\n        match_result = [\\n            (\\n                Document(\\n                    metadata=search.get(\"metadata\", {}),  # type: ignore\\n                    page_content=search.get(\"content\", \"\"),\\n                ),\\n                search.get(\"similarity\", 0.0),\\n            )\\n            for search in res.data\\n            if search.get(\"content\")\\n        ]\\n        return match_result\\n[docs]    def similarity_search_by_vector_returning_embeddings(\\n        self, query: List[float], k: int\\n    ) -> List[Tuple[Document, float, np.ndarray[np.float32, Any]]]:\\n        match_documents_params = dict(query_embedding=query, match_count=k)\\n        res = self._client.rpc(self.query_name, match_documents_params).execute()\\n        match_result = [\\n            (\\n                Document(\\n                    metadata=search.get(\"metadata\", {}),  # type: ignore\\n                    page_content=search.get(\"content\", \"\"),\\n                ),\\n                search.get(\"similarity\", 0.0),\\n                # Supabase returns a vector type as its string represation (!).\\n                # This is a hack to convert the string to numpy array.\\n                np.fromstring(\\n                    search.get(\"embedding\", \"\").strip(\"[]\"), np.float32, sep=\",\"\\n                ),\\n            )\\n            for search in res.data\\n            if search.get(\"content\")\\n        ]\\n        return match_result\\n    @staticmethod\\n    def _texts_to_documents(\\n        texts: Iterable[str],\\n        metadatas: Optional[Iterable[dict[Any, Any]]] = None,\\n    ) -> List[Document]:\\n        \"\"\"Return list of Documents from list of texts and metadatas.\"\"\"\\n        if metadatas is None:\\n            metadatas = repeat({})\\n        docs = [\\n            Document(page_content=text, metadata=metadata)\\n            for text, metadata in zip(texts, metadatas)\\n        ]\\n        return docs\\n    @staticmethod\\n    def _add_vectors(\\n        client: supabase.client.Client,\\n        table_name: str,\\n        vectors: List[List[float]],\\n        documents: List[Document],\\n    ) -> List[str]:\\n        \"\"\"Add vectors to Supabase table.\"\"\"\\n        rows: List[dict[str, Any]] = [\\n            {\\n                \"content\": documents[idx].page_content,\\n                \"embedding\": embedding,\\n                \"metadata\": documents[idx].metadata,  # type: ignore\\n            }\\n            for idx, embedding in enumerate(vectors)\\n        ]\\n        # According to the SupabaseVectorStore JS implementation, the best chunk size\\n        # is 500\\n        chunk_size = 500\\n        id_list: List[str] = []\\n        for i in range(0, len(rows), chunk_size):\\n            chunk = rows[i : i + chunk_size]\\n            result = client.from_(table_name).insert(chunk).execute()  # type: ignore\\n            if len(result.data) == 0:\\n                raise Exception(\"Error inserting: No rows added\")\\n            # VectorStore.add_vectors returns ids as strings\\n            ids = [str(i.get(\"id\")) for i in result.data if i.get(\"id\")]\\n            id_list.extend(ids)\\n        return id_list\\n[docs]    def max_marginal_relevance_search_by_vector(\\n        self,\\n        embedding: List[float],\\n        k: int = 4,\\n        fetch_k: int = 20,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        \"\"\"Return docs selected using the maximal marginal relevance.\\n        Maximal marginal relevance optimizes for similarity to query AND diversity\\n        among selected documents.\\n        Args:\\n            embedding: Embedding to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n        Returns:\\n            List of Documents selected by maximal marginal relevance.\\n        \"\"\"\\n        result = self.similarity_search_by_vector_returning_embeddings(\\n            embedding, fetch_k\\n        )\\n        matched_documents = [doc_tuple[0] for doc_tuple in result]\\n        matched_embeddings = [doc_tuple[2] for doc_tuple in result]\\n        mmr_selected = maximal_marginal_relevance(\\n            np.array([embedding], dtype=np.float32), matched_embeddings, k=k\\n        )\\n        filtered_documents = [matched_documents[i] for i in mmr_selected]\\n        return filtered_documents\\n[docs]    def max_marginal_relevance_search(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        fetch_k: int = 20,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        \"\"\"Return docs selected using the maximal marginal relevance.\\n        Maximal marginal relevance optimizes for similarity to query AND diversity\\n        among selected documents.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n        Returns:\\n            List of Documents selected by maximal marginal relevance.\\n        `max_marginal_relevance_search` requires that `query_name` returns matched\\n        embeddings alongside the match documents. The following function function\\n        demonstrates how to do this:\\n        ```sql\\n        CREATE FUNCTION match_documents_embeddings(query_embedding vector(1536),\\n                                                   match_count int)\\n            RETURNS TABLE(\\n                id bigint,\\n                content text,\\n                metadata jsonb,\\n                embedding vector(1536),\\n                similarity float)\\n            LANGUAGE plpgsql\\n            AS $$\\n            # variable_conflict use_column\\n        BEGIN\\n            RETURN query\\n            SELECT\\n                id,\\n                content,\\n                metadata,\\n                embedding,\\n                1 -(docstore.embedding <=> query_embedding) AS similarity\\n            FROM\\n                docstore\\n            ORDER BY\\n                docstore.embedding <=> query_embedding\\n            LIMIT match_count;\\n        END;\\n        $$;```\\n        \"\"\"\\n        embedding = self._embedding.embed_documents([query])\\n        docs = self.max_marginal_relevance_search_by_vector(embedding[0], k, fetch_k)\\n        return docs\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\vectorstores\\\\supabase.html'}),\n",
              " Document(page_content='Source code for langchain.vectorstores.weaviate\\n\"\"\"Wrapper around weaviate vector database.\"\"\"\\nfrom __future__ import annotations\\nfrom typing import Any, Dict, Iterable, List, Optional, Type\\nfrom uuid import uuid4\\nimport numpy as np\\nfrom langchain.docstore.document import Document\\nfrom langchain.embeddings.base import Embeddings\\nfrom langchain.utils import get_from_dict_or_env\\nfrom langchain.vectorstores.base import VectorStore\\nfrom langchain.vectorstores.utils import maximal_marginal_relevance\\ndef _default_schema(index_name: str) -> Dict:\\n    return {\\n        \"class\": index_name,\\n        \"properties\": [\\n            {\\n                \"name\": \"text\",\\n                \"dataType\": [\"text\"],\\n            }\\n        ],\\n    }\\n[docs]class Weaviate(VectorStore):\\n    \"\"\"Wrapper around Weaviate vector database.\\n    To use, you should have the ``weaviate-client`` python package installed.\\n    Example:\\n        .. code-block:: python\\n            import weaviate\\n            from langchain.vectorstores import Weaviate\\n            client = weaviate.Client(url=os.environ[\"WEAVIATE_URL\"], ...)\\n            weaviate = Weaviate(client, index_name, text_key)\\n    \"\"\"\\n    def __init__(\\n        self,\\n        client: Any,\\n        index_name: str,\\n        text_key: str,\\n        embedding: Optional[Embeddings] = None,\\n        attributes: Optional[List[str]] = None,\\n    ):\\n        \"\"\"Initialize with Weaviate client.\"\"\"\\n        try:\\n            import weaviate\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import weaviate python package. \"\\n                \"Please install it with `pip install weaviate-client`.\"\\n            )\\n        if not isinstance(client, weaviate.Client):\\n            raise ValueError(\\n                f\"client should be an instance of weaviate.Client, got {type(client)}\"\\n            )\\n        self._client = client\\n        self._index_name = index_name\\n        self._embedding = embedding\\n        self._text_key = text_key\\n        self._query_attrs = [self._text_key]\\n        if attributes is not None:\\n            self._query_attrs.extend(attributes)\\n[docs]    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        \"\"\"Upload texts with metadata (properties) to Weaviate.\"\"\"\\n        from weaviate.util import get_valid_uuid\\n        with self._client.batch as batch:\\n            ids = []\\n            for i, doc in enumerate(texts):\\n                data_properties = {\\n                    self._text_key: doc,\\n                }\\n                if metadatas is not None:\\n                    for key in metadatas[i].keys():\\n                        data_properties[key] = metadatas[i][key]\\n                _id = get_valid_uuid(uuid4())\\n                batch.add_data_object(\\n                    data_object=data_properties, class_name=self._index_name, uuid=_id\\n                )\\n                ids.append(_id)\\n        return ids\\n[docs]    def similarity_search(\\n        self, query: str, k: int = 4, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs most similar to query.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n        Returns:\\n            List of Documents most similar to the query.\\n        \"\"\"\\n        content: Dict[str, Any] = {\"concepts\": [query]}\\n        if kwargs.get(\"search_distance\"):\\n            content[\"certainty\"] = kwargs.get(\"search_distance\")\\n        query_obj = self._client.query.get(self._index_name, self._query_attrs)\\n        result = query_obj.with_near_text(content).with_limit(k).do()\\n        if \"errors\" in result:\\n            raise ValueError(f\"Error during query: {result[\\'errors\\']}\")\\n        docs = []\\n        for res in result[\"data\"][\"Get\"][self._index_name]:\\n            text = res.pop(self._text_key)\\n            docs.append(Document(page_content=text, metadata=res))\\n        return docs\\n[docs]    def similarity_search_by_vector(\\n        self, embedding: List[float], k: int = 4, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Look up similar documents by embedding vector in Weaviate.\"\"\"\\n        vector = {\"vector\": embedding}\\n        query_obj = self._client.query.get(self._index_name, self._query_attrs)\\n        result = query_obj.with_near_vector(vector).with_limit(k).do()\\n        if \"errors\" in result:\\n            raise ValueError(f\"Error during query: {result[\\'errors\\']}\")\\n        docs = []\\n        for res in result[\"data\"][\"Get\"][self._index_name]:\\n            text = res.pop(self._text_key)\\n            docs.append(Document(page_content=text, metadata=res))\\n        return docs\\n[docs]    def max_marginal_relevance_search(\\n        self, query: str, k: int = 4, fetch_k: int = 20, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Return docs selected using the maximal marginal relevance.\\n        Maximal marginal relevance optimizes for similarity to query AND diversity\\n        among selected documents.\\n        Args:\\n            query: Text to look up documents similar to.\\n            k: Number of Documents to return. Defaults to 4.\\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\\n        Returns:\\n            List of Documents selected by maximal marginal relevance.\\n        \"\"\"\\n        lambda_mult = kwargs.get(\"lambda_mult\", 0.5)\\n        if self._embedding is not None:\\n            embedding = self._embedding.embed_query(query)\\n        else:\\n            raise ValueError(\\n                \"max_marginal_relevance_search requires a suitable Embeddings object\"\\n            )\\n        vector = {\"vector\": embedding}\\n        query_obj = self._client.query.get(self._index_name, self._query_attrs)\\n        results = (\\n            query_obj.with_additional(\"vector\")\\n            .with_near_vector(vector)\\n            .with_limit(fetch_k)\\n            .do()\\n        )\\n        payload = results[\"data\"][\"Get\"][self._index_name]\\n        embeddings = [result[\"_additional\"][\"vector\"] for result in payload]\\n        mmr_selected = maximal_marginal_relevance(\\n            np.array(embedding), embeddings, k=k, lambda_mult=lambda_mult\\n        )\\n        docs = []\\n        for idx in mmr_selected:\\n            text = payload[idx].pop(self._text_key)\\n            payload[idx].pop(\"_additional\")\\n            meta = payload[idx]\\n            docs.append(Document(page_content=text, metadata=meta))\\n        return docs\\n[docs]    @classmethod\\n    def from_texts(\\n        cls: Type[Weaviate],\\n        texts: List[str],\\n        embedding: Embeddings,\\n        metadatas: Optional[List[dict]] = None,\\n        **kwargs: Any,\\n    ) -> Weaviate:\\n        \"\"\"Construct Weaviate wrapper from raw documents.\\n        This is a user-friendly interface that:\\n            1. Embeds documents.\\n            2. Creates a new index for the embeddings in the Weaviate instance.\\n            3. Adds the documents to the newly created Weaviate index.\\n        This is intended to be a quick way to get started.\\n        Example:\\n            .. code-block:: python\\n                from langchain.vectorstores.weaviate import Weaviate\\n                from langchain.embeddings import OpenAIEmbeddings\\n                embeddings = OpenAIEmbeddings()\\n                weaviate = Weaviate.from_texts(\\n                    texts,\\n                    embeddings,\\n                    weaviate_url=\"http://localhost:8080\"\\n                )\\n        \"\"\"\\n        weaviate_url = get_from_dict_or_env(kwargs, \"weaviate_url\", \"WEAVIATE_URL\")\\n        try:\\n            from weaviate import Client\\n            from weaviate.util import get_valid_uuid\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import weaviate python  package. \"\\n                \"Please install it with `pip instal weaviate-client`\"\\n            )\\n        client = Client(weaviate_url)\\n        index_name = kwargs.get(\"index_name\", f\"LangChain_{uuid4().hex}\")\\n        embeddings = embedding.embed_documents(texts) if embedding else None\\n        text_key = \"text\"\\n        schema = _default_schema(index_name)\\n        attributes = list(metadatas[0].keys()) if metadatas else None\\n        # check whether the index already exists\\n        if not client.schema.contains(schema):\\n            client.schema.create_class(schema)\\n        with client.batch as batch:\\n            for i, text in enumerate(texts):\\n                data_properties = {\\n                    text_key: text,\\n                }\\n                if metadatas is not None:\\n                    for key in metadatas[i].keys():\\n                        data_properties[key] = metadatas[i][key]\\n                _id = get_valid_uuid(uuid4())\\n                # if an embedding strategy is not provided, we let\\n                # weaviate create the embedding. Note that this will only\\n                # work if weaviate has been installed with a vectorizer module\\n                # like text2vec-contextionary for example\\n                params = {\\n                    \"uuid\": _id,\\n                    \"data_object\": data_properties,\\n                    \"class_name\": index_name,\\n                }\\n                if embeddings is not None:\\n                    params[\"vector\"] = embeddings[i]\\n                batch.add_data_object(**params)\\n            batch.flush()\\n        return cls(client, index_name, text_key, embedding, attributes)\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 21, 2023.\\n  ', metadata={'source': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\_modules\\\\langchain\\\\vectorstores\\\\weaviate.html'})]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcIkny_6xiZJ",
        "outputId": "8d4814bf-22cc-4af8-bd95-dae2c511b625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oKw0q8xqyHYr",
        "outputId": "08441141-0548-4d27-8246-6fce90a105e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\glossary.html'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[5].metadata['source'].replace('LangChaindocs\\\\', 'https://')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Rb7KxUqYzsuV"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('p50k_base')\n",
        "\n",
        "# create the length function\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    return len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OKO8e3Dp0dQS"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400,\n",
        "    chunk_overlap=20,\n",
        "    length_function=tiktoken_len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "31aa526b635e4eaea7c7c8101184e1d8",
            "fffa66a9213e4b2e93b626d199156d25",
            "833417b68a9d4b94a57f1595d5a4fdd9",
            "88924dd9e4394d99b942c118be840e65",
            "c08c76a37c324d0393a0852ec315869b",
            "5f61d01ca80e4a9086d91973ff22043b",
            "8f4ecd72531845099d00b3eba1209c9d",
            "8d9cca0327b2425fb6a053f467558c58",
            "e82da63021374013aa2189d53f6eb9c4",
            "047d3e83cd2745879529a34cdf4c52fe",
            "c84a063d79f340a7bde611d9f4e2c739"
          ]
        },
        "id": "uOdPyiAQ0uWs",
        "outputId": "80dc69d1-8ec3-468f-f91b-d400dbc98e1c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 326/326 [00:02<00:00, 158.44it/s]\n"
          ]
        }
      ],
      "source": [
        "from uuid import uuid4\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "chunks = []\n",
        "\n",
        "for idx, record in enumerate(tqdm(docs)):\n",
        "    texts = text_splitter.split_text(record.page_content)\n",
        "    chunks.extend([{\n",
        "        'id': str(uuid4()),\n",
        "        'text': texts[i],\n",
        "        'chunk': i,\n",
        "        'url': record.metadata['source'].replace('rtdocs/', 'https://')\n",
        "    } for i in range(len(texts))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kteZ69Z5M55S"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "# initialize openai API key\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY_FREE\")  #platform.openai.com\n",
        "embed_model = \"text-embedding-ada-002\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "res = openai.Embedding.create(\n",
        "    input=[\n",
        "        \"Sample document text goes here\",\n",
        "        \"there will be several phrases in each batch\"\n",
        "    ], engine=embed_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esagZj6iNLPZ",
        "outputId": "b3d53d15-72ae-4c8f-ab29-8f2ac55835b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['object', 'data', 'model', 'usage'])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVoP9VcINWAC",
        "outputId": "c3bc3623-9149-400f-894a-671d46f00906"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(res['data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-zraDCjNeC6",
        "outputId": "7634cb0e-f755-4931-a66d-08e728c97c4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1536, 1536)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(res['data'][0]['embedding']), len(res['data'][1]['embedding'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO8sbJFZNyIZ",
        "outputId": "62a49c74-1629-4819-d113-e94dbf682e38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'': {'vector_count': 5040}},\n",
              " 'total_vector_count': 5040}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pinecone\n",
        "\n",
        "index_name = 'langchain-docs'\n",
        "\n",
        "# initialize connection to pinecone\n",
        "pinecone.init(\n",
        "    api_key=os.getenv(\"pinecone_api_key\"),  # app.pinecone.io (console)\n",
        "    environment=\"asia-northeast1-gcp\"  # next to API key in console\n",
        ")\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # if does not exist, create index\n",
        "    pinecone.create_index(\n",
        "        index_name,\n",
        "        dimension=len(res['data'][0]['embedding']),\n",
        "        metric='dotproduct'\n",
        "    )\n",
        "# connect to index\n",
        "index = pinecone.Index(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "eadd92cbd46d4d8888c51b2f339b845e",
            "2ae8008c252741879e738da71f5d977d",
            "31f45f7fd88a4080b94565baf25f380f",
            "933f662b8c314369b874a6e46eee3872",
            "91ebdba428564d399d4090ba121531fc",
            "950e452b63d04bea9b810826658a22ba",
            "57469d00b3d84fb585621dbceed72871",
            "3d8ce02491cc470c88d7cf917c21815f",
            "1c06721baa384e2188d5e0d7765dd30c",
            "5ba56e481c62412fb1788a1e56d99cfa",
            "99a48256c20048a58d9d7e6bb58b8458"
          ]
        },
        "id": "iZbFbulAPeop",
        "outputId": "714e410e-08b4-43ba-858c-680598589baa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 18/18 [01:02<00:00,  3.45s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import datetime\n",
        "from time import sleep\n",
        "\n",
        "batch_size = 100  # how many embeddings we create and insert at once\n",
        "\n",
        "for i in tqdm(range(0, len(chunks), batch_size)):\n",
        "    # find end of batch\n",
        "    i_end = min(len(chunks), i+batch_size)\n",
        "    meta_batch = chunks[i:i_end]\n",
        "    # get ids\n",
        "    ids_batch = [x['id'] for x in meta_batch]\n",
        "    # get texts to encode\n",
        "    texts = [x['text'] for x in meta_batch]\n",
        "    # create embeddings (try-except added to avoid RateLimitError)\n",
        "    try:\n",
        "        res = openai.Embedding.create(input=texts, engine=embed_model)\n",
        "    except:\n",
        "        done = False\n",
        "        while not done:\n",
        "            sleep(5)\n",
        "            try:\n",
        "                res = openai.Embedding.create(input=texts, engine=embed_model)\n",
        "                done = True\n",
        "            except:\n",
        "                pass\n",
        "    embeds = [record['embedding'] for record in res['data']]\n",
        "    # cleanup metadata\n",
        "    meta_batch = [{\n",
        "        'text': x['text'],\n",
        "        'chunk': x['chunk'],\n",
        "        'url': x['url']\n",
        "    } for x in meta_batch]\n",
        "    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
        "    # upsert to Pinecone\n",
        "    index.upsert(vectors=to_upsert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FMUPdX9cQQYC"
      },
      "outputs": [],
      "source": [
        "query = \"how do I use the tiktoken\"\n",
        "\n",
        "res = openai.Embedding.create(\n",
        "    input=[query],\n",
        "    engine=embed_model\n",
        ")\n",
        "\n",
        "# retrieve from Pinecone\n",
        "xq = res['data'][0]['embedding']\n",
        "\n",
        "# get relevant contexts (including the questions)\n",
        "res = index.query(xq, top_k=16, include_metadata=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl9SrFPkQjg-",
        "outputId": "a4ec350c-7a19-4bc0-de9c-9838ca58546c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'matches': [{'id': 'ac55e4b2-b80e-48bb-a07a-dd4648dc18b0',\n",
              "              'metadata': {'chunk': 3.0,\n",
              "                           'text': 'for all LLMs, and common utilities for '\n",
              "                                   'working with LLMs.\\\\n\\\\nð\\\\x9f”\\\\x97 '\n",
              "                                   'Chains:\\\\n\\\\nChains go beyond just a '\n",
              "                                   'single LLM call, and are sequences of '\n",
              "                                   'calls (whether to an LLM or a different '\n",
              "                                   'utility). LangChain provides a standard '\n",
              "                                   'interface for chains, lots of integrations '\n",
              "                                   'with other tools, and end-to-end chains '\n",
              "                                   'for common applications.\\\\n\\\\nð\\\\x9f“\\\\x9a '\n",
              "                                   'Data Augmented Generation:\\\\n\\\\nData '\n",
              "                                   'Augmented Generation involves specific '\n",
              "                                   'types of chains that first interact with '\n",
              "                                   'an external datasource to fetch data to '\n",
              "                                   'use in the generation step. Examples of '\n",
              "                                   'this include summarization of long pieces '\n",
              "                                   'of text and question/answering over '\n",
              "                                   'specific data sources.\\\\n\\\\nð\\\\x9f¤\\\\x96 '\n",
              "                                   'Agents:\\\\n\\\\nAgents involve an LLM making '\n",
              "                                   'decisions about which Actions to take, '\n",
              "                                   'taking that Action, seeing an Observation, '\n",
              "                                   'and repeating that until done. LangChain '\n",
              "                                   'provides a standard interface for agents, '\n",
              "                                   'a selection of agents to choose from, and '\n",
              "                                   'examples of end to end',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/markdown.html'},\n",
              "              'score': 0.875683,\n",
              "              'values': []},\n",
              "             {'id': 'fe32467f-41bf-49c8-988f-138d2f0ed5c4',\n",
              "              'metadata': {'chunk': 1.0,\n",
              "                           'text': 'Chains: Chains go beyond just a single LLM '\n",
              "                                   'call, and are sequences of calls (whether '\n",
              "                                   'to an LLM or a different utility). '\n",
              "                                   'LangChain provides a standard interface '\n",
              "                                   'for chains, lots of integrations with '\n",
              "                                   'other tools, and end-to-end chains for '\n",
              "                                   'common applications.\\n'\n",
              "                                   'Agents: Agents involve an LLM making '\n",
              "                                   'decisions about which Actions to take, '\n",
              "                                   'taking that Action, seeing an Observation, '\n",
              "                                   'and repeating that until done. LangChain '\n",
              "                                   'provides a standard interface for agents, '\n",
              "                                   'a selection of agents to choose from, and '\n",
              "                                   'examples of end to end agents.\\n'\n",
              "                                   'Use Cases#\\n'\n",
              "                                   'The above modules can be used in a variety '\n",
              "                                   'of ways. LangChain also provides guidance '\n",
              "                                   'and assistance in this. Below are some of '\n",
              "                                   'the common use cases LangChain supports.\\n'\n",
              "                                   'Autonomous Agents: Autonomous agents are '\n",
              "                                   'long running agents that take many steps '\n",
              "                                   'in an attempt to accomplish an objective. '\n",
              "                                   'Examples include AutoGPT and BabyAGI.\\n'\n",
              "                                   'Agent Simulations: Putting agents in a '\n",
              "                                   'sandbox and observing how they interact '\n",
              "                                   'with each other or to events can be an '\n",
              "                                   'interesting way to observe their long-term '\n",
              "                                   'memory abilities.\\n'\n",
              "                                   'Personal Assistants: The main LangChain '\n",
              "                                   'use case. Personal assistants need to take '\n",
              "                                   'actions, remember interactions, and have '\n",
              "                                   'knowledge about your data.\\n'\n",
              "                                   'Question Answering: The second big '\n",
              "                                   'LangChain use case. Answering questions '\n",
              "                                   'over specific documents, only utilizing '\n",
              "                                   'the information in those documents to '\n",
              "                                   'construct an answer.\\n'\n",
              "                                   'Chatbots: Since language models are good '\n",
              "                                   'at producing text, that makes them ideal '\n",
              "                                   'for creating chatbots.\\n'\n",
              "                                   'Querying Tabular Data: If you want to '\n",
              "                                   'understand how to use LLMs to query data '\n",
              "                                   'that is stored in a tabular format (csvs, '\n",
              "                                   'SQL, dataframes, etc) you should read this '\n",
              "                                   'page.\\n'\n",
              "                                   'Code Understanding: If you want to '\n",
              "                                   'understand how to use LLMs to query source '\n",
              "                                   'code from github, you should read this '\n",
              "                                   'page.',\n",
              "                           'url': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\index.html'},\n",
              "              'score': 0.8711887,\n",
              "              'values': []},\n",
              "             {'id': 'b2c8e5e6-4e47-4575-bd07-a5b8227d5605',\n",
              "              'metadata': {'chunk': 1.0,\n",
              "                           'text': 'Chains: Chains go beyond just a single LLM '\n",
              "                                   'call, and are sequences of calls (whether '\n",
              "                                   'to an LLM or a different utility). '\n",
              "                                   'LangChain provides a standard interface '\n",
              "                                   'for chains, lots of integrations with '\n",
              "                                   'other tools, and end-to-end chains for '\n",
              "                                   'common applications.\\n'\n",
              "                                   'Agents: Agents involve an LLM making '\n",
              "                                   'decisions about which Actions to take, '\n",
              "                                   'taking that Action, seeing an Observation, '\n",
              "                                   'and repeating that until done. LangChain '\n",
              "                                   'provides a standard interface for agents, '\n",
              "                                   'a selection of agents to choose from, and '\n",
              "                                   'examples of end to end agents.\\n'\n",
              "                                   'Use Cases#\\n'\n",
              "                                   'The above modules can be used in a variety '\n",
              "                                   'of ways. LangChain also provides guidance '\n",
              "                                   'and assistance in this. Below are some of '\n",
              "                                   'the common use cases LangChain supports.\\n'\n",
              "                                   'Autonomous Agents: Autonomous agents are '\n",
              "                                   'long running agents that take many steps '\n",
              "                                   'in an attempt to accomplish an objective. '\n",
              "                                   'Examples include AutoGPT and BabyAGI.\\n'\n",
              "                                   'Agent Simulations: Putting agents in a '\n",
              "                                   'sandbox and observing how they interact '\n",
              "                                   'with each other or to events can be an '\n",
              "                                   'interesting way to observe their long-term '\n",
              "                                   'memory abilities.\\n'\n",
              "                                   'Personal Assistants: The main LangChain '\n",
              "                                   'use case. Personal assistants need to take '\n",
              "                                   'actions, remember interactions, and have '\n",
              "                                   'knowledge about your data.\\n'\n",
              "                                   'Question Answering: The second big '\n",
              "                                   'LangChain use case. Answering questions '\n",
              "                                   'over specific documents, only utilizing '\n",
              "                                   'the information in those documents to '\n",
              "                                   'construct an answer.\\n'\n",
              "                                   'Chatbots: Since language models are good '\n",
              "                                   'at producing text, that makes them ideal '\n",
              "                                   'for creating chatbots.\\n'\n",
              "                                   'Querying Tabular Data: If you want to '\n",
              "                                   'understand how to use LLMs to query data '\n",
              "                                   'that is stored in a tabular format (csvs, '\n",
              "                                   'SQL, dataframes, etc) you should read this '\n",
              "                                   'page.\\n'\n",
              "                                   'Code Understanding: If you want to '\n",
              "                                   'understand how to use LLMs to query source '\n",
              "                                   'code from github, you should read this '\n",
              "                                   'page.',\n",
              "                           'url': 'https://python.langchain.com/en/latest/index.html'},\n",
              "              'score': 0.871176839,\n",
              "              'values': []},\n",
              "             {'id': '9a68088c-30d0-4b45-98e9-76ca12d1aef8',\n",
              "              'metadata': {'chunk': 27.0,\n",
              "                           'text': 'in a variety of ways. LangChain also '\n",
              "                                   'provides guidance and assistance in this. '\n",
              "                                   'Below are some of the common use cases '\n",
              "                                   'LangChain supports.\\\\n\\\\nPersonal '\n",
              "                                   'Assistants: The main LangChain use case. '\n",
              "                                   'Personal assistants need to take actions, '\n",
              "                                   'remember interactions, and have knowledge '\n",
              "                                   'about your data.\\\\nQuestion Answering: The '\n",
              "                                   'second big LangChain use case. Answering '\n",
              "                                   'questions over specific documents, only '\n",
              "                                   'utilizing the information in those '\n",
              "                                   'documents to construct an '\n",
              "                                   'answer.\\\\nChatbots: Since language models '\n",
              "                                   'are good at producing text, that makes '\n",
              "                                   'them ideal for creating '\n",
              "                                   'chatbots.\\\\nQuerying Tabular Data: If you '\n",
              "                                   'want to understand how to use LLMs to '\n",
              "                                   'query data that is stored in a tabular '\n",
              "                                   'format (csvs, SQL, dataframes, etc) you '\n",
              "                                   'should read this page.\\\\nInteracting with '\n",
              "                                   'APIs: Enabling LLMs to interact with APIs '\n",
              "                                   'is extremely powerful in order to give '\n",
              "                                   'them more up-to-date information and allow '\n",
              "                                   'them to take actions.\\\\nExtraction: '\n",
              "                                   'Extract structured information from '\n",
              "                                   'text.\\\\nSummarization: Summarizing longer '\n",
              "                                   'documents into',\n",
              "                           'url': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\indexes\\\\document_loaders\\\\examples\\\\sitemap.html'},\n",
              "              'score': 0.862068176,\n",
              "              'values': []},\n",
              "             {'id': '5b937aaf-549c-4490-b58b-d7dd4f2e9744',\n",
              "              'metadata': {'chunk': 27.0,\n",
              "                           'text': 'in a variety of ways. LangChain also '\n",
              "                                   'provides guidance and assistance in this. '\n",
              "                                   'Below are some of the common use cases '\n",
              "                                   'LangChain supports.\\\\n\\\\nPersonal '\n",
              "                                   'Assistants: The main LangChain use case. '\n",
              "                                   'Personal assistants need to take actions, '\n",
              "                                   'remember interactions, and have knowledge '\n",
              "                                   'about your data.\\\\nQuestion Answering: The '\n",
              "                                   'second big LangChain use case. Answering '\n",
              "                                   'questions over specific documents, only '\n",
              "                                   'utilizing the information in those '\n",
              "                                   'documents to construct an '\n",
              "                                   'answer.\\\\nChatbots: Since language models '\n",
              "                                   'are good at producing text, that makes '\n",
              "                                   'them ideal for creating '\n",
              "                                   'chatbots.\\\\nQuerying Tabular Data: If you '\n",
              "                                   'want to understand how to use LLMs to '\n",
              "                                   'query data that is stored in a tabular '\n",
              "                                   'format (csvs, SQL, dataframes, etc) you '\n",
              "                                   'should read this page.\\\\nInteracting with '\n",
              "                                   'APIs: Enabling LLMs to interact with APIs '\n",
              "                                   'is extremely powerful in order to give '\n",
              "                                   'them more up-to-date information and allow '\n",
              "                                   'them to take actions.\\\\nExtraction: '\n",
              "                                   'Extract structured information from '\n",
              "                                   'text.\\\\nSummarization: Summarizing longer '\n",
              "                                   'documents into',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'},\n",
              "              'score': 0.862068176,\n",
              "              'values': []},\n",
              "             {'id': 'c27ad49e-43eb-442a-bbf8-8e4cb5eb5a49',\n",
              "              'metadata': {'chunk': 9.0,\n",
              "                           'text': 'are several main modules that LangChain '\n",
              "                                   'provides support for.\\\\nFor each module we '\n",
              "                                   'provide some examples to get started, '\n",
              "                                   'how-to guides, reference docs, and '\n",
              "                                   'conceptual guides.\\\\nThese modules are, in '\n",
              "                                   'increasing order of '\n",
              "                                   'complexity:\\\\n\\\\nPrompts: This includes '\n",
              "                                   'prompt management, prompt optimization, '\n",
              "                                   'and prompt serialization.\\\\nLLMs: This '\n",
              "                                   'includes a generic interface for all LLMs, '\n",
              "                                   'and common utilities for working with '\n",
              "                                   'LLMs.\\\\nDocument Loaders: This includes a '\n",
              "                                   'standard interface for loading documents, '\n",
              "                                   'as well as specific integrations to all '\n",
              "                                   'types of text data sources.\\\\nUtils: '\n",
              "                                   'Language models are often more powerful '\n",
              "                                   'when interacting with other sources of '\n",
              "                                   'knowledge or computation. This can include '\n",
              "                                   'Python REPLs, embeddings, search engines, '\n",
              "                                   'and more. LangChain provides a large '\n",
              "                                   'collection of common utils to use in your '\n",
              "                                   'application.\\\\nChains: Chains go beyond '\n",
              "                                   'just a single LLM call, and are sequences '\n",
              "                                   'of calls (whether to an LLM or a different '\n",
              "                                   'utility). LangChain provides a standard '\n",
              "                                   'interface for chains, lots of integrations '\n",
              "                                   'with other',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'},\n",
              "              'score': 0.854617953,\n",
              "              'values': []},\n",
              "             {'id': '0b3e2832-ee15-41a2-bdeb-493f814ae01b',\n",
              "              'metadata': {'chunk': 9.0,\n",
              "                           'text': 'are several main modules that LangChain '\n",
              "                                   'provides support for.\\\\nFor each module we '\n",
              "                                   'provide some examples to get started, '\n",
              "                                   'how-to guides, reference docs, and '\n",
              "                                   'conceptual guides.\\\\nThese modules are, in '\n",
              "                                   'increasing order of '\n",
              "                                   'complexity:\\\\n\\\\nPrompts: This includes '\n",
              "                                   'prompt management, prompt optimization, '\n",
              "                                   'and prompt serialization.\\\\nLLMs: This '\n",
              "                                   'includes a generic interface for all LLMs, '\n",
              "                                   'and common utilities for working with '\n",
              "                                   'LLMs.\\\\nDocument Loaders: This includes a '\n",
              "                                   'standard interface for loading documents, '\n",
              "                                   'as well as specific integrations to all '\n",
              "                                   'types of text data sources.\\\\nUtils: '\n",
              "                                   'Language models are often more powerful '\n",
              "                                   'when interacting with other sources of '\n",
              "                                   'knowledge or computation. This can include '\n",
              "                                   'Python REPLs, embeddings, search engines, '\n",
              "                                   'and more. LangChain provides a large '\n",
              "                                   'collection of common utils to use in your '\n",
              "                                   'application.\\\\nChains: Chains go beyond '\n",
              "                                   'just a single LLM call, and are sequences '\n",
              "                                   'of calls (whether to an LLM or a different '\n",
              "                                   'utility). LangChain provides a standard '\n",
              "                                   'interface for chains, lots of integrations '\n",
              "                                   'with other',\n",
              "                           'url': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\indexes\\\\document_loaders\\\\examples\\\\sitemap.html'},\n",
              "              'score': 0.854617953,\n",
              "              'values': []},\n",
              "             {'id': '719ff28c-1027-48ae-8299-53420201d18f',\n",
              "              'metadata': {'chunk': 1.0,\n",
              "                           'text': '[Document(page_content=\"ð\\\\x9f¦\\\\x9cï¸\\\\x8fð\\\\x9f”\\\\x97 '\n",
              "                                   'LangChain\\\\n\\\\nâ\\\\x9a¡ Building '\n",
              "                                   'applications with LLMs through '\n",
              "                                   'composability â\\\\x9a¡\\\\n\\\\nProduction '\n",
              "                                   'Support: As you move your LangChains into '\n",
              "                                   \"production, we'd love to offer more \"\n",
              "                                   'comprehensive support.\\\\nPlease fill out '\n",
              "                                   \"this form and we'll set up a dedicated \"\n",
              "                                   'support Slack channel.\\\\n\\\\nQuick '\n",
              "                                   'Install\\\\n\\\\npip install '\n",
              "                                   'langchain\\\\n\\\\nð\\\\x9f¤” What is '\n",
              "                                   'this?\\\\n\\\\nLarge language models (LLMs) '\n",
              "                                   'are emerging as a transformative '\n",
              "                                   'technology, enabling\\\\ndevelopers to build '\n",
              "                                   'applications that they previously could '\n",
              "                                   'not.\\\\nBut using these LLMs in isolation '\n",
              "                                   'is often not enough to\\\\ncreate a truly '\n",
              "                                   'powerful app - the real power comes when '\n",
              "                                   'you can combine them with other sources of '\n",
              "                                   'computation or knowledge.\\\\n\\\\nThis '\n",
              "                                   'library is aimed at assisting in the '\n",
              "                                   'development of those types of '\n",
              "                                   'applications. Common examples of these '\n",
              "                                   'types of applications '\n",
              "                                   'include:\\\\n\\\\nâ\\\\x9d“ Question Answering '\n",
              "                                   'over specific',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/markdown.html'},\n",
              "              'score': 0.853299081,\n",
              "              'values': []},\n",
              "             {'id': 'b8e3307b-a5c8-4e94-b317-dbe985213c5c',\n",
              "              'metadata': {'chunk': 0.0,\n",
              "                           'text': '.rst\\n'\n",
              "                                   '.pdf\\n'\n",
              "                                   'Chains\\n'\n",
              "                                   'Chains#\\n'\n",
              "                                   'Note\\n'\n",
              "                                   'Conceptual Guide\\n'\n",
              "                                   'Using an LLM in isolation is fine for some '\n",
              "                                   'simple applications,\\n'\n",
              "                                   'but many more complex ones require '\n",
              "                                   'chaining LLMs - either with each other or '\n",
              "                                   'with other experts.\\n'\n",
              "                                   'LangChain provides a standard interface '\n",
              "                                   'for Chains, as well as some common '\n",
              "                                   'implementations of chains for ease of '\n",
              "                                   'use.\\n'\n",
              "                                   'The following sections of documentation '\n",
              "                                   'are provided:\\n'\n",
              "                                   'Getting Started: A getting started guide '\n",
              "                                   'for chains, to get you up and running '\n",
              "                                   'quickly.\\n'\n",
              "                                   'How-To Guides: A collection of how-to '\n",
              "                                   'guides. These highlight how to use various '\n",
              "                                   'types of chains.\\n'\n",
              "                                   'Reference: API reference documentation for '\n",
              "                                   'all Chain classes.\\n'\n",
              "                                   'previous\\n'\n",
              "                                   'Redis Chat Message History\\n'\n",
              "                                   'next\\n'\n",
              "                                   'Getting Started\\n'\n",
              "                                   'By Harrison Chase\\n'\n",
              "                                   '    \\n'\n",
              "                                   '      © Copyright 2023, Harrison Chase.\\n'\n",
              "                                   '      \\n'\n",
              "                                   '  Last updated on Apr 21, 2023.',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/chains.html'},\n",
              "              'score': 0.851718605,\n",
              "              'values': []},\n",
              "             {'id': '43b888e0-30a7-4c82-958e-f6cd8aba25d0',\n",
              "              'metadata': {'chunk': 0.0,\n",
              "                           'text': '.rst\\n'\n",
              "                                   '.pdf\\n'\n",
              "                                   'Chains\\n'\n",
              "                                   'Chains#\\n'\n",
              "                                   'Note\\n'\n",
              "                                   'Conceptual Guide\\n'\n",
              "                                   'Using an LLM in isolation is fine for some '\n",
              "                                   'simple applications,\\n'\n",
              "                                   'but many more complex ones require '\n",
              "                                   'chaining LLMs - either with each other or '\n",
              "                                   'with other experts.\\n'\n",
              "                                   'LangChain provides a standard interface '\n",
              "                                   'for Chains, as well as some common '\n",
              "                                   'implementations of chains for ease of '\n",
              "                                   'use.\\n'\n",
              "                                   'The following sections of documentation '\n",
              "                                   'are provided:\\n'\n",
              "                                   'Getting Started: A getting started guide '\n",
              "                                   'for chains, to get you up and running '\n",
              "                                   'quickly.\\n'\n",
              "                                   'How-To Guides: A collection of how-to '\n",
              "                                   'guides. These highlight how to use various '\n",
              "                                   'types of chains.\\n'\n",
              "                                   'Reference: API reference documentation for '\n",
              "                                   'all Chain classes.\\n'\n",
              "                                   'previous\\n'\n",
              "                                   'Redis Chat Message History\\n'\n",
              "                                   'next\\n'\n",
              "                                   'Getting Started\\n'\n",
              "                                   'By Harrison Chase\\n'\n",
              "                                   '    \\n'\n",
              "                                   '      © Copyright 2023, Harrison Chase.\\n'\n",
              "                                   '      \\n'\n",
              "                                   '  Last updated on Apr 21, 2023.',\n",
              "                           'url': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\chains.html'},\n",
              "              'score': 0.851718605,\n",
              "              'values': []},\n",
              "             {'id': 'aed60880-cf29-4cd3-9809-8d3f026f0776',\n",
              "              'metadata': {'chunk': 8.0,\n",
              "                           'text': 'to LangChain#\\\\nLarge language models '\n",
              "                                   '(LLMs) are emerging as a transformative '\n",
              "                                   'technology, enabling\\\\ndevelopers to build '\n",
              "                                   'applications that they previously could '\n",
              "                                   'not.\\\\nBut using these LLMs in isolation '\n",
              "                                   'is often not enough to\\\\ncreate a truly '\n",
              "                                   'powerful app - the real power comes when '\n",
              "                                   'you are able to\\\\ncombine them with other '\n",
              "                                   'sources of computation or '\n",
              "                                   'knowledge.\\\\nThis library is aimed at '\n",
              "                                   'assisting in the development of those '\n",
              "                                   'types of applications. Common examples of '\n",
              "                                   'these types of applications include:\\\\n❓ '\n",
              "                                   'Question Answering over specific '\n",
              "                                   'documents\\\\n\\\\nDocumentation\\\\nEnd-to-end '\n",
              "                                   'Example: Question Answering over Notion '\n",
              "                                   'Database\\\\n\\\\n💬 '\n",
              "                                   'Chatbots\\\\n\\\\nDocumentation\\\\nEnd-to-end '\n",
              "                                   'Example: Chat-LangChain\\\\n\\\\n🤖 '\n",
              "                                   'Agents\\\\n\\\\nDocumentation\\\\nEnd-to-end '\n",
              "                                   'Example: GPT+WolframAlpha\\\\n\\\\n\\\\nGetting '\n",
              "                                   'Started#\\\\nCheckout the below guide for a '\n",
              "                                   'walkthrough of how to get started using '\n",
              "                                   'LangChain to create an Language Model '\n",
              "                                   'application.\\\\n\\\\nGetting Started '\n",
              "                                   'Documentation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nModules#\\\\nThere '\n",
              "                                   'are several main',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'},\n",
              "              'score': 0.851256132,\n",
              "              'values': []},\n",
              "             {'id': 'e22f6c49-788b-4649-9801-43d1eef2a9e8',\n",
              "              'metadata': {'chunk': 8.0,\n",
              "                           'text': 'to LangChain#\\\\nLarge language models '\n",
              "                                   '(LLMs) are emerging as a transformative '\n",
              "                                   'technology, enabling\\\\ndevelopers to build '\n",
              "                                   'applications that they previously could '\n",
              "                                   'not.\\\\nBut using these LLMs in isolation '\n",
              "                                   'is often not enough to\\\\ncreate a truly '\n",
              "                                   'powerful app - the real power comes when '\n",
              "                                   'you are able to\\\\ncombine them with other '\n",
              "                                   'sources of computation or '\n",
              "                                   'knowledge.\\\\nThis library is aimed at '\n",
              "                                   'assisting in the development of those '\n",
              "                                   'types of applications. Common examples of '\n",
              "                                   'these types of applications include:\\\\n❓ '\n",
              "                                   'Question Answering over specific '\n",
              "                                   'documents\\\\n\\\\nDocumentation\\\\nEnd-to-end '\n",
              "                                   'Example: Question Answering over Notion '\n",
              "                                   'Database\\\\n\\\\n💬 '\n",
              "                                   'Chatbots\\\\n\\\\nDocumentation\\\\nEnd-to-end '\n",
              "                                   'Example: Chat-LangChain\\\\n\\\\n🤖 '\n",
              "                                   'Agents\\\\n\\\\nDocumentation\\\\nEnd-to-end '\n",
              "                                   'Example: GPT+WolframAlpha\\\\n\\\\n\\\\nGetting '\n",
              "                                   'Started#\\\\nCheckout the below guide for a '\n",
              "                                   'walkthrough of how to get started using '\n",
              "                                   'LangChain to create an Language Model '\n",
              "                                   'application.\\\\n\\\\nGetting Started '\n",
              "                                   'Documentation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nModules#\\\\nThere '\n",
              "                                   'are several main',\n",
              "                           'url': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\indexes\\\\document_loaders\\\\examples\\\\sitemap.html'},\n",
              "              'score': 0.851256132,\n",
              "              'values': []},\n",
              "             {'id': 'cf5b4962-5338-4993-ac39-b0dc62ec691c',\n",
              "              'metadata': {'chunk': 10.0,\n",
              "                           'text': 'standard interface for chains, lots of '\n",
              "                                   'integrations with other tools, and '\n",
              "                                   'end-to-end chains for common '\n",
              "                                   'applications.\\\\nIndexes: Language models '\n",
              "                                   'are often more powerful when combined with '\n",
              "                                   'your own text data - this module covers '\n",
              "                                   'best practices for doing exactly '\n",
              "                                   'that.\\\\nAgents: Agents involve an LLM '\n",
              "                                   'making decisions about which Actions to '\n",
              "                                   'take, taking that Action, seeing an '\n",
              "                                   'Observation, and repeating that until '\n",
              "                                   'done. LangChain provides a standard '\n",
              "                                   'interface for agents, a selection of '\n",
              "                                   'agents to choose from, and examples of end '\n",
              "                                   'to end agents.\\\\nMemory: Memory is the '\n",
              "                                   'concept of persisting state between calls '\n",
              "                                   'of a chain/agent. LangChain provides a '\n",
              "                                   'standard interface for memory, a '\n",
              "                                   'collection of memory implementations, and '\n",
              "                                   'examples of chains/agents that use '\n",
              "                                   'memory.\\\\nChat: Chat models are a '\n",
              "                                   'variation on Language Models that expose a '\n",
              "                                   'different API - rather than working with '\n",
              "                                   'raw text, they work with messages. '\n",
              "                                   'LangChain provides a standard interface '\n",
              "                                   'for working with them and doing all the '\n",
              "                                   'same things as above.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse',\n",
              "                           'url': 'rtdocs\\\\python.langchain.com\\\\en\\\\latest\\\\modules\\\\indexes\\\\document_loaders\\\\examples\\\\sitemap.html'},\n",
              "              'score': 0.847109675,\n",
              "              'values': []},\n",
              "             {'id': '1c7d7733-e9de-40df-9763-22bc9765ac11',\n",
              "              'metadata': {'chunk': 10.0,\n",
              "                           'text': 'standard interface for chains, lots of '\n",
              "                                   'integrations with other tools, and '\n",
              "                                   'end-to-end chains for common '\n",
              "                                   'applications.\\\\nIndexes: Language models '\n",
              "                                   'are often more powerful when combined with '\n",
              "                                   'your own text data - this module covers '\n",
              "                                   'best practices for doing exactly '\n",
              "                                   'that.\\\\nAgents: Agents involve an LLM '\n",
              "                                   'making decisions about which Actions to '\n",
              "                                   'take, taking that Action, seeing an '\n",
              "                                   'Observation, and repeating that until '\n",
              "                                   'done. LangChain provides a standard '\n",
              "                                   'interface for agents, a selection of '\n",
              "                                   'agents to choose from, and examples of end '\n",
              "                                   'to end agents.\\\\nMemory: Memory is the '\n",
              "                                   'concept of persisting state between calls '\n",
              "                                   'of a chain/agent. LangChain provides a '\n",
              "                                   'standard interface for memory, a '\n",
              "                                   'collection of memory implementations, and '\n",
              "                                   'examples of chains/agents that use '\n",
              "                                   'memory.\\\\nChat: Chat models are a '\n",
              "                                   'variation on Language Models that expose a '\n",
              "                                   'different API - rather than working with '\n",
              "                                   'raw text, they work with messages. '\n",
              "                                   'LangChain provides a standard interface '\n",
              "                                   'for working with them and doing all the '\n",
              "                                   'same things as above.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'},\n",
              "              'score': 0.847109675,\n",
              "              'values': []},\n",
              "             {'id': 'fd2fa433-d35d-4f22-b4c7-f173b4ef3cdc',\n",
              "              'metadata': {'chunk': 3.0,\n",
              "                           'text': 'a standard interface for agents, a '\n",
              "                                   'selection of agents to choose from, and '\n",
              "                                   'examples of end to end agents.\\\\nUse '\n",
              "                                   'Cases\\\\nThe above modules can be used in a '\n",
              "                                   'variety of ways. LangChain also provides '\n",
              "                                   'guidance and assistance in this. Below are '\n",
              "                                   'some of the common use cases LangChain '\n",
              "                                   'supports.\\\\nPersonal Assistants: The main '\n",
              "                                   'LangChain use case. Personal assistants '\n",
              "                                   'need to take actions, remember '\n",
              "                                   'interactions, and have knowledge about '\n",
              "                                   'your data.\\\\nQuestion Answering: The '\n",
              "                                   'second big LangChain use case. Answering '\n",
              "                                   'questions over specific documents, only '\n",
              "                                   'utilizing the information in those '\n",
              "                                   'documents to construct an '\n",
              "                                   'answer.\\\\nChatbots: Since language models '\n",
              "                                   'are good at producing text, that makes '\n",
              "                                   'them ideal for creating '\n",
              "                                   'chatbots.\\\\nQuerying Tabular Data: If you '\n",
              "                                   'want to understand how to use LLMs to '\n",
              "                                   'query data that is stored in a tabular '\n",
              "                                   'format (csvs, SQL, dataframes, etc) you '\n",
              "                                   'should read this page.\\\\nInteracting with '\n",
              "                                   'APIs: Enabling LLMs to interact with APIs '\n",
              "                                   'is extremely powerful in order to give '\n",
              "                                   'them',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/diffbot.html'},\n",
              "              'score': 0.844662786,\n",
              "              'values': []},\n",
              "             {'id': 'c1ea9b7a-5917-49f1-9aa5-fd9226a2700f',\n",
              "              'metadata': {'chunk': 26.0,\n",
              "                           'text': 'the concept of persisting state between '\n",
              "                                   'calls of a chain/agent. LangChain provides '\n",
              "                                   'a standard interface for memory, a '\n",
              "                                   'collection of memory implementations, and '\n",
              "                                   'examples of chains/agents that use '\n",
              "                                   'memory.\\\\nIndexes: Language models are '\n",
              "                                   'often more powerful when combined with '\n",
              "                                   'your own text data - this module covers '\n",
              "                                   'best practices for doing exactly '\n",
              "                                   'that.\\\\nChains: Chains go beyond just a '\n",
              "                                   'single LLM call, and are sequences of '\n",
              "                                   'calls (whether to an LLM or a different '\n",
              "                                   'utility). LangChain provides a standard '\n",
              "                                   'interface for chains, lots of integrations '\n",
              "                                   'with other tools, and end-to-end chains '\n",
              "                                   'for common applications.\\\\nAgents: Agents '\n",
              "                                   'involve an LLM making decisions about '\n",
              "                                   'which Actions to take, taking that Action, '\n",
              "                                   'seeing an Observation, and repeating that '\n",
              "                                   'until done. LangChain provides a standard '\n",
              "                                   'interface for agents, a selection of '\n",
              "                                   'agents to choose from, and examples of end '\n",
              "                                   'to end agents.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse '\n",
              "                                   'Cases#\\\\nThe above modules can be used in '\n",
              "                                   'a variety of ways. LangChain also provides',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'},\n",
              "              'score': 0.838196,\n",
              "              'values': []}],\n",
              " 'namespace': ''}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "unZstoHNUHeG"
      },
      "outputs": [],
      "source": [
        "# get list of retrieved text\n",
        "contexts = [item['metadata']['text'] for item in res['matches']]\n",
        "\n",
        "augmented_query = \"\\n\\n---\\n\\n\".join(contexts)+\"\\n\\n-----\\n\\n\"+query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRcEHm0Z9fXE",
        "outputId": "1a6b518c-df79-4ff7-eb3b-63237fe899e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "for all LLMs, and common utilities for working with LLMs.\\n\\nð\\x9f”\\x97 Chains:\\n\\nChains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\n\\nð\\x9f“\\x9a Data Augmented Generation:\\n\\nData Augmented Generation involves specific types of chains that first interact with an external datasource to fetch data to use in the generation step. Examples of this include summarization of long pieces of text and question/answering over specific data sources.\\n\\nð\\x9f¤\\x96 Agents:\\n\\nAgents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end\n",
            "\n",
            "---\n",
            "\n",
            "Chains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\n",
            "Agents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\n",
            "Use Cases#\n",
            "The above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\n",
            "Autonomous Agents: Autonomous agents are long running agents that take many steps in an attempt to accomplish an objective. Examples include AutoGPT and BabyAGI.\n",
            "Agent Simulations: Putting agents in a sandbox and observing how they interact with each other or to events can be an interesting way to observe their long-term memory abilities.\n",
            "Personal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\n",
            "Question Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\n",
            "Chatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\n",
            "Querying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\n",
            "Code Understanding: If you want to understand how to use LLMs to query source code from github, you should read this page.\n",
            "\n",
            "---\n",
            "\n",
            "Chains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\n",
            "Agents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\n",
            "Use Cases#\n",
            "The above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\n",
            "Autonomous Agents: Autonomous agents are long running agents that take many steps in an attempt to accomplish an objective. Examples include AutoGPT and BabyAGI.\n",
            "Agent Simulations: Putting agents in a sandbox and observing how they interact with each other or to events can be an interesting way to observe their long-term memory abilities.\n",
            "Personal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\n",
            "Question Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\n",
            "Chatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\n",
            "Querying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\n",
            "Code Understanding: If you want to understand how to use LLMs to query source code from github, you should read this page.\n",
            "\n",
            "---\n",
            "\n",
            "in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\n\\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\\nQuestion Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\\nInteracting with APIs: Enabling LLMs to interact with APIs is extremely powerful in order to give them more up-to-date information and allow them to take actions.\\nExtraction: Extract structured information from text.\\nSummarization: Summarizing longer documents into\n",
            "\n",
            "---\n",
            "\n",
            "in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\n\\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\\nQuestion Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\\nInteracting with APIs: Enabling LLMs to interact with APIs is extremely powerful in order to give them more up-to-date information and allow them to take actions.\\nExtraction: Extract structured information from text.\\nSummarization: Summarizing longer documents into\n",
            "\n",
            "---\n",
            "\n",
            "are several main modules that LangChain provides support for.\\nFor each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides.\\nThese modules are, in increasing order of complexity:\\n\\nPrompts: This includes prompt management, prompt optimization, and prompt serialization.\\nLLMs: This includes a generic interface for all LLMs, and common utilities for working with LLMs.\\nDocument Loaders: This includes a standard interface for loading documents, as well as specific integrations to all types of text data sources.\\nUtils: Language models are often more powerful when interacting with other sources of knowledge or computation. This can include Python REPLs, embeddings, search engines, and more. LangChain provides a large collection of common utils to use in your application.\\nChains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other\n",
            "\n",
            "---\n",
            "\n",
            "are several main modules that LangChain provides support for.\\nFor each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides.\\nThese modules are, in increasing order of complexity:\\n\\nPrompts: This includes prompt management, prompt optimization, and prompt serialization.\\nLLMs: This includes a generic interface for all LLMs, and common utilities for working with LLMs.\\nDocument Loaders: This includes a standard interface for loading documents, as well as specific integrations to all types of text data sources.\\nUtils: Language models are often more powerful when interacting with other sources of knowledge or computation. This can include Python REPLs, embeddings, search engines, and more. LangChain provides a large collection of common utils to use in your application.\\nChains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other\n",
            "\n",
            "---\n",
            "\n",
            "[Document(page_content=\"ð\\x9f¦\\x9cï¸\\x8fð\\x9f”\\x97 LangChain\\n\\nâ\\x9a¡ Building applications with LLMs through composability â\\x9a¡\\n\\nProduction Support: As you move your LangChains into production, we'd love to offer more comprehensive support.\\nPlease fill out this form and we'll set up a dedicated support Slack channel.\\n\\nQuick Install\\n\\npip install langchain\\n\\nð\\x9f¤” What is this?\\n\\nLarge language models (LLMs) are emerging as a transformative technology, enabling\\ndevelopers to build applications that they previously could not.\\nBut using these LLMs in isolation is often not enough to\\ncreate a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.\\n\\nThis library is aimed at assisting in the development of those types of applications. Common examples of these types of applications include:\\n\\nâ\\x9d“ Question Answering over specific\n",
            "\n",
            "---\n",
            "\n",
            ".rst\n",
            ".pdf\n",
            "Chains\n",
            "Chains#\n",
            "Note\n",
            "Conceptual Guide\n",
            "Using an LLM in isolation is fine for some simple applications,\n",
            "but many more complex ones require chaining LLMs - either with each other or with other experts.\n",
            "LangChain provides a standard interface for Chains, as well as some common implementations of chains for ease of use.\n",
            "The following sections of documentation are provided:\n",
            "Getting Started: A getting started guide for chains, to get you up and running quickly.\n",
            "How-To Guides: A collection of how-to guides. These highlight how to use various types of chains.\n",
            "Reference: API reference documentation for all Chain classes.\n",
            "previous\n",
            "Redis Chat Message History\n",
            "next\n",
            "Getting Started\n",
            "By Harrison Chase\n",
            "    \n",
            "      © Copyright 2023, Harrison Chase.\n",
            "      \n",
            "  Last updated on Apr 21, 2023.\n",
            "\n",
            "---\n",
            "\n",
            ".rst\n",
            ".pdf\n",
            "Chains\n",
            "Chains#\n",
            "Note\n",
            "Conceptual Guide\n",
            "Using an LLM in isolation is fine for some simple applications,\n",
            "but many more complex ones require chaining LLMs - either with each other or with other experts.\n",
            "LangChain provides a standard interface for Chains, as well as some common implementations of chains for ease of use.\n",
            "The following sections of documentation are provided:\n",
            "Getting Started: A getting started guide for chains, to get you up and running quickly.\n",
            "How-To Guides: A collection of how-to guides. These highlight how to use various types of chains.\n",
            "Reference: API reference documentation for all Chain classes.\n",
            "previous\n",
            "Redis Chat Message History\n",
            "next\n",
            "Getting Started\n",
            "By Harrison Chase\n",
            "    \n",
            "      © Copyright 2023, Harrison Chase.\n",
            "      \n",
            "  Last updated on Apr 21, 2023.\n",
            "\n",
            "---\n",
            "\n",
            "to LangChain#\\nLarge language models (LLMs) are emerging as a transformative technology, enabling\\ndevelopers to build applications that they previously could not.\\nBut using these LLMs in isolation is often not enough to\\ncreate a truly powerful app - the real power comes when you are able to\\ncombine them with other sources of computation or knowledge.\\nThis library is aimed at assisting in the development of those types of applications. Common examples of these types of applications include:\\n❓ Question Answering over specific documents\\n\\nDocumentation\\nEnd-to-end Example: Question Answering over Notion Database\\n\\n💬 Chatbots\\n\\nDocumentation\\nEnd-to-end Example: Chat-LangChain\\n\\n🤖 Agents\\n\\nDocumentation\\nEnd-to-end Example: GPT+WolframAlpha\\n\\n\\nGetting Started#\\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\\n\\nGetting Started Documentation\\n\\n\\n\\n\\n\\nModules#\\nThere are several main\n",
            "\n",
            "---\n",
            "\n",
            "to LangChain#\\nLarge language models (LLMs) are emerging as a transformative technology, enabling\\ndevelopers to build applications that they previously could not.\\nBut using these LLMs in isolation is often not enough to\\ncreate a truly powerful app - the real power comes when you are able to\\ncombine them with other sources of computation or knowledge.\\nThis library is aimed at assisting in the development of those types of applications. Common examples of these types of applications include:\\n❓ Question Answering over specific documents\\n\\nDocumentation\\nEnd-to-end Example: Question Answering over Notion Database\\n\\n💬 Chatbots\\n\\nDocumentation\\nEnd-to-end Example: Chat-LangChain\\n\\n🤖 Agents\\n\\nDocumentation\\nEnd-to-end Example: GPT+WolframAlpha\\n\\n\\nGetting Started#\\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\\n\\nGetting Started Documentation\\n\\n\\n\\n\\n\\nModules#\\nThere are several main\n",
            "\n",
            "---\n",
            "\n",
            "standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\nIndexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\\nAgents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\nMemory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\nChat: Chat models are a variation on Language Models that expose a different API - rather than working with raw text, they work with messages. LangChain provides a standard interface for working with them and doing all the same things as above.\\n\\n\\n\\n\\n\\nUse\n",
            "\n",
            "---\n",
            "\n",
            "standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\nIndexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\\nAgents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\nMemory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\nChat: Chat models are a variation on Language Models that expose a different API - rather than working with raw text, they work with messages. LangChain provides a standard interface for working with them and doing all the same things as above.\\n\\n\\n\\n\\n\\nUse\n",
            "\n",
            "---\n",
            "\n",
            "a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\nUse Cases\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\\nQuestion Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\\nInteracting with APIs: Enabling LLMs to interact with APIs is extremely powerful in order to give them\n",
            "\n",
            "---\n",
            "\n",
            "the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\nIndexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\\nChains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\nAgents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\n\\n\\n\\n\\n\\nUse Cases#\\nThe above modules can be used in a variety of ways. LangChain also provides\n",
            "\n",
            "-----\n",
            "\n",
            "how do I use the LLMChain in LangChain?\n"
          ]
        }
      ],
      "source": [
        "print(augmented_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IThBqBi8V70d"
      },
      "outputs": [],
      "source": [
        "# system message to 'prime' the model\n",
        "primer = f\"\"\"You are Q&A bot. A highly intelligent system that answers\n",
        "user questions based on the information provided by the user above\n",
        "each question. If the information can not be found in the information\n",
        "provided by the user you truthfully say \"I don't know\".\n",
        "\"\"\"\n",
        "\n",
        "res = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": augmented_query}\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "RDo2qeMHWto1",
        "outputId": "39f6ca9c-1818-416d-866e-2a93811a7f3c"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "You can use the tiktoken tokenizer package for Python to estimate the number of tokens in a piece of text. To use it, you can install it with pip install tiktoken, and import it with import tiktoken. Once imported, you can use it to count tokens in text by calling tiktoken.encoding_for_model(\"model_name\").encode(text), where \"model_name\" is the name of the tokenizer model you want to use and \"text\" is the text you want to count the tokens for."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "display(Markdown(res['choices'][0]['message']['content']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "vwhaSgdF0ZDX",
        "outputId": "f2e63d6c-0607-4496-85da-a14f52f89901"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I'm sorry, but I don't have enough information to answer your question. Could you provide me more detail about LangChain and LLMChain?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "res = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        ")\n",
        "display(Markdown(res['choices'][0]['message']['content']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "Z3svdTCZ0iJ2",
        "outputId": "2c586206-b8a1-4225-9edb-8175bba2e2e2"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LLMChain is a blockchain-based platform developed by LangChain that is designed to help language learners improve their language proficiency skills. Here are the basic steps to use the platform:\n",
              "\n",
              "1. Navigate to the LLMChain page on LangChain's website and create an account.\n",
              "2. Choose the language you want to learn and find the right course for your level.\n",
              "3. Start learning by watching video lessons, reading texts, and practicing exercises.\n",
              "4. Earn LLMChain tokens (LLM) by completing courses and participating in various activities.\n",
              "5. Use your LLM tokens to buy additional courses or exchange them for other cryptocurrencies on supported exchanges.\n",
              "\n",
              "Additionally, LLMChain is also designed to provide a social learning experience. You can interact with other learners and language experts through the platform's messaging system, participate in language challenges, and receive feedback and tips from native speakers."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "res = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are Q&A bot. A highly intelligent system that answers user questions\"},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        ")\n",
        "display(Markdown(res['choices'][0]['message']['content']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pinecone.delete_index(index_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "047d3e83cd2745879529a34cdf4c52fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c06721baa384e2188d5e0d7765dd30c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ae8008c252741879e738da71f5d977d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_950e452b63d04bea9b810826658a22ba",
            "placeholder": "​",
            "style": "IPY_MODEL_57469d00b3d84fb585621dbceed72871",
            "value": "100%"
          }
        },
        "31aa526b635e4eaea7c7c8101184e1d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fffa66a9213e4b2e93b626d199156d25",
              "IPY_MODEL_833417b68a9d4b94a57f1595d5a4fdd9",
              "IPY_MODEL_88924dd9e4394d99b942c118be840e65"
            ],
            "layout": "IPY_MODEL_c08c76a37c324d0393a0852ec315869b"
          }
        },
        "31f45f7fd88a4080b94565baf25f380f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d8ce02491cc470c88d7cf917c21815f",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c06721baa384e2188d5e0d7765dd30c",
            "value": 33
          }
        },
        "3d8ce02491cc470c88d7cf917c21815f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57469d00b3d84fb585621dbceed72871": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ba56e481c62412fb1788a1e56d99cfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f61d01ca80e4a9086d91973ff22043b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "833417b68a9d4b94a57f1595d5a4fdd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d9cca0327b2425fb6a053f467558c58",
            "max": 513,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e82da63021374013aa2189d53f6eb9c4",
            "value": 513
          }
        },
        "88924dd9e4394d99b942c118be840e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_047d3e83cd2745879529a34cdf4c52fe",
            "placeholder": "​",
            "style": "IPY_MODEL_c84a063d79f340a7bde611d9f4e2c739",
            "value": " 513/513 [00:05&lt;00:00, 260.98it/s]"
          }
        },
        "8d9cca0327b2425fb6a053f467558c58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f4ecd72531845099d00b3eba1209c9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91ebdba428564d399d4090ba121531fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "933f662b8c314369b874a6e46eee3872": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ba56e481c62412fb1788a1e56d99cfa",
            "placeholder": "​",
            "style": "IPY_MODEL_99a48256c20048a58d9d7e6bb58b8458",
            "value": " 33/33 [00:37&lt;00:00,  1.02s/it]"
          }
        },
        "950e452b63d04bea9b810826658a22ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99a48256c20048a58d9d7e6bb58b8458": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c08c76a37c324d0393a0852ec315869b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c84a063d79f340a7bde611d9f4e2c739": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e82da63021374013aa2189d53f6eb9c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eadd92cbd46d4d8888c51b2f339b845e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ae8008c252741879e738da71f5d977d",
              "IPY_MODEL_31f45f7fd88a4080b94565baf25f380f",
              "IPY_MODEL_933f662b8c314369b874a6e46eee3872"
            ],
            "layout": "IPY_MODEL_91ebdba428564d399d4090ba121531fc"
          }
        },
        "fffa66a9213e4b2e93b626d199156d25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f61d01ca80e4a9086d91973ff22043b",
            "placeholder": "​",
            "style": "IPY_MODEL_8f4ecd72531845099d00b3eba1209c9d",
            "value": "100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
